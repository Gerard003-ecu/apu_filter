### Propuesta 1 test_generate_embeddings.py ###

# tests/test_generate_embeddings.py

import json
import shutil
import sys
import time
from dataclasses import dataclass
from pathlib import Path
from typing import Dict, List, Optional, Tuple
from unittest.mock import MagicMock, Mock, PropertyMock, call, patch

import faiss
import numpy as np
import pandas as pd
import pytest

# Importar las clases y funciones a probar
sys.path.insert(0, str(Path(__file__).resolve().parent.parent))
from scripts.generate_embeddings import (
    Config,
    DataValidationError,
    DataValidator,
    EmbeddingGenerationError,
    EmbeddingGenerator,
    EmbeddingPipeline,
    FileManager,
    InsufficientMemoryError,
    MemoryMonitor,
    ModelLoadError,
)


# ============================================================================
# FIXTURES
# ============================================================================

@pytest.fixture
def config():
    """Proporciona una configuraci贸n de prueba."""
    config = Config()
    config.MAX_BATCH_SIZE = 32
    config.MEMORY_LIMIT_GB = 1.0
    config.VALIDATION_SAMPLE_SIZE = 10
    config.BACKUP_ENABLED = True
    config.NORMALIZE_EMBEDDINGS = True
    config.SHOW_PROGRESS = False
    return config


@pytest.fixture
def sample_dataframe():
    """Crea un DataFrame de prueba con datos v谩lidos."""
    return pd.DataFrame({
        "CODIGO_APU": ["APU-001", "APU-002", "APU-003", "APU-004", "APU-005"],
        "original_description": [
            "Instalaci贸n de muro de ladrillo com煤n de 15cm",
            "Suministro de tuber铆a de PVC de 2 pulgadas",
            "Excavaci贸n manual en tierra hasta 2m de profundidad",
            "Colocaci贸n de concreto premezclado f'c=210",
            "Pintura acr铆lica en muros interiores dos manos",
        ],
        "unidad": ["m2", "m", "m3", "m3", "m2"],
        "precio": [100.5, 45.2, 78.9, 234.5, 67.8],
    })


@pytest.fixture
def invalid_dataframe():
    """Crea un DataFrame con datos problem谩ticos para pruebas de validaci贸n."""
    return pd.DataFrame({
        "CODIGO_APU": ["APU-001", "APU-002", None, "APU-001", "APU-005"],
        "original_description": [
            "Texto v谩lido",
            None,  # Descripci贸n nula
            "Texto v谩lido 2",
            "Duplicado",  # ID duplicado
            "ab",  # Texto muy corto
        ],
        "otra_columna": [1, 2, 3, 4, 5],
    })


@pytest.fixture
def sample_csv_file(tmp_path: Path, sample_dataframe: pd.DataFrame) -> Path:
    """Crea un archivo CSV temporal con datos de prueba."""
    file_path = tmp_path / "test_apus.csv"
    sample_dataframe.to_csv(file_path, index=False, encoding='utf-8')
    return file_path


@pytest.fixture
def sample_json_file(tmp_path: Path, sample_dataframe: pd.DataFrame) -> Path:
    """Crea un archivo JSON temporal con datos de prueba."""
    file_path = tmp_path / "test_apus.json"
    sample_dataframe.to_json(file_path, orient='records', force_ascii=False)
    return file_path


@pytest.fixture
def output_dir(tmp_path: Path) -> Path:
    """Crea un directorio de salida temporal."""
    out_dir = tmp_path / "test_output"
    out_dir.mkdir(exist_ok=True)
    return out_dir


@pytest.fixture
def mock_model():
    """Crea un mock del modelo SentenceTransformer."""
    model = MagicMock()
    model.get_sentence_embedding_dimension.return_value = 384
    model.encode.return_value = np.random.rand(5, 384).astype(np.float32)
    return model


@pytest.fixture
def mock_faiss_index():
    """Crea un mock del 铆ndice FAISS."""
    index = MagicMock(spec=faiss.IndexFlatIP)
    index.ntotal = 5
    index.d = 384
    return index


# ============================================================================
# TESTS - DataValidator
# ============================================================================

class TestDataValidator:
    """Pruebas para la clase DataValidator."""
    
    def test_validate_dataframe_success(self, sample_dataframe, config):
        """Prueba validaci贸n exitosa con datos correctos."""
        validator = DataValidator()
        
        result = validator.validate_dataframe(
            sample_dataframe,
            "original_description",
            "CODIGO_APU",
            config
        )
        
        assert len(result) == 5
        assert result["CODIGO_APU"].nunique() == 5
        assert result["original_description"].notna().all()
    
    def test_validate_dataframe_missing_columns(self, sample_dataframe, config):
        """Prueba que se detecten columnas faltantes."""
        validator = DataValidator()
        
        with pytest.raises(DataValidationError) as exc_info:
            validator.validate_dataframe(
                sample_dataframe,
                "columna_inexistente",
                "CODIGO_APU",
                config
            )
        
        assert "Columnas faltantes" in str(exc_info.value)
    
    def test_validate_dataframe_null_removal(self, invalid_dataframe, config):
        """Prueba eliminaci贸n de valores nulos."""
        validator = DataValidator()
        
        result = validator.validate_dataframe(
            invalid_dataframe,
            "original_description",
            "CODIGO_APU",
            config
        )
        
        # Debe eliminar filas con valores nulos
        assert len(result) < len(invalid_dataframe)
        assert result["CODIGO_APU"].notna().all()
        assert result["original_description"].notna().all()
    
    def test_validate_dataframe_duplicate_removal(self, config):
        """Prueba eliminaci贸n de duplicados por ID."""
        df = pd.DataFrame({
            "CODIGO_APU": ["APU-001", "APU-001", "APU-002"],
            "original_description": ["Texto 1", "Texto 2", "Texto 3"],
        })
        
        validator = DataValidator()
        result = validator.validate_dataframe(
            df, "original_description", "CODIGO_APU", config
        )
        
        assert len(result) == 2
        assert result["CODIGO_APU"].nunique() == 2
    
    def test_validate_dataframe_text_length_filtering(self, config):
        """Prueba filtrado por longitud de texto."""
        df = pd.DataFrame({
            "CODIGO_APU": ["APU-001", "APU-002", "APU-003"],
            "original_description": [
                "ab",  # Muy corto
                "Texto de longitud normal para procesar",
                "x" * 6000,  # Muy largo
            ],
        })
        
        validator = DataValidator()
        result = validator.validate_dataframe(
            df, "original_description", "CODIGO_APU", config
        )
        
        assert len(result) == 1
        assert result.iloc[0]["CODIGO_APU"] == "APU-002"
    
    def test_validate_dataframe_empty_after_cleaning(self, config):
        """Prueba error cuando no quedan datos despu茅s de limpieza."""
        df = pd.DataFrame({
            "CODIGO_APU": [None, None],
            "original_description": [None, None],
        })
        
        validator = DataValidator()
        
        with pytest.raises(DataValidationError) as exc_info:
            validator.validate_dataframe(
                df, "original_description", "CODIGO_APU", config
            )
        
        assert "No quedan datos v谩lidos" in str(exc_info.value)


# ============================================================================
# TESTS - MemoryMonitor
# ============================================================================

class TestMemoryMonitor:
    """Pruebas para la clase MemoryMonitor."""
    
    @patch('psutil.virtual_memory')
    def test_check_memory_sufficient(self, mock_memory):
        """Prueba cuando hay memoria suficiente."""
        mock_memory.return_value = MagicMock(available=10 * 1024**3)  # 10 GB
        
        monitor = MemoryMonitor()
        # No debe lanzar excepci贸n
        monitor.check_memory_availability(2.0, 8.0)
    
    @patch('psutil.virtual_memory')
    def test_check_memory_exceeds_limit(self, mock_memory):
        """Prueba cuando se excede el l铆mite configurado."""
        mock_memory.return_value = MagicMock(available=10 * 1024**3)
        
        monitor = MemoryMonitor()
        
        with pytest.raises(InsufficientMemoryError) as exc_info:
            monitor.check_memory_availability(10.0, 5.0)
        
        assert "el l铆mite es" in str(exc_info.value)
    
    @patch('psutil.virtual_memory')
    def test_check_memory_insufficient_available(self, mock_memory):
        """Prueba cuando no hay suficiente memoria disponible."""
        mock_memory.return_value = MagicMock(available=2 * 1024**3)  # 2 GB
        
        monitor = MemoryMonitor()
        
        with pytest.raises(InsufficientMemoryError) as exc_info:
            monitor.check_memory_availability(1.8, 5.0)
        
        assert "Memoria insuficiente" in str(exc_info.value)


# ============================================================================
# TESTS - FileManager
# ============================================================================

class TestFileManager:
    """Pruebas para la clase FileManager."""
    
    def test_create_backup_existing_file(self, tmp_path):
        """Prueba creaci贸n de backup de archivo existente."""
        original = tmp_path / "test.json"
        original.write_text('{"test": "data"}')
        
        manager = FileManager()
        backup = manager.create_backup(original)
        
        assert backup is not None
        assert backup.exists()
        assert backup.read_text() == '{"test": "data"}'
        assert "backup" in str(backup)
    
    def test_create_backup_nonexistent_file(self, tmp_path):
        """Prueba backup de archivo inexistente."""
        nonexistent = tmp_path / "nonexistent.json"
        
        manager = FileManager()
        backup = manager.create_backup(nonexistent)
        
        assert backup is None
    
    def test_load_data_json(self, sample_json_file):
        """Prueba carga de archivo JSON."""
        manager = FileManager()
        df = manager.load_data(sample_json_file)
        
        assert isinstance(df, pd.DataFrame)
        assert len(df) == 5
        assert "CODIGO_APU" in df.columns
    
    def test_load_data_csv(self, sample_csv_file):
        """Prueba carga de archivo CSV."""
        manager = FileManager()
        df = manager.load_data(sample_csv_file)
        
        assert isinstance(df, pd.DataFrame)
        assert len(df) == 5
        assert "CODIGO_APU" in df.columns
    
    def test_load_data_nonexistent_file(self, tmp_path):
        """Prueba error con archivo inexistente."""
        nonexistent = tmp_path / "nonexistent.csv"
        
        manager = FileManager()
        
        with pytest.raises(FileNotFoundError):
            manager.load_data(nonexistent)
    
    def test_load_data_unsupported_format(self, tmp_path):
        """Prueba error con formato no soportado."""
        unsupported = tmp_path / "file.txt"
        unsupported.write_text("texto")
        
        manager = FileManager()
        
        with pytest.raises(ValueError) as exc_info:
            manager.load_data(unsupported)
        
        assert "Formato de archivo no soportado" in str(exc_info.value)


# ============================================================================
# TESTS - EmbeddingGenerator
# ============================================================================

class TestEmbeddingGenerator:
    """Pruebas para la clase EmbeddingGenerator."""
    
    @patch('scripts.generate_embeddings.SentenceTransformer')
    def test_load_model_success(self, mock_transformer_class, config):
        """Prueba carga exitosa del modelo."""
        mock_model = MagicMock()
        mock_model.encode.return_value = np.array([[0.1, 0.2, 0.3]])
        mock_transformer_class.return_value = mock_model
        
        generator = EmbeddingGenerator(config)
        generator.load_model("test-model")
        
        assert generator.model is not None
        mock_transformer_class.assert_called_once_with("test-model")
    
    @patch('scripts.generate_embeddings.SentenceTransformer')
    def test_load_model_failure(self, mock_transformer_class, config):
        """Prueba error al cargar modelo."""
        mock_transformer_class.side_effect = Exception("Model not found")
        
        generator = EmbeddingGenerator(config)
        
        with pytest.raises(ModelLoadError) as exc_info:
            generator.load_model("invalid-model")
        
        assert "Error al cargar el modelo" in str(exc_info.value)
    
    def test_generate_embeddings_batch(self, config, mock_model):
        """Prueba generaci贸n de embeddings por lotes."""
        generator = EmbeddingGenerator(config)
        generator.model = mock_model
        
        texts = ["Texto 1", "Texto 2", "Texto 3", "Texto 4", "Texto 5"]
        mock_model.encode.return_value = np.random.rand(len(texts), 384)
        
        embeddings = generator.generate_embeddings_batch(texts, batch_size=2)
        
        assert embeddings.shape == (5, 384)
        # Debe haberse llamado 3 veces (batches de 2, 2, 1)
        assert mock_model.encode.call_count == 3
    
    def test_generate_embeddings_no_model(self, config):
        """Prueba error cuando no hay modelo cargado."""
        generator = EmbeddingGenerator(config)
        
        with pytest.raises(RuntimeError) as exc_info:
            generator.generate_embeddings_batch(["texto"], 32)
        
        assert "El modelo no ha sido cargado" in str(exc_info.value)
    
    @patch('scripts.generate_embeddings.faiss')
    def test_build_faiss_index(self, mock_faiss, config):
        """Prueba construcci贸n del 铆ndice FAISS."""
        mock_index = MagicMock()
        mock_index.ntotal = 5
        mock_faiss.IndexFlatIP.return_value = mock_index
        
        generator = EmbeddingGenerator(config)
        embeddings = np.random.rand(5, 384).astype(np.float32)
        
        index = generator.build_faiss_index(embeddings)
        
        assert index == mock_index
        mock_faiss.IndexFlatIP.assert_called_once_with(384)
        mock_index.add.assert_called_once()
    
    def test_validate_index_success(self, config, mock_faiss_index):
        """Prueba validaci贸n exitosa del 铆ndice."""
        generator = EmbeddingGenerator(config)
        embeddings = np.random.rand(5, 384).astype(np.float32)
        
        # Configurar el mock para que siempre encuentre el 铆ndice correcto
        mock_faiss_index.search.side_effect = [
            (np.array([[0.9]]), np.array([[i]])) for i in range(5)
        ]
        
        result = generator.validate_index(mock_faiss_index, embeddings, 5)
        
        assert result is True
    
    def test_validate_index_failure(self, config, mock_faiss_index):
        """Prueba fallo en validaci贸n del 铆ndice."""
        generator = EmbeddingGenerator(config)
        embeddings = np.random.rand(5, 384).astype(np.float32)
        
        # Configurar el mock para que devuelva 铆ndice incorrecto
        mock_faiss_index.search.return_value = (
            np.array([[0.9]]), np.array([[1]])  # Siempre devuelve 1
        )
        
        result = generator.validate_index(mock_faiss_index, embeddings, 2)
        
        assert result is False


# ============================================================================
# TESTS - EmbeddingPipeline (Integration Tests)
# ============================================================================

class TestEmbeddingPipeline:
    """Pruebas de integraci贸n para el pipeline completo."""
    
    @patch('scripts.generate_embeddings.faiss')
    @patch('scripts.generate_embeddings.SentenceTransformer')
    @patch('psutil.virtual_memory')
    def test_run_complete_success(
        self,
        mock_memory,
        mock_transformer_class,
        mock_faiss,
        config,
        sample_csv_file,
        output_dir
    ):
        """Prueba ejecuci贸n completa exitosa del pipeline."""
        # Configurar mocks
        mock_memory.return_value = MagicMock(available=10 * 1024**3)
        
        mock_model = MagicMock()
        mock_model.get_sentence_embedding_dimension.return_value = 384
        mock_model.encode.return_value = np.random.rand(5, 384).astype(np.float32)
        mock_transformer_class.return_value = mock_model
        
        mock_index = MagicMock()
        mock_index.ntotal = 5
        mock_index.search.side_effect = [
            (np.array([[0.9]]), np.array([[i]])) for i in range(100)
        ]
        mock_faiss.IndexFlatIP.return_value = mock_index
        
        # Ejecutar pipeline
        pipeline = EmbeddingPipeline(config)
        metrics = pipeline.run(
            input_file=sample_csv_file,
            output_dir=output_dir,
            model_name="test-model",
            text_column="original_description",
            id_column="CODIGO_APU"
        )
        
        # Verificar m茅tricas
        assert "processing_time" in metrics
        assert "embeddings_generated" in metrics
        assert metrics["embeddings_generated"] == 5
        assert metrics["index_size"] == 5
        
        # Verificar archivos guardados
        assert (output_dir / "faiss.index").exists()
        assert (output_dir / "id_map.json").exists()
        assert (output_dir / "metadata.json").exists()
        
        # Verificar contenido del mapeo
        with open(output_dir / "id_map.json", "r") as f:
            id_map = json.load(f)
        assert len(id_map) == 5
        assert "0" in id_map
        
        # Verificar metadata
        with open(output_dir / "metadata.json", "r") as f:
            metadata = json.load(f)
        assert metadata["model_name"] == "test-model"
        assert metadata["embedding_dimension"] == 384
        assert metadata["total_vectors"] == 5
    
    def test_run_with_invalid_input_file(self, config, output_dir):
        """Prueba error con archivo de entrada inv谩lido."""
        pipeline = EmbeddingPipeline(config)
        
        with pytest.raises(FileNotFoundError):
            pipeline.run(
                input_file=Path("nonexistent.csv"),
                output_dir=output_dir,
                model_name="test-model",
                text_column="desc",
                id_column="id"
            )
    
    @patch('scripts.generate_embeddings.SentenceTransformer')
    def test_run_with_model_load_error(
        self,
        mock_transformer_class,
        config,
        sample_csv_file,
        output_dir
    ):
        """Prueba manejo de error al cargar modelo."""
        mock_transformer_class.side_effect = Exception("Model error")
        
        pipeline = EmbeddingPipeline(config)
        
        with pytest.raises(ModelLoadError):
            pipeline.run(
                input_file=sample_csv_file,
                output_dir=output_dir,
                model_name="invalid-model",
                text_column="original_description",
                id_column="CODIGO_APU"
            )
    
    @patch('psutil.virtual_memory')
    @patch('scripts.generate_embeddings.SentenceTransformer')
    def test_run_with_insufficient_memory(
        self,
        mock_transformer_class,
        mock_memory,
        config,
        sample_csv_file,
        output_dir
    ):
        """Prueba detecci贸n de memoria insuficiente."""
        mock_memory.return_value = MagicMock(available=0.5 * 1024**3)  # 0.5 GB
        
        mock_model = MagicMock()
        mock_model.get_sentence_embedding_dimension.return_value = 384
        mock_transformer_class.return_value = mock_model
        
        pipeline = EmbeddingPipeline(config)
        
        with pytest.raises(InsufficientMemoryError):
            pipeline.run(
                input_file=sample_csv_file,
                output_dir=output_dir,
                model_name="test-model",
                text_column="original_description",
                id_column="CODIGO_APU"
            )
    
    def test_run_with_empty_dataframe(self, config, tmp_path, output_dir):
        """Prueba con DataFrame vac铆o despu茅s de limpieza."""
        # Crear archivo con datos inv谩lidos
        df = pd.DataFrame({
            "CODIGO_APU": [None, None],
            "original_description": [None, None]
        })
        input_file = tmp_path / "empty.csv"
        df.to_csv(input_file, index=False)
        
        pipeline = EmbeddingPipeline(config)
        
        with pytest.raises(DataValidationError) as exc_info:
            pipeline.run(
                input_file=input_file,
                output_dir=output_dir,
                model_name="test-model",
                text_column="original_description",
                id_column="CODIGO_APU"
            )
        
        assert "No quedan datos v谩lidos" in str(exc_info.value)
    
    @patch('scripts.generate_embeddings.faiss')
    @patch('scripts.generate_embeddings.SentenceTransformer')
    @patch('psutil.virtual_memory')
    def test_run_with_backup_creation(
        self,
        mock_memory,
        mock_transformer_class,
        mock_faiss,
        config,
        sample_csv_file,
        output_dir
    ):
        """Prueba creaci贸n de backups cuando existen archivos previos."""
        # Configurar mocks
        mock_memory.return_value = MagicMock(available=10 * 1024**3)
        
        mock_model = MagicMock()
        mock_model.get_sentence_embedding_dimension.return_value = 384
        mock_model.encode.return_value = np.random.rand(5, 384).astype(np.float32)
        mock_transformer_class.return_value = mock_model
        
        mock_index = MagicMock()
        mock_index.ntotal = 5
        mock_index.search.return_value = (np.array([[0.9]]), np.array([[0]]))
        mock_faiss.IndexFlatIP.return_value = mock_index
        
        # Crear archivos existentes
        (output_dir / "faiss.index").write_text("old index")
        (output_dir / "id_map.json").write_text('{"old": "map"}')
        
        # Ejecutar pipeline con backup habilitado
        pipeline = EmbeddingPipeline(config)
        pipeline.run(
            input_file=sample_csv_file,
            output_dir=output_dir,
            model_name="test-model",
            text_column="original_description",
            id_column="CODIGO_APU"
        )
        
        # Verificar que se crearon backups
        backup_files = list(output_dir.glob("*backup*"))
        assert len(backup_files) >= 2
        
        # Verificar que al menos un backup contiene los datos originales
        index_backups = [f for f in backup_files if "faiss" in f.name]
        assert any("old index" == f.read_text() for f in index_backups)


# ============================================================================
# TESTS - Performance and Edge Cases
# ============================================================================

class TestPerformanceAndEdgeCases:
    """Pruebas de rendimiento y casos extremos."""
    
    @pytest.mark.parametrize("n_samples", [100, 1000, 5000])
    def test_large_dataset_handling(self, config, tmp_path, n_samples):
        """Prueba manejo de datasets grandes."""
        # Crear dataset grande
        df = pd.DataFrame({
            "CODIGO_APU": [f"APU-{i:05d}" for i in range(n_samples)],
            "original_description": [
                f"Descripci贸n de prueba n煤mero {i} con texto suficiente"
                for i in range(n_samples)
            ]
        })
        
        input_file = tmp_path / f"large_{n_samples}.csv"
        df.to_csv(input_file, index=False)
        
        # Validar que no falla con datasets grandes
        validator = DataValidator()
        result = validator.validate_dataframe(
            df, "original_description", "CODIGO_APU", config
        )
        
        assert len(result) == n_samples
    
    def test_special_characters_in_text(self, config):
        """Prueba manejo de caracteres especiales."""
        df = pd.DataFrame({
            "CODIGO_APU": ["APU-001", "APU-002", "APU-003"],
            "original_description": [
                "Texto con 帽, 谩, 茅, 铆, 贸, 煤",
                "Text with 规瀛绗 and 茅mojis ",
                "Symbols: @#$%^&*()_+-=[]{}|;:',.<>?/",
            ]
        })
        
        validator = DataValidator()
        result = validator.validate_dataframe(
            df, "original_description", "CODIGO_APU", config
        )
        
        assert len(result) == 3
        assert all(result["original_description"].notna())
    
    def test_memory_estimation_accuracy(self, config):
        """Prueba precisi贸n de estimaci贸n de memoria."""
        pipeline = EmbeddingPipeline(config)
        
        # Test con diferentes tama帽os
        test_cases = [
            (1000, 384, 0.0015),  # ~1.5 MB
            (10000, 768, 0.029),  # ~29 MB
            (100000, 1024, 0.39),  # ~390 MB
        ]
        
        for n_samples, embedding_dim, expected_gb in test_cases:
            estimated = pipeline.estimate_memory_usage(n_samples, embedding_dim)
            # Permitir 20% de margen de error
            assert abs(estimated - expected_gb) / expected_gb < 0.2
    
    @pytest.mark.parametrize("batch_size", [1, 10, 100, 512])
    def test_different_batch_sizes(self, config, mock_model, batch_size):
        """Prueba diferentes tama帽os de batch."""
        config.MAX_BATCH_SIZE = batch_size
        generator = EmbeddingGenerator(config)
        generator.model = mock_model
        
        texts = ["Text"] * 50
        mock_model.encode.return_value = np.random.rand(
            min(batch_size, len(texts)), 384
        )
        
        # Reset mock para contar llamadas
        mock_model.encode.reset_mock()
        
        embeddings = generator.generate_embeddings_batch(texts, batch_size)
        
        expected_calls = (len(texts) + batch_size - 1) // batch_size
        assert mock_model.encode.call_count == expected_calls


# ============================================================================
# TESTS - CLI Integration
# ============================================================================

class TestCLIIntegration:
    """Pruebas de integraci贸n con la interfaz de l铆nea de comandos."""
    
    @patch('scripts.generate_embeddings.EmbeddingPipeline')
    def test_main_function_success(self, mock_pipeline_class, sample_csv_file):
        """Prueba funci贸n main con argumentos v谩lidos."""
        from scripts.generate_embeddings import main
        
        mock_pipeline = MagicMock()
        mock_pipeline.run.return_value = {"processing_time": 1.0}
        mock_pipeline_class.return_value = mock_pipeline
        
        # Simular argumentos de l铆nea de comandos
        test_args = [
            "generate_embeddings.py",
            "--input", str(sample_csv_file),
            "--output", "test_output",
            "--model", "test-model",
            "--batch-size", "64",
        ]
        
        with patch.object(sys, 'argv', test_args):
            with pytest.raises(SystemExit) as exc_info:
                main()
            
            assert exc_info.value.code == 0
            mock_pipeline.run.assert_called_once()
    
    @patch('scripts.generate_embeddings.EmbeddingPipeline')
    def test_main_function_with_error(self, mock_pipeline_class):
        """Prueba manejo de errores en funci贸n main."""
        from scripts.generate_embeddings import main
        
        mock_pipeline = MagicMock()
        mock_pipeline.run.side_effect = EmbeddingGenerationError("Test error")
        mock_pipeline_class.return_value = mock_pipeline
        
        test_args = [
            "generate_embeddings.py",
            "--input", "test.csv",
        ]
        
        with patch.object(sys, 'argv', test_args):
            with pytest.raises(SystemExit) as exc_info:
                main()
            
            assert exc_info.value.code == 1


# ============================================================================
# TESTS - Concurrency and Thread Safety
# ============================================================================

class TestConcurrencyAndThreadSafety:
    """Pruebas de concurrencia y seguridad de hilos."""
    
    def test_concurrent_file_access(self, tmp_path):
        """Prueba acceso concurrente a archivos."""
        import threading
        import time
        
        test_file = tmp_path / "concurrent_test.json"
        test_file.write_text('{"test": "data"}')
        
        manager = FileManager()
        errors = []
        
        def load_file():
            try:
                for _ in range(10):
                    df = manager.load_data(test_file)
                    time.sleep(0.001)
            except Exception as e:
                errors.append(e)
        
        threads = [threading.Thread(target=load_file) for _ in range(5)]
        for t in threads:
            t.start()
        for t in threads:
            t.join()
        
        assert len(errors) == 0, f"Concurrent access failed: {errors}"


# ============================================================================
# RUN CONFIGURATION
# ============================================================================

if __name__ == "__main__":
    # Configurar pytest para ejecutar con verbosidad y cobertura
    pytest.main([
        __file__,
        "-v",
        "--tb=short",
        "--cov=scripts.generate_embeddings",
        "--cov-report=html",
        "--cov-report=term-missing",
    ])

### propuesta 2 test_generate_embeddings.py ###

import json
from pathlib import Path
from unittest.mock import patch, MagicMock

import numpy as np
import pandas as pd
import pytest

# Importar la funci贸n a probar
from scripts.generate_embeddings import generate_embeddings


# --- Fixtures de Pytest ---

@pytest.fixture
def sample_data_file_csv(tmp_path: Path) -> Path:
    """Crea un archivo CSV de prueba en un directorio temporal."""
    data = {
        "CODIGO_APU": ["APU-001", "APU-002", "APU-003"],
        "original_description": [
            "Instalaci贸n de muro de ladrillo",
            "Suministro de tuber铆a de PVC",
            "Excavaci贸n manual en tierra",
        ],
        "otra_columna": [1, 2, 3],
    }
    df = pd.DataFrame(data)
    file_path = tmp_path / "test_apus.csv"
    df.to_csv(file_path, index=False)
    return file_path


@pytest.fixture
def sample_data_file_json(tmp_path: Path) -> Path:
    """Crea un archivo JSON de prueba en un directorio temporal."""
    data = {
        "CODIGO_APU": ["APU-001", "APU-002", "APU-003"],
        "original_description": [
            "Instalaci贸n de muro de ladrillo",
            "Suministro de tuber铆a de PVC",
            "Excavaci贸n manual en tierra",
        ],
        "otra_columna": [1, 2, 3],
    }
    file_path = tmp_path / "test_apus.json"
    with open(file_path, "w", encoding="utf-8") as f:
        json.dump(data, f, ensure_ascii=False, indent=2)
    return file_path


@pytest.fixture
def output_dir(tmp_path: Path) -> Path:
    """Crea un directorio de salida temporal."""
    return tmp_path / "test_output"


# --- Suite de Pruebas ---

@patch("scripts.generate_embeddings.faiss")
@patch("scripts.generate_embeddings.SentenceTransformer")
def test_generate_embeddings_success_flow_csv(
    mock_sentence_transformer: MagicMock,
    mock_faiss: MagicMock,
    sample_data_file_csv: Path,
    output_dir: Path,
):
    """
    Prueba el flujo exitoso de la generaci贸n de embeddings con archivo CSV.
    """
    # --- Configuraci贸n de los Mocks ---
    mock_model_instance = MagicMock()
    embedding_dim = 384
    fake_embeddings = np.random.rand(3, embedding_dim).astype(np.float32)
    mock_model_instance.get_sentence_embedding_dimension.return_value = embedding_dim
    mock_model_instance.encode.return_value = fake_embeddings
    mock_sentence_transformer.return_value = mock_model_instance

    mock_index_instance = MagicMock()
    mock_faiss.IndexFlatIP.return_value = mock_index_instance

    # --- Ejecuci贸n ---
    generate_embeddings(
        input_file=sample_data_file_csv,
        output_dir=output_dir,
        model_name="test-model",
        text_column="original_description",
        id_column="CODIGO_APU",
    )

    # --- Aserciones ---
    mock_sentence_transformer.assert_called_once_with("test-model")
    mock_model_instance.encode.assert_called_once()
    call_args = mock_model_instance.encode.call_args[0][0]
    assert call_args == [
        "Instalaci贸n de muro de ladrillo",
        "Suministro de tuber铆a de PVC",
        "Excavaci贸n manual en tierra",
    ]

    mock_faiss.IndexFlatIP.assert_called_once_with(embedding_dim)
    mock_index_instance.add.assert_called_once()
    np.testing.assert_array_equal(
        mock_index_instance.add.call_args[0][0], fake_embeddings
    )

    mock_faiss.write_index.assert_called_once_with(
        mock_index_instance, str(output_dir / "faiss.index")
    )

    map_path = output_dir / "id_map.json"
    assert map_path.exists()
    with open(map_path, "r", encoding="utf-8") as f:
        id_map = json.load(f)
    expected_map = {"0": "APU-001", "1": "APU-002", "2": "APU-003"}
    assert id_map == expected_map


@patch("scripts.generate_embeddings.faiss")
@patch("scripts.generate_embeddings.SentenceTransformer")
def test_generate_embeddings_success_flow_json(
    mock_sentence_transformer: MagicMock,
    mock_faiss: MagicMock,
    sample_data_file_json: Path,
    output_dir: Path,
):
    """
    Prueba el flujo exitoso de la generaci贸n de embeddings con archivo JSON.
    """
    mock_model_instance = MagicMock()
    embedding_dim = 384
    fake_embeddings = np.random.rand(3, embedding_dim).astype(np.float32)
    mock_model_instance.get_sentence_embedding_dimension.return_value = embedding_dim
    mock_model_instance.encode.return_value = fake_embeddings
    mock_sentence_transformer.return_value = mock_model_instance

    mock_index_instance = MagicMock()
    mock_faiss.IndexFlatIP.return_value = mock_index_instance

    generate_embeddings(
        input_file=sample_data_file_json,
        output_dir=output_dir,
        model_name="test-model",
        text_column="original_description",
        id_column="CODIGO_APU",
    )

    mock_sentence_transformer.assert_called_once_with("test-model")
    mock_model_instance.encode.assert_called_once()
    call_args = mock_model_instance.encode.call_args[0][0]
    assert call_args == [
        "Instalaci贸n de muro de ladrillo",
        "Suministro de tuber铆a de PVC",
        "Excavaci贸n manual en tierra",
    ]

    mock_faiss.IndexFlatIP.assert_called_once_with(embedding_dim)
    mock_index_instance.add.assert_called_once()
    np.testing.assert_array_equal(
        mock_index_instance.add.call_args[0][0], fake_embeddings
    )

    mock_faiss.write_index.assert_called_once_with(
        mock_index_instance, str(output_dir / "faiss.index")
    )

    map_path = output_dir / "id_map.json"
    assert map_path.exists()
    with open(map_path, "r", encoding="utf-8") as f:
        id_map = json.load(f)
    expected_map = {"0": "APU-001", "1": "APU-002", "2": "APU-003"}
    assert id_map == expected_map


def test_generate_embeddings_file_not_found():
    """Verifica que se lance FileNotFoundError si el archivo no existe."""
    with pytest.raises(FileNotFoundError):
        generate_embeddings(
            input_file=Path("non_existent_file.csv"),
            output_dir=Path("some_dir"),
            model_name="test-model",
            text_column="desc",
            id_column="id",
        )


def test_generate_embeddings_unsupported_format(tmp_path: Path, output_dir: Path):
    """Verifica que se lance ValueError si el formato no es soportado."""
    invalid_file = tmp_path / "test.txt"
    invalid_file.write_text("contenido inv谩lido")
    with pytest.raises(ValueError, match="Formato de archivo no soportado"):
        generate_embeddings(
            input_file=invalid_file,
            output_dir=output_dir,
            model_name="test-model",
            text_column="desc",
            id_column="id",
        )


def test_generate_embeddings_missing_columns(sample_data_file_csv: Path, output_dir: Path):
    """Verifica que se lance ValueError si faltan columnas requeridas."""
    with pytest.raises(ValueError, match="debe contener las columnas: \\['missing_col'\\]"):
        generate_embeddings(
            input_file=sample_data_file_csv,
            output_dir=output_dir,
            model_name="test-model",
            text_column="missing_col",
            id_column="CODIGO_APU",
        )


def test_generate_embeddings_empty_dataframe_after_cleaning(tmp_path: Path, output_dir: Path):
    """Verifica que no se generen archivos si el DataFrame queda vac铆o tras limpieza."""
    empty_file = tmp_path / "empty_data.csv"
    df = pd.DataFrame({"CODIGO_APU": ["APU-001"], "original_description": [None]})
    df.to_csv(empty_file, index=False)

    try:
        generate_embeddings(
            input_file=empty_file,
            output_dir=output_dir,
            model_name="test-model",
            text_column="original_description",
            id_column="CODIGO_APU",
        )
    except Exception as e:
        pytest.fail(f"La funci贸n no deber铆a fallar con datos vac铆os, pero lanz贸: {e}")

    # No se deben crear archivos
    assert not (output_dir / "faiss.index").exists()
    assert not (output_dir / "id_map.json").exists()


def test_generate_embeddings_duplicate_ids(tmp_path: Path, output_dir: Path):
    """Verifica que se eliminen duplicados por ID correctamente."""
    dup_file = tmp_path / "duplicates.csv"
    data = {
        "CODIGO_APU": ["APU-001", "APU-001", "APU-002"],
        "original_description": [
            "Descripci贸n 1",
            "Descripci贸n duplicada",
            "Descripci贸n 2",
        ],
    }
    df = pd.DataFrame(data)
    df.to_csv(dup_file, index=False)

    # Mockear para evitar llamadas reales
    with patch("scripts.generate_embeddings.faiss") as mock_faiss, \
         patch("scripts.generate_embeddings.SentenceTransformer") as mock_st:

        mock_model = MagicMock()
        mock_model.get_sentence_embedding_dimension.return_value = 384
        mock_model.encode.return_value = np.random.rand(2, 384).astype(np.float32)
        mock_st.return_value = mock_model

        mock_index = MagicMock()
        mock_faiss.IndexFlatIP.return_value = mock_index

        generate_embeddings(
            input_file=dup_file,
            output_dir=output_dir,
            model_name="test-model",
            text_column="original_description",
            id_column="CODIGO_APU",
        )

        # Solo 2 registros 煤nicos deben ser procesados
        mock_model.encode.assert_called_once()
        call_args = mock_model.encode.call_args[0][0]
        assert len(call_args) == 2
        assert call_args == ["Descripci贸n 1", "Descripci贸n 2"]  # Duplicado eliminado

        mock_index.add.assert_called_once()
        added_embeddings = mock_index.add.call_args[0][0]
        assert added_embeddings.shape[0] == 2  # Dos vectores

        # Verificar mapeo
        map_path = output_dir / "id_map.json"
        assert map_path.exists()
        with open(map_path, "r", encoding="utf-8") as f:
            id_map = json.load(f)
        assert len(id_map) == 2
        assert id_map["0"] == "APU-001"
        assert id_map["1"] == "APU-002"


def test_generate_embeddings_model_load_error(tmp_path: Path, output_dir: Path):
    """Verifica que se maneje un error al cargar el modelo."""
    # Crear un archivo v谩lido
    valid_file = tmp_path / "valid.csv"
    df = pd.DataFrame({
        "CODIGO_APU": ["APU-001"],
        "original_description": ["Descripci贸n"]
    })
    df.to_csv(valid_file, index=False)

    with patch("scripts.generate_embeddings.SentenceTransformer") as mock_st:
        mock_st.side_effect = Exception("Modelo no encontrado")

        with pytest.raises(RuntimeError, match="Error al cargar el modelo"):
            generate_embeddings(
                input_file=valid_file,
                output_dir=output_dir,
                model_name="invalid-model",
                text_column="original_description",
                id_column="CODIGO_APU",
            )


def test_generate_embeddings_faiss_write_error(tmp_path: Path, output_dir: Path):
    """Verifica que se maneje un error al escribir el 铆ndice FAISS."""
    valid_file = tmp_path / "valid.csv"
    df = pd.DataFrame({
        "CODIGO_APU": ["APU-001"],
        "original_description": ["Descripci贸n"]
    })
    df.to_csv(valid_file, index=False)

    with patch("scripts.generate_embeddings.faiss") as mock_faiss, \
         patch("scripts.generate_embeddings.SentenceTransformer") as mock_st:

        mock_model = MagicMock()
        mock_model.get_sentence_embedding_dimension.return_value = 384
        mock_model.encode.return_value = np.array([[0.1, 0.2]], dtype=np.float32)
        mock_st.return_value = mock_model

        mock_index = MagicMock()
        mock_faiss.IndexFlatIP.return_value = mock_index
        mock_faiss.write_index.side_effect = Exception("Error de escritura")

        with pytest.raises(RuntimeError, match="Error al guardar el 铆ndice FAISS"):
            generate_embeddings(
                input_file=valid_file,
                output_dir=output_dir,
                model_name="test-model",
                text_column="original_description",
                id_column="CODIGO_APU",
            )