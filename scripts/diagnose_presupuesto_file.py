# scripts/diagnose_presupuesto_file.py
"""
Herramienta de diagn√≥stico avanzada para analizar archivos de presupuesto.

Este m√≥dulo proporciona capacidades robustas de an√°lisis de estructura,
detecci√≥n autom√°tica de encoding, identificaci√≥n de separadores y
generaci√≥n de reportes detallados con recomendaciones.
"""
import logging
import re
import sys
from collections import Counter, defaultdict
from pathlib import Path
from typing import Any, Dict, List, Optional, Tuple, Union
from dataclasses import dataclass, field
from enum import Enum

try:
    import chardet
    CHARDET_AVAILABLE = True
except ImportError:
    CHARDET_AVAILABLE = False
    logging.warning("chardet no disponible. La detecci√≥n autom√°tica de encoding ser√° limitada.")

# Configuraci√≥n robusta del logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    datefmt='%Y-%m-%d %H:%M:%S'
)
logger = logging.getLogger("PresupuestoDiagnostic")


class DiagnosticError(Exception):
    """Excepci√≥n base para errores de diagn√≥stico."""
    pass


class FileReadError(DiagnosticError):
    """Error al leer el archivo."""
    pass


class EncodingDetectionError(DiagnosticError):
    """Error al detectar el encoding del archivo."""
    pass


class ConfidenceLevel(Enum):
    """Niveles de confianza para las detecciones."""
    HIGH = "alta"
    MEDIUM = "media"
    LOW = "baja"
    NONE = "ninguna"


@dataclass
class HeaderCandidate:
    """Representa un candidato a encabezado detectado."""
    line_num: int
    content: str
    matches: List[str]
    match_count: int
    column_count: int
    confidence: ConfidenceLevel = ConfidenceLevel.MEDIUM
    
    def __post_init__(self):
        """Valida la consistencia de los datos."""
        if self.line_num < 1:
            raise ValueError("line_num debe ser >= 1")
        if self.match_count != len(self.matches):
            raise ValueError("match_count debe coincidir con len(matches)")
        if self.column_count < 1:
            raise ValueError("column_count debe ser >= 1")


@dataclass
class SampleLine:
    """Representa una l√≠nea de muestra del archivo."""
    line_num: int
    content: str
    column_count: int
    
    def __post_init__(self):
        """Valida los datos de la l√≠nea."""
        if self.line_num < 1:
            raise ValueError("line_num debe ser >= 1")
        if self.column_count < 0:
            raise ValueError("column_count debe ser >= 0")


@dataclass
class ColumnStatistics:
    """Estad√≠sticas sobre columnas con un n√∫mero espec√≠fico."""
    count: int = 0
    samples: List[str] = field(default_factory=list)
    percentage: float = 0.0
    
    def add_sample(self, sample: str, max_samples: int = 3) -> None:
        """Agrega una muestra si no se ha alcanzado el l√≠mite."""
        if len(self.samples) < max_samples:
            self.samples.append(sample)
        self.count += 1


class PresupuestoFileDiagnostic:
    """
    Herramienta de diagn√≥stico para analizar la estructura de un archivo de Presupuesto.
    
    Esta clase proporciona m√©todos para:
    - Detectar autom√°ticamente el encoding del archivo con m√∫ltiples estrategias
    - Identificar el separador de columnas mediante an√°lisis estad√≠stico robusto
    - Localizar la fila de encabezado con coincidencias ponderadas de palabras clave
    - Analizar la distribuci√≥n y consistencia de columnas en los datos
    - Generar reportes detallados con recomendaciones accionables
    
    Attributes:
        file_path (Path): Ruta absoluta al archivo a diagnosticar
        ENCODINGS_TO_TRY (List[str]): Encodings a probar en orden de preferencia
        HEADER_KEYWORDS (List[str]): Palabras clave para identificar encabezados
        MIN_HEADER_KEYWORD_MATCHES (int): Coincidencias m√≠nimas para validar encabezado
        MAX_SAMPLE_LINES (int): L√≠mite de l√≠neas de muestra a almacenar
        MAX_REPORT_SAMPLE_LINES (int): L√≠mite de l√≠neas a mostrar en reporte
        MAX_LINES_TO_ANALYZE (int): L√≠mite de l√≠neas para archivos grandes
    """
    
    # Configuraci√≥n de encodings a probar
    ENCODINGS_TO_TRY = ['utf-8', 'utf-8-sig', 'latin1', 'cp1252', 'iso-8859-1', 'utf-16']
    
    # Palabras clave para detecci√≥n de encabezados (normalizadas)
    HEADER_KEYWORDS = [
        'ITEM', 'DESCRIPCION', 'CANT', 'CANTIDAD', 'UNIDAD', 'UND',
        'VR UNITARIO', 'VALOR UNITARIO', 'PRECIO', 'TOTAL', 'IMPORTE', 
        'PU', 'P U', 'SUBTOTAL', 'PARCIAL', 'COSTO'
    ]
    
    # Configuraci√≥n de l√≠mites y umbrales
    MIN_HEADER_KEYWORD_MATCHES = 2
    MAX_SAMPLE_LINES = 20
    MAX_REPORT_SAMPLE_LINES = 15
    MAX_LINES_TO_ANALYZE = 1000
    CHARDET_SAMPLE_SIZE = 50000  # Bytes a leer para detecci√≥n de encoding
    CHARDET_MIN_CONFIDENCE = 0.7  # Confianza m√≠nima para aceptar detecci√≥n autom√°tica
    SEPARATOR_DETECTION_SAMPLE_LINES = 100  # L√≠neas para analizar separador
    COLUMN_CONSISTENCY_THRESHOLD = 0.85  # Umbral para considerar columnas consistentes
    MAX_SAMPLES_PER_COLUMN_COUNT = 3  # Muestras a guardar por cada conteo de columnas

    def __init__(self, file_path: Union[str, Path]):
        """
        Inicializa el diagnosticador con la ruta del archivo.
        
        Args:
            file_path (Union[str, Path]): Ruta al archivo de presupuesto a analizar
            
        Raises:
            ValueError: Si la ruta es inv√°lida o el archivo no existe
            PermissionError: Si no hay permisos de lectura
        """
        self.file_path = Path(file_path).resolve()
        
        # Validaciones exhaustivas del archivo
        if not self.file_path.exists():
            raise ValueError(f"El archivo no existe: {self.file_path}")
        
        if not self.file_path.is_file():
            raise ValueError(f"La ruta no apunta a un archivo: {self.file_path}")
        
        if self.file_path.stat().st_size == 0:
            raise ValueError(f"El archivo est√° vac√≠o: {self.file_path}")
        
        # Verificar permisos de lectura
        if not self._check_read_permissions():
            raise PermissionError(f"No hay permisos de lectura para: {self.file_path}")
        
        self._reset_state()
        logger.info(f"Inicializado diagnosticador para: {self.file_path}")

    def _check_read_permissions(self) -> bool:
        """
        Verifica que el archivo tenga permisos de lectura.
        
        Returns:
            bool: True si tiene permisos de lectura, False en caso contrario
        """
        try:
            with self.file_path.open('r', encoding='utf-8', errors='ignore') as f:
                f.read(1)
            return True
        except PermissionError:
            return False
        except Exception as e:
            logger.warning(f"Error al verificar permisos: {e}")
            return True  # Permitir continuar en caso de error inesperado

    def _reset_state(self) -> None:
        """Reinicia el estado interno para un nuevo diagn√≥stico."""
        self.stats: Counter = Counter()
        self.sample_lines: List[SampleLine] = []
        self.header_candidate: Optional[HeaderCandidate] = None
        self.column_analysis: Dict[int, ColumnStatistics] = defaultdict(ColumnStatistics)
        self.data_start_line: Optional[int] = None
        self._encoding: Optional[str] = None
        self._separator: Optional[str] = None
        self._content_cache: Optional[str] = None

    def diagnose(self) -> Dict[str, Any]:
        """
        Ejecuta el diagn√≥stico completo del archivo.
        
        Returns:
            Dict[str, Any]: Diccionario con estad√≠sticas y hallazgos del diagn√≥stico
            
        Raises:
            FileReadError: Si no se puede leer el archivo
            DiagnosticError: Para errores durante el diagn√≥stico
        """
        try:
            self._reset_state()
            logger.info(f"üîç Iniciando diagn√≥stico del archivo: {self.file_path}")
            
            # Obtener informaci√≥n b√°sica del archivo
            file_size = self.file_path.stat().st_size
            self.stats['file_size_bytes'] = file_size
            self.stats['file_size_human'] = self._human_readable_size(file_size)
            
            # Leer contenido con detecci√≥n robusta de encoding
            content = self._read_with_fallback_encoding()
            if not content:
                raise FileReadError(
                    "No se pudo leer el contenido del archivo con ning√∫n encoding soportado"
                )
            
            # Validar que el contenido no est√© vac√≠o despu√©s de leerlo
            if not content.strip():
                raise FileReadError("El archivo no contiene datos v√°lidos (solo espacios/saltos)")
            
            # Procesar l√≠neas
            lines = content.splitlines()
            total_lines = len(lines)
            self.stats['total_lines'] = total_lines
            logger.info(f"Archivo contiene {total_lines} l√≠neas.")
            
            # Validar cantidad m√≠nima de l√≠neas
            if total_lines < 2:
                logger.warning("‚ö†Ô∏è El archivo tiene muy pocas l√≠neas para un an√°lisis significativo.")
            
            # Limitar an√°lisis para archivos muy grandes
            lines_to_analyze = self._get_lines_to_analyze(lines)
            
            # Ejecutar an√°lisis
            self._detect_separator_from_lines(lines_to_analyze)
            self._analyze_structure_single_pass(lines_to_analyze)
            self._calculate_column_statistics()
            self._determine_confidence_levels()
            self._generate_diagnostic_report()
            
            # Preparar resultado
            result = self._build_result_dict()
            logger.info("‚úÖ Diagn√≥stico completado exitosamente.")
            return result
            
        except FileReadError:
            raise
        except Exception as e:
            logger.exception(f"‚ùå Error inesperado durante el diagn√≥stico: {str(e)}")
            raise DiagnosticError(f"Fallo en el diagn√≥stico: {str(e)}") from e

    def _get_lines_to_analyze(self, lines: List[str]) -> List[str]:
        """
        Determina qu√© l√≠neas analizar seg√∫n el tama√±o del archivo.
        
        Args:
            lines (List[str]): Todas las l√≠neas del archivo
            
        Returns:
            List[str]: L√≠neas a analizar (limitadas si es necesario)
        """
        total_lines = len(lines)
        
        if total_lines > self.MAX_LINES_TO_ANALYZE:
            logger.warning(
                f"‚ö†Ô∏è Archivo grande ({total_lines} l√≠neas). "
                f"Analizando las primeras {self.MAX_LINES_TO_ANALYZE} l√≠neas."
            )
            self.stats['truncated_analysis'] = True
            self.stats['lines_analyzed'] = self.MAX_LINES_TO_ANALYZE
            return lines[:self.MAX_LINES_TO_ANALYZE]
        else:
            self.stats['truncated_analysis'] = False
            self.stats['lines_analyzed'] = total_lines
            return lines

    def _human_readable_size(self, size_bytes: int) -> str:
        """
        Convierte bytes a formato legible por humanos.
        
        Args:
            size_bytes (int): Tama√±o en bytes
            
        Returns:
            str: Tama√±o formateado (ej: "1.5 MB")
        """
        if size_bytes == 0:
            return "0 B"
        
        units = ["B", "KB", "MB", "GB", "TB"]
        size = float(size_bytes)
        unit_index = 0
        
        while size >= 1024.0 and unit_index < len(units) - 1:
            size /= 1024.0
            unit_index += 1
        
        # Formato con decimales apropiados
        if size < 10:
            return f"{size:.2f} {units[unit_index]}"
        elif size < 100:
            return f"{size:.1f} {units[unit_index]}"
        else:
            return f"{size:.0f} {units[unit_index]}"

    def _read_with_fallback_encoding(self) -> Optional[str]:
        """
        Intenta leer el archivo con m√∫ltiples encodings.
        
        Estrategia:
        1. Probar encodings predefinidos en orden
        2. Si falla, usar chardet para detecci√≥n autom√°tica
        3. Como √∫ltimo recurso, leer con errores ignorados
        
        Returns:
            Optional[str]: Contenido del archivo o None si falla completamente
        """
        # Estrategia 1: Encodings predefinidos
        for encoding in self.ENCODINGS_TO_TRY:
            try:
                with self.file_path.open('r', encoding=encoding, errors='strict') as f:
                    content = f.read()
                
                self._encoding = encoding
                self.stats['encoding'] = encoding
                self.stats['encoding_method'] = 'predefined'
                logger.info(f"‚úÖ Archivo le√≠do con encoding: {encoding}")
                return content
                
            except (UnicodeDecodeError, LookupError):
                continue
            except Exception as e:
                logger.debug(f"Error inesperado con encoding {encoding}: {e}")
                continue
        
        # Estrategia 2: Detecci√≥n autom√°tica con chardet
        if CHARDET_AVAILABLE:
            content = self._read_with_chardet()
            if content:
                return content
        
        # Estrategia 3: √öltimo recurso - leer con errores reemplazados
        logger.warning("‚ö†Ô∏è Usando estrategia de √∫ltimo recurso: lectura con errores reemplazados")
        try:
            with self.file_path.open('r', encoding='utf-8', errors='replace') as f:
                content = f.read()
            
            self._encoding = 'utf-8'
            self.stats['encoding'] = 'utf-8 (con errores)'
            self.stats['encoding_method'] = 'fallback_with_errors'
            logger.warning("‚ö†Ô∏è Archivo le√≠do con reemplazo de caracteres inv√°lidos")
            return content
            
        except Exception as e:
            logger.error(f"‚ùå Error fatal al leer archivo: {e}")
            return None

    def _read_with_chardet(self) -> Optional[str]:
        """
        Intenta leer el archivo usando chardet para detectar el encoding.
        
        Returns:
            Optional[str]: Contenido del archivo o None si falla
        """
        try:
            logger.info("üîç Intentando detecci√≥n autom√°tica de encoding con chardet...")
            
            # Leer muestra para detecci√≥n
            sample_size = min(self.CHARDET_SAMPLE_SIZE, self.stats['file_size_bytes'])
            with self.file_path.open('rb') as f:
                raw_data = f.read(sample_size)
            
            detection = chardet.detect(raw_data)
            confidence = detection.get('confidence', 0)
            detected_encoding = detection.get('encoding')
            
            logger.info(
                f"Chardet detect√≥: {detected_encoding} "
                f"(confianza: {confidence:.2%})"
            )
            
            # Validar confianza m√≠nima
            if confidence < self.CHARDET_MIN_CONFIDENCE:
                logger.warning(
                    f"‚ö†Ô∏è Confianza insuficiente en detecci√≥n autom√°tica "
                    f"({confidence:.2%} < {self.CHARDET_MIN_CONFIDENCE:.2%})"
                )
                return None
            
            if not detected_encoding:
                logger.warning("‚ö†Ô∏è Chardet no pudo determinar un encoding")
                return None
            
            # Intentar leer con el encoding detectado
            try:
                with self.file_path.open('r', encoding=detected_encoding) as f:
                    content = f.read()
                
                self._encoding = detected_encoding
                self.stats['encoding'] = detected_encoding
                self.stats['encoding_method'] = 'chardet'
                self.stats['encoding_confidence'] = f"{confidence:.2%}"
                logger.info(f"‚úÖ Archivo le√≠do con encoding detectado: {detected_encoding}")
                return content
                
            except (UnicodeDecodeError, LookupError) as e:
                logger.error(f"‚ùå Fall√≥ lectura con encoding detectado ({detected_encoding}): {e}")
                return None
                
        except Exception as e:
            logger.error(f"‚ùå Error durante detecci√≥n autom√°tica de encoding: {e}")
            return None

    def _detect_separator_from_lines(self, lines: List[str]) -> None:
        """
        Detecta el separador de columnas analizando l√≠neas no vac√≠as.
        
        Args:
            lines (List[str]): L√≠neas a analizar
        """
        # Filtrar l√≠neas √∫tiles (no vac√≠as, no comentarios)
        useful_lines = [
            line.strip() 
            for line in lines[:self.SEPARATOR_DETECTION_SAMPLE_LINES]
            if line.strip() and not self._is_comment_line(line.strip())
        ]
        
        if not useful_lines:
            logger.warning("‚ö†Ô∏è No hay l√≠neas √∫tiles para detectar separador. Usando ';'")
            self._separator = ';'
            self.stats['detected_separator'] = ';'
            self.stats['separator_confidence'] = ConfidenceLevel.NONE.value
            return
        
        # Analizar frecuencia de separadores comunes
        separators = {
            ';': 'punto y coma',
            ',': 'coma',
            '\t': 'tabulaci√≥n',
            '|': 'pipe',
            '^': 'circunflejo'
        }
        
        separator_stats = {}
        
        for sep, name in separators.items():
            counts = [line.count(sep) for line in useful_lines]
            # Calcular estad√≠sticas robustas
            if counts:
                avg_count = sum(counts) / len(counts)
                max_count = max(counts)
                min_count = min(counts)
                # Varianza para medir consistencia
                variance = sum((c - avg_count) ** 2 for c in counts) / len(counts)
                consistency = 1.0 / (1.0 + variance) if variance > 0 else 1.0
                
                separator_stats[sep] = {
                    'name': name,
                    'avg': avg_count,
                    'max': max_count,
                    'min': min_count,
                    'variance': variance,
                    'consistency': consistency,
                    'score': avg_count * consistency  # Score ponderado
                }
        
        # Seleccionar mejor candidato
        if separator_stats:
            # Ordenar por score (frecuencia * consistencia)
            best_sep = max(separator_stats.items(), key=lambda x: x[1]['score'])
            separator = best_sep[0]
            stats = best_sep[1]
            
            # Determinar nivel de confianza
            if stats['avg'] >= 3 and stats['consistency'] > 0.7:
                confidence = ConfidenceLevel.HIGH
            elif stats['avg'] >= 2 and stats['consistency'] > 0.5:
                confidence = ConfidenceLevel.MEDIUM
            else:
                confidence = ConfidenceLevel.LOW
            
            self._separator = separator
            self.stats['detected_separator'] = separator
            self.stats['separator_confidence'] = confidence.value
            self.stats['separator_avg_count'] = f"{stats['avg']:.1f}"
            
            logger.info(
                f"‚úÖ Separador detectado: '{separator}' ({stats['name']}) - "
                f"Confianza: {confidence.value} "
                f"(promedio: {stats['avg']:.1f}, consistencia: {stats['consistency']:.2f})"
            )
        else:
            # Fallback
            logger.warning("‚ö†Ô∏è No se pudo detectar separador. Usando ';' por defecto")
            self._separator = ';'
            self.stats['detected_separator'] = ';'
            self.stats['separator_confidence'] = ConfidenceLevel.LOW.value

    def _is_comment_line(self, line: str) -> bool:
        """
        Determina si una l√≠nea es un comentario.
        
        Args:
            line (str): L√≠nea a evaluar (ya debe estar stripped)
            
        Returns:
            bool: True si es comentario, False en caso contrario
        """
        comment_markers = ('#', '//', '*', '--', '/*', '\'', 'REM')
        return any(line.startswith(marker) for marker in comment_markers)

    def _analyze_structure_single_pass(self, lines: List[str]) -> None:
        """
        Analiza la estructura del archivo en una sola pasada (optimizado).
        
        Args:
            lines (List[str]): L√≠neas a analizar
        """
        if not self._separator:
            logger.error("‚ùå Separador no detectado. No se puede analizar estructura.")
            return
        
        separator = self._separator
        potential_headers: List[HeaderCandidate] = []
        header_line_num = None
        
        for line_num, line in enumerate(lines, 1):
            stripped = line.strip()
            
            # Clasificar l√≠nea
            if not stripped:
                self.stats['empty_lines'] += 1
                continue
            
            self.stats['non_empty_lines'] += 1
            
            if self._is_comment_line(stripped):
                self.stats['comment_lines'] += 1
                continue
            
            # Si a√∫n no encontramos el encabezado, buscar candidatos
            if header_line_num is None:
                header_match_info = self._evaluate_header_candidate(stripped, line_num, separator)
                if header_match_info:
                    potential_headers.append(header_match_info)
                    # Si tenemos un candidato muy fuerte, dejamos de buscar
                    if header_match_info.match_count >= 4:
                        header_line_num = line_num
                        self.header_candidate = header_match_info
                        logger.info(
                            f"‚úÖ Encabezado fuerte detectado en l√≠nea {line_num} "
                            f"({header_match_info.match_count} coincidencias)"
                        )
            else:
                # Ya encontramos el encabezado, procesar como datos
                self._process_data_line(stripped, line_num, separator)
        
        # Si no se encontr√≥ un encabezado fuerte, seleccionar el mejor candidato
        if header_line_num is None and potential_headers:
            # Ordenar por n√∫mero de coincidencias y n√∫mero de columnas
            best_candidate = max(
                potential_headers,
                key=lambda x: (x.match_count, x.column_count, -x.line_num)
            )
            self.header_candidate = best_candidate
            header_line_num = best_candidate.line_num
            logger.info(
                f"‚úÖ Mejor candidato a encabezado: l√≠nea {header_line_num} "
                f"({best_candidate.match_count} coincidencias)"
            )
            
            # Re-procesar l√≠neas posteriores al encabezado como datos
            for line_num, line in enumerate(lines, 1):
                if line_num <= header_line_num:
                    continue
                stripped = line.strip()
                if stripped and not self._is_comment_line(stripped):
                    self._process_data_line(stripped, line_num, separator)
        
        # Registrar resultados
        if self.header_candidate:
            self.stats['header_found_at_line'] = self.header_candidate.line_num
            self.stats['header_column_count'] = self.header_candidate.column_count
        else:
            logger.warning("‚ö†Ô∏è No se detect√≥ ning√∫n encabezado v√°lido")
            self.stats['header_found_at_line'] = None

    def _evaluate_header_candidate(
        self, 
        line: str, 
        line_num: int, 
        separator: str
    ) -> Optional[HeaderCandidate]:
        """
        Eval√∫a si una l√≠nea puede ser el encabezado.
        
        Args:
            line (str): L√≠nea a evaluar
            line_num (int): N√∫mero de l√≠nea
            separator (str): Separador de columnas
            
        Returns:
            Optional[HeaderCandidate]: Informaci√≥n del candidato si cumple criterios, None en caso contrario
        """
        normalized = self._normalize_header_text(line)
        matches = [kw for kw in self.HEADER_KEYWORDS if kw in normalized]
        match_count = len(matches)
        
        if match_count < self.MIN_HEADER_KEYWORD_MATCHES:
            return None
        
        columns = [col.strip() for col in line.split(separator)]
        column_count = len(columns)
        
        # Determinar confianza preliminar
        if match_count >= 5:
            confidence = ConfidenceLevel.HIGH
        elif match_count >= 3:
            confidence = ConfidenceLevel.MEDIUM
        else:
            confidence = ConfidenceLevel.LOW
        
        try:
            return HeaderCandidate(
                line_num=line_num,
                content=line,
                matches=matches,
                match_count=match_count,
                column_count=column_count,
                confidence=confidence
            )
        except ValueError as e:
            logger.warning(f"Error al crear HeaderCandidate: {e}")
            return None

    def _process_data_line(self, line: str, line_num: int, separator: str) -> None:
        """
        Procesa una l√≠nea de datos (posterior al encabezado).
        
        Args:
            line (str): L√≠nea a procesar
            line_num (int): N√∫mero de l√≠nea
            separator (str): Separador de columnas
        """
        # Ignorar l√≠neas de totales o res√∫menes
        if line.strip().upper().startswith('TOTAL'):
            self.stats['summary_lines_ignored'] += 1
            return

        columns = [col.strip() for col in line.split(separator)]
        num_cols = len(columns)
        
        # Actualizar estad√≠sticas de columnas
        self.stats[f'lines_with_{num_cols}_columns'] += 1
        
        # Agregar a an√°lisis de columnas
        self.column_analysis[num_cols].add_sample(
            line, 
            max_samples=self.MAX_SAMPLES_PER_COLUMN_COUNT
        )
        
        # Guardar muestra si no hemos alcanzado el l√≠mite
        if len(self.sample_lines) < self.MAX_SAMPLE_LINES:
            try:
                sample = SampleLine(
                    line_num=line_num,
                    content=line,
                    column_count=num_cols
                )
                self.sample_lines.append(sample)
            except ValueError as e:
                logger.warning(f"Error al crear SampleLine: {e}")
        
        # Establecer l√≠nea de inicio de datos (primera l√≠nea de datos)
        if self.data_start_line is None:
            self.data_start_line = line_num

    def _normalize_header_text(self, text: str) -> str:
        """
        Normaliza texto para detecci√≥n de encabezados.
        
        Args:
            text (str): Texto a normalizar
            
        Returns:
            str: Texto normalizado (may√∫sculas, sin acentos, sin puntuaci√≥n)
        """
        # Convertir a may√∫sculas
        normalized = text.upper()
        
        # Eliminar acentos comunes
        accent_map = {
            '√Å': 'A', '√â': 'E', '√ç': 'I', '√ì': 'O', '√ö': 'U',
            '√Ä': 'A', '√à': 'E', '√å': 'I', '√í': 'O', '√ô': 'U',
            '√Ç': 'A', '√ä': 'E', '√é': 'I', '√î': 'O', '√õ': 'U',
            '√Ñ': 'A', '√ã': 'E', '√è': 'I', '√ñ': 'O', '√ú': 'U',
            '√ë': 'N', '√á': 'C'
        }
        for accented, plain in accent_map.items():
            normalized = normalized.replace(accented, plain)
        
        # Eliminar caracteres especiales comunes en encabezados
        chars_to_remove = '.,$%:"/\\\'*#@!¬°¬ø?()[]{}¬´¬ª""''<>~`¬¥¬®^'
        for char in chars_to_remove:
            normalized = normalized.replace(char, '')
        
        # Normalizar espacios m√∫ltiples a uno solo
        normalized = re.sub(r'\s+', ' ', normalized)
        
        return normalized.strip()

    def _calculate_column_statistics(self) -> None:
        """Calcula estad√≠sticas porcentuales para cada conteo de columnas."""
        total_data_lines = sum(stats.count for stats in self.column_analysis.values())
        
        if total_data_lines == 0:
            logger.warning("‚ö†Ô∏è No se encontraron l√≠neas de datos para an√°lisis")
            return
        
        for num_cols, stats in self.column_analysis.items():
            stats.percentage = (stats.count / total_data_lines) * 100
        
        # Identificar conteo de columnas dominante
        if self.column_analysis:
            dominant = max(self.column_analysis.items(), key=lambda x: x[1].count)
            self.stats['dominant_column_count'] = dominant[0]
            self.stats['column_consistency'] = dominant[1].percentage / 100

    def _determine_confidence_levels(self) -> None:
        """Determina y actualiza los niveles de confianza de las detecciones."""
        # Confianza en la consistencia de columnas
        consistency = self.stats.get('column_consistency', 0)
        
        if consistency >= 0.95:
            self.stats['column_consistency_level'] = ConfidenceLevel.HIGH.value
        elif consistency >= self.COLUMN_CONSISTENCY_THRESHOLD:
            self.stats['column_consistency_level'] = ConfidenceLevel.MEDIUM.value
        elif consistency >= 0.7:
            self.stats['column_consistency_level'] = ConfidenceLevel.LOW.value
        else:
            self.stats['column_consistency_level'] = ConfidenceLevel.NONE.value
        
        # Actualizar confianza del encabezado basado en consistencia
        if self.header_candidate:
            if (self.header_candidate.column_count == self.stats.get('dominant_column_count') 
                and consistency >= 0.9):
                self.header_candidate.confidence = ConfidenceLevel.HIGH

    def _build_result_dict(self) -> Dict[str, Any]:
        """
        Construye el diccionario de resultados del diagn√≥stico.
        
        Returns:
            Dict[str, Any]: Diccionario con todos los resultados
        """
        result = {
            'success': True,
            'file_path': str(self.file_path),
            'stats': dict(self.stats),
            'encoding': self._encoding,
            'separator': self._separator,
            'file_size': self.stats['file_size_human'],
        }
        
        if self.header_candidate:
            result['header_candidate'] = {
                'line_num': self.header_candidate.line_num,
                'content': self.header_candidate.content,
                'matches': self.header_candidate.matches,
                'match_count': self.header_candidate.match_count,
                'column_count': self.header_candidate.column_count,
                'confidence': self.header_candidate.confidence.value
            }
            result['data_start_line'] = self.data_start_line
        else:
            result['header_candidate'] = None
            result['data_start_line'] = None
        
        # Informaci√≥n de columnas
        if self.column_analysis:
            result['column_distribution'] = {
                num_cols: {
                    'count': stats.count,
                    'percentage': f"{stats.percentage:.1f}%"
                }
                for num_cols, stats in sorted(self.column_analysis.items())
            }
        
        return result

    def _generate_diagnostic_report(self) -> None:
        """Genera un reporte formateado detallado con hallazgos y recomendaciones."""
        report_lines = [
            "\n" + "=" * 90,
            "üìä REPORTE DE DIAGN√ìSTICO DEL ARCHIVO DE PRESUPUESTO".center(90),
            "=" * 90
        ]
        
        # Secci√≥n 1: Informaci√≥n b√°sica del archivo
        report_lines.extend([
            "\nüìÅ INFORMACI√ìN B√ÅSICA DEL ARCHIVO:",
            f"  üìÇ Ruta: {self.file_path}",
            f"  üíæ Tama√±o: {self.stats.get('file_size_human', 'desconocido')}",
            f"  üìè Total de l√≠neas: {self.stats.get('total_lines', 0):,}",
            f"  üìù L√≠neas analizadas: {self.stats.get('lines_analyzed', 0):,}"
        ])
        
        if self.stats.get('truncated_analysis'):
            report_lines.append("  ‚ö†Ô∏è  An√°lisis limitado debido al tama√±o del archivo")
        
        # Secci√≥n 2: Detalles de encoding
        report_lines.extend([
            f"\nüî§ ENCODING Y FORMATO:",
            f"  Encoding detectado: {self._encoding or 'desconocido'}",
            f"  M√©todo de detecci√≥n: {self.stats.get('encoding_method', 'desconocido')}"
        ])
        
        if 'encoding_confidence' in self.stats:
            report_lines.append(f"  Confianza: {self.stats['encoding_confidence']}")
        
        # Secci√≥n 3: Separador de columnas
        sep_display = repr(self._separator) if self._separator else 'desconocido'
        report_lines.extend([
            f"  Separador detectado: {sep_display}",
            f"  Confianza del separador: {self.stats.get('separator_confidence', 'desconocida')}"
        ])
        
        # Secci√≥n 4: Estad√≠sticas generales
        report_lines.extend([
            f"\nüìà ESTAD√çSTICAS GENERALES:",
            f"  ‚úì L√≠neas no vac√≠as: {self.stats.get('non_empty_lines', 0):,}",
            f"  ‚àÖ L√≠neas vac√≠as: {self.stats.get('empty_lines', 0):,}",
            f"  # L√≠neas de comentario: {self.stats.get('comment_lines', 0):,}"
        ])
        
        # Secci√≥n 5: Resultados de detecci√≥n de encabezado
        if self.header_candidate:
            report_lines.extend([
                f"\n‚úÖ ENCABEZADO DETECTADO (L√≠nea {self.header_candidate.line_num}):",
                f"  Contenido: {self.header_candidate.content[:100]}{'...' if len(self.header_candidate.content) > 100 else ''}",
                f"  Columnas detectadas: {self.header_candidate.column_count}",
                f"  Palabras clave encontradas ({self.header_candidate.match_count}):",
                f"    {', '.join(self.header_candidate.matches)}",
                f"  Nivel de confianza: {self.header_candidate.confidence.value.upper()}"
            ])
            
            if self.data_start_line:
                report_lines.append(f"  Datos comienzan en l√≠nea: {self.data_start_line}")
        else:
            report_lines.extend([
                "\n‚ö†Ô∏è  NO SE DETECT√ì UNA FILA DE ENCABEZADO CLARA",
                "  Posibles causas:",
                "    ‚Ä¢ El archivo no contiene un encabezado est√°ndar",
                "    ‚Ä¢ Las palabras clave del encabezado son diferentes a las esperadas",
                "    ‚Ä¢ El formato del archivo no es compatible"
            ])
        
        # Secci√≥n 6: An√°lisis de distribuci√≥n de columnas
        if self.column_analysis:
            report_lines.append("\nüìä AN√ÅLISIS DE DISTRIBUCI√ìN DE COLUMNAS:")
            
            total_data_lines = sum(stats.count for stats in self.column_analysis.values())
            dominant_count = self.stats.get('dominant_column_count')
            
            for num_cols in sorted(self.column_analysis.keys()):
                stats = self.column_analysis[num_cols]
                is_dominant = (num_cols == dominant_count)
                marker = "‚úì" if is_dominant else "‚ö†"
                status = "DOMINANTE" if is_dominant else "MINORITARIA"
                
                report_lines.append(
                    f"  {marker} {num_cols} columna(s): {stats.count:,} l√≠neas "
                    f"({stats.percentage:.1f}%) [{status}]"
                )
                
                # Mostrar ejemplos de l√≠neas inconsistentes
                if not is_dominant and stats.samples:
                    example = stats.samples[0]
                    truncated = example[:80] + "..." if len(example) > 80 else example
                    report_lines.append(f"      Ejemplo: {truncated}")
        
        # Secci√≥n 7: Muestra de l√≠neas de datos
        if self.sample_lines:
            report_lines.append(f"\nüìù MUESTRA DE L√çNEAS DE DATOS:")
            
            for sample in self.sample_lines[:self.MAX_REPORT_SAMPLE_LINES]:
                truncated = sample.content[:85] + "..." if len(sample.content) > 85 else sample.content
                report_lines.append(
                    f"  L√≠nea {sample.line_num:>5} ({sample.column_count} cols): {truncated}"
                )
            
            if len(self.sample_lines) > self.MAX_REPORT_SAMPLE_LINES:
                remaining = len(self.sample_lines) - self.MAX_REPORT_SAMPLE_LINES
                report_lines.append(f"  ... y {remaining} l√≠nea(s) m√°s")
        
        # Secci√≥n 8: Recomendaciones
        report_lines.append("\nüí° RECOMENDACIONES PARA PROCESAMIENTO:")
        
        if self.header_candidate:
            # Par√°metro header para pandas (0-indexed)
            pandas_header = self.header_candidate.line_num - 1
            report_lines.append(f"  ‚úì Usar header={pandas_header} al leer con pandas")
            
            # Advertencia si el encabezado no est√° en l√≠nea 0
            if pandas_header > 5:
                report_lines.append(
                    f"    ‚ö† El encabezado est√° en l√≠nea {self.header_candidate.line_num}. "
                    f"Considere skiprows=[0-{pandas_header-1}]"
                )
        else:
            report_lines.extend([
                "  ‚ö† No se detect√≥ encabezado autom√°ticamente",
                "    ‚Ä¢ Revisar manualmente las primeras l√≠neas del archivo",
                "    ‚Ä¢ Especificar header=None y proporcionar names=[...] al leer"
            ])
        
        # Recomendaciones de lectura
        if self._separator and self._encoding:
            report_lines.append(
                f"  ‚úì Par√°metros de lectura: sep={repr(self._separator)}, "
                f"encoding='{self._encoding}'"
            )
        
        # Advertencias sobre consistencia
        consistency = self.stats.get('column_consistency', 0)
        if consistency < self.COLUMN_CONSISTENCY_THRESHOLD:
            report_lines.extend([
                f"  ‚ö† ADVERTENCIA: Inconsistencia en columnas detectada "
                f"(consistencia: {consistency:.1%})",
                "    ‚Ä¢ Verificar que el separador detectado sea correcto",
                "    ‚Ä¢ Revisar si hay l√≠neas de resumen o totales mezcladas con datos",
                "    ‚Ä¢ Considerar filtrar l√≠neas por n√∫mero de columnas durante el procesamiento"
            ])
        else:
            report_lines.append(
                f"  ‚úì Consistencia de columnas: {consistency:.1%} "
                f"({self.stats.get('column_consistency_level', 'desconocida')})"
            )
        
        # C√≥digo de ejemplo para pandas
        if self.header_candidate and self._separator and self._encoding:
            report_lines.extend([
                "\nüêç EJEMPLO DE C√ìDIGO PANDAS:",
                "```python",
                "import pandas as pd",
                "",
                f"df = pd.read_csv(",
                f"    '{self.file_path.name}',",
                f"    sep={repr(self._separator)},",
                f"    encoding='{self._encoding}',",
                f"    header={self.header_candidate.line_num - 1}"
            ])
            
            # Agregar par√°metros opcionales seg√∫n el an√°lisis
            if consistency < 0.95:
                dominant_cols = self.stats.get('dominant_column_count')
                if dominant_cols:
                    report_lines.append(f"    # on_bad_lines='warn'  # Advertir sobre l√≠neas inconsistentes")
            
            report_lines.extend([
                ")",
                "```"
            ])
        
        # Secci√≥n final
        report_lines.extend([
            "\n" + "=" * 90,
            "‚úÖ FIN DEL REPORTE DE DIAGN√ìSTICO".center(90),
            "=" * 90 + "\n"
        ])
        
        # Imprimir reporte
        for line in report_lines:
            logger.info(line)


def main() -> int:
    """
    Funci√≥n principal para ejecuci√≥n desde l√≠nea de comandos.
    
    Returns:
        int: C√≥digo de salida (0 = √©xito, 1 = error)
    """
    if len(sys.argv) < 2:
        logger.error("‚ùå Error: Debe proporcionar la ruta al archivo de presupuesto")
        print("\n" + "=" * 70)
        print("USO DEL SCRIPT DE DIAGN√ìSTICO".center(70))
        print("=" * 70)
        print("\nSintaxis:")
        print("  python diagnose_presupuesto_file.py <ruta_al_archivo>")
        print("\nEjemplos:")
        print("  python diagnose_presupuesto_file.py presupuesto.csv")
        print("  python diagnose_presupuesto_file.py /ruta/completa/datos.txt")
        print("\nDescripci√≥n:")
        print("  Analiza la estructura de un archivo de presupuesto y genera")
        print("  un reporte detallado con recomendaciones para su procesamiento.")
        print("=" * 70 + "\n")
        return 1
    
    file_path = sys.argv[1]
    
    try:
        logger.info("=" * 80)
        logger.info(f"üöÄ INICIANDO DIAGN√ìSTICO".center(80))
        logger.info(f"Archivo: {file_path}".center(80))
        logger.info("=" * 80)
        
        diagnostic = PresupuestoFileDiagnostic(file_path)
        result = diagnostic.diagnose()
        
        if not result or not result.get('success'):
            logger.error("‚ùå El diagn√≥stico no pudo completarse exitosamente")
            return 1
        
        logger.info("\n" + "üéâ DIAGN√ìSTICO COMPLETADO EXITOSAMENTE üéâ".center(80))
        return 0
        
    except ValueError as ve:
        logger.error(f"‚ùå Error de validaci√≥n: {ve}")
        return 1
    except PermissionError as pe:
        logger.error(f"‚ùå Error de permisos: {pe}")
        return 1
    except FileReadError as fre:
        logger.error(f"‚ùå Error al leer archivo: {fre}")
        return 1
    except DiagnosticError as de:
        logger.error(f"‚ùå Error de diagn√≥stico: {de}")
        return 1
    except KeyboardInterrupt:
        logger.warning("\n‚ö†Ô∏è  Diagn√≥stico interrumpido por el usuario")
        return 130
    except Exception as e:
        logger.exception(f"‚ùå Error inesperado: {e}")
        return 1


if __name__ == "__main__":
    sys.exit(main())