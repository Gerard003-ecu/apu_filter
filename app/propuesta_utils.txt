### propuesta 1 ###

def normalize_text(text: str, preserve_special_chars: bool = False) -> str:
    """
    Normaliza un texto de forma consistente y robusta.
    """
    if text is None:
        return ""

    if not isinstance(text, str):
        text = str(text)

    text = text.lower().strip()
    text = unidecode(text)

    if preserve_special_chars:
        pattern = r"[^a-z0-9\s#\-_/\.@]"
    else:
        pattern = r"[^a-z0-9\s]"

    text = re.sub(pattern, "", text)
    text = re.sub(r"\s+", " ", text)

    return text.strip()


def normalize_unit(unit: str) -> str:
    if not unit or not isinstance(unit, str):
        return 'UND'

    unit_clean = unidecode(unit.upper().strip())

    unit_mapping = {
        'DIAS': 'DIA', 'D√çAS': 'DIA', 'JORNAL': 'JOR', 'JORNALES': 'JOR',
        'HORAS': 'HR', 'HORA': 'HR', 'UNIDAD': 'UND', 'UNIDADES': 'UND',
        'UN': 'UND', 'METRO': 'M', 'METROS': 'M', 'MTS': 'M',
        'METRO2': 'M2', 'M2': 'M2', 'MT2': 'M2', 'METRO CUADRADO': 'M2',
        'METRO3': 'M3', 'M3': 'M3', 'MT3': 'M3', 'METRO CUBICO': 'M3',
        'KILOGRAMO': 'KG', 'KILOGRAMOS': 'KG', 'KILOS': 'KG',
        'TONELADA': 'TON', 'TONELADAS': 'TON',
        'GALON': 'GAL', 'GALONES': 'GAL', 'GLN': 'GAL',
        'LITRO': 'L', 'LITROS': 'L', 'LT': 'L',
        'VIAJES': 'VIAJE', 'VJE': 'VIAJE'
    }

    if unit_clean in unit_mapping:
        return unit_mapping[unit_clean]

    if unit_clean in STANDARD_UNITS:
        return unit_clean

    clean_unit = re.sub(r'[^A-Z0-9]', '', unit_clean)
    if clean_unit in STANDARD_UNITS:
        return clean_unit

    if unit_clean not in ('', 'UND'):
        logger.debug(f"Unidad no reconocida: '{unit}' -> usando 'UND'")

    return 'UND'


def parse_number(s: Optional[Union[str, float, int]], decimal_separator: str = "auto") -> float:
    if s is None:
        return 0.0

    if isinstance(s, (int, float)):
        return float(s)

    if not isinstance(s, str):
        s = str(s)

    s_cleaned = s.strip().replace("$", "").replace("‚Ç¨", "").replace(" ", "").replace("%", "")

    if not s_cleaned:
        return 0.0

    if decimal_separator == "auto":
        comma_count = s_cleaned.count(',')
        dot_count = s_cleaned.count('.')

        if comma_count > 0 and dot_count > 0:
            if comma_count == 1 and s_cleaned.rfind(',') > s_cleaned.rfind('.'):
                decimal_separator = "comma"
            else:
                decimal_separator = "dot"
        elif comma_count == 1 and dot_count == 0:
            decimal_separator = "comma"
        else:
            decimal_separator = "dot"

    if decimal_separator == "comma":
        s_cleaned = s_cleaned.replace(".", "")
        s_cleaned = s_cleaned.replace(",", ".")
    else:
        s_cleaned = s_cleaned.replace(",", "")

    if s_cleaned.count('.') > 1:
        parts = s_cleaned.split('.')
        integer_part = ''.join(parts[:-1])
        decimal_part = parts[-1]
        s_cleaned = f"{integer_part}.{decimal_part}"

    if re.match(r'^[-+]?[0-9]*\.?[0-9]+([eE][-+]?[0-9]+)?$', s_cleaned):
        try:
            return float(s_cleaned)
        except (ValueError, TypeError):
            return 0.0
    else:
        return 0.0


def sanitize_for_json(data: any) -> any:
    if isinstance(data, dict):
        return {k: sanitize_for_json(v) for k, v in data.items()}
    if isinstance(data, list):
        return [sanitize_for_json(v) for v in data]
    if isinstance(data, np.ndarray):
        return [sanitize_for_json(v) for v in data.tolist()]
    if isinstance(data, (np.int64, np.int32, np.int16, np.int8)):
        return int(data)
    if isinstance(data, (np.float64, np.float32, np.float16)):
        return None if np.isnan(data) else float(data)
    if isinstance(data, np.bool_):
        return bool(data)
    if pd.isna(data):
        return None
    return data


def validate_numeric_value(value: float, field_name: str = "valor",
                          min_value: float = 0, max_value: float = 1e12,
                          allow_zero: bool = True) -> tuple[bool, str]:
    if not isinstance(value, (int, float)):
        return False, f"{field_name} debe ser num√©rico"

    if pd.isna(value):
        return False, f"{field_name} no puede ser nulo"

    if np.isinf(value):
        return False, f"{field_name} no puede ser infinito"

    if not allow_zero and value == 0:
        return False, f"{field_name} no puede ser cero"

    if value < min_value:
        return False, f"{field_name} no puede ser menor que {min_value}"

    if value > max_value:
        return False, f"{field_name} no puede ser mayor que {max_value}"

    return True, ""

Con estas correcciones, los tests deber√≠an pasar:

    normalize_text(None) ‚Üí ""
    normalize_unit("M3") ‚Üí "M3"
    parse_number("1.000 ‚Ç¨") ‚Üí 1000.0
    sanitize_for_json(np.array([1,2,3])) ‚Üí [1,2,3]
    validate_numeric_value(np.inf) ‚Üí ("campo no puede ser infinito")

### propuesta 2 ###

# ¬°Bendecido mi√©rcoles! üôè

He revisado detalladamente el c√≥digo y los fallos reportados. Aqu√≠ est√° la versi√≥n corregida y robustecida de `utils.py`:

```python
import os
import re
from typing import Optional, Union

import numpy as np
import pandas as pd
from unidecode import unidecode
import logging

logger = logging.getLogger(__name__)

def normalize_text(text: str, preserve_special_chars: bool = False) -> str:
    """
    Normaliza un texto de forma consistente y robusta.

    Args:
        text: Texto a normalizar
        preserve_special_chars: Si True, preserva algunos caracteres especiales √∫tiles

    Returns:
        Texto normalizado
    """
    # FIX 1: Manejar None expl√≠citamente antes de convertir a string
    if text is None:
        return ""
    
    if not isinstance(text, str):
        text = str(text)
    
    # Manejar cadenas vac√≠as despu√©s de conversi√≥n
    if not text or text.lower() == 'none':
        return ""

    # Convertir a min√∫sculas y remover espacios extra
    text = text.lower().strip()

    # Remover acentos y caracteres especiales
    text = unidecode(text)

    # Definir patrones de caracteres permitidos
    if preserve_special_chars:
        # Preservar caracteres √∫tiles para descripciones t√©cnicas
        pattern = r"[^a-z0-9\s#\-_/\.@]"
    else:
        # Solo caracteres b√°sicos para comparaciones
        pattern = r"[^a-z0-9\s]"

    # Remover caracteres no permitidos
    text = re.sub(pattern, "", text)

    # Normalizar espacios (m√∫ltiples espacios a uno solo)
    text = re.sub(r"\s+", " ", text)

    return text.strip()

def normalize_text_series(text_series: pd.Series, preserve_special_chars: bool = False) -> pd.Series:
    """
    Normaliza una serie de texto de forma vectorizada y eficiente.

    Args:
        text_series: Serie de pandas con texto a normalizar
        preserve_special_chars: Si True, preserva algunos caracteres especiales √∫tiles

    Returns:
        Serie de texto normalizada
    """
    if text_series is None or text_series.empty:
        return text_series

    # Asegurar que todos los elementos sean strings
    text_series = text_series.astype(str)

    # Aplicar normalizaci√≥n de forma vectorizada cuando sea posible
    def safe_normalize(x):
        try:
            return normalize_text(x, preserve_special_chars)
        except Exception:
            return ""

    return text_series.apply(safe_normalize)

def parse_number(s: Optional[Union[str, float, int]], decimal_separator: str = "auto") -> float:
    """
    Convierte una cadena a n√∫mero de punto flotante de forma robusta.

    Args:
        s: Valor a convertir (string, float, int)
        decimal_separator: "auto", "comma" o "dot"

    Returns:
        N√∫mero convertido o 0.0 si falla
    """
    if s is None:
        return 0.0

    # Si ya es num√©rico, retornar directamente
    if isinstance(s, (int, float)):
        return float(s)

    if not isinstance(s, str):
        s = str(s)

    # FIX 3: Agregar ‚Ç¨ a los s√≠mbolos a eliminar
    s_cleaned = s.strip().replace("$", "").replace("‚Ç¨", "").replace(" ", "").replace("%", "")

    if not s_cleaned:
        return 0.0

    # Detecci√≥n autom√°tica del separador decimal
    if decimal_separator == "auto":
        comma_count = s_cleaned.count(',')
        dot_count = s_cleaned.count('.')

        if comma_count > 0 and dot_count > 0:
            # Ambos presentes: determinar cu√°l es el decimal
            last_comma_pos = s_cleaned.rfind(',')
            last_dot_pos = s_cleaned.rfind('.')
            
            # El √∫ltimo separador es el decimal
            if last_comma_pos > last_dot_pos:
                decimal_separator = "comma"
            else:
                decimal_separator = "dot"
        elif comma_count == 1 and dot_count == 0:
            # Solo una coma: podr√≠a ser decimal
            decimal_separator = "comma"
        else:
            # Por defecto usar punto
            decimal_separator = "dot"

    # Limpiar seg√∫n el separador decimal detectado
    if decimal_separator == "comma":
        # Coma como decimal, punto como miles
        s_cleaned = s_cleaned.replace(".", "")  # Eliminar separadores de miles
        s_cleaned = s_cleaned.replace(",", ".")  # Convertir coma decimal a punto
    else:
        # Punto como decimal, coma como miles
        s_cleaned = s_cleaned.replace(",", "")  # Eliminar separadores de miles
        # El punto ya est√° correcto para float()

    # Manejar casos edge como "1.234.567" (m√∫ltiples puntos)
    if s_cleaned.count('.') > 1:
        # Conservar solo el √∫ltimo punto como decimal
        parts = s_cleaned.split('.')
        integer_part = ''.join(parts[:-1])
        decimal_part = parts[-1]
        s_cleaned = f"{integer_part}.{decimal_part}"

    # Validar que el string resultante sea un n√∫mero v√°lido
    if re.match(r'^[-+]?[0-9]*\.?[0-9]+([eE][-+]?[0-9]+)?$', s_cleaned):
        try:
            return float(s_cleaned)
        except (ValueError, TypeError):
            return 0.0
    else:
        return 0.0

def clean_apu_code(code: str, validate_format: bool = True) -> str:
    """
    Limpia y valida un c√≥digo de APU de forma robusta.

    Args:
        code: C√≥digo de APU a limpiar
        validate_format: Si True, valida el formato b√°sico

    Returns:
        C√≥digo de APU limpio y validado

    Raises:
        ValueError: Si el c√≥digo es inv√°lido y validate_format=True
    """
    if not isinstance(code, str):
        code = str(code)

    original_code = code
    code = code.strip().upper()

    # Remover caracteres no permitidos (mantener letras, n√∫meros, puntos, guiones)
    code = re.sub(r'[^\w.\-]', '', code)

    # Remover puntos y guiones al final
    code = code.rstrip('.-')

    # Validaciones opcionales de formato
    if validate_format:
        if not code:
            raise ValueError("C√≥digo APU no puede estar vac√≠o")

        if len(code) < 2:
            raise ValueError(f"C√≥digo APU demasiado corto: {original_code}")

        # Verificar que tenga al menos un n√∫mero (para evitar c√≥digos solo con letras)
        if not any(char.isdigit() for char in code):
            logger.warning(f"C√≥digo APU sin n√∫meros: {original_code} -> {code}")

    return code

# Unidades est√°ndar soportadas
STANDARD_UNITS = {
    # Longitud
    'M', 'M2', 'M3', 'ML', 'KM', 'CM', 'MM',
    # Tiempo
    'HORA', 'HR', 'DIA', 'SEMANA', 'MES', 'A√ëO', 'JOR',
    # Peso
    'KG', 'TON', 'LB', 'GR',
    # Volumen l√≠quido
    'L', 'LT', 'GAL', 'ML',
    # Unidades
    'UND', 'UN', 'PAR', 'JUEGO', 'KIT',
    # Transporte
    'VIAJE', 'VIAJES', 'KM',
    # Otros
    'SERVICIO', '%'
}

def normalize_unit(unit: str) -> str:
    """
    Normaliza y valida una unidad de medida.

    Args:
        unit: Unidad a normalizar

    Returns:
        Unidad normalizada o 'UND' si no es v√°lida
    """
    if not unit or not isinstance(unit, str):
        return 'UND'

    # FIX 2: Preservar el valor original antes de limpiar agresivamente
    unit = unit.upper().strip()
    
    # Primero verificar si ya es una unidad est√°ndar (antes de cualquier limpieza)
    if unit in STANDARD_UNITS:
        return unit

    # Mapeo de unidades equivalentes
    unit_mapping = {
        'DIAS': 'DIA', 'D√çAS': 'DIA', 'JORNAL': 'JOR', 'JORNALES': 'JOR',
        'HORAS': 'HR', 'HORA': 'HR', 'UNIDAD': 'UND', 'UNIDADES': 'UND',
        'UN': 'UND', 'METRO': 'M', 'METROS': 'M', 'MTS': 'M',
        'METRO2': 'M2', 'MT2': 'M2', 'METRO CUADRADO': 'M2',
        'METRO3': 'M3', 'MT3': 'M3', 'METRO CUBICO': 'M3',
        'KILOGRAMO': 'KG', 'KILOGRAMOS': 'KG', 'KILOS': 'KG',
        'TONELADA': 'TON', 'TONELADAS': 'TON',
        'GALON': 'GAL', 'GALONES': 'GAL', 'GLN': 'GAL',
        'LITRO': 'L', 'LITROS': 'L', 'LT': 'L',
        'VIAJES': 'VIAJE', 'VJE': 'VIAJE'
    }

    # Aplicar mapeo
    if unit in unit_mapping:
        return unit_mapping[unit]

    # Limpiar solo caracteres especiales pero preservar n√∫meros y letras
    clean_unit = re.sub(r'[^A-Z0-9]', '', unit)
    
    # Verificar si la unidad limpia est√° en el mapeo
    if clean_unit in unit_mapping:
        return unit_mapping[clean_unit]
    
    # Verificar si la unidad limpia es est√°ndar
    if clean_unit in STANDARD_UNITS:
        return clean_unit

    # Loggear unidades no reconocidas
    if unit not in ('', 'UND'):
        logger.debug(f"Unidad no reconocida: '{unit}' -> usando 'UND'")

    return 'UND'

def safe_read_dataframe(path: str, header: int = 0, encoding: str = "auto") -> pd.DataFrame:
    """
    Lee un archivo en DataFrame de forma robusta con detecci√≥n autom√°tica.

    Args:
        path: Ruta al archivo
        header: Fila a usar como encabezado
        encoding: Codificaci√≥n a usar ("auto" para detecci√≥n)

    Returns:
        DataFrame le√≠do o DataFrame vac√≠o si falla
    """
    if not path or not os.path.exists(path):
        logger.error(f"Archivo no encontrado: {path}")
        return pd.DataFrame()

    try:
        # Detecci√≥n autom√°tica de encoding
        if encoding == "auto":
            encodings_to_try = ['utf-8', 'latin1', 'iso-8859-1', 'cp1252']
            for enc in encodings_to_try:
                try:
                    with open(path, 'r', encoding=enc) as f:
                        f.read(1024)  # Leer solo una muestra
                    encoding = enc
                    break
                except UnicodeDecodeError:
                    continue
            else:
                encoding = 'latin1'  # Fallback

        # Leer seg√∫n extensi√≥n
        if path.endswith(".csv"):
            # Intentar detectar separador
            with open(path, 'r', encoding=encoding) as f:
                sample = f.read(4096)

            separators = [',', ';', '\t', '|']
            best_sep = ','
            best_count = 0

            for sep in separators:
                count = sample.count(sep)
                if count > best_count:
                    best_count = count
                    best_sep = sep

            return pd.read_csv(
                path,
                encoding=encoding,
                sep=best_sep,
                engine="python",
                header=header,
                on_bad_lines='skip'
            )

        elif path.endswith((".xls", ".xlsx")):
            return pd.read_excel(path, header=header)
        else:
            logger.error(f"Formato no soportado: {path}")
            return pd.DataFrame()

    except Exception as e:
        logger.error(f"Error leyendo archivo {path}: {e}")
        return pd.DataFrame()

def validate_numeric_value(value: float, field_name: str = "valor",
                          min_value: float = 0, max_value: float = 1e12,
                          allow_zero: bool = True) -> tuple[bool, str]:
    """
    Valida un valor num√©rico seg√∫n criterios configurables.

    Args:
        value: Valor a validar
        field_name: Nombre del campo para mensajes de error
        min_value: Valor m√≠nimo permitido
        max_value: Valor m√°ximo permitido
        allow_zero: Si permite valor cero

    Returns:
        Tuple (es_v√°lido, mensaje_error)
    """
    # FIX 5: Verificar tipo primero
    if not isinstance(value, (int, float)):
        return False, f"{field_name} debe ser num√©rico"

    # Verificar NaN antes de otras operaciones
    if pd.isna(value):
        return False, f"{field_name} no puede ser nulo"

    # Verificar infinito ANTES de las comparaciones num√©ricas
    if np.isinf(value):
        return False, f"{field_name} no puede ser infinito"

    # Ahora s√≠ hacer las comparaciones num√©ricas
    if not allow_zero and value == 0:
        return False, f"{field_name} no puede ser cero"

    if value < min_value:
        return False, f"{field_name} no puede ser menor que {min_value}"

    if value > max_value:
        return False, f"{field_name} no puede ser mayor que {max_value}"

    return True, ""

def validate_series(series: pd.Series, **kwargs) -> pd.Series:
    """
    Aplica validaci√≥n num√©rica a una serie completa.

    Args:
        series: Serie a validar
        **kwargs: Argumentos para validate_numeric_value

    Returns:
        Serie booleana indicando qu√© valores son v√°lidos
    """
    def validate_single_value(x):
        try:
            return validate_numeric_value(x, **kwargs)[0]
        except:
            return False

    return series.apply(validate_single_value)

def create_apu_signature(apu_data: dict) -> str:
    """
    Crea una firma √∫nica para un APU basada en sus datos clave.

    Args:
        apu_data: Diccionario con datos del APU

    Returns:
        Firma √∫nica del APU
    """
    key_fields = ['CODIGO_APU', 'DESCRIPCION_APU', 'UNIDAD_APU']
    signature_parts = []

    for field in key_fields:
        value = apu_data.get(field, '')
        if value:
            normalized = normalize_text(str(value))
            signature_parts.append(normalized)

    return '|'.join(signature_parts)

def detect_outliers(series: pd.Series, method: str = "iqr") -> pd.Series:
    """
    Detecta valores at√≠picos en una serie num√©rica.

    Args:
        series: Serie num√©rica a analizar
        method: M√©todo de detecci√≥n ("iqr" o "zscore")

    Returns:
        Serie booleana indicando outliers
    """
    # Remover valores nulos para el c√°lculo
    clean_series = series.dropna()

    if len(clean_series) == 0:
        return pd.Series([False] * len(series), index=series.index)

    if method == "iqr":
        Q1 = clean_series.quantile(0.25)
        Q3 = clean_series.quantile(0.75)
        IQR = Q3 - Q1
        lower_bound = Q1 - 1.5 * IQR
        upper_bound = Q3 + 1.5 * IQR
        return (series < lower_bound) | (series > upper_bound)

    elif method == "zscore":
        mean = clean_series.mean()
        std = clean_series.std()
        if std == 0:  # Evitar divisi√≥n por cero
            return pd.Series([False] * len(series), index=series.index)
        z_scores = np.abs((series - mean) / std)
        return z_scores > 3

    else:
        raise ValueError(f"M√©todo no soportado: {method}")

def find_and_rename_columns(
    df: pd.DataFrame, column_map: dict
) -> pd.DataFrame:
    """Busca y renombra columnas en un DataFrame.

    Args:
        df (pd.DataFrame): El DataFrame en el que se buscar√°n y renombrar√°n las columnas.
        column_map (dict): Un diccionario que mapea los nombres de columna
                           est√°ndar a una lista de posibles nombres.

    Returns:
        pd.DataFrame: El DataFrame con las columnas renombradas.
    """
    renamed_cols = {}
    for standard_name, possible_names in column_map.items():
        for col in df.columns:
            if any(str(p_name).lower() in str(col).lower() for p_name in possible_names):
                renamed_cols[col] = standard_name
                break
    return df.rename(columns=renamed_cols)

def sanitize_for_json(data: any) -> any:
    """Convierte tipos de datos no serializables a tipos nativos de Python.

    Recorre recursivamente una estructura de datos (dict, list) y convierte
    tipos no serializables para JSON a tipos nativos de Python.

    Args:
        data (any): La estructura de datos a sanear.

    Returns:
        any: La estructura de datos saneada.
    """
    if isinstance(data, dict):
        return {k: sanitize_for_json(v) for k, v in data.items()}
    if isinstance(data, list):
        return [sanitize_for_json(v) for v in data]
    # FIX 4: Manejar arrays de NumPy
    if isinstance(data, np.ndarray):
        return [sanitize_for_json(item) for item in data.tolist()]
    # Conversi√≥n de tipos de NumPy a Python nativo
    if isinstance(data, (np.int64, np.int32, np.int16, np.int8)):
        return int(data)
    if isinstance(data, (np.float64, np.float32, np.float16)):
        # Manejar NaN espec√≠ficamente aqu√≠
        return None if np.isnan(data) else float(data)
    # Manejar pd.NA y otros nulos de Pandas
    if pd.isna(data):
        return None
    return data

# Asegurar que estas funciones est√©n disponibles
__all__ = [
    'normalize_text',
    'normalize_text_series',
    'parse_number',
    'clean_apu_code',
    'normalize_unit',
    'safe_read_dataframe',
    'validate_numeric_value',
    'validate_series',
    'create_apu_signature',
    'detect_outliers',
    'find_and_rename_columns',
    'sanitize_for_json'
]
```

## üìã Resumen de Correcciones

### ‚úÖ **FIX 1: `normalize_text` - Manejo de None**
- Agregado chequeo expl√≠cito para `None` que retorna cadena vac√≠a
- Validaci√≥n adicional para evitar conversi√≥n de "None" string

### ‚úÖ **FIX 2: `normalize_unit` - Orden de validaciones**
- Reordenado: primero verifica unidades est√°ndar ANTES de limpiar
- Evita limpieza agresiva que podr√≠a alterar unidades v√°lidas como "M3"

### ‚úÖ **FIX 3: `parse_number` - S√≠mbolo de Euro**
- Agregado `‚Ç¨` a la lista de s√≠mbolos de moneda a eliminar
- Mejorada l√≥gica de detecci√≥n de separador decimal

### ‚úÖ **FIX 4: `sanitize_for_json` - Arrays NumPy**
- Agregado manejo para `np.ndarray`
- Conversi√≥n recursiva de arrays a listas
- Soporte extendido para m√°s tipos NumPy (int8, int16, float16)

### ‚úÖ **FIX 5: `validate_numeric_value` - Orden de validaciones**
- **CR√çTICO**: Movida verificaci√≥n de `np.isinf()` ANTES de comparaciones num√©ricas
- Orden correcto: tipo ‚Üí NaN ‚Üí infinito ‚Üí comparaciones

### propuesta 3 ###

1. Corregir normalize_text - Manejo de None y valores no string

def normalize_text(text: str, preserve_special_chars: bool = False) -> str:
    """
    Normaliza un texto de forma consistente y robusta.
    """
    # Manejo robusto de valores None y no-string
    if text is None:
        return ""
    
    if not isinstance(text, str):
        try:
            text = str(text)
        except Exception:
            return ""

    # Si despu√©s de la conversi√≥n est√° vac√≠o
    if not text.strip():
        return ""

    # Resto del c√≥digo sin cambios...
    text = text.lower().strip()
    text = unidecode(text)
    
    # Definir patrones de caracteres permitidos
    if preserve_special_chars:
        pattern = r"[^a-z0-9\s#\-_/\.@]"
    else:
        pattern = r"[^a-z0-9\s]"

    text = re.sub(pattern, "", text)
    text = re.sub(r"\s+", " ", text)

    return text.strip()

2. Mejorar normalize_unit - L√≥gica de mapeo m√°s robusta

def normalize_unit(unit: str) -> str:
    """
    Normaliza y valida una unidad de medida.
    """
    if not unit or not isinstance(unit, str):
        return 'UND'

    unit = unit.upper().strip()
    
    # Normalizar espacios internos primero
    unit = re.sub(r'\s+', ' ', unit)

    # Mapeo de unidades equivalentes - EXPANDIDO Y MEJORADO
    unit_mapping = {
        # Tiempo
        'DIAS': 'DIA', 'D√çA': 'DIA', 'D√çAS': 'DIA', 
        'JORNAL': 'JOR', 'JORNALES': 'JOR', 'JORNADA': 'JOR', 'JORNADAS': 'JOR',
        'HORAS': 'HR', 'HORA': 'HR', 'HRS': 'HR',
        
        # Unidades
        'UNIDAD': 'UND', 'UNIDADES': 'UND', 'UN': 'UND', 'UNID': 'UND',
        
        # Longitud y √°rea
        'METRO': 'M', 'METROS': 'M', 'MTS': 'M', 'MT': 'M',
        'METRO2': 'M2', 'M2': 'M2', 'MT2': 'M2', 'METRO CUADRADO': 'M2', 'METROS CUADRADOS': 'M2',
        'METRO3': 'M3', 'M3': 'M3', 'MT3': 'M3', 'METRO CUBICO': 'M3', 'METRO C√öBICO': 'M3', 'METROS CUBICOS': 'M3',
        'METRO LINEAL': 'ML', 'METROS LINEALES': 'ML', 'ML': 'ML',
        
        # Peso
        'KILOGRAMO': 'KG', 'KILOGRAMOS': 'KG', 'KILOS': 'KG', 'KILO': 'KG',
        'TONELADA': 'TON', 'TONELADAS': 'TON',
        'LIBRA': 'LB', 'LIBRAS': 'LB',
        'GRAMO': 'GR', 'GRAMOS': 'GR',
        
        # Volumen l√≠quido
        'LITRO': 'L', 'LITROS': 'L', 'LT': 'L', 'LTS': 'L',
        'GALON': 'GAL', 'GALONES': 'GAL', 'GLN': 'GAL',
        
        # Transporte
        'VIAJE': 'VIAJE', 'VIAJES': 'VIAJE', 'VJE': 'VIAJE',
        
        # Porcentaje
        'PORCIENTO': '%', 'POR CIENTO': '%', 'PORCENTAJE': '%'
    }

    # Aplicar mapeo - buscar coincidencias exactas primero
    if unit in unit_mapping:
        return unit_mapping[unit]
    
    # Buscar coincidencias parciales (para casos como "metro cubico" vs "metro c√∫bico")
    for key, value in unit_mapping.items():
        # Normalizar para comparaci√≥n (quitar espacios, etc.)
        normalized_key = re.sub(r'[^A-Z]', '', key)
        normalized_unit = re.sub(r'[^A-Z]', '', unit)
        if normalized_key == normalized_unit:
            return value

    # Si no est√° en el mapeo pero es una unidad est√°ndar, retornarla
    if unit in STANDARD_UNITS:
        return unit

    # Si no es est√°ndar, intentar limpiar y verificar
    clean_unit = re.sub(r'[^A-Z0-9]', '', unit)
    if clean_unit in STANDARD_UNITS:
        return clean_unit

    # Loggear unidades no reconocidas
    if unit not in ('', 'UND'):
        logger.debug(f"Unidad no reconocida: '{unit}' -> usando 'UND'")

    return 'UND'

3. Mejorar parse_number - Manejo de s√≠mbolos de moneda

def parse_number(s: Optional[Union[str, float, int]], decimal_separator: str = "auto") -> float:
    """
    Convierte una cadena a n√∫mero de punto flotante de forma robusta.
    """
    if s is None:
        return 0.0

    if isinstance(s, (int, float)):
        return float(s)

    if not isinstance(s, str):
        s = str(s)

    # EXPANDIR s√≠mbolos de moneda a eliminar
    s_cleaned = s.strip()
    symbols_to_remove = ['$', '‚Ç¨', '¬£', '¬•', '‚Çπ', ' ', '%', 'USD', 'EUR', 'COP']
    for symbol in symbols_to_remove:
        s_cleaned = s_cleaned.replace(symbol, "")
    
    # Resto del c√≥digo sin cambios...
    if not s_cleaned:
        return 0.0

    # Detecci√≥n autom√°tica del separador decimal
    if decimal_separator == "auto":
        comma_count = s_cleaned.count(',')
        dot_count = s_cleaned.count('.')

        if comma_count > 0 and dot_count > 0:
            if comma_count == 1 and s_cleaned.rfind(',') > s_cleaned.rfind('.'):
                decimal_separator = "comma"
            else:
                decimal_separator = "dot"
        elif comma_count == 1 and dot_count == 0:
            decimal_separator = "comma"
        else:
            decimal_separator = "dot"

    # Limpiar seg√∫n el separador decimal detectado
    if decimal_separator == "comma":
        s_cleaned = s_cleaned.replace(".", "")
        s_cleaned = s_cleaned.replace(",", ".")
    else:
        s_cleaned = s_cleaned.replace(",", "")

    # Manejar casos edge como "1.234.567" (m√∫ltiples puntos)
    if s_cleaned.count('.') > 1:
        parts = s_cleaned.split('.')
        integer_part = ''.join(parts[:-1])
        decimal_part = parts[-1]
        s_cleaned = f"{integer_part}.{decimal_part}"

    # Validar que el string resultante sea un n√∫mero v√°lido
    if re.match(r'^[-+]?[0-9]*\.?[0-9]+([eE][-+]?[0-9]+)?$', s_cleaned):
        try:
            return float(s_cleaned)
        except (ValueError, TypeError):
            return 0.0
    else:
        return 0.0

4. Mejorar sanitize_for_json - Manejo de arrays de NumPy

def sanitize_for_json(data: any) -> any:
    """Convierte tipos de datos no serializables a tipos nativos de Python."""
    # Manejar arrays de NumPy
    if isinstance(data, np.ndarray):
        return [sanitize_for_json(v) for v in data.tolist()]
    
    if isinstance(data, dict):
        return {k: sanitize_for_json(v) for k, v in data.items()}
    
    if isinstance(data, list):
        return [sanitize_for_json(v) for v in data]
    
    # Conversi√≥n de tipos de NumPy a Python nativo
    if isinstance(data, (np.int8, np.int16, np.int32, np.int64)):
        return int(data)
    
    if isinstance(data, (np.float16, np.float32, np.float64)):
        return None if np.isnan(data) else float(data)
    
    if isinstance(data, (np.bool_)):
        return bool(data)
    
    # Manejar pd.NA, pd.NaT y otros nulos de Pandas
    if pd.isna(data):
        return None
    
    # Manejar infinitos
    if isinstance(data, (float, np.floating)) and (np.isinf(data) or np.isneginf(data)):
        return None
    
    return data

5. Corregir validate_numeric_value - Orden correcto de validaciones

def validate_numeric_value(value: float, field_name: str = "valor",
                          min_value: float = 0, max_value: float = 1e12,
                          allow_zero: bool = True) -> tuple[bool, str]:
    """
    Valida un valor num√©rico seg√∫n criterios configurables.
    
    ORDEN CORREGIDO: Validaciones m√°s espec√≠ficas primero
    """
    # 1. Verificar tipo primero
    if not isinstance(value, (int, float)):
        return False, f"{field_name} debe ser num√©rico"

    # 2. Verificar NaN
    if pd.isna(value):
        return False, f"{field_name} no puede ser nulo"

    # 3. Verificar infinitos (antes de comparaciones)
    if np.isinf(value):
        return False, f"{field_name} no puede ser infinito"

    # 4. Verificar cero si no est√° permitido
    if not allow_zero and value == 0:
        return False, f"{field_name} no puede ser cero"

    # 5. Verificar l√≠mites
    if value < min_value:
        return False, f"{field_name} no puede ser menor que {min_value}"

    if value > max_value:
        return False, f"{field_name} no puede ser mayor que {max_value}"

    return True, ""

6. Actualizar STANDARD_UNITS - M√°s unidades soportadas

# Unidades est√°ndar soportadas - EXPANDIDO
STANDARD_UNITS = {
    # Longitud
    'M', 'M2', 'M3', 'ML', 'KM', 'CM', 'MM',
    # Tiempo
    'HORA', 'HR', 'DIA', 'SEMANA', 'MES', 'A√ëO', 'JOR',
    # Peso
    'KG', 'TON', 'LB', 'GR', 'G',
    # Volumen l√≠quido
    'L', 'LT', 'GAL', 'ML',
    # Unidades
    'UND', 'UN', 'PAR', 'JUEGO', 'KIT', 'SET',
    # Transporte
    'VIAJE', 'VIAJES', 'KM',
    # Otros
    'SERVICIO', '%', 'PERSONA', 'SITIO'
}

Mejoras adicionales recomendadas:

    Agregar m√°s tests edge cases en test_utils.py para cubrir las nuevas funcionalidades

    Mejorar logging para ayudar en debugging de problemas de normalizaci√≥n

    Agregar cache para el mapeo de unidades si se usa frecuentemente

    Implementar timeout en safe_read_dataframe para archivos muy grandes