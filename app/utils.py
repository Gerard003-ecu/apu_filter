"""
M√≥dulo de utilidades para procesamiento de datos de APU.

Este m√≥dulo proporciona funciones robustas para normalizaci√≥n de texto,
conversi√≥n de n√∫meros, validaci√≥n de datos y manejo de archivos.
"""

import logging
import re
from functools import lru_cache
from pathlib import Path
from typing import Any, Dict, List, Optional, Tuple, Union
from decimal import Decimal, InvalidOperation

import numpy as np
import pandas as pd
from unidecode import unidecode

# Configuraci√≥n del logger
logger = logging.getLogger(__name__)

# ============================================================================
# CONSTANTES Y CONFIGURACI√ìN
# ============================================================================

# Unidades est√°ndar soportadas (como frozenset para mejor rendimiento)
STANDARD_UNITS = frozenset({
    # Longitud
    'M', 'M2', 'M3', 'ML', 'KM', 'CM', 'MM',
    # Tiempo
    'HORA', 'HR', 'DIA', 'SEMANA', 'MES', 'A√ëO', 'JOR',
    # Peso
    'KG', 'TON', 'LB', 'GR',
    # Volumen l√≠quido
    'L', 'LT', 'GAL', 'ML',
    # Unidades
    'UND', 'UN', 'PAR', 'JUEGO', 'KIT',
    # Transporte
    'VIAJE', 'VIAJES', 'KM',
    # Otros
    'SERVICIO', '%'
})

# Mapeo de unidades equivalentes (optimizado como dict constante)
UNIT_MAPPING = {
    'DIAS': 'DIA', 'D√çAS': 'DIA', 'JORNAL': 'JOR', 'JORNALES': 'JOR',
    'HORAS': 'HR', 'HORA': 'HR', 'UNIDAD': 'UND', 'UNIDADES': 'UND',
    'UN': 'UND', 'METRO': 'M', 'METROS': 'M', 'MTS': 'M',
    'METRO2': 'M2', 'M2': 'M2', 'MT2': 'M2', 'METRO CUADRADO': 'M2',
    'METRO3': 'M3', 'M3': 'M3', 'MT3': 'M3', 'METRO CUBICO': 'M3',
    'KILOGRAMO': 'KG', 'KILOGRAMOS': 'KG', 'KILOS': 'KG',
    'TONELADA': 'TON', 'TONELADAS': 'TON',
    'GALON': 'GAL', 'GALONES': 'GAL', 'GLN': 'GAL',
    'LITRO': 'L', 'LITROS': 'L', 'LT': 'L',
    'VIAJES': 'VIAJE', 'VJE': 'VIAJE'
}

# Configuraciones por defecto
DEFAULT_ENCODING_ATTEMPTS = ['utf-8', 'latin1', 'iso-8859-1', 'cp1252']
DEFAULT_CSV_SEPARATORS = [',', ';', '\t', '|']
NUMERIC_VALIDATION_LIMITS = {
    'min': 0,
    'max': 1e12,
    'allow_zero': True
}

# Patrones regex compilados (m√°s eficiente)
NUMERIC_PATTERN = re.compile(r'^[-+]?[0-9]*\.?[0-9]+([eE][-+]?[0-9]+)?$')
APU_INVALID_CHARS_PATTERN = re.compile(r'[^\w.\-]')
WHITESPACE_PATTERN = re.compile(r'\s+')

# ============================================================================
# FUNCIONES DE NORMALIZACI√ìN DE TEXTO
# ============================================================================

@lru_cache(maxsize=1024)
def normalize_text(text: str, preserve_special_chars: bool = False) -> str:
    """
    Normaliza un texto de forma consistente y robusta con cache.

    Args:
        text: Texto a normalizar
        preserve_special_chars: Si True, preserva algunos caracteres especiales √∫tiles

    Returns:
        Texto normalizado

    Raises:
        TypeError: Si text no puede convertirse a string
    """
    if not isinstance(text, str):
        try:
            text = str(text)
        except Exception as e:
            raise TypeError(f"No se puede convertir a string: {e}")

    # Validaci√≥n de entrada vac√≠a
    if not text:
        return ""

    # Convertir a min√∫sculas y remover espacios extra
    text = text.lower().strip()

    # Remover acentos y caracteres especiales
    text = unidecode(text)

    # Definir patrones de caracteres permitidos
    if preserve_special_chars:
        # Preservar caracteres √∫tiles para descripciones t√©cnicas
        pattern = r"[^a-z0-9\s#\-_/\.@]"
    else:
        # Solo caracteres b√°sicos para comparaciones
        pattern = r"[^a-z0-9\s]"

    # Remover caracteres no permitidos
    text = re.sub(pattern, "", text)

    # Normalizar espacios (m√∫ltiples espacios a uno solo)
    text = WHITESPACE_PATTERN.sub(" ", text)

    return text.strip()


def normalize_text_series(
    text_series: pd.Series,
    preserve_special_chars: bool = False,
    chunk_size: int = 10000
) -> pd.Series:
    """
    Normaliza una serie de texto de forma vectorizada y eficiente.
    
    Incluye procesamiento por chunks para series grandes.

    Args:
        text_series: Serie de pandas con texto a normalizar
        preserve_special_chars: Si True, preserva algunos caracteres especiales √∫tiles
        chunk_size: Tama√±o del chunk para procesamiento de series grandes

    Returns:
        Serie de texto normalizada
    """
    if text_series is None or text_series.empty:
        return text_series

    # Asegurar que todos los elementos sean strings
    text_series = text_series.astype(str)

    # Para series grandes, procesar por chunks
    if len(text_series) > chunk_size:
        result_chunks = []
        for i in range(0, len(text_series), chunk_size):
            chunk = text_series.iloc[i:i+chunk_size]
            normalized_chunk = chunk.apply(
                lambda x: _safe_normalize(x, preserve_special_chars)
            )
            result_chunks.append(normalized_chunk)
        return pd.concat(result_chunks)

    # Para series peque√±as, procesar directamente
    return text_series.apply(lambda x: _safe_normalize(x, preserve_special_chars))


def _safe_normalize(text: str, preserve_special_chars: bool) -> str:
    """Funci√≥n auxiliar para normalizaci√≥n segura."""
    try:
        return normalize_text(text, preserve_special_chars)
    except Exception as e:
        logger.warning(f"Error normalizando texto '{text}': {e}")
        return str(text)

# ============================================================================
# FUNCIONES DE CONVERSI√ìN NUM√âRICA
# ============================================================================

def parse_number(
    s: Optional[Union[str, float, int]],
    default_value: float = 0.0,
    decimal_separator: Optional[str] = None,
    strict: bool = False,
    allow_percentage: bool = True,
    allow_scientific: bool = True,
    debug: bool = False
) -> float:
    """
    Convierte una cadena a n√∫mero de punto flotante de forma robusta.

    Maneja diferentes formatos num√©ricos incluyendo:
    - Separadores de miles y decimales (punto o coma)
    - N√∫meros negativos y positivos
    - Porcentajes (opcional)
    - Notaci√≥n cient√≠fica (opcional)
    - Espacios en blanco
    - Caracteres no num√©ricos comunes

    Args:
        s: Valor a convertir (string, float, int o None)
        default_value: Valor por defecto si la conversi√≥n falla
        decimal_separator: Forzar separador decimal espec√≠fico ('dot', 'comma', None=auto)
        strict: Si True, falla con excepciones en lugar de retornar default
        allow_percentage: Si True, maneja valores como "15%" -> 0.15
        allow_scientific: Si True, maneja notaci√≥n cient√≠fica como "1.5e-3"
        debug: Si True, registra informaci√≥n de depuraci√≥n

    Returns:
        float: N√∫mero parseado o default_value si falla

    Raises:
        ValueError: Solo si strict=True y la conversi√≥n falla

    Examples:
        >>> parse_number("1,234.56")
        1234.56
        >>> parse_number("1.234,56", decimal_separator="comma")
        1234.56
        >>> parse_number("‚Ç¨ 1,500.00")
        1500.0
        >>> parse_number("15%", allow_percentage=True)
        0.15
        >>> parse_number("1.5e-3", allow_scientific=True)
        0.0015
        >>> parse_number("N/A", default_value=-1)
        -1.0
    """

    # ============================================================
    # 1. VALIDACI√ìN INICIAL Y CASOS R√ÅPIDOS
    # ============================================================

    # Caso None
    if s is None:
        if debug:
            logger.debug(f"parse_number: Input is None, returning default {default_value}")
        return default_value

    # Casos num√©ricos directos (int, float)
    if isinstance(s, (int, float)):
        result = float(s)
        if debug:
            logger.debug(f"parse_number: Direct numeric conversion: {s} -> {result}")
        return result

    # Convertir a string si no lo es
    if not isinstance(s, str):
        s = str(s)

    # Cache para strings comunes
    cached_result = _get_cached_parse(s, decimal_separator)
    if cached_result is not None:
        if debug:
            logger.debug(f"parse_number: Cache hit for '{s}' -> {cached_result}")
        return cached_result

    # Guardar string original para logging
    original_s = s

    # ============================================================
    # 2. LIMPIEZA INICIAL Y VALIDACIONES
    # ============================================================

    # Eliminar espacios en blanco al inicio y final
    s = s.strip()

    # Verificar si est√° vac√≠o despu√©s de strip
    if not s:
        if debug:
            logger.debug(f"parse_number: Empty string after strip, returning default {default_value}")
        return _handle_parse_error(original_s, "empty string", default_value, strict)

    # Verificar casos especiales comunes que no son n√∫meros
    if _is_non_numeric_text(s):
        if debug:
            logger.debug(f"parse_number: Non-numeric text detected: '{s}'")
        return _handle_parse_error(original_s, "non-numeric text", default_value, strict)

    # ============================================================
    # 3. MANEJO DE PORCENTAJES
    # ============================================================

    if allow_percentage and '%' in s:
        result = _parse_percentage(s, default_value, strict, debug)
        if result is not None:
            _cache_parse_result(original_s, decimal_separator, result)
            return result

    # ============================================================
    # 4. MANEJO DE NOTACI√ìN CIENT√çFICA
    # ============================================================

    if allow_scientific and ('e' in s.lower() or 'E' in s):
        result = _parse_scientific(s, default_value, strict, debug)
        if result is not None:
            _cache_parse_result(original_s, decimal_separator, result)
            return result

    # ============================================================
    # 5. LIMPIEZA DE CARACTERES NO NUM√âRICOS
    # ============================================================

    # Extraer signo si existe
    sign = 1.0
    s_work = s

    # Manejar m√∫ltiples signos al inicio
    sign_match = re.match(r'^([+-]+)', s_work)
    if sign_match:
        signs = sign_match.group(1)
        # Contar signos negativos
        neg_count = signs.count('-')
        sign = -1.0 if neg_count % 2 == 1 else 1.0
        s_work = s_work[len(signs):]
        if debug:
            logger.debug(f"parse_number: Extracted sign: {sign} from '{signs}'")

    # Eliminar s√≠mbolos de moneda comunes y otros caracteres no num√©ricos
    # pero preservar puntos, comas, espacios (pueden ser separadores)
    s_cleaned = re.sub(r'[^\d,.\s-]', '', s_work)

    # Eliminar espacios que pueden ser separadores de miles
    # pero solo si est√°n entre d√≠gitos
    s_cleaned = re.sub(r'(?<=\d)\s+(?=\d)', '', s_cleaned)
    s_cleaned = s_cleaned.strip()

    if not s_cleaned or not re.search(r'\d', s_cleaned):
        if debug:
            logger.debug(f"parse_number: No digits found after cleaning: '{s}' -> '{s_cleaned}'")
        return _handle_parse_error(original_s, "no digits found", default_value, strict)

    # ============================================================
    # 6. DETECCI√ìN INTELIGENTE DE SEPARADORES
    # ============================================================

    if decimal_separator:
        # Usar separador especificado por el usuario
        s_standard = _apply_separator_format(s_cleaned, decimal_separator, debug)
    else:
        # Detecci√≥n autom√°tica mejorada
        s_standard = _auto_detect_and_convert_separators(s_cleaned, debug)

    # ============================================================
    # 7. VALIDACI√ìN Y CONVERSI√ìN FINAL
    # ============================================================

    # Validar formato antes de intentar conversi√≥n
    if not _is_valid_number_format(s_standard):
        if debug:
            logger.debug(f"parse_number: Invalid format after standardization: '{s_standard}'")
        return _handle_parse_error(original_s, f"invalid format: {s_standard}", default_value, strict)

    # Intentar conversi√≥n con manejo de errores robusto
    try:
        # Usar Decimal para mayor precisi√≥n en la conversi√≥n intermedia
        if '.' in s_standard and len(s_standard.split('.')[1]) > 15:
            # Para n√∫meros con muchos decimales, usar Decimal
            result = float(Decimal(s_standard)) * sign
        else:
            # Conversi√≥n directa para n√∫meros normales
            result = float(s_standard) * sign

        # Validar resultado
        if not _is_finite(result):
            if debug:
                logger.debug(f"parse_number: Non-finite result: {result}")
            return _handle_parse_error(original_s, f"non-finite result: {result}", default_value, strict)

        # Cache el resultado exitoso
        _cache_parse_result(original_s, decimal_separator, result)

        if debug:
            logger.debug(f"parse_number: Success: '{original_s}' -> {result}")

        return result

    except (ValueError, TypeError, InvalidOperation) as e:
        if debug:
            logger.debug(f"parse_number: Conversion error for '{s_standard}': {e}")
        return _handle_parse_error(original_s, str(e), default_value, strict)


# ============================================================
# FUNCIONES AUXILIARES ROBUSTAS
# ============================================================

@lru_cache(maxsize=1024)
def _get_cached_parse(s: str, decimal_separator: Optional[str]) -> Optional[float]:
    """
    Busca en cache valores previamente parseados.
    Cache key incluye el string y el separador para evitar colisiones.
    """
    # Solo cachear strings comunes y cortos
    if len(s) > 50:
        return None

    # Valores comunes pre-computados
    common_values = {
        "0": 0.0,
        "1": 1.0,
        "-1": -1.0,
        "0.0": 0.0,
        "1.0": 1.0,
        "0,0": 0.0,
        "1,0": 1.0,
    }

    return common_values.get(s)


def _cache_parse_result(s: str, decimal_separator: Optional[str], result: float):
    """Almacena resultado en cache si es apropiado."""
    # Solo cachear strings cortos para evitar uso excesivo de memoria
    if len(s) <= 50:
        # El LRU cache de _get_cached_parse manejar√° esto autom√°ticamente
        pass


def _is_non_numeric_text(s: str) -> bool:
    """
    Detecta texto que claramente no es un n√∫mero.
    """
    non_numeric_indicators = [
        r'^N[/\\]?A$',           # N/A, NA
        r'^-+$',                 # Solo guiones
        r'^\?+$',                # Solo signos de interrogaci√≥n
        r'^TBD$',                # To Be Determined
        r'^NULL$',               # NULL
        r'^NONE$',               # None
        r'^NAN$',                # NaN
        r'^#.*',                 # Errores de Excel (#DIV/0!, #VALUE!, etc)
    ]

    s_upper = s.upper().strip()
    return any(re.match(pattern, s_upper) for pattern in non_numeric_indicators)


def _parse_percentage(s: str, default_value: float, strict: bool, debug: bool) -> Optional[float]:
    """
    Parsea un valor de porcentaje (e.g., "15%" -> 0.15).
    """
    try:
        # Eliminar el s√≠mbolo de porcentaje y espacios
        s_percent = s.replace('%', '').strip()

        if not s_percent:
            return None

        # Parsear recursivamente sin el %
        base_value = parse_number(
            s_percent,
            default_value=None,  # Usar None para detectar fallo
            allow_percentage=False,  # Evitar recursi√≥n infinita
            debug=debug
        )

        if base_value is None:
            return None

        # Convertir a decimal (dividir por 100)
        result = base_value / 100.0

        if debug:
            logger.debug(f"parse_percentage: '{s}' -> {base_value} -> {result}")

        return result

    except Exception as e:
        if debug:
            logger.debug(f"parse_percentage: Error parsing '{s}': {e}")
        return None


def _parse_scientific(s: str, default_value: float, strict: bool, debug: bool) -> Optional[float]:
    """
    Parsea notaci√≥n cient√≠fica (e.g., "1.5e-3" -> 0.0015).
    """
    try:
        # Limpiar espacios alrededor de 'e' o 'E'
        s_sci = re.sub(r'\s*([eE])\s*', r'\1', s.strip())

        # Validar formato b√°sico de notaci√≥n cient√≠fica
        if not re.match(r'^[+-]?\d+\.?\d*[eE][+-]?\d+$', s_sci):
            return None

        result = float(s_sci)

        if not _is_finite(result):
            return None

        if debug:
            logger.debug(f"parse_scientific: '{s}' -> {result}")

        return result

    except (ValueError, OverflowError) as e:
        if debug:
            logger.debug(f"parse_scientific: Error parsing '{s}': {e}")
        return None


def _apply_separator_format(s: str, separator_format: str, debug: bool) -> str:
    """
    Aplica formato de separador especificado por el usuario.
    """
    if separator_format == "comma":
        # Coma es decimal, punto es miles
        s_standard = s.replace('.', '').replace(',', '.')
        if debug:
            logger.debug(f"apply_separator: Comma format: '{s}' -> '{s_standard}'")
    elif separator_format == "dot":
        # Punto es decimal, coma es miles
        s_standard = s.replace(',', '')
        if debug:
            logger.debug(f"apply_separator: Dot format: '{s}' -> '{s_standard}'")
    else:
        # Formato no reconocido, usar auto-detecci√≥n
        s_standard = _auto_detect_and_convert_separators(s, debug)

    return s_standard


def _auto_detect_and_convert_separators(s: str, debug: bool) -> str:
    """
    Detecci√≥n autom√°tica mejorada de separadores decimales y de miles.
    """
    # Contar ocurrencias
    comma_count = s.count(',')
    dot_count = s.count('.')

    # Casos simples sin ambig√ºedad
    if comma_count == 0 and dot_count == 0:
        # No hay separadores
        return s

    if comma_count == 0:
        # Solo puntos, asumir que el punto es decimal
        return s

    if dot_count == 0:
        # Solo comas
        if comma_count == 1:
            # Una sola coma, probablemente decimal
            return s.replace(',', '.')
        else:
            # M√∫ltiples comas, probablemente miles
            return s.replace(',', '')

    # Ambos separadores presentes - an√°lisis m√°s detallado
    last_comma = s.rfind(',')
    last_dot = s.rfind('.')

    # El √∫ltimo separador suele ser el decimal
    if last_comma > last_dot:
        # Coma es decimal
        # Verificar consistencia: despu√©s de la coma decimal debe haber 1-4 d√≠gitos t√≠picamente
        after_comma = s[last_comma + 1:]
        if after_comma and after_comma.isdigit() and len(after_comma) <= 4:
            s_standard = s.replace('.', '').replace(',', '.')
            if debug:
                logger.debug(f"auto_detect: Comma as decimal: '{s}' -> '{s_standard}'")
            return s_standard

    # Punto es decimal (caso m√°s com√∫n)
    # Verificar consistencia: despu√©s del punto decimal debe haber d√≠gitos
    after_dot = s[last_dot + 1:]
    if after_dot and after_dot.isdigit():
        s_standard = s.replace(',', '')
        if debug:
            logger.debug(f"auto_detect: Dot as decimal: '{s}' -> '{s_standard}'")
        return s_standard

    # Caso ambiguo, usar heur√≠stica adicional
    return _resolve_ambiguous_separators(s, debug)


def _resolve_ambiguous_separators(s: str, debug: bool) -> str:
    """
    Resuelve casos ambiguos usando heur√≠sticas adicionales.
    """
    # Analizar patrones de agrupamiento
    # En formato de miles, los grupos son t√≠picamente de 3 d√≠gitos

    # Buscar patrones como 1.234.567 o 1,234,567
    thousand_dot_pattern = re.match(r'^\d{1,3}(?:\.\d{3})+(?:,\d+)?$', s)
    thousand_comma_pattern = re.match(r'^\d{1,3}(?:,\d{3})+(?:\.\d+)?$', s)

    if thousand_comma_pattern:
        # Formato americano: 1,234,567.89
        result = s.replace(',', '')
        if debug:
            logger.debug(f"resolve_ambiguous: American format detected: '{s}' -> '{result}'")
        return result

    if thousand_dot_pattern:
        # Formato europeo: 1.234.567,89
        result = s.replace('.', '').replace(',', '.')
        if debug:
            logger.debug(f"resolve_ambiguous: European format detected: '{s}' -> '{result}'")
        return result

    # Si no hay patr√≥n claro, usar la posici√≥n del √∫ltimo separador
    last_comma = s.rfind(',')
    last_dot = s.rfind('.')

    if last_comma > last_dot:
        return s.replace('.', '').replace(',', '.')
    else:
        return s.replace(',', '')


def _is_valid_number_format(s: str) -> bool:
    """
    Valida que la string tenga un formato num√©rico v√°lido despu√©s de la estandarizaci√≥n.
    """
    # Debe tener al menos un d√≠gito
    if not re.search(r'\d', s):
        return False

    # No debe tener m√∫ltiples puntos decimales
    if s.count('.') > 1:
        return False

    # Validar formato general
    # Permitir: d√≠gitos, m√°ximo un punto decimal, signo opcional al inicio
    pattern = r'^-?\d+\.?\d*$'
    return bool(re.match(pattern, s))


def _is_finite(value: float) -> bool:
    """
    Verifica que el valor sea finito (no inf, -inf o nan).
    """
    import math
    return math.isfinite(value)


def _handle_parse_error(
    original: str,
    error: str,
    default_value: float,
    strict: bool
) -> float:
    """
    Maneja errores de parsing de forma consistente.
    """
    error_msg = f"Failed to parse '{original}': {error}"

    if strict:
        raise ValueError(error_msg)
    else:
        logger.debug(f"parse_number: {error_msg}, returning default {default_value}")
        return default_value

# ============================================================================
# FUNCIONES DE VALIDACI√ìN Y LIMPIEZA DE C√ìDIGOS APU
# ============================================================================

@lru_cache(maxsize=512)
def clean_apu_code(code: str, validate_format: bool = True) -> str:
    """
    Limpia y valida un c√≥digo de APU de forma robusta con cache.

    Args:
        code: C√≥digo de APU a limpiar
        validate_format: Si True, valida el formato b√°sico

    Returns:
        C√≥digo de APU limpio y validado

    Raises:
        ValueError: Si el c√≥digo es inv√°lido y validate_format=True
        TypeError: Si code no puede convertirse a string
    """
    if not isinstance(code, str):
        try:
            code = str(code)
        except Exception as e:
            raise TypeError(f"No se puede convertir c√≥digo APU a string: {e}")

    original_code = code
    code = code.strip().upper()

    # Remover caracteres no permitidos (mantener letras, n√∫meros, puntos, guiones)
    code = code.replace(',', '.')
    code = APU_INVALID_CHARS_PATTERN.sub('', code)

    # Remover puntos y guiones al final
    code = code.rstrip('.-')

    # Validaciones opcionales de formato
    if validate_format:
        if not code:
            raise ValueError(f"C√≥digo APU no puede estar vac√≠o: '{original_code}'")

        if len(code) < 2:
            raise ValueError(f"C√≥digo APU demasiado corto: '{original_code}'")

        # Verificar que tenga al menos un n√∫mero
        if not any(char.isdigit() for char in code):
            logger.warning(f"C√≥digo APU sin n√∫meros: '{original_code}' -> '{code}'")

    return code

# ============================================================================
# FUNCIONES DE NORMALIZACI√ìN DE UNIDADES
# ============================================================================

@lru_cache(maxsize=256)
def normalize_unit(unit: str) -> str:
    """
    Normaliza y valida una unidad de medida con cache.

    Args:
        unit: Unidad a normalizar

    Returns:
        Unidad normalizada o 'UND' si no es v√°lida
    """
    if not unit or not isinstance(unit, str):
        return 'UND'

    unit = unit.upper().strip()

    # Verificar en mapeo primero (m√°s com√∫n)
    if unit in UNIT_MAPPING:
        return UNIT_MAPPING[unit]

    # Si es una unidad est√°ndar, retornarla
    if unit in STANDARD_UNITS:
        return unit

    # Intentar limpiar y verificar
    clean_unit = re.sub(r'[^A-Z0-9]', '', unit)
    if clean_unit in STANDARD_UNITS:
        return clean_unit

    # Log solo para unidades no triviales
    if unit not in ('', 'UND') and len(unit) > 1:
        logger.debug(f"Unidad no reconocida: '{unit}' -> usando '{unit}'")

    return unit

# ============================================================================
# FUNCIONES DE LECTURA DE ARCHIVOS
# ============================================================================

def safe_read_dataframe(
    path: Union[str, Path],
    header: int = 0,
    encoding: str = "auto",
    nrows: Optional[int] = None,
    usecols: Optional[List[str]] = None
) -> pd.DataFrame:
    """
    Lee un archivo en DataFrame de forma robusta con detecci√≥n autom√°tica.

    Args:
        path: Ruta al archivo
        header: Fila a usar como encabezado
        encoding: Codificaci√≥n a usar ("auto" para detecci√≥n)
        nrows: N√∫mero de filas a leer (None para todas)
        usecols: Lista de columnas a leer (None para todas)

    Returns:
        DataFrame le√≠do o DataFrame vac√≠o si falla
    """
    path = Path(path) if not isinstance(path, Path) else path

    if not path.exists():
        logger.error(f"Archivo no encontrado: {path}")
        return pd.DataFrame()

    try:
        # Detecci√≥n autom√°tica de encoding
        if encoding == "auto":
            encoding = _detect_file_encoding(path)

        # Leer seg√∫n extensi√≥n
        file_extension = path.suffix.lower()

        if file_extension == ".csv":
            return _read_csv_robust(path, encoding, header, nrows, usecols)
        elif file_extension in [".xls", ".xlsx"]:
            return _read_excel_robust(path, header, nrows, usecols)
        else:
            logger.error(f"Formato no soportado: {file_extension}")
            return pd.DataFrame()

    except Exception as e:
        logger.error(f"Error leyendo archivo {path}: {e}")
        return pd.DataFrame()


def _detect_file_encoding(path: Path) -> str:
    """Detecta la codificaci√≥n de un archivo de texto."""
    for enc in DEFAULT_ENCODING_ATTEMPTS:
        try:
            with open(path, 'r', encoding=enc) as f:
                f.read(1024)  # Leer solo una muestra
            return enc
        except UnicodeDecodeError:
            continue
    return 'latin1'  # Fallback


def _read_csv_robust(
    path: Path,
    encoding: str,
    header: int,
    nrows: Optional[int],
    usecols: Optional[List[str]]
) -> pd.DataFrame:
    """Lee un archivo CSV de forma robusta."""
    # Detectar separador
    separator = _detect_csv_separator(path, encoding)

    # Configurar par√°metros de lectura
    read_params = {
        'filepath_or_buffer': path,
        'encoding': encoding,
        'sep': separator,
        'engine': 'python',
        'header': header,
        'on_bad_lines': 'skip'
    }

    if nrows is not None:
        read_params['nrows'] = nrows
    if usecols is not None:
        read_params['usecols'] = usecols

    return pd.read_csv(**read_params)


def _read_excel_robust(
    path: Path,
    header: int,
    nrows: Optional[int],
    usecols: Optional[List[str]]
) -> pd.DataFrame:
    """Lee un archivo Excel de forma robusta."""
    read_params = {
        'io': path,
        'header': header
    }

    if nrows is not None:
        read_params['nrows'] = nrows
    if usecols is not None:
        read_params['usecols'] = usecols

    return pd.read_excel(**read_params)


def _detect_csv_separator(path: Path, encoding: str) -> str:
    """Detecta el separador de un archivo CSV."""
    try:
        with open(path, 'r', encoding=encoding) as f:
            sample = f.read(4096)

        best_sep = ','
        best_count = 0

        for sep in DEFAULT_CSV_SEPARATORS:
            # Contar ocurrencias considerando saltos de l√≠nea
            lines = sample.split('\n')[:5]  # Primeras 5 l√≠neas
            if len(lines) > 1:
                counts = [line.count(sep) for line in lines if line]
                if counts and min(counts) > 0:
                    avg_count = sum(counts) / len(counts)
                    if avg_count > best_count:
                        best_count = avg_count
                        best_sep = sep

        return best_sep
    except Exception:
        return ','

# ============================================================================
# FUNCIONES DE VALIDACI√ìN
# ============================================================================

def validate_numeric_value(
    value: float,
    field_name: str = "valor",
    min_value: float = None,
    max_value: float = None,
    allow_zero: bool = True,
    allow_negative: bool = False,
    allow_inf: bool = False
) -> Tuple[bool, str]:
    """
    Valida un valor num√©rico seg√∫n criterios configurables.

    Args:
        value: Valor a validar
        field_name: Nombre del campo para mensajes de error
        min_value: Valor m√≠nimo permitido (None para sin l√≠mite)
        max_value: Valor m√°ximo permitido (None para sin l√≠mite)
        allow_zero: Si permite valor cero
        allow_negative: Si permite valores negativos
        allow_inf: Si permite valores infinitos

    Returns:
        Tuple (es_v√°lido, mensaje_error)
    """
    # Validar tipo
    if not isinstance(value, (int, float, np.integer, np.floating)):
        return False, f"{field_name} debe ser num√©rico"

    # Validar nulos
    if pd.isna(value):
        return False, f"{field_name} no puede ser nulo"

    # Validar infinitos
    if np.isinf(value):
        if not allow_inf:
            return False, f"{field_name} no puede ser infinito"
        return True, ""

    # Validar cero
    if not allow_zero and value == 0:
        return False, f"{field_name} no puede ser cero"

    # Validar negativos
    if not allow_negative and value < 0:
        return False, f"{field_name} no puede ser negativo"

    # Validar rango m√≠nimo
    if min_value is not None and value < min_value:
        return False, f"{field_name} no puede ser menor que {min_value}"

    # Validar rango m√°ximo
    if max_value is not None and value > max_value:
        return False, f"{field_name} no puede ser mayor que {max_value}"

    return True, ""


def validate_series(
    series: pd.Series,
    return_mask: bool = True,
    **kwargs
) -> Union[pd.Series, pd.DataFrame]:
    """
    Aplica validaci√≥n num√©rica a una serie completa.

    Args:
        series: Serie a validar
        return_mask: Si True, retorna m√°scara booleana. Si False, retorna DataFrame con detalles
        **kwargs: Argumentos para validate_numeric_value

    Returns:
        Serie booleana o DataFrame con validaci√≥n y mensajes
    """
    if series.empty:
        return series if return_mask else pd.DataFrame()

    if return_mask:
        # Retornar solo m√°scara booleana
        return series.apply(
            lambda x: validate_numeric_value(x, **kwargs)[0]
        )
    else:
        # Retornar DataFrame con detalles
        validation_results = series.apply(
            lambda x: validate_numeric_value(x, **kwargs)
        )

        return pd.DataFrame({
            'value': series,
            'is_valid': validation_results.apply(lambda x: x[0]),
            'error_message': validation_results.apply(lambda x: x[1])
        })

# ============================================================================
# FUNCIONES DE AN√ÅLISIS Y DETECCI√ìN
# ============================================================================

def create_apu_signature(
    apu_data: Dict[str, Any],
    key_fields: Optional[List[str]] = None
) -> str:
    """
    Crea una firma √∫nica para un APU basada en sus datos clave.

    Args:
        apu_data: Diccionario con datos del APU
        key_fields: Campos a usar para la firma (None para usar default)

    Returns:
        Firma √∫nica del APU
    """
    if key_fields is None:
        key_fields = ['CODIGO_APU', 'DESCRIPCION_APU', 'UNIDAD_APU']

    signature_parts = []

    for field in key_fields:
        value = apu_data.get(field, '')
        if value:
            # Normalizar el valor para la firma
            if isinstance(value, (int, float)):
                normalized = str(value)
            else:
                normalized = normalize_text(str(value))

            if normalized:  # Solo a√±adir si no est√° vac√≠o
                signature_parts.append(normalized)

    return '|'.join(signature_parts) if signature_parts else 'empty_signature'


def detect_outliers(
    series: pd.Series,
    method: str = "iqr",
    threshold: float = 1.5,
    return_bounds: bool = False
) -> Union[pd.Series, Tuple[pd.Series, Dict[str, float]]]:
    """
    Detecta valores at√≠picos en una serie num√©rica con m√©todos configurables.

    Args:
        series: Serie num√©rica a analizar
        method: M√©todo de detecci√≥n ("iqr", "zscore", "modified_zscore")
        threshold: Umbral para detecci√≥n (1.5 para IQR, 3 para z-score)
        return_bounds: Si True, retorna tambi√©n los l√≠mites utilizados

    Returns:
        Serie booleana indicando outliers, opcionalmente con l√≠mites

    Raises:
        ValueError: Si el m√©todo no es soportado
    """
    # Validar entrada
    if series.empty:
        result = pd.Series(dtype=bool)
        bounds = {}
        return (result, bounds) if return_bounds else result

    # Remover valores nulos para el c√°lculo
    clean_series = series.dropna()

    if len(clean_series) == 0:
        result = pd.Series([False] * len(series), index=series.index)
        bounds = {}
        return (result, bounds) if return_bounds else result

    outliers = None
    bounds = {}

    if method == "iqr":
        outliers, bounds = _detect_outliers_iqr(series, clean_series, threshold)
    elif method == "zscore":
        outliers, bounds = _detect_outliers_zscore(series, clean_series, threshold)
    elif method == "modified_zscore":
        outliers, bounds = _detect_outliers_modified_zscore(series, clean_series, threshold)
    else:
        raise ValueError(f"M√©todo no soportado: {method}")

    return (outliers, bounds) if return_bounds else outliers


def _detect_outliers_iqr(
    series: pd.Series,
    clean_series: pd.Series,
    threshold: float
) -> Tuple[pd.Series, Dict[str, float]]:
    """Detecci√≥n de outliers usando IQR."""
    Q1 = clean_series.quantile(0.25)
    Q3 = clean_series.quantile(0.75)
    IQR = Q3 - Q1

    lower_bound = Q1 - threshold * IQR
    upper_bound = Q3 + threshold * IQR

    outliers = (series < lower_bound) | (series > upper_bound)
    bounds = {
        'Q1': Q1, 'Q3': Q3, 'IQR': IQR,
        'lower_bound': lower_bound,
        'upper_bound': upper_bound
    }

    return outliers, bounds


def _detect_outliers_zscore(
    series: pd.Series,
    clean_series: pd.Series,
    threshold: float
) -> Tuple[pd.Series, Dict[str, float]]:
    """Detecci√≥n de outliers usando z-score."""
    mean = clean_series.mean()
    std = clean_series.std()

    if std == 0:  # Evitar divisi√≥n por cero
        outliers = pd.Series([False] * len(series), index=series.index)
        bounds = {'mean': mean, 'std': 0, 'threshold': threshold}
    else:
        z_scores = np.abs((series - mean) / std)
        outliers = z_scores > threshold
        bounds = {
            'mean': mean, 'std': std,
            'threshold': threshold,
            'lower_bound': mean - threshold * std,
            'upper_bound': mean + threshold * std
        }

    return outliers, bounds


def _detect_outliers_modified_zscore(
    series: pd.Series,
    clean_series: pd.Series,
    threshold: float
) -> Tuple[pd.Series, Dict[str, float]]:
    """Detecci√≥n de outliers usando Modified Z-score (m√°s robusto)."""
    median = clean_series.median()
    mad = np.median(np.abs(clean_series - median))

    if mad == 0:
        # Usar MAD m√≠nimo para evitar divisi√≥n por cero
        mad = 1.4826 * clean_series.std()
        if mad == 0:
            outliers = pd.Series([False] * len(series), index=series.index)
            bounds = {'median': median, 'mad': 0, 'threshold': threshold}
            return outliers, bounds

    modified_z_scores = 0.6745 * (series - median) / mad
    outliers = np.abs(modified_z_scores) > threshold

    bounds = {
        'median': median,
        'mad': mad,
        'threshold': threshold,
        'lower_bound': median - threshold * mad / 0.6745,
        'upper_bound': median + threshold * mad / 0.6745
    }

    return outliers, bounds

# ============================================================================
# FUNCIONES DE MANIPULACI√ìN DE DATAFRAMES
# ============================================================================

def find_and_rename_columns(
    df: pd.DataFrame,
    column_map: Dict[str, List[str]],
    case_sensitive: bool = False
) -> pd.DataFrame:
    """
    Busca y renombra columnas en un DataFrame con b√∫squeda flexible.

    Args:
        df: El DataFrame en el que se buscar√°n y renombrar√°n las columnas
        column_map: Diccionario que mapea nombres est√°ndar a posibles nombres
        case_sensitive: Si la b√∫squeda debe ser sensible a may√∫sculas

    Returns:
        DataFrame con las columnas renombradas

    Raises:
        ValueError: Si hay conflictos en el mapeo
    """
    if df.empty:
        return df

    renamed_cols = {}
    used_cols = set()

    for standard_name, possible_names in column_map.items():
        found = False
        for col in df.columns:
            if col in used_cols:
                continue

            col_compare = col if case_sensitive else str(col).lower()

            for p_name in possible_names:
                p_name_compare = p_name if case_sensitive else str(p_name).lower()

                # B√∫squeda flexible: coincidencia exacta o parcial
                if (p_name_compare == col_compare or
                    p_name_compare in col_compare or
                    col_compare in p_name_compare):

                    if standard_name in renamed_cols.values():
                        logger.warning(
                            f"Columna '{standard_name}' ya mapeada. "
                            f"Ignorando '{col}'"
                        )
                        continue

                    renamed_cols[col] = standard_name
                    used_cols.add(col)
                    found = True
                    break

            if found:
                break

    # Log columnas no mapeadas si hay muchas
    unmapped = set(df.columns) - used_cols
    if len(unmapped) > 0 and len(unmapped) <= 5:
        logger.debug(f"Columnas no mapeadas: {unmapped}")

    return df.rename(columns=renamed_cols)

# ============================================================================
# FUNCIONES DE SERIALIZACI√ìN
# ============================================================================

def sanitize_for_json(data: Any, max_depth: int = 100) -> Any:
    """
    Convierte tipos de datos no serializables a tipos nativos de Python.

    Args:
        data: La estructura de datos a sanear
        max_depth: Profundidad m√°xima de recursi√≥n

    Returns:
        La estructura de datos saneada

    Raises:
        RecursionError: Si se excede la profundidad m√°xima
    """
    if max_depth <= 0:
        raise RecursionError("Profundidad m√°xima de recursi√≥n alcanzada")

    # Manejar diccionarios
    if isinstance(data, dict):
        return {
            k: sanitize_for_json(v, max_depth - 1)
            for k, v in data.items()
        }

    # Manejar listas y tuplas
    if isinstance(data, (list, tuple)):
        return [
            sanitize_for_json(v, max_depth - 1)
            for v in data
        ]

    # Manejar Series de pandas
    if isinstance(data, pd.Series):
        return sanitize_for_json(data.to_list(), max_depth - 1)

    # Manejar DataFrames de pandas
    if isinstance(data, pd.DataFrame):
        return sanitize_for_json(data.to_dict('records'), max_depth - 1)

    # Conversi√≥n de tipos de NumPy a Python nativo
    if isinstance(data, (np.integer, np.int32, np.int64)):
        return int(data)

    if isinstance(data, (np.floating, np.float32, np.float64)):
        if np.isnan(data) or np.isinf(data):
            return None
        return float(data)

    if isinstance(data, np.ndarray):
        return sanitize_for_json(data.tolist(), max_depth - 1)

    if isinstance(data, (np.bool_, bool)):
        return bool(data)

    # Manejar pd.NA, pd.NaT y otros nulos de Pandas
    if pd.isna(data):
        return None

    # Manejar fechas
    if hasattr(data, 'isoformat'):
        return data.isoformat()

    # Para otros tipos, intentar conversi√≥n a string
    if hasattr(data, '__dict__'):
        return sanitize_for_json(data.__dict__, max_depth - 1)

    return data

# ============================================================================
# FUNCIONES ADICIONALES DE UTILIDAD
# ============================================================================

def calculate_std_dev(values: List[float]) -> float:
    """Calcula la desviaci√≥n est√°ndar de una lista de valores."""
    if not values or len(values) < 2:
        return 0.0

    mean = sum(values) / len(values)
    variance = sum((x - mean) ** 2 for x in values) / len(values)
    return variance ** 0.5


def calculate_statistics(series: pd.Series) -> Dict[str, float]:
    """
    Calcula estad√≠sticas descriptivas robustas para una serie num√©rica.

    Args:
        series: Serie num√©rica

    Returns:
        Diccionario con estad√≠sticas
    """
    clean_series = series.dropna()

    if len(clean_series) == 0:
        return {
            'count': 0, 'mean': None, 'std': None,
            'min': None, 'max': None, 'median': None
        }

    return {
        'count': len(clean_series),
        'mean': float(clean_series.mean()),
        'std': float(clean_series.std()),
        'min': float(clean_series.min()),
        'max': float(clean_series.max()),
        'median': float(clean_series.median()),
        'q1': float(clean_series.quantile(0.25)),
        'q3': float(clean_series.quantile(0.75)),
        'null_count': len(series) - len(clean_series),
        'null_percentage': (len(series) - len(clean_series)) / len(series) * 100
    }


def batch_process_dataframe(
    df: pd.DataFrame,
    process_func: callable,
    batch_size: int = 1000,
    **kwargs
) -> pd.DataFrame:
    """
    Procesa un DataFrame en lotes para optimizar memoria.

    Args:
        df: DataFrame a procesar
        process_func: Funci√≥n de procesamiento
        batch_size: Tama√±o del lote
        **kwargs: Argumentos adicionales para process_func

    Returns:
        DataFrame procesado
    """
    if len(df) <= batch_size:
        return process_func(df, **kwargs)

    results = []
    for i in range(0, len(df), batch_size):
        batch = df.iloc[i:i+batch_size]
        processed = process_func(batch, **kwargs)
        results.append(processed)

    return pd.concat(results, ignore_index=True)

def calculate_unit_costs(df: pd.DataFrame,
                        config: Optional[Dict[str, Any]] = None) -> pd.DataFrame:
    """
    Calcula costos unitarios por APU con validaci√≥n robusta.
    Args:
        df: DataFrame procesado
        config: Configuraci√≥n opcional para c√°lculos
    Returns:
        DataFrame con costos unitarios por APU
    """
    if df.empty:
        logger.error("‚ùå DataFrame vac√≠o para c√°lculo de costos")
        return pd.DataFrame()

    required_cols = ['CODIGO_APU', 'TIPO_INSUMO', 'VALOR_TOTAL_APU']
    missing = [col for col in required_cols if col not in df.columns]
    if missing:
        logger.error(f"‚ùå Columnas faltantes: {missing}")
        return pd.DataFrame()

    logger.info("üîÑ Calculando costos unitarios por APU...")
    try:
        # Agrupar y sumar por APU y tipo
        grouped = df.groupby(
            ['CODIGO_APU', 'DESCRIPCION_APU', 'UNIDAD_APU', 'TIPO_INSUMO']
        )['VALOR_TOTAL_APU'].sum().reset_index()

        # Pivotear
        pivot = grouped.pivot_table(
            index=['CODIGO_APU', 'DESCRIPCION_APU', 'UNIDAD_APU'],
            columns='TIPO_INSUMO',
            values='VALOR_TOTAL_APU',
            fill_value=0,
            aggfunc='sum'
        ).reset_index()

        # Asegurar todas las columnas necesarias
        expected_columns = ['SUMINISTRO', 'MANO_DE_OBRA', 'EQUIPO', 'TRANSPORTE', 'OTRO']
        for col in expected_columns:
            if col not in pivot.columns:
                pivot[col] = 0

        # Calcular componentes
        pivot['VALOR_SUMINISTRO_UN'] = pivot.get('SUMINISTRO', 0)
        pivot['VALOR_INSTALACION_UN'] = (
            pivot.get('MANO_DE_OBRA', 0) +
            pivot.get('EQUIPO', 0)
        )
        pivot['VALOR_TRANSPORTE_UN'] = pivot.get('TRANSPORTE', 0)
        pivot['VALOR_OTRO_UN'] = pivot.get('OTRO', 0)

        # Total
        pivot['COSTO_UNITARIO_TOTAL'] = (
            pivot['VALOR_SUMINISTRO_UN'] +
            pivot['VALOR_INSTALACION_UN'] +
            pivot['VALOR_TRANSPORTE_UN'] +
            pivot['VALOR_OTRO_UN']
        )

        # Porcentajes con manejo de divisi√≥n por cero
        total = pivot['COSTO_UNITARIO_TOTAL'].replace(0, np.nan)
        pivot['PCT_SUMINISTRO'] = (pivot['VALOR_SUMINISTRO_UN'] / total * 100).fillna(0).round(2)
        pivot['PCT_INSTALACION'] = (pivot['VALOR_INSTALACION_UN'] / total * 100).fillna(0).round(2)
        pivot['PCT_TRANSPORTE'] = (pivot['VALOR_TRANSPORTE_UN'] / total * 100).fillna(0).round(2)
        pivot['PCT_OTRO'] = (pivot['VALOR_OTRO_UN'] / total * 100).fillna(0).round(2)

        # Ordenar y limpiar
        pivot = pivot.sort_values('CODIGO_APU')

        # Optimizar tipos
        for col in pivot.select_dtypes(include=['float64']).columns:
            pivot[col] = pivot[col].astype('float32')

        # Log resumen
        logger.info(f"‚úÖ Costos calculados para {len(pivot):,} APUs √∫nicos")
        logger.info(f"   üí∞ Suministros: ${pivot['VALOR_SUMINISTRO_UN'].sum():,.2f}")
        logger.info(f"   üë∑ Instalaci√≥n: ${pivot['VALOR_INSTALACION_UN'].sum():,.2f}")
        logger.info(f"   üöö Transporte: ${pivot['VALOR_TRANSPORTE_UN'].sum():,.2f}")
        logger.info(f"   üìä Total: ${pivot['COSTO_UNITARIO_TOTAL'].sum():,.2f}")

        # Validar resultados
        if pivot['COSTO_UNITARIO_TOTAL'].sum() == 0:
            logger.error("‚ö†Ô∏è Todos los costos calculados son cero")

        negative_costs = (pivot['COSTO_UNITARIO_TOTAL'] < 0).sum()
        if negative_costs > 0:
            logger.error(f"‚ö†Ô∏è {negative_costs} APUs con costos negativos")

        return pivot
    except Exception as e:
        logger.error(f"‚ùå Error calculando costos unitarios: {str(e)}")
        return pd.DataFrame()

# ============================================================================
# LISTA DE EXPORTACI√ìN
# ============================================================================

__all__ = [
    # Funciones principales
    'normalize_text',
    'normalize_text_series',
    'parse_number',
    'clean_apu_code',
    'normalize_unit',
    'safe_read_dataframe',
    'validate_numeric_value',
    'validate_series',
    'create_apu_signature',
    'detect_outliers',
    'find_and_rename_columns',
    'sanitize_for_json',
    'calculate_unit_costs',
    # Funciones adicionales
    'calculate_std_dev',
    'calculate_statistics',
    'batch_process_dataframe',
    # Constantes
    'STANDARD_UNITS',
    'UNIT_MAPPING'
]
