import logging
from abc import ABC, abstractmethod
from dataclasses import dataclass
from pathlib import Path
from typing import Dict, List, Optional, Tuple

import numpy as np
import pandas as pd

from scripts.diagnose_apus_file import APUFileDiagnostic

from .apu_processor import APUProcessor
from .data_loader import load_data
from .data_validator import validate_and_clean_data
from .report_parser_crudo import ReportParserCrudo
from .utils import (
    clean_apu_code,
    find_and_rename_columns,
    normalize_text_series,
)

logger = logging.getLogger(__name__)

# ==================== CONSTANTES ====================


class ColumnNames:
    """Nombres de columnas estandarizados para evitar strings m√°gicos."""

    # Columnas de APU
    CODIGO_APU = "CODIGO_APU"
    DESCRIPCION_APU = "DESCRIPCION_APU"
    DESCRIPCION_SECUNDARIA = "descripcion_secundaria"
    ORIGINAL_DESCRIPTION = "original_description"
    UNIDAD_APU = "UNIDAD_APU"
    CANTIDAD_APU = "CANTIDAD_APU"

    # Columnas de Presupuesto
    CANTIDAD_PRESUPUESTO = "CANTIDAD_PRESUPUESTO"

    # Columnas de Insumos
    GRUPO_INSUMO = "GRUPO_INSUMO"
    DESCRIPCION_INSUMO = "DESCRIPCION_INSUMO"
    DESCRIPCION_INSUMO_NORM = "DESCRIPCION_INSUMO_NORM"
    VR_UNITARIO_INSUMO = "VR_UNITARIO_INSUMO"
    UNIDAD_INSUMO = "UNIDAD_INSUMO"
    NORMALIZED_DESC = "NORMALIZED_DESC"

    # Columnas de Costos
    COSTO_INSUMO_EN_APU = "COSTO_INSUMO_EN_APU"
    VR_UNITARIO_FINAL = "VR_UNITARIO_FINAL"
    VALOR_TOTAL_APU = "VALOR_TOTAL_APU"
    PRECIO_UNIT_APU = "PRECIO_UNIT_APU"

    # Columnas de Resultados
    MATERIALES = "MATERIALES"
    MANO_DE_OBRA = "MANO DE OBRA"
    EQUIPO = "EQUIPO"
    OTROS = "OTROS"

    VALOR_SUMINISTRO_UN = "VALOR_SUMINISTRO_UN"
    VALOR_INSTALACION_UN = "VALOR_INSTALACION_UN"
    VALOR_CONSTRUCCION_UN = "VALOR_CONSTRUCCION_UN"

    VALOR_SUMINISTRO_TOTAL = "VALOR_SUMINISTRO_TOTAL"
    VALOR_INSTALACION_TOTAL = "VALOR_INSTALACION_TOTAL"
    VALOR_CONSTRUCCION_TOTAL = "VALOR_CONSTRUCCION_TOTAL"

    TIPO_INSUMO = "TIPO_INSUMO"
    TIPO_APU = "tipo_apu"
    CATEGORIA = "CATEGORIA"

    # Columnas de Tiempo
    TIEMPO_INSTALACION = "TIEMPO_INSTALACION"
    RENDIMIENTO = "RENDIMIENTO"
    RENDIMIENTO_DIA = "RENDIMIENTO_DIA"


class InsumoTypes:
    """Tipos de insumos reconocidos."""

    SUMINISTRO = "SUMINISTRO"
    MANO_DE_OBRA = "MANO_DE_OBRA"
    EQUIPO = "EQUIPO"
    TRANSPORTE = "TRANSPORTE"
    OTRO = "OTRO"


class APUTypes:
    """Tipos de APU clasificados."""

    INSTALACION = "Instalaci√≥n"
    SUMINISTRO = "Suministro"
    SUMINISTRO_PREFABRICADO = "Suministro (Pre-fabricado)"
    OBRA_COMPLETA = "Obra Completa"
    INDEFINIDO = "Indefinido"


class ProcessingStep(ABC):
    @abstractmethod
    def execute(self, context: dict) -> dict:
        pass


class ProcessingPipeline:
    def __init__(self, steps: list[ProcessingStep]):
        self.steps = steps

    def run(self, initial_context: dict) -> dict:
        context = initial_context
        for step in self.steps:
            context = step.execute(context)
        return context


@dataclass
class ProcessingThresholds:
    """Umbrales configurables para validaciones y clasificaci√≥n."""

    # Detecci√≥n de outliers
    outlier_std_multiplier: float = 3.0

    # L√≠mites de costos an√≥malos
    max_quantity: float = 1e6
    max_cost_per_item: float = 1e9
    max_total_cost: float = 1e11

    # Clasificaci√≥n de APU (porcentajes)
    instalacion_mo_threshold: float = 75.0
    suministro_mat_threshold: float = 75.0
    suministro_mo_max: float = 15.0
    prefabricado_mat_threshold: float = 65.0
    prefabricado_mo_min: float = 15.0

    # N√∫mero m√°ximo de filas para b√∫squeda de encabezado
    max_header_search_rows: int = 10


@dataclass
class ProcessingResult:
    """Resultado estandarizado del procesamiento."""

    success: bool
    data: Optional[Dict] = None
    error: Optional[str] = None
    warnings: List[str] = None

    def __post_init__(self):
        if self.warnings is None:
            self.warnings = []


# ==================== VALIDADORES ====================


class DataValidator:
    """Validador centralizado para verificaci√≥n de integridad de datos."""

    @staticmethod
    def validate_dataframe_not_empty(
        df: pd.DataFrame, name: str
    ) -> Tuple[bool, Optional[str]]:
        """Valida que un DataFrame no est√© vac√≠o."""
        if df is None or df.empty:
            error_msg = f"DataFrame '{name}' est√° vac√≠o o es None"
            logger.error(f"‚ùå {error_msg}")
            return False, error_msg
        return True, None

    @staticmethod
    def validate_required_columns(
        df: pd.DataFrame, required_cols: List[str], df_name: str
    ) -> Tuple[bool, Optional[str]]:
        """Valida que existan las columnas requeridas."""
        missing_cols = [col for col in required_cols if col not in df.columns]
        if missing_cols:
            error_msg = f"Faltan columnas requeridas en '{df_name}': {missing_cols}"
            logger.error(f"‚ùå {error_msg}")
            return False, error_msg
        return True, None

    @staticmethod
    def detect_and_log_duplicates(
        df: pd.DataFrame, subset_cols: List[str], df_name: str, keep: str = "first"
    ) -> pd.DataFrame:
        """Detecta, registra y elimina duplicados."""
        duplicates = df[df.duplicated(subset=subset_cols, keep=False)]

        if not duplicates.empty:
            unique_dupes = duplicates[subset_cols[0]].unique()
            logger.warning(
                f"‚ö†Ô∏è  Se encontraron {len(duplicates)} filas duplicadas en '{df_name}' "
                f"por {subset_cols}. Se conservar√°: '{keep}'"
            )
            logger.debug(f"Valores duplicados en '{df_name}': {unique_dupes.tolist()[:10]}")

            df_clean = df.drop_duplicates(subset=subset_cols, keep=keep)
            logger.info(f"‚úÖ Duplicados eliminados: {len(df)} -> {len(df_clean)} filas")
            return df_clean

        return df

    @staticmethod
    def validate_numeric_range(
        series: pd.Series, column_name: str, max_value: float, min_value: float = 0
    ) -> bool:
        """Valida que los valores num√©ricos est√©n en un rango aceptable."""
        invalid_values = series[(series < min_value) | (series > max_value)]

        if not invalid_values.empty:
            logger.warning(
                f"‚ö†Ô∏è  Se encontraron {len(invalid_values)} valores fuera de rango "
                f"en '{column_name}' (rango v√°lido: {min_value} - {max_value})"
            )
            logger.debug(
                f"Valores an√≥malos en '{column_name}':\n{invalid_values.describe()}"
            )
            return False

        return True


class FileValidator:
    """Validador para archivos de entrada."""

    @staticmethod
    def validate_file_exists(file_path: str, file_type: str) -> Tuple[bool, Optional[str]]:
        """Valida que un archivo exista."""
        path = Path(file_path)

        if not path.exists():
            error_msg = f"Archivo de {file_type} no encontrado: {file_path}"
            logger.error(f"‚ùå {error_msg}")
            return False, error_msg

        if not path.is_file():
            error_msg = f"La ruta de {file_type} no es un archivo: {file_path}"
            logger.error(f"‚ùå {error_msg}")
            return False, error_msg

        logger.debug(f"‚úÖ Archivo de {file_type} validado: {file_path}")
        return True, None


# ==================== PROCESADORES ====================


class LoadDataStep(ProcessingStep):
    def __init__(self, config: dict, thresholds: ProcessingThresholds):
        self.config = config
        self.thresholds = thresholds

    def execute(self, context: dict) -> dict:
        presupuesto_path = context["presupuesto_path"]
        apus_path = context["apus_path"]
        insumos_path = context["insumos_path"]

        # ... (validaciones de existencia de archivos) ...
        file_validator = FileValidator()
        validations = [
            (presupuesto_path, "presupuesto"),
            (apus_path, "APUs"),
            (insumos_path, "insumos"),
        ]
        for file_path, file_type in validations:
            is_valid, error = file_validator.validate_file_exists(file_path, file_type)
            if not is_valid:
                raise ValueError(error)
        # --- CAMBIO CLAVE: Leer y usar los perfiles ---
        file_profiles = self.config.get("file_profiles", {})

        # Cargar Presupuesto con su perfil
        presupuesto_profile = file_profiles.get("presupuesto_default")
        if not presupuesto_profile:
            raise ValueError("No se encontr√≥ el perfil 'presupuesto_default' en config.json")
        presupuesto_processor = PresupuestoProcessor(
            self.config, self.thresholds, presupuesto_profile
        )
        df_presupuesto = presupuesto_processor.process(presupuesto_path)

        # Cargar Insumos con su perfil
        insumos_profile = file_profiles.get("insumos_default")
        if not insumos_profile:
            raise ValueError("No se encontr√≥ el perfil 'insumos_default' en config.json")
        insumos_processor = InsumosProcessor(self.thresholds, insumos_profile)
        df_insumos = insumos_processor.process(insumos_path)

        # Cargar APUs con su perfil
        apus_profile = file_profiles.get("apus_default")
        if not apus_profile:
            raise ValueError("No se encontr√≥ el perfil 'apus_default' en config.json")
        parser = ReportParserCrudo(apus_path, apus_profile)
        raw_records = parser.parse_to_raw()

        # El resto del pipeline sigue igual, pero ahora APUProcessor tambi√©n necesita la config
        processor = APUProcessor(raw_records, self.config)
        df_apus_raw = processor.process_all()

        data_validator = DataValidator()
        validations = [
            (df_presupuesto, "presupuesto"),
            (df_insumos, "insumos"),
            (df_apus_raw, "APUs"),
        ]
        for df, name in validations:
            is_valid, error = data_validator.validate_dataframe_not_empty(df, name)
            if not is_valid:
                raise ValueError(error)

        context["df_presupuesto"] = df_presupuesto
        context["df_insumos"] = df_insumos
        context["df_apus_raw"] = df_apus_raw
        return context


class MergeDataStep(ProcessingStep):
    def __init__(self, thresholds: ProcessingThresholds):
        self.thresholds = thresholds

    def execute(self, context: dict) -> dict:
        df_apus_raw = context["df_apus_raw"]
        df_insumos = context["df_insumos"]

        merger = DataMerger(self.thresholds)
        df_merged = merger.merge_apus_with_insumos(df_apus_raw, df_insumos)

        context["df_merged"] = df_merged
        return context


class CalculateCostsStep(ProcessingStep):
    def __init__(self, config: dict, thresholds: ProcessingThresholds):
        self.config = config
        self.thresholds = thresholds

    def execute(self, context: dict) -> dict:
        df_merged = context["df_merged"]

        df_merged = calculate_insumo_costs(df_merged, self.thresholds)

        cost_calculator = APUCostCalculator(self.config, self.thresholds)
        df_apu_costos, df_tiempo, df_rendimiento = cost_calculator.calculate(df_merged)

        context["df_merged"] = df_merged
        context["df_apu_costos"] = df_apu_costos
        context["df_tiempo"] = df_tiempo
        context["df_rendimiento"] = df_rendimiento
        return context


class FinalMergeStep(ProcessingStep):
    def __init__(self, thresholds: ProcessingThresholds):
        self.thresholds = thresholds

    def execute(self, context: dict) -> dict:
        df_presupuesto = context["df_presupuesto"]
        df_apu_costos = context["df_apu_costos"]
        df_tiempo = context["df_tiempo"]

        merger = DataMerger(self.thresholds)
        df_final = merger.merge_with_presupuesto(df_presupuesto, df_apu_costos)

        df_final = pd.merge(df_final, df_tiempo, on=ColumnNames.CODIGO_APU, how="left")

        df_final = group_and_split_description(df_final)

        df_final = calculate_total_costs(df_final, self.thresholds)

        context["df_final"] = df_final
        return context


class BuildOutputStep(ProcessingStep):
    def execute(self, context: dict) -> dict:
        df_final = context["df_final"]
        df_insumos = context["df_insumos"]
        df_merged = context["df_merged"]
        df_apus_raw = context["df_apus_raw"]
        df_apu_costos = context["df_apu_costos"]
        df_tiempo = context["df_tiempo"]
        df_rendimiento = context["df_rendimiento"]

        df_merged = synchronize_data_sources(df_merged, df_final)

        df_processed_apus = build_processed_apus_dataframe(
            df_apu_costos, df_apus_raw, df_tiempo, df_rendimiento
        )

        result_dict = build_output_dictionary(
            df_final, df_insumos, df_merged, df_apus_raw, df_processed_apus
        )

        validated_result = validate_and_clean_data(result_dict)
        validated_result["raw_insumos_df"] = df_insumos.to_dict("records")

        context["final_result"] = validated_result
        return context


class PresupuestoProcessor:
    """Procesador especializado para archivos de presupuesto."""

    def __init__(self, config: dict, thresholds: ProcessingThresholds, profile: dict):
        self.config = config
        self.thresholds = thresholds
        self.profile = profile  # Guardar el perfil
        self.validator = DataValidator()

    def process(self, path: str) -> pd.DataFrame:
        """Procesa el archivo de presupuesto (CSV o Excel)."""
        try:
            # CAMBIO: Usar los par√°metros del perfil para la carga
            loader_params = self.profile.get("loader_params", {})
            logger.info(f"Cargando presupuesto con perfil: {loader_params}")
            df = load_data(path, **loader_params)

            if df is None or df.empty:
                logger.error("‚ùå No se pudo leer el archivo de presupuesto o est√° vac√≠o")
                return pd.DataFrame()
            df = self._find_and_set_header(df)
            if df.empty:
                return pd.DataFrame()

            df = self._rename_columns(df)
            if not self._validate_required_columns(df):
                return pd.DataFrame()

            df = self._clean_and_convert_data(df)
            df = self._remove_duplicates(df)

            logger.info(f"‚úÖ Presupuesto cargado: {len(df)} APUs √∫nicos")

            return df[
                [
                    ColumnNames.CODIGO_APU,
                    ColumnNames.DESCRIPCION_APU,
                    ColumnNames.CANTIDAD_PRESUPUESTO,
                ]
            ]

        except Exception as e:
            logger.error(f"‚ùå Error procesando presupuesto: {e}", exc_info=True)
            return pd.DataFrame()

    def _find_and_set_header(self, df: pd.DataFrame) -> pd.DataFrame:
        """Busca y establece la fila de encabezado."""
        header_row_index = -1
        search_rows = min(self.thresholds.max_header_search_rows, len(df))

        for i in range(search_rows):
            row_str = " ".join(df.iloc[i].astype(str).str.upper())
            if all(keyword in row_str for keyword in ["ITEM", "DESCRIPCION", "CANT"]):
                header_row_index = i
                break

        if header_row_index == -1:
            logger.error(
                f"‚ùå No se encontr√≥ encabezado v√°lido en las primeras "
                f"{search_rows} filas del presupuesto"
            )
            return pd.DataFrame()

        df.columns = df.iloc[header_row_index]
        df = df.iloc[header_row_index + 1 :].reset_index(drop=True)

        logger.debug(f"‚úÖ Encabezado encontrado en fila {header_row_index}")
        return df

    def _rename_columns(self, df: pd.DataFrame) -> pd.DataFrame:
        """Renombra columnas seg√∫n configuraci√≥n."""
        column_map = self.config.get("presupuesto_column_map", {})
        return find_and_rename_columns(df, column_map)

    def _validate_required_columns(self, df: pd.DataFrame) -> bool:
        """Valida columnas requeridas."""
        is_valid, error = self.validator.validate_required_columns(
            df, [ColumnNames.CODIGO_APU], "presupuesto"
        )
        return is_valid

    def _clean_and_convert_data(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        Limpia y convierte tipos de datos.
        Maneja c√≥digos de ITEM de cualquier longitud, incluyendo d√≠gitos simples.
        """
        # CAMBIO: Leer los par√°metros desde la configuraci√≥n
        clean_code_params = (
            self.config.get("clean_apu_code_params", {}).get("presupuesto_item", {})
        )

        df[ColumnNames.CODIGO_APU] = (
            df[ColumnNames.CODIGO_APU]
            .astype(str)
            .apply(lambda code: clean_apu_code(code, **clean_code_params))
        )

        # Filtrar c√≥digos vac√≠os (solo despu√©s de la limpieza)
        df = df[df[ColumnNames.CODIGO_APU].notna() & (df[ColumnNames.CODIGO_APU] != "")]

        # Convertir cantidades con manejo robusto
        if ColumnNames.CANTIDAD_PRESUPUESTO in df.columns:
            cantidad_str = (
                df[ColumnNames.CANTIDAD_PRESUPUESTO]
                .astype(str)
                .str.replace(",", ".", regex=False)
            )
            df[ColumnNames.CANTIDAD_PRESUPUESTO] = pd.to_numeric(
                cantidad_str, errors="coerce"
            )

        # Validar rango con logging mejorado
        invalid_quantities = df[
            (df[ColumnNames.CANTIDAD_PRESUPUESTO] < 0)
            | (df[ColumnNames.CANTIDAD_PRESUPUESTO] > self.thresholds.max_quantity)
        ]
        if not invalid_quantities.empty:
            logger.warning(
                f"‚ö†Ô∏è Se encontraron {len(invalid_quantities)} cantidades "
                f"fuera de rango (0 - {self.thresholds.max_quantity:,.0f})"
            )
            # Mostrar ejemplos
            for idx, row in invalid_quantities.head(3).iterrows():
                logger.warning(
                    f" ITEM {row[ColumnNames.CODIGO_APU]}: "
                    f"Cantidad = {row[ColumnNames.CANTIDAD_PRESUPUESTO]}"
                )

        logger.debug(
            f"‚úÖ Limpieza completada: {len(df)} ITEMs v√°lidos "
            f"(incluyendo c√≥digos de 1 car√°cter)"
        )

        return df

    def _remove_duplicates(self, df: pd.DataFrame) -> pd.DataFrame:
        """Elimina duplicados de c√≥digos APU."""
        return self.validator.detect_and_log_duplicates(
            df, [ColumnNames.CODIGO_APU], "presupuesto", keep="first"
        )


class InsumosProcessor:
    """Procesador especializado para archivos de insumos."""

    def __init__(self, thresholds: ProcessingThresholds, profile: dict):
        self.thresholds = thresholds
        self.profile = profile  # Guardar el perfil
        self.validator = DataValidator()

    def process(self, file_path: str) -> pd.DataFrame:
        """Procesa el archivo CSV de insumos con formato no est√°ndar."""
        try:
            records = self._parse_file(file_path)

            if not records:
                logger.warning("‚ö†Ô∏è  No se encontraron registros en archivo de insumos")
                return pd.DataFrame()

            df = pd.DataFrame(records)
            df = self._rename_and_select_columns(df)
            df = self._convert_and_normalize(df)
            df = self._remove_duplicates(df)

            logger.info(f"‚úÖ Insumos cargados: {len(df)} insumos √∫nicos")
            return df

        except Exception as e:
            logger.error(f"‚ùå Error procesando archivo de insumos: {e}", exc_info=True)
            return pd.DataFrame()

    def _parse_file(self, file_path: str) -> List[Dict]:
        """Parsea el archivo de insumos con formato especial."""
        # CAMBIO: Usar el encoding del perfil
        encoding = self.profile.get("encoding", "latin1")  # latin1 como fallback seguro
        with open(file_path, "r", encoding=encoding) as f:
            lines = f.readlines()

        records = []
        current_group = None
        header = None

        for line in lines:
            parts = [p.strip().replace('"', "") for p in line.strip().split(";")]

            if not any(parts):
                continue

            # Detectar grupo
            if parts[0].startswith("G"):
                current_group = parts[1]
                header = None
                continue

            # Detectar encabezado
            if "CODIGO" in parts[0] and "DESCRIPCION" in parts[1]:
                header = ["CODIGO", "DESCRIPCION", "UND", "CANT.", "VR. UNIT."]
                continue

            # Parsear registro
            if header and current_group:
                record = {ColumnNames.GRUPO_INSUMO: current_group}
                for i, col in enumerate(header):
                    record[col] = parts[i] if i < len(parts) else None
                records.append(record)

        return records

    def _rename_and_select_columns(self, df: pd.DataFrame) -> pd.DataFrame:
        """Renombra y selecciona columnas relevantes."""
        df = df.rename(
            columns={
                "DESCRIPCION": ColumnNames.DESCRIPCION_INSUMO,
                "VR. UNIT.": ColumnNames.VR_UNITARIO_INSUMO,
            }
        )

        final_cols = [
            ColumnNames.GRUPO_INSUMO,
            ColumnNames.DESCRIPCION_INSUMO,
            ColumnNames.VR_UNITARIO_INSUMO,
        ]

        return df[final_cols]

    def _convert_and_normalize(self, df: pd.DataFrame) -> pd.DataFrame:
        """Convierte valores num√©ricos y normaliza texto."""
        # Convertir precios
        df[ColumnNames.VR_UNITARIO_INSUMO] = pd.to_numeric(
            df[ColumnNames.VR_UNITARIO_INSUMO].astype(str).str.replace(",", "."),
            errors="coerce",
        )

        # Normalizar descripciones
        df[ColumnNames.DESCRIPCION_INSUMO_NORM] = normalize_text_series(
            df[ColumnNames.DESCRIPCION_INSUMO]
        )

        # Eliminar filas sin descripci√≥n
        df = df.dropna(subset=[ColumnNames.DESCRIPCION_INSUMO])

        return df

    def _remove_duplicates(self, df: pd.DataFrame) -> pd.DataFrame:
        """Elimina duplicados conservando el de mayor precio."""
        duplicates = df[
            df.duplicated(subset=[ColumnNames.DESCRIPCION_INSUMO_NORM], keep=False)
        ]

        if not duplicates.empty:
            logger.warning(
                f"‚ö†Ô∏è  Se encontraron {len(duplicates)} insumos con descripciones "
                f"normalizadas duplicadas. Se conservar√° el de mayor precio."
            )

            df = df.sort_values(
                ColumnNames.VR_UNITARIO_INSUMO, ascending=False
            ).drop_duplicates(subset=[ColumnNames.DESCRIPCION_INSUMO_NORM], keep="first")

        return df


class APUCostCalculator:
    """Calculador de costos y metadatos de APUs."""

    def __init__(self, config: dict, thresholds: ProcessingThresholds):
        self.config = config
        self.thresholds = thresholds

    def calculate(
        self, df_merged: pd.DataFrame
    ) -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:
        """Calcula costos, clasifica APUs y extrae metadatos."""

        df_apu_costos = self._aggregate_costs_by_type(df_merged)
        df_apu_costos = self._calculate_unit_values(df_apu_costos)
        df_apu_costos = self._detect_cost_outliers(df_apu_costos)
        df_apu_costos = self._classify_apus(df_apu_costos)

        df_tiempo = self._calculate_installation_time(df_merged)
        df_rendimiento = self._calculate_performance(df_merged)

        return df_apu_costos, df_tiempo, df_rendimiento

    def _aggregate_costs_by_type(self, df_merged: pd.DataFrame) -> pd.DataFrame:
        """Agrupa costos por APU y tipo de insumo."""
        logger.info("üìä Agregando costos por APU y tipo de insumo...")

        df_apu_costos = (
            df_merged.groupby([ColumnNames.CODIGO_APU, ColumnNames.TIPO_INSUMO])[
                ColumnNames.COSTO_INSUMO_EN_APU
            ]
            .sum()
            .unstack(fill_value=0)
            .reset_index()
        )

        # Mapear tipos de insumo a columnas de costo
        cost_cols_map = {
            InsumoTypes.SUMINISTRO: ColumnNames.MATERIALES,
            InsumoTypes.MANO_DE_OBRA: ColumnNames.MANO_DE_OBRA,
            InsumoTypes.EQUIPO: ColumnNames.EQUIPO,
            InsumoTypes.TRANSPORTE: ColumnNames.OTROS,
            InsumoTypes.OTRO: ColumnNames.OTROS,
        }

        df_apu_costos = df_apu_costos.rename(columns=cost_cols_map)

        # Consolidar columna OTROS si se generaron m√∫ltiples
        if ColumnNames.OTROS in df_apu_costos.columns:
            if isinstance(df_apu_costos[ColumnNames.OTROS], pd.DataFrame):
                df_apu_costos[ColumnNames.OTROS] = df_apu_costos[ColumnNames.OTROS].sum(
                    axis=1
                )

        # Asegurar que existan todas las columnas de costo
        final_cost_cols = [
            ColumnNames.MATERIALES,
            ColumnNames.MANO_DE_OBRA,
            ColumnNames.EQUIPO,
            ColumnNames.OTROS,
        ]

        for col in final_cost_cols:
            if col not in df_apu_costos.columns:
                df_apu_costos[col] = 0

        logger.info(f"‚úÖ Costos agregados: {len(df_apu_costos)} APUs √∫nicos")
        return df_apu_costos

    def _calculate_unit_values(self, df: pd.DataFrame) -> pd.DataFrame:
        """Calcula valores unitarios por APU."""
        df[ColumnNames.VALOR_SUMINISTRO_UN] = df[ColumnNames.MATERIALES]

        df[ColumnNames.VALOR_INSTALACION_UN] = (
            df[ColumnNames.MANO_DE_OBRA] + df[ColumnNames.EQUIPO]
        )

        cost_cols = [
            ColumnNames.MATERIALES,
            ColumnNames.MANO_DE_OBRA,
            ColumnNames.EQUIPO,
            ColumnNames.OTROS,
        ]
        df[ColumnNames.VALOR_CONSTRUCCION_UN] = df[cost_cols].sum(axis=1)

        return df

    def _detect_cost_outliers(self, df: pd.DataFrame) -> pd.DataFrame:
        """Detecta y registra valores de costo an√≥malos."""
        valor_max = df[ColumnNames.VALOR_CONSTRUCCION_UN].max()
        valor_avg = df[ColumnNames.VALOR_CONSTRUCCION_UN].mean()
        valor_std = df[ColumnNames.VALOR_CONSTRUCCION_UN].std()

        logger.info(
            f"üìà Estad√≠sticas de costos unitarios: "
            f"Max=${valor_max:,.2f}, Avg=${valor_avg:,.2f}, Std=${valor_std:,.2f}"
        )

        outlier_threshold = valor_avg + (self.thresholds.outlier_std_multiplier * valor_std)

        outliers = df[df[ColumnNames.VALOR_CONSTRUCCION_UN] > outlier_threshold]

        if not outliers.empty:
            logger.warning(
                f"‚ö†Ô∏è  Se detectaron {len(outliers)} APUs con costos at√≠picos "
                f"(>{outlier_threshold:,.2f})"
            )

            for _, outlier in outliers.head(5).iterrows():
                logger.warning(
                    f"   APU {outlier[ColumnNames.CODIGO_APU]}: "
                    f"${outlier[ColumnNames.VALOR_CONSTRUCCION_UN]:,.2f} "
                    f"(MO: ${outlier.get(ColumnNames.MANO_DE_OBRA, 0):,.2f}, "
                    f"MAT: ${outlier.get(ColumnNames.MATERIALES, 0):,.2f})"
                )

        return df

    def _classify_apus(self, df: pd.DataFrame) -> pd.DataFrame:
        """Clasifica APUs seg√∫n distribuci√≥n de costos."""

        def classify_apu(row) -> str:
            costo_total = row[ColumnNames.VALOR_CONSTRUCCION_UN]

            if costo_total == 0:
                return APUTypes.INDEFINIDO

            # Contexto para eval()
            context = {
                "porcentaje_mo_eq": (
                    (row.get(ColumnNames.MANO_DE_OBRA, 0) + row.get(ColumnNames.EQUIPO, 0))
                    / costo_total
                )
                * 100,
                "porcentaje_materiales": (row.get(ColumnNames.MATERIALES, 0) / costo_total)
                * 100,
            }

            # Motor de reglas desde config
            for rule in self.config.get("apu_classification_rules", []):
                try:
                    if eval(rule["condition"], {"__builtins__": {}}, context):
                        return rule["type"]
                except Exception as e:
                    logger.error(f"Error evaluando regla: {rule}. Error: {e}")

            return APUTypes.OBRA_COMPLETA

        df[ColumnNames.TIPO_APU] = df.apply(classify_apu, axis=1)

        # Registrar distribuci√≥n de tipos
        tipo_counts = df[ColumnNames.TIPO_APU].value_counts()
        logger.info(f"üìã Clasificaci√≥n de APUs:\n{tipo_counts}")

        return df

    def _calculate_installation_time(self, df_merged: pd.DataFrame) -> pd.DataFrame:
        """Calcula tiempo de instalaci√≥n basado en mano de obra."""
        df_tiempo = (
            df_merged[df_merged[ColumnNames.TIPO_INSUMO] == InsumoTypes.MANO_DE_OBRA]
            .groupby(ColumnNames.CODIGO_APU)[ColumnNames.CANTIDAD_APU]
            .sum()
            .reset_index()
        )

        df_tiempo = df_tiempo.rename(
            columns={ColumnNames.CANTIDAD_APU: ColumnNames.TIEMPO_INSTALACION}
        )

        return df_tiempo

    def _calculate_performance(self, df_merged: pd.DataFrame) -> pd.DataFrame:
        """Calcula rendimiento diario si la columna existe."""
        if ColumnNames.RENDIMIENTO not in df_merged.columns:
            return pd.DataFrame(
                columns=[ColumnNames.CODIGO_APU, ColumnNames.RENDIMIENTO_DIA]
            )

        df_rendimiento = (
            df_merged[df_merged[ColumnNames.TIPO_INSUMO] == InsumoTypes.MANO_DE_OBRA]
            .groupby(ColumnNames.CODIGO_APU)[ColumnNames.RENDIMIENTO]
            .sum()
            .reset_index()
        )

        df_rendimiento = df_rendimiento.rename(
            columns={ColumnNames.RENDIMIENTO: ColumnNames.RENDIMIENTO_DIA}
        )

        return df_rendimiento


class DataMerger:
    """Gestor de merge de datos con validaciones robustas."""

    def __init__(self, thresholds: ProcessingThresholds):
        self.thresholds = thresholds
        self.validator = DataValidator()

    def merge_apus_with_insumos(
        self, df_apus: pd.DataFrame, df_insumos: pd.DataFrame
    ) -> pd.DataFrame:
        """Merge de APUs con cat√°logo de insumos."""
        logger.info("üîó Iniciando merge de APUs con cat√°logo de insumos...")

        # Validar que existe la columna de normalizaci√≥n
        if ColumnNames.NORMALIZED_DESC not in df_apus.columns:
            logger.warning(
                f"‚ö†Ô∏è  Columna {ColumnNames.NORMALIZED_DESC} no encontrada. "
                f"Creando como fallback..."
            )
            df_apus[ColumnNames.NORMALIZED_DESC] = normalize_text_series(
                df_apus[ColumnNames.DESCRIPCION_INSUMO]
            )

        rows_before = len(df_apus)

        df_merged = pd.merge(
            df_apus,
            df_insumos,
            left_on=ColumnNames.NORMALIZED_DESC,
            right_on=ColumnNames.DESCRIPCION_INSUMO_NORM,
            how="left",
            suffixes=("_apu", ""),
            validate="m:1",
        )

        rows_after = len(df_merged)

        if rows_after != rows_before:
            logger.warning(
                f"‚ö†Ô∏è  Cambio en n√∫mero de filas durante merge: {rows_before} -> {rows_after}"
            )

        # Consolidar columnas duplicadas
        df_merged[ColumnNames.TIPO_INSUMO] = df_merged[ColumnNames.CATEGORIA]

        df_merged[ColumnNames.DESCRIPCION_INSUMO] = df_merged[
            ColumnNames.DESCRIPCION_INSUMO
        ].fillna(df_merged[f"{ColumnNames.DESCRIPCION_INSUMO}_apu"])

        # Renombrar unidad de insumo
        if f"{ColumnNames.UNIDAD_INSUMO}_apu" in df_merged.columns:
            df_merged = df_merged.rename(
                columns={f"{ColumnNames.UNIDAD_INSUMO}_apu": ColumnNames.UNIDAD_INSUMO}
            )

        logger.info(f"‚úÖ Merge completado: {len(df_merged)} registros")

        return df_merged

    def merge_with_presupuesto(
        self, df_presupuesto: pd.DataFrame, df_apu_costos: pd.DataFrame
    ) -> pd.DataFrame:
        """Merge de presupuesto con costos de APUs (1:1 validado)."""
        logger.info("üîó Realizando merge final: presupuesto + costos...")

        logger.debug(
            f"   - Presupuesto: {len(df_presupuesto)} filas, "
            f"{df_presupuesto[ColumnNames.CODIGO_APU].nunique()} APUs √∫nicos"
        )
        logger.debug(
            f"   - Costos: {len(df_apu_costos)} filas, "
            f"{df_apu_costos[ColumnNames.CODIGO_APU].nunique()} APUs √∫nicos"
        )

        rows_before = len(df_presupuesto)

        try:
            df_final = pd.merge(
                df_presupuesto,
                df_apu_costos,
                on=ColumnNames.CODIGO_APU,
                how="left",
                validate="1:1",  # Validaci√≥n cr√≠tica: solo 1:1
            )
        except pd.errors.MergeError as e:
            logger.error(f"‚ùå Error en merge 1:1: Se detectaron duplicados. {e}")

            # Diagnosticar duplicados
            dupes_presupuesto = df_presupuesto[
                df_presupuesto.duplicated(ColumnNames.CODIGO_APU, keep=False)
            ]
            dupes_costos = df_apu_costos[
                df_apu_costos.duplicated(ColumnNames.CODIGO_APU, keep=False)
            ]

            if not dupes_presupuesto.empty:
                logger.error(
                    f"Duplicados en presupuesto:\n"
                    f"{dupes_presupuesto[ColumnNames.CODIGO_APU].unique()}"
                )

            if not dupes_costos.empty:
                logger.error(
                    f"Duplicados en costos:\n{dupes_costos[ColumnNames.CODIGO_APU].unique()}"
                )

            raise

        rows_after = len(df_final)

        if rows_after != rows_before:
            logger.error(
                f"‚ùå Explosi√≥n cartesiana detectada: {rows_before} -> {rows_after} filas"
            )
            raise ValueError(
                "Merge produjo explosi√≥n cartesiana. Revise duplicados en datos."
            )

        logger.info(f"‚úÖ Merge 1:1 exitoso: {len(df_final)} filas")

        return df_final


# ==================== FUNCIONES AUXILIARES ====================


def group_and_split_description(df: pd.DataFrame) -> pd.DataFrame:
    """Conserva la descripci√≥n original y la divide en principal y secundaria.

    Args:
        df: DataFrame que contiene la columna DESCRIPCION_APU.

    Returns:
        DataFrame con descripci√≥n dividida.
    """
    if ColumnNames.DESCRIPCION_APU not in df.columns:
        logger.warning(
            f"‚ö†Ô∏è  Columna {ColumnNames.DESCRIPCION_APU} no encontrada. "
            f"Saltando divisi√≥n de descripci√≥n."
        )
        return df

    df_result = df.copy()

    # Conservar original
    df_result[ColumnNames.ORIGINAL_DESCRIPTION] = df_result[ColumnNames.DESCRIPCION_APU]

    # Dividir por separador
    split_desc = df_result[ColumnNames.DESCRIPCION_APU].str.split(" / ", n=1, expand=True)

    df_result[ColumnNames.DESCRIPCION_APU] = split_desc[0]

    if split_desc.shape[1] > 1:
        df_result[ColumnNames.DESCRIPCION_SECUNDARIA] = split_desc[1]
    else:
        df_result[ColumnNames.DESCRIPCION_SECUNDARIA] = ""

    return df_result


def calculate_total_costs(
    df: pd.DataFrame, thresholds: ProcessingThresholds
) -> pd.DataFrame:
    """Calcula valores totales del presupuesto con validaciones."""
    logger.info("üíµ Calculando valores totales del presupuesto...")

    # Validar que existe la columna de cantidad
    if ColumnNames.CANTIDAD_PRESUPUESTO not in df.columns:
        logger.error(f"‚ùå Columna {ColumnNames.CANTIDAD_PRESUPUESTO} no encontrada")
        raise ValueError(
            f"Columna requerida no encontrada: {ColumnNames.CANTIDAD_PRESUPUESTO}"
        )

    # Convertir y validar cantidades
    df[ColumnNames.CANTIDAD_PRESUPUESTO] = pd.to_numeric(
        df[ColumnNames.CANTIDAD_PRESUPUESTO], errors="coerce"
    ).fillna(0)

    logger.debug(
        f"Estad√≠sticas de cantidades:\n{df[ColumnNames.CANTIDAD_PRESUPUESTO].describe()}"
    )

    # Calcular valores totales
    df[ColumnNames.VALOR_SUMINISTRO_TOTAL] = (
        df[ColumnNames.VALOR_SUMINISTRO_UN] * df[ColumnNames.CANTIDAD_PRESUPUESTO]
    )

    df[ColumnNames.VALOR_INSTALACION_TOTAL] = (
        df[ColumnNames.VALOR_INSTALACION_UN] * df[ColumnNames.CANTIDAD_PRESUPUESTO]
    )

    df[ColumnNames.VALOR_CONSTRUCCION_TOTAL] = (
        df[ColumnNames.VALOR_CONSTRUCCION_UN] * df[ColumnNames.CANTIDAD_PRESUPUESTO]
    )

    # Validar costo total
    total_construccion = df[ColumnNames.VALOR_CONSTRUCCION_TOTAL].sum()

    logger.info(f"üí∞ COSTO TOTAL CONSOLIDADO: ${total_construccion:,.2f}")

    if total_construccion > thresholds.max_total_cost:
        logger.error(
            f"‚ùå COSTO TOTAL ANORMALMENTE ALTO: ${total_construccion:,.2f} "
            f"(l√≠mite: ${thresholds.max_total_cost:,.2f})"
        )

        # Mostrar principales contribuyentes
        top_contributors = df.nlargest(10, ColumnNames.VALOR_CONSTRUCCION_TOTAL)[
            [
                ColumnNames.CODIGO_APU,
                ColumnNames.DESCRIPCION_APU,
                ColumnNames.CANTIDAD_PRESUPUESTO,
                ColumnNames.VALOR_CONSTRUCCION_UN,
                ColumnNames.VALOR_CONSTRUCCION_TOTAL,
            ]
        ]

        logger.error(f"Top 10 APUs contributores:\n{top_contributors}")

        raise ValueError(f"Costo total excede l√≠mite permitido: ${total_construccion:,.2f}")

    return df


def calculate_insumo_costs(
    df: pd.DataFrame, thresholds: ProcessingThresholds
) -> pd.DataFrame:
    """Calcula costos de insumos con validaciones robustas."""
    logger.info("üí∞ Calculando costos de insumos...")

    # Validar columnas requeridas
    required_cols = [
        ColumnNames.CANTIDAD_APU,
        ColumnNames.VR_UNITARIO_INSUMO,
        ColumnNames.VALOR_TOTAL_APU,
    ]

    for col in required_cols:
        if col not in df.columns:
            logger.warning(f"‚ö†Ô∏è  Columna {col} no encontrada. Creando con valor 0.")
            df[col] = 0

    # Estad√≠sticas de cantidad
    qty_stats = df[ColumnNames.CANTIDAD_APU].describe()
    logger.debug(f"Estad√≠sticas de cantidad:\n{qty_stats}")

    if df[ColumnNames.CANTIDAD_APU].max() > thresholds.max_quantity:
        logger.warning(
            f"‚ö†Ô∏è  Cantidad extremadamente alta detectada: "
            f"{df[ColumnNames.CANTIDAD_APU].max():,.2f}"
        )

    # Estad√≠sticas de precio
    precio_stats = df[ColumnNames.VR_UNITARIO_INSUMO].describe()
    logger.debug(f"Estad√≠sticas de precio unitario:\n{precio_stats}")

    # Calcular costo usando numpy.where para eficiencia
    costo_insumo = np.where(
        df[ColumnNames.VR_UNITARIO_INSUMO].notna(),
        df[ColumnNames.CANTIDAD_APU] * df[ColumnNames.VR_UNITARIO_INSUMO],
        df[ColumnNames.VALOR_TOTAL_APU],
    )

    df[ColumnNames.COSTO_INSUMO_EN_APU] = pd.Series(costo_insumo).fillna(0)

    # Validar costos an√≥malos
    costo_max = df[ColumnNames.COSTO_INSUMO_EN_APU].max()

    if costo_max > thresholds.max_cost_per_item:
        logger.error(
            f"‚ùå COSTO AN√ìMALO DETECTADO: ${costo_max:,.2f} "
            f"(l√≠mite: ${thresholds.max_cost_per_item:,.2f})"
        )

        anomalies = df[
            df[ColumnNames.COSTO_INSUMO_EN_APU] > thresholds.max_cost_per_item
        ].head(10)

        logger.error(
            "Registros an√≥malos:\n%s",
            anomalies[
                [
                    ColumnNames.CODIGO_APU,
                    ColumnNames.DESCRIPCION_INSUMO,
                    ColumnNames.CANTIDAD_APU,
                    ColumnNames.VR_UNITARIO_INSUMO,
                    ColumnNames.COSTO_INSUMO_EN_APU,
                ]
            ],
        )

    # Calcular valor unitario final con fallbacks
    df[ColumnNames.VR_UNITARIO_FINAL] = df[ColumnNames.VR_UNITARIO_INSUMO].fillna(
        df.get(ColumnNames.PRECIO_UNIT_APU, 0)
    )

    # Fallback adicional: calcular desde costo total
    cantidad_safe = df[ColumnNames.CANTIDAD_APU].replace(0, 1)
    df[ColumnNames.VR_UNITARIO_FINAL] = df[ColumnNames.VR_UNITARIO_FINAL].fillna(
        df[ColumnNames.COSTO_INSUMO_EN_APU] / cantidad_safe
    )

    logger.info(f"‚úÖ Costos de insumos calculados: {len(df)} registros")

    return df


def build_processed_apus_dataframe(
    df_apu_costos: pd.DataFrame,
    df_apus_raw: pd.DataFrame,
    df_tiempo: pd.DataFrame,
    df_rendimiento: pd.DataFrame,
) -> pd.DataFrame:
    """Construye el DataFrame de APUs procesados consolidado."""
    logger.info("üì¶ Construyendo DataFrame de APUs procesados...")

    # Extraer descripciones √∫nicas de APUs
    df_apu_descriptions = df_apus_raw[
        [ColumnNames.CODIGO_APU, ColumnNames.DESCRIPCION_APU, ColumnNames.UNIDAD_APU]
    ].drop_duplicates(subset=[ColumnNames.CODIGO_APU])

    # Merge costos + descripciones
    df_processed = pd.merge(
        df_apu_costos, df_apu_descriptions, on=ColumnNames.CODIGO_APU, how="left"
    )

    # Merge tiempo
    df_processed = pd.merge(df_processed, df_tiempo, on=ColumnNames.CODIGO_APU, how="left")

    # Merge rendimiento
    if not df_rendimiento.empty:
        df_processed = pd.merge(
            df_processed, df_rendimiento, on=ColumnNames.CODIGO_APU, how="left"
        )

    # Renombrar unidad
    if ColumnNames.UNIDAD_APU in df_processed.columns:
        df_processed = df_processed.rename(columns={ColumnNames.UNIDAD_APU: "UNIDAD"})

    # Normalizar descripci√≥n
    if ColumnNames.DESCRIPCION_APU in df_processed.columns:
        df_processed["DESC_NORMALIZED"] = normalize_text_series(
            df_processed[ColumnNames.DESCRIPCION_APU]
        )

    # Dividir descripci√≥n
    df_processed = group_and_split_description(df_processed)

    logger.info(f"‚úÖ APUs procesados consolidados: {len(df_processed)} registros")

    return df_processed


def synchronize_data_sources(
    df_merged: pd.DataFrame, df_final: pd.DataFrame
) -> pd.DataFrame:
    """Sincroniza fuentes de datos para consistencia.

    Filtra df_merged para incluir solo APUs presentes en el presupuesto final.
    """
    logger.info("üîÑ Sincronizando fuentes de datos...")

    codigos_apu_validos = df_final[ColumnNames.CODIGO_APU].unique()

    insumos_antes = len(df_merged)
    df_merged_sync = df_merged[
        df_merged[ColumnNames.CODIGO_APU].isin(codigos_apu_validos)
    ].copy()
    insumos_despues = len(df_merged_sync)

    logger.info(
        f"‚úÖ Sincronizaci√≥n completada. Insumos: {insumos_antes} -> {insumos_despues}"
    )

    return df_merged_sync


def build_output_dictionary(
    df_final: pd.DataFrame,
    df_insumos: pd.DataFrame,
    df_merged: pd.DataFrame,
    df_apus_raw: pd.DataFrame,
    df_processed_apus: pd.DataFrame,
) -> dict:
    """Construye el diccionario de salida estandarizado."""
    logger.info("üì¶ Construyendo diccionario de salida...")

    # Renombrar columnas en df_merged para salida
    df_merged_output = df_merged.rename(
        columns={
            ColumnNames.VR_UNITARIO_FINAL: "VR_UNITARIO",
            ColumnNames.COSTO_INSUMO_EN_APU: "VR_TOTAL",
        }
    )

    # Construir diccionario de insumos por grupo
    insumos_dict = {}
    for name, group in df_insumos.groupby(ColumnNames.GRUPO_INSUMO):
        if name and isinstance(name, str):
            insumos_dict[name.strip()] = (
                group[[ColumnNames.DESCRIPCION_INSUMO, ColumnNames.VR_UNITARIO_INSUMO]]
                .dropna()
                .to_dict("records")
            )

    # Construir diccionario final
    result_dict = {
        "presupuesto": df_final.to_dict("records"),
        "insumos": insumos_dict,
        "apus_detail": df_merged_output.to_dict("records"),
        "all_apus": df_apus_raw.to_dict("records"),
        "raw_insumos_df": df_insumos,
        "processed_apus": df_processed_apus.to_dict("records"),
    }

    logger.info("‚úÖ Diccionario de salida construido")

    return result_dict


# ==================== L√ìGICA PRINCIPAL ====================


def _do_processing(
    presupuesto_path: str, apus_path: str, insumos_path: str, config: dict
) -> dict:
    """
    L√≥gica central para procesar, unificar y calcular todos los datos.
    Procesa archivos de entrada y genera archivos JSON de salida necesarios
    para an√°lisis posteriores y generaci√≥n de embeddings.
    Args:
    presupuesto_path: Ruta al archivo de presupuesto.
    apus_path: Ruta al archivo de APUs.
    insumos_path: Ruta al archivo de insumos.
    config: Configuraci√≥n de la aplicaci√≥n.
    Returns:
    Diccionario con datos procesados o error.
    Genera archivos:
    - data/processed_apus.json: APUs procesados para embeddings
    - data/presupuesto_final.json: Presupuesto consolidado
    - data/insumos_detalle.json: Detalle de insumos por APU
    """
    logger.info("=" * 80)
    logger.info("üöÄ Iniciando procesamiento de archivos con patr√≥n pipeline...")
    logger.info("=" * 80)

    # ============================================================
    # 1. CONFIGURACI√ìN DE UMBRALES Y RUTAS DE SALIDA
    # ============================================================
    thresholds = ProcessingThresholds()
    if "processing_thresholds" in config:
        custom_thresholds = config["processing_thresholds"]
        for key, value in custom_thresholds.items():
            if hasattr(thresholds, key):
                setattr(thresholds, key, value)
                logger.debug(f"Umbral configurado: {key} = {value}")

    # Configurar rutas de salida desde config o usar defaults
    output_dir = Path(config.get("output_dir", "data"))
    output_files = {
        "processed_apus": output_dir
        / config.get("processed_apus_file", "processed_apus.json"),
        "presupuesto_final": output_dir
        / config.get("presupuesto_final_file", "presupuesto_final.json"),
        "insumos_detalle": output_dir
        / config.get("insumos_detalle_file", "insumos_detalle.json"),
    }

    try:
        # ============================================================
        # 2. EJECUTAR PIPELINE DE PROCESAMIENTO
        # ============================================================
        pipeline = ProcessingPipeline(
            [
                LoadDataStep(config, thresholds),
                MergeDataStep(thresholds),
                CalculateCostsStep(config, thresholds),
                FinalMergeStep(thresholds),
                BuildOutputStep(),
            ]
        )

        initial_context = {
            "presupuesto_path": presupuesto_path,
            "apus_path": apus_path,
            "insumos_path": insumos_path,
        }

        logger.info("üîÑ Ejecutando pipeline de procesamiento...")
        final_context = pipeline.run(initial_context)

        final_result = final_context.get("final_result")
        if not final_result:
            error_msg = "Pipeline no produjo resultado final"
            logger.error(f"‚ùå {error_msg}")
            return {"error": error_msg}

        if "error" in final_result:
            logger.error(f"‚ùå Error en resultado final: {final_result['error']}")
            return final_result

        # ============================================================
        # 3. VALIDAR DATOS ANTES DE GUARDAR
        # ============================================================
        logger.info("üîç Validando datos procesados antes de guardar...")
        validation_results = _validate_output_data(final_result)
        if not validation_results["is_valid"]:
            logger.error(f"‚ùå Validaci√≥n de datos fall√≥: {validation_results['errors']}")
            # Continuar pero registrar advertencias
            for warning in validation_results["warnings"]:
                logger.warning(f"‚ö†Ô∏è {warning}")

        # ============================================================
        # 4. GUARDAR ARCHIVOS JSON DE SALIDA
        # ============================================================
        logger.info("=" * 80)
        logger.info("üíæ Guardando archivos JSON de salida...")
        logger.info("=" * 80)

        saved_files = _save_output_files(final_result, output_files, config)

        # ============================================================
        # 5. REGISTRAR ESTAD√çSTICAS Y COMPLETAR
        # ============================================================
        _log_processing_statistics(final_result, saved_files)

        logger.info("=" * 80)
        logger.info("üéâ PROCESAMIENTO COMPLETADO EXITOSAMENTE")
        logger.info("=" * 80)

        # A√±adir informaci√≥n de archivos guardados al resultado
        final_result["output_files"] = {
            name: str(path) for name, path in saved_files.items()
        }
        final_result["processing_timestamp"] = pd.Timestamp.now().isoformat()

        return final_result

    except (ValueError, pd.errors.MergeError) as e:
        error_msg = f"Error en el pipeline: {e}"
        logger.error(f"‚ùå {error_msg}", exc_info=True)
        # Diagn√≥stico espec√≠fico para errores de APUs
        if "apus" in str(e).lower():
            logger.info("=" * 80)
            logger.info("üîç Ejecutando diagn√≥stico del archivo de APUs...")
            logger.info("=" * 80)
            try:
                diagnostic = APUFileDiagnostic(apus_path)
                diagnostic.diagnose()
            except Exception as diag_e:
                logger.error(f"‚ùå Error durante el diagn√≥stico: {diag_e}", exc_info=True)
        logger.info("=" * 80)
        return {"error": error_msg}
    except Exception as e:
        error_msg = f"Error cr√≠tico en el pipeline: {e}"
        logger.error(f"‚ùå {error_msg}", exc_info=True)
        return {"error": error_msg}


# ============================================================
# FUNCIONES AUXILIARES PARA GUARDADO Y VALIDACI√ìN
# ============================================================


def _validate_output_data(result: dict) -> dict:
    """
    Valida la integridad de los datos de salida.
    Args:
    result: Diccionario con los datos procesados
    Returns:
    Diccionario con resultado de validaci√≥n
    """
    validation = {"is_valid": True, "errors": [], "warnings": []}

    # Validar presencia de secciones clave
    required_keys = ["presupuesto", "processed_apus", "apus_detail"]
    for key in required_keys:
        if key not in result:
            validation["errors"].append(f"Falta secci√≥n requerida: {key}")
            validation["is_valid"] = False
        elif not result[key]:
            validation["warnings"].append(f"Secci√≥n vac√≠a: {key}")

    # Validar que processed_apus tenga datos
    if "processed_apus" in result:
        processed_count = len(result["processed_apus"])
        if processed_count == 0:
            validation["errors"].append("processed_apus est√° vac√≠o")
            validation["is_valid"] = False
        else:
            logger.info(f"‚úÖ {processed_count} APUs procesados listos para guardar")

    # Validar estructura de processed_apus
    if "processed_apus" in result and result["processed_apus"]:
        sample_apu = result["processed_apus"][0]
        expected_fields = [
            ColumnNames.CODIGO_APU,
            ColumnNames.DESCRIPCION_APU,
            ColumnNames.VALOR_CONSTRUCCION_UN,
        ]
        missing_fields = [field for field in expected_fields if field not in sample_apu]
        if missing_fields:
            validation["warnings"].append(
                f"Campos faltantes en processed_apus: {missing_fields}"
            )

    return validation


def _save_output_files(result: dict, output_files: dict, config: dict) -> dict:
    """
    Guarda los archivos JSON de salida de forma robusta.
    Args:
    result: Diccionario con los datos procesados
    output_files: Diccionario con rutas de archivos a guardar
    config: Configuraci√≥n de la aplicaci√≥n
    Returns:
    Diccionario con rutas de archivos guardados exitosamente
    """
    import json

    from .utils import sanitize_for_json

    saved_files = {}

    # Asegurar que el directorio de salida existe
    for file_path in output_files.values():
        file_path.parent.mkdir(parents=True, exist_ok=True)
        logger.debug(f"Directorio verificado: {file_path.parent}")

    # ============================================================
    # 1. GUARDAR PROCESSED_APUS.JSON (CR√çTICO PARA EMBEDDINGS)
    # ============================================================
    if "processed_apus" in result and result["processed_apus"]:
        try:
            processed_apus_path = output_files["processed_apus"]
            logger.info(f"üíæ Guardando APUs procesados en: {processed_apus_path}")
            # Sanitizar datos para JSON
            processed_data = sanitize_for_json(result["processed_apus"])
            # Guardar con formato legible
            with open(processed_apus_path, "w", encoding="utf-8") as f:
                json.dump(processed_data, f, indent=2, ensure_ascii=False)
            file_size = processed_apus_path.stat().st_size
            logger.info(
                f"‚úÖ processed_apus.json guardado exitosamente "
                f"({len(processed_data)} registros, {file_size:,} bytes)"
            )
            saved_files["processed_apus"] = processed_apus_path
        except Exception as e:
            logger.error(f"‚ùå Error guardando processed_apus.json: {e}", exc_info=True)
            # Este es cr√≠tico, pero no falla el proceso completo
    else:
        logger.warning("‚ö†Ô∏è No hay datos de processed_apus para guardar")

    # ============================================================
    # 2. GUARDAR PRESUPUESTO_FINAL.JSON
    # ============================================================
    if "presupuesto" in result and result["presupuesto"]:
        try:
            presupuesto_path = output_files["presupuesto_final"]
            logger.info(f"üíæ Guardando presupuesto final en: {presupuesto_path}")
            presupuesto_data = sanitize_for_json(result["presupuesto"])
            with open(presupuesto_path, "w", encoding="utf-8") as f:
                json.dump(presupuesto_data, f, indent=2, ensure_ascii=False)
            file_size = presupuesto_path.stat().st_size
            logger.info(
                f"‚úÖ presupuesto_final.json guardado "
                f"({len(presupuesto_data)} registros, {file_size:,} bytes)"
            )
            saved_files["presupuesto_final"] = presupuesto_path
        except Exception as e:
            logger.error(f"‚ùå Error guardando presupuesto_final.json: {e}", exc_info=True)

    # ============================================================
    # 3. GUARDAR INSUMOS_DETALLE.JSON
    # ============================================================
    if "apus_detail" in result and result["apus_detail"]:
        try:
            insumos_path = output_files["insumos_detalle"]
            logger.info(f"üíæ Guardando detalle de insumos en: {insumos_path}")
            insumos_data = sanitize_for_json(result["apus_detail"])
            with open(insumos_path, "w", encoding="utf-8") as f:
                json.dump(insumos_data, f, indent=2, ensure_ascii=False)
            file_size = insumos_path.stat().st_size
            logger.info(
                f"‚úÖ insumos_detalle.json guardado "
                f"({len(insumos_data)} registros, {file_size:,} bytes)"
            )
            saved_files["insumos_detalle"] = insumos_path
        except Exception as e:
            logger.error(f"‚ùå Error guardando insumos_detalle.json: {e}", exc_info=True)

    # ============================================================
    # 4. GUARDAR ARCHIVO COMPLETO DE RESPALDO (OPCIONAL)
    # ============================================================
    if config.get("save_full_backup", False):
        try:
            backup_path = output_files.get(
                "full_backup", Path(config.get("output_dir", "data")) / "full_backup.json"
            )
            logger.info(f"üíæ Guardando respaldo completo en: {backup_path}")
            # Crear copia del resultado sin DataFrames crudos
            backup_data = {
                k: v
                for k, v in result.items()
                if k not in ["raw_insumos_df"] and not isinstance(v, pd.DataFrame)
            }
            backup_data_clean = sanitize_for_json(backup_data)
            with open(backup_path, "w", encoding="utf-8") as f:
                json.dump(backup_data_clean, f, indent=2, ensure_ascii=False)
            file_size = backup_path.stat().st_size
            logger.info(f"‚úÖ Respaldo completo guardado ({file_size:,} bytes)")
            saved_files["full_backup"] = backup_path
        except Exception as e:
            logger.error(f"‚ö†Ô∏è Error guardando respaldo completo: {e}", exc_info=True)
            # Este es opcional, no afecta el proceso

    return saved_files


def _log_processing_statistics(result: dict, saved_files: dict) -> None:
    """
    Registra estad√≠sticas del procesamiento completado.
    Args:
    result: Diccionario con los datos procesados
    saved_files: Diccionario con archivos guardados
    """
    logger.info("=" * 80)
    logger.info("üìä ESTAD√çSTICAS DEL PROCESAMIENTO")
    logger.info("=" * 80)

    # Estad√≠sticas de datos
    stats = []
    if "presupuesto" in result:
        count = len(result["presupuesto"])
        stats.append(f" üìã APUs en Presupuesto: {count:,}")
        # Calcular totales si existe la columna
        if result["presupuesto"]:
            try:
                total_construccion = sum(
                    item.get(ColumnNames.VALOR_CONSTRUCCION_TOTAL, 0)
                    for item in result["presupuesto"]
                )
                stats.append(f" üí∞ Costo Total Construcci√≥n: ${total_construccion:,.2f}")
            except Exception:
                pass

    if "processed_apus" in result:
        count = len(result["processed_apus"])
        stats.append(f" üèóÔ∏è APUs Procesados: {count:,}")

    if "apus_detail" in result:
        count = len(result["apus_detail"])
        stats.append(f" üîß Insumos Detallados: {count:,}")

    if "insumos" in result:
        grupos = len(result["insumos"])
        total_insumos = sum(len(items) for items in result["insumos"].values())
        stats.append(f" üì¶ Grupos de Insumos: {grupos}")
        stats.append(f" üì¶ Total Insumos: {total_insumos:,}")

    for stat in stats:
        logger.info(stat)

    # Archivos guardados
    logger.info("")
    logger.info("üìÅ ARCHIVOS GENERADOS:")
    if saved_files:
        for name, path in saved_files.items():
            file_size = path.stat().st_size if path.exists() else 0
            size_mb = file_size / (1024 * 1024)
            logger.info(f" ‚úÖ {name}: {path} ({size_mb:.2f} MB)")
    else:
        logger.warning(" ‚ö†Ô∏è No se guardaron archivos de salida")

    logger.info("=" * 80)


# ============================================================
# FUNCI√ìN PRINCIPAL (SIN CAMBIOS)
# ============================================================


def process_all_files(
    presupuesto_path: str, apus_path: str, insumos_path: str, config: dict
) -> dict:
    """
    Orquesta el procesamiento completo de los archivos de entrada.
    Args:
    presupuesto_path: Ruta al archivo del presupuesto.
    apus_path: Ruta al archivo de APUs.
    insumos_path: Ruta al archivo de insumos.
    config: Configuraci√≥n de la aplicaci√≥n.
    Returns:
    Diccionario con los datos procesados o informaci√≥n de error.
    """
    return _do_processing(presupuesto_path, apus_path, insumos_path, config)
