"""
M√≥dulo para el parseo crudo de reportes de An√°lisis de Precios Unitarios (APU).

Este m√≥dulo proporciona una clase `ReportParserCrudo` que implementa una m√°quina
de estados robusta para procesar, l√≠nea por l√≠nea, archivos de APU con un
formato semi-estructurado. Su objetivo principal es identificar y extraer los
registros de insumos asociados a cada APU, manteniendo el contexto del APU
al que pertenecen.

V2: Incorpora validaci√≥n topol√≥gica y m√©tricas estructurales para garantizar
la integridad matem√°tica del parsing.
"""

import hashlib
import logging
import re
from abc import ABC, abstractmethod
from collections import Counter
from dataclasses import dataclass, field
from pathlib import Path
from typing import Any, Dict, List, Optional, Tuple, Union

from lark import Lark
from lark.exceptions import (
    LarkError,
    UnexpectedCharacters,
    UnexpectedEOF,
    UnexpectedInput,
    UnexpectedToken,
)

from .utils import clean_apu_code

logger = logging.getLogger(__name__)


@dataclass
class LineValidationResult:
    """Resultado detallado de la validaci√≥n de una l√≠nea."""

    is_valid: bool
    reason: str = ""
    fields_count: int = 0
    has_numeric_fields: bool = False
    validation_layer: str = ""  # "basic", "lark", "both", "full_homeomorphism"
    lark_tree: Any = None  # √Årbol de parsing si fue exitoso


@dataclass
class ValidationStats:
    """Estad√≠sticas detalladas de validaci√≥n."""

    total_evaluated: int = 0
    passed_basic: int = 0
    passed_lark: int = 0
    passed_both: int = 0

    failed_basic_fields: int = 0
    failed_basic_numeric: int = 0
    failed_basic_subtotal: int = 0
    failed_basic_junk: int = 0

    failed_lark_parse: int = 0
    failed_lark_unexpected_input: int = 0
    failed_lark_unexpected_chars: int = 0

    cached_parses: int = 0

    failed_samples: List[Dict[str, Any]] = field(default_factory=list)


class ParserError(Exception):
    """Excepci√≥n base para errores ocurridos durante el parseo."""

    pass


class FileReadError(ParserError):
    """Indica un error al leer el archivo de entrada."""

    pass


class ParseStrategyError(ParserError):
    """Indica un error en la l√≥gica de la estrategia de parseo."""

    pass


@dataclass
class APUContext:
    """
    Almacena el contexto de un APU mientras se procesan sus l√≠neas.

    Attributes:
        apu_code: El c√≥digo (ITEM) del APU.
        apu_desc: La descripci√≥n del APU.
        apu_unit: La unidad de medida del APU.
        source_line: El n√∫mero de l√≠nea donde se detect√≥ el APU.
    """

    apu_code: str
    apu_desc: str
    apu_unit: str
    source_line: int
    default_unit: str = "UND"

    def __post_init__(self):
        """Realiza validaci√≥n y normalizaci√≥n despu√©s de la inicializaci√≥n."""
        self.apu_code = self.apu_code.strip() if self.apu_code else ""
        self.apu_desc = self.apu_desc.strip() if self.apu_desc else ""
        self.apu_unit = (
            self.apu_unit.strip().upper() if self.apu_unit else self.default_unit
        )
        if not self.apu_code:
            raise ValueError("El c√≥digo del APU no puede estar vac√≠o.")

    @property
    def is_valid(self) -> bool:
        """Comprueba si el contexto del APU es v√°lido."""
        return bool(self.apu_code and len(self.apu_code) >= 2)


@dataclass
class ParserContext:
    """
    Mantiene el estado mutable del parseo (La Pir√°mide en construcci√≥n).

    Act√∫a como la 'Memoria de Corto Plazo' del sistema.
    """

    current_apu: Optional[APUContext] = None  # El 'Padre' actual (Nivel 2)
    current_category: str = "INDEFINIDO"
    current_line_number: int = 0
    raw_records: List[Dict[str, Any]] = field(default_factory=list)
    errors: List[str] = field(default_factory=list)

    # Telemetr√≠a interna
    stats: Counter = field(default_factory=Counter)

    def has_active_parent(self) -> bool:
        """Valida la l√≥gica piramidal: ¬øExiste un nodo padre activo?"""
        return self.current_apu is not None


class LineHandler(ABC):
    """
    Unidad de Trabajo Discreta.

    Patr√≥n: Chain of Responsibility.
    """

    def __init__(self, parent_parser):
        self.parent = parent_parser  # Acceso a utilidades (Lark, Regex)

    @abstractmethod
    def can_handle(self, line: str, next_line: Optional[str] = None) -> bool:
        """Determina si este handler es responsable de la l√≠nea."""
        pass

    @abstractmethod
    def handle(
        self, line: str, context: ParserContext, next_line: Optional[str] = None
    ) -> bool:
        """
        Procesa la l√≠nea y actualiza el contexto (mutaci√≥n de estado).

        Aqu√≠ se aplica la l√≥gica de negocio.
        Returns: True si debe avanzar una l√≠nea extra (por encabezados multil√≠nea), False si no.
        """
        pass


class JunkHandler(LineHandler):
    """Detecta y descarta basura, separadores o l√≠neas decorativas."""

    def can_handle(self, line: str, next_line: Optional[str] = None) -> bool:
        return self.parent._is_junk_line(line.upper())

    def handle(
        self, line: str, context: ParserContext, next_line: Optional[str] = None
    ) -> bool:
        context.stats["junk_lines_skipped"] += 1
        return False


class HeaderHandler(LineHandler):
    """Detecta encabezados de APU (Nivel 2)."""

    def can_handle(self, line: str, next_line: Optional[str] = None) -> bool:
        line_upper = line.upper()
        is_header_line = "UNIDAD:" in line_upper
        is_item_line_next = next_line is not None and "ITEM:" in next_line.upper()
        return is_header_line and is_item_line_next

    def handle(
        self, line: str, context: ParserContext, next_line: Optional[str] = None
    ) -> bool:
        header_line = line
        item_line = next_line.strip() if next_line else ""

        try:
            apu_context_result = self.parent._extract_apu_header(
                header_line, item_line, context.current_line_number
            )

            if apu_context_result is not None:
                context.current_apu = apu_context_result
                context.current_category = "INDEFINIDO"
                context.stats["apus_detected"] += 1

                logger.info(
                    f"‚úì APU detectado [l√≠nea {context.current_line_number}]: "
                    f"{context.current_apu.apu_code} - "
                    f"{context.current_apu.apu_desc[:50]}"
                )
            else:
                logger.warning(
                    f"Encabezado APU inv√°lido en l√≠nea {context.current_line_number}"
                )
        except Exception as e:
            logger.warning(
                f"‚úó Fallo al parsear encabezado de APU en l√≠nea {context.current_line_number}: {e}"
            )
            context.current_apu = None

        return True  # Consume la siguiente l√≠nea (ITEM)


class CategoryHandler(LineHandler):
    """Detecta cambios de categor√≠a."""

    def can_handle(self, line: str, next_line: Optional[str] = None) -> bool:
        return self.parent._detect_category(line.upper()) is not None

    def handle(
        self, line: str, context: ParserContext, next_line: Optional[str] = None
    ) -> bool:
        new_category = self.parent._detect_category(line.upper())
        if new_category:
            context.current_category = new_category
            context.stats[f"category_{new_category}"] += 1
            logger.debug(f"  ‚Üí Categor√≠a: {new_category}")
        return False


class InsumoHandler(LineHandler):
    """Detecta y procesa l√≠neas de insumos (Nivel 3)."""

    def can_handle(self, line: str, next_line: Optional[str] = None) -> bool:
        # Validaci√≥n ligera preliminar: debe tener al menos un separador y alg√∫n n√∫mero
        return ";" in line and any(c.isdigit() for c in line)

    def handle(
        self, line: str, context: ParserContext, next_line: Optional[str] = None
    ) -> bool:
        # 1. VALIDACI√ìN PIRAMIDAL (L√≥gica Estructural)
        if not context.has_active_parent():
            # ERROR CR√çTICO DE NEGOCIO: Recurso Hu√©rfano
            logger.warning(
                f"‚ö†Ô∏è Recurso Hu√©rfano detectado en l√≠nea {context.current_line_number}. Ignorando."
            )
            context.stats["orphans_discarded"] += 1
            return False

        fields = [f.strip() for f in line.split(";")]
        validation_result = self.parent._validate_insumo_line(line, fields)

        if validation_result.is_valid:
            record = self.parent._build_insumo_record(
                context.current_apu,
                context.current_category,
                line,
                context.current_line_number,
                validation_result,
                fields,  # Pasar fields para evitar re-split
            )
            context.raw_records.append(record)
            context.stats["insumos_extracted"] += 1

            if self.parent.debug_mode:
                logger.debug(
                    f"  ‚úì Insumo v√°lido [l√≠nea {context.current_line_number}] "
                    f"[{validation_result.validation_layer}]: "
                    f"{fields[0][:40]}... ({validation_result.fields_count} campos)"
                )
        else:
            context.stats["lines_ignored_in_context"] += 1
            if self.parent.debug_mode:
                logger.debug(
                    f"  ‚úó Rechazada [l√≠nea {context.current_line_number}]: {validation_result.reason}"
                )
        return False


class ReportParserCrudo:
    """
    Parser robusto tipo m√°quina de estados para archivos APU semi-estructurados.

    ROBUSTECIDO: Constantes centralizadas, l√≠mites de recursos, manejo defensivo.
    V2: Integraci√≥n de an√°lisis topol√≥gico.
    """

    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    # CONSTANTES DE CLASE
    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

    # L√≠mites de recursos
    _MAX_CACHE_SIZE: int = 50000
    _MAX_FAILED_SAMPLES: int = 20
    _MAX_LINE_LENGTH: int = 5000
    _MIN_FIELDS_FOR_INSUMO: int = 5
    _MIN_LINE_LENGTH: int = 3

    # Configuraci√≥n de validaci√≥n
    _CACHE_KEY_MAX_LENGTH: int = 2000

    CATEGORY_KEYWORDS = {
        "MATERIALES": {"MATERIALES", "MATERIAL", "MAT.", "INSUMOS"},
        "MANO DE OBRA": {
            "MANO DE OBRA",
            "MANO OBRA",
            "M.O.",
            "MO",
            "PERSONAL",
            "OBRERO",
        },
        "EQUIPO": {"EQUIPO", "EQUIPOS", "MAQUINARIA", "MAQ."},
        "TRANSPORTE": {"TRANSPORTE", "TRANSPORTES", "TRANS.", "ACARREO"},
        "HERRAMIENTA": {"HERRAMIENTA", "HERRAMIENTAS", "HERR.", "UTILES"},
        "OTROS": {"OTROS", "OTRO", "VARIOS", "ADICIONALES"},
    }

    JUNK_KEYWORDS = frozenset(
        {  # ROBUSTECIDO: frozenset para inmutabilidad y rendimiento
            "SUBTOTAL",
            "COSTO DIRECTO",
            "DESCRIPCION",
            "IMPUESTOS",
            "POLIZAS",
            "TOTAL",
            "IVA",
            "AIU",
        }
    )

    # Patrones pre-compilados para rendimiento
    _NUMERIC_PATTERN = re.compile(r"\d+[.,]\d+|\d+")
    _DECORATIVE_PATTERN = re.compile(r"^[=\-_\s*]+$")
    _UNIT_PATTERN = re.compile(r"UNIDAD:\s*(\S+)", re.IGNORECASE)
    _ITEM_PATTERN = re.compile(r"ITEM:\s*([\S,]+)", re.IGNORECASE)

    def __init__(
        self,
        file_path: Union[str, Path],
        profile: dict,
        config: Optional[Dict] = None,
        telemetry: Optional[Any] = None,
    ):
        """Inicializa el parser con validaci√≥n exhaustiva de par√°metros."""
        # ROBUSTECIDO: Conversi√≥n segura de file_path
        if file_path is None:
            raise ValueError("file_path no puede ser None")
        self.file_path = (
            Path(file_path) if not isinstance(file_path, Path) else file_path
        )

        # ROBUSTECIDO: Validaci√≥n de tipos para profile y config
        if profile is not None and not isinstance(profile, dict):
            logger.warning(
                f"profile no es dict ({type(profile).__name__}), usando vac√≠o"
            )
            profile = {}
        if config is not None and not isinstance(config, dict):
            logger.warning(f"config no es dict ({type(config).__name__}), usando vac√≠o")
            config = {}

        self.profile = profile or {}
        self.config = config or {}
        self.telemetry = telemetry

        # Validar archivo antes de continuar
        self._validate_file_path()

        # ROBUSTECIDO: Inicializaci√≥n segura del parser Lark
        self.lark_parser: Optional[Lark] = None
        self._parse_cache: Dict[str, Tuple[bool, Any]] = {}
        self.validation_stats = ValidationStats()

        try:
            from .apu_processor import APU_GRAMMAR

            self.lark_parser = self._initialize_lark_parser(APU_GRAMMAR)
        except ImportError as ie:
            logger.error(
                f"No se pudo importar APU_GRAMMAR: {ie}\n"
                f"  El parser funcionar√° sin validaci√≥n Lark"
            )
        except Exception as e:
            logger.error(
                f"Error inicializando parser Lark: {e}\n"
                f"  El parser funcionar√° sin validaci√≥n Lark"
            )

        # Estado del parser
        self.raw_records: List[Dict[str, Any]] = []
        self.stats: Counter = Counter()
        self._parsed: bool = False

        # ROBUSTECIDO: Modo debug desde config
        self.debug_mode = self.config.get("debug_mode", False)

        logger.debug(
            f"ReportParserCrudo inicializado:\n"
            f"  Archivo: {self.file_path.name}\n"
            f"  Lark parser: {'‚úì' if self.lark_parser else '‚úó'}\n"
            f"  Debug mode: {self.debug_mode}"
        )

    def _initialize_handlers(self) -> List[LineHandler]:
        """Fabrica la cadena de responsabilidad en orden de prioridad."""
        return [
            JunkHandler(self),  # 1. Descartar basura obvia
            HeaderHandler(self),  # 2. Detectar cambios de estructura (Nuevos APUs)
            CategoryHandler(self),  # 3. Detectar cambios de categor√≠a
            InsumoHandler(self),  # 4. Procesar datos (Hojas del √°rbol)
        ]

    def _initialize_lark_parser(self, grammar: Optional[str] = None) -> Optional[Lark]:
        """Inicializa el parser Lark con la MISMA gram√°tica que usa APUProcessor."""
        try:
            from lark import Lark
            from lark.exceptions import ConfigurationError, GrammarError
        except ImportError as ie:
            logger.error(
                f"No se pudo importar Lark: {ie}\n  Ejecute: pip install lark"
            )
            return None

        # ROBUSTECIDO: Obtener gram√°tica si no se proporcion√≥
        if grammar is None:
            try:
                from .apu_processor import APU_GRAMMAR

                grammar = APU_GRAMMAR
            except ImportError:
                logger.error("No se pudo importar APU_GRAMMAR desde apu_processor")
                return None

        # ROBUSTECIDO: Validar que la gram√°tica no est√° vac√≠a
        if not grammar or not isinstance(grammar, str) or not grammar.strip():
            logger.error("La gram√°tica proporcionada est√° vac√≠a o no es v√°lida")
            return None

        try:
            # ROBUSTECIDO: Configuraci√≥n id√©ntica a APUProcessor para coherencia
            parser_config = {
                "start": "line",
                "parser": "lalr",
                "maybe_placeholders": False,
                "propagate_positions": True,  # V2: Necesario para validaci√≥n topol√≥gica
                "cache": True,
            }

            parser = Lark(grammar, **parser_config)
            return parser

        except GrammarError as ge:
            logger.error(
                f"Error de gram√°tica Lark:\n"
                f"  Mensaje: {ge}\n"
                f"  Revise que APU_GRAMMAR sea v√°lida"
            )
            return None

        except ConfigurationError as ce:
            logger.error(f"Error de configuraci√≥n Lark: {ce}")
            return None

        except Exception as e:
            logger.error(f"Error inesperado inicializando parser Lark: {e}")
            return None

    def _validate_with_lark(
        self, line: str, use_cache: bool = True
    ) -> Tuple[bool, Optional[Any], str]:
        """
        Valida una l√≠nea usando el parser Lark con optimizaci√≥n topol√≥gica.

        Refuerzo: Prefiltrado estricto, cache sem√°ntica y manejo jer√°rquico de errores.
        """
        # === PRECONDICIONES TOPOL√ìGICAS ===
        if self.lark_parser is None:
            return (True, None, "Lark no disponible - validaci√≥n omitida")

        if not line or not isinstance(line, str):
            return (False, None, "L√≠nea vac√≠a o tipo inv√°lido")

        line_clean = line.strip()
        line_len = len(line_clean)

        if line_len > self._MAX_LINE_LENGTH:
            return (
                False,
                None,
                f"L√≠nea excede l√≠mite topol√≥gico: {line_len} > {self._MAX_LINE_LENGTH}",
            )
        if line_len < self._MIN_LINE_LENGTH:
            return (
                False,
                None,
                f"L√≠nea insuficiente topol√≥gicamente: {line_len} < {self._MIN_LINE_LENGTH}",
            )

        # === CACHE SEM√ÅNTICO ===
        cache_key = (
            self._compute_semantic_cache_key(line_clean) if use_cache else None
        )

        if use_cache and cache_key in self._parse_cache:
            self.validation_stats.cached_parses += 1
            cached_result = self._parse_cache[cache_key]

            if isinstance(cached_result, tuple) and len(cached_result) == 2:
                is_valid, tree = cached_result
                # Solo revalidar si fue exitoso pero el √°rbol es inv√°lido (invariante roto)
                if is_valid and tree is not None and not self._is_valid_tree(tree):
                    del self._parse_cache[cache_key]
                else:
                    reason = "" if is_valid else "Fall√≥ previamente (cache v√°lido)"
                    return (is_valid, tree, reason)

        # === VALIDACI√ìN DE CONECTIVIDAD ESTRUCTURAL ===
        if not self._has_minimal_structural_connectivity(line_clean):
            if use_cache and cache_key:
                self._cache_result(cache_key, False, None)
            return (False, None, "Falta conectividad estructural m√≠nima")

        # === PARSING CON MANEJO JER√ÅRQUICO DE ERRORES ===
        try:
            tree = self.lark_parser.parse(line_clean)

            if not self._validate_tree_homotopy(tree):
                if use_cache and cache_key:
                    self._cache_result(cache_key, False, None)
                return (False, None, "√Årbol no cumple invariantes de homotop√≠a")

            if use_cache and cache_key:
                self._cache_result(cache_key, True, tree)

            return (True, tree, "")

        except UnexpectedCharacters as uc:
            self.validation_stats.failed_lark_unexpected_chars += 1
            context = self._get_topological_context(
                line_clean, getattr(uc, "column", 0)
            )
            error_msg = f"Car√°cter discontinuo en vecindad {context}"

        except UnexpectedToken as ut:
            self.validation_stats.failed_lark_parse += 1
            expected = (
                list(ut.expected) if hasattr(ut, "expected") and ut.expected else []
            )
            expected_space = self._map_tokens_to_topological_space(expected)
            token_repr = getattr(ut, "token", "desconocido")
            error_msg = f"Token '{token_repr}' fuera del espacio {expected_space}"

        except UnexpectedEOF:
            self.validation_stats.failed_lark_parse += 1
            completeness = self._calculate_topological_completeness(line_clean)
            error_msg = f"Fin prematuro (compleci√≥n {completeness:.0%})"

        except LarkError as le:
            self.validation_stats.failed_lark_parse += 1
            error_msg = f"Error Lark: {str(le)[:100]}"

        except Exception as e:
            self.validation_stats.failed_lark_parse += 1
            logger.error(
                f"Error inesperado en validaci√≥n Lark: {type(e).__name__}: {e}"
            )
            error_msg = f"Error inesperado: {type(e).__name__}"

        # Punto de salida unificado para errores
        if use_cache and cache_key:
            self._cache_result(cache_key, False, None)
        return (False, None, error_msg)

    def _compute_semantic_cache_key(self, line: str) -> str:
        """
        Computa clave de cache basada en invariantes topol√≥gicos.

        Preserva sem√°ntica mientras normaliza variaciones sint√°cticas superficiales.
        """
        # Normalizaci√≥n de espacios (homeomorfismo de espaciado)
        normalized = re.sub(r"\s+", " ", line.strip())

        # Normalizaci√≥n de ceros no significativos en posiciones decimales
        # Preservamos formato de miles (1,000) vs decimales contextuales
        normalized = re.sub(r"\b0+(\d+\.\d+)", r"\1", normalized)

        # Para l√≠neas muy largas: firma topol√≥gica compacta
        if len(normalized) > self._CACHE_KEY_MAX_LENGTH:
            # Caracter√≠sticas estructurales que preservan la topolog√≠a
            num_groups = len(re.findall(r"\d+[.,]?\d*", normalized))
            alpha_groups = len(re.findall(r"[A-Za-z√Å√â√ç√ì√ö√°√©√≠√≥√∫√ë√±]+", normalized))
            sep_count = normalized.count(";")
            total_len = len(normalized)

            # Incluir prefijo para colisiones reducidas
            prefix = normalized[:50]
            suffix = normalized[-30:]

            feature_string = f"{prefix}|{num_groups}|{alpha_groups}|{sep_count}|{total_len}|{suffix}"
            return hashlib.sha256(feature_string.encode()).hexdigest()[:32]

        return normalized

    def _cache_result(self, key: str, is_valid: bool, tree: Any) -> None:
        """Almacena un resultado en cache con control de tama√±o."""
        if len(self._parse_cache) >= self._MAX_CACHE_SIZE:
            keys_to_remove = list(self._parse_cache.keys())[
                : self._MAX_CACHE_SIZE // 10
            ]
            for k in keys_to_remove:
                del self._parse_cache[k]

        self._parse_cache[key] = (is_valid, tree)

    def _is_valid_tree(self, tree: Any) -> bool:
        """Verifica que un √°rbol Lark es v√°lido y usable."""
        if tree is None:
            return False

        try:
            if not hasattr(tree, "data"):
                return False
            if not hasattr(tree, "children"):
                return False
            if not isinstance(tree.data, str):
                return False
            return True
        except Exception:
            return False

    def _validate_tree_homotopy(self, tree: Any) -> bool:
        """
        Verifica que el √°rbol de parsing sea homot√≥picamente v√°lido.

        Un √°rbol es homot√≥picamente v√°lido si puede deformarse continuamente
        a la estructura can√≥nica esperada.
        """
        if tree is None:
            return False

        try:
            if not hasattr(tree, "data") or not hasattr(tree, "children"):
                return False

            # Invariante 1: La ra√≠z debe pertenecer al espacio de no-terminales v√°lidos
            if not isinstance(tree.data, str):
                return False

            # Invariante 2: L√≠mite de ramificaci√≥n (evita estructuras degeneradas)
            child_count = len(tree.children) if tree.children else 0
            if child_count > 50:  # √Årbol anormalmente ancho
                return False

            # Invariante 3: Profundidad acotada (evita recursi√≥n infinita)
            max_depth = 20

            def check_depth_and_validity(node, current_depth: int) -> bool:
                if current_depth > max_depth:
                    return False

                if hasattr(node, "children") and node.children:
                    for child in node.children:
                        if hasattr(child, "data"):  # Es un nodo no-terminal
                            if not check_depth_and_validity(
                                child, current_depth + 1
                            ):
                                return False
                return True

            if not check_depth_and_validity(tree, 0):
                return False

            # Invariante 4: Debe existir al menos un token terminal
            def has_terminal(node) -> bool:
                if not hasattr(node, "children") or not node.children:
                    return True  # Nodo hoja
                for child in node.children:
                    if not hasattr(child, "data"):  # Es un Token
                        return True
                    if has_terminal(child):
                        return True
                return False

            return has_terminal(tree)

        except Exception:
            return False

    def _has_minimal_structural_connectivity(self, line: str) -> bool:
        """
        Verifica conectividad topol√≥gica m√≠nima.

        Una l√≠nea tiene conectividad si sus componentes est√°n distribuidos
        y relacionados mediante separadores.
        """
        alpha_sequences = re.findall(r"[A-Za-z√Å√â√ç√ì√ö√°√©√≠√≥√∫√ë√±]{2,}", line)
        numeric_sequences = re.findall(r"\d+\.?\d*", line)
        separator_count = line.count(";")

        has_alpha = len(alpha_sequences) >= 1
        has_numeric = len(numeric_sequences) >= 1
        min_separators = self._MIN_FIELDS_FOR_INSUMO - 1
        has_separators = separator_count >= min_separators

        if not (has_alpha and has_numeric and has_separators):
            return False

        # An√°lisis de distribuci√≥n topol√≥gica
        line_len = len(line)
        if line_len < 10:
            return True  # L√≠neas cortas: conectividad trivial

        midpoint = line_len // 2
        first_half = line[:midpoint]
        second_half = line[midpoint:]

        # Verificar que informaci√≥n sem√°ntica existe en ambas mitades
        has_content_first = bool(re.search(r"[A-Za-z0-9]", first_half))
        has_content_second = bool(re.search(r"[A-Za-z0-9]", second_half))

        # Verificar distribuci√≥n de separadores (conexiones entre componentes)
        seps_first = first_half.count(";")
        seps_second = second_half.count(";")

        # Debe haber separadores en ambas mitades para buena conectividad
        # o al menos contenido en ambas
        well_distributed = (has_content_first and has_content_second) and (
            seps_first >= 1 or seps_second >= 1
        )

        return well_distributed

    def _get_topological_context(
        self, line: str, position: int, radius: int = 10
    ) -> str:
        """
        Obtiene el contexto topol√≥gico alrededor de una posici√≥n.

        Muestra la vecindad Œµ del punto de error.
        """
        start = max(0, position - radius)
        end = min(len(line), position + radius)

        context = line[start:end]
        error_pos = position - start

        # Marcar el punto de error en el contexto
        if error_pos < len(context):
            marked = (
                context[:error_pos]
                + "‚ü™"
                + context[error_pos]
                + "‚ü´"
                + context[error_pos + 1 :]
            )
        else:
            marked = context + "‚ü™‚ê£‚ü´"

        return f"[...]{marked}[...]"

    def _map_tokens_to_topological_space(self, expected_tokens: List[str]) -> str:
        """Mapea tokens esperados a espacios topol√≥gicos (categor√≠as)."""
        token_spaces = {
            "NUMBER": "Espacio Num√©rico ‚Ñù",
            "WORD": "Espacio Lexical Œ£*",
            "UNIT": "Espacio de Unidades ùí∞",
            "SEPARATOR": "Espacio de Separaci√≥n ùíÆ",
            "DESCRIPTION": "Espacio Descriptivo ùíü",
        }

        # Clasificar tokens esperados en espacios
        spaces = set()
        for token in expected_tokens:
            found = False
            for key, space in token_spaces.items():
                if key in token.upper():
                    spaces.add(space)
                    found = True
                    break
            if not found:
                spaces.add("Espacio Desconocido ùí≥")

        return " ‚à™ ".join(sorted(spaces)) if spaces else "‚àÖ"

    def _calculate_topological_completeness(self, line: str) -> float:
        """
        Calcula el grado de compleci√≥n topol√≥gica de una l√≠nea.

        Basado en la teor√≠a de compleci√≥n de espacios m√©tricos.
        """
        # Normalizar l√≠nea para facilitar regex (reemplazar comas decimales por puntos temporalmente)
        normalized_line = line.replace(",", ".")

        # Componentes esenciales para un insumo APU completo
        components = {
            "descripcion": bool(re.search(r"[A-Za-z]{3,}", line)),  # 0.3
            "cantidad": bool(
                re.search(r"\d+\.?\d*\s*[A-Za-z]*$", normalized_line)
            ),  # 0.25
            "unidad": bool(
                re.search(r"\b(UND|M|M2|M3|KG|L|GLN|HR|DIA)\b", line, re.I)
            ),  # 0.2
            "precio": bool(re.search(r"\d", line)),  # 0.15 (Simplificado)
            "separadores": line.count(";") >= 3,  # 0.1
        }

        weights = [0.3, 0.25, 0.2, 0.15, 0.1]
        score = sum(w for c, w in zip(components.values(), weights) if c)

        # Ajuste por densidad de informaci√≥n: penalizar l√≠neas muy dispersas o vac√≠as
        info_chunks = len(re.findall(r"\S+", line))
        separators = line.count(";")

        # Densidad relativa al n√∫mero de campos esperados
        expected_chunks = separators + 1
        density_factor = min(info_chunks / max(expected_chunks, 1), 1.0)

        # Penalizaci√≥n suave si la densidad es baja
        if density_factor < 0.5:
            score *= 0.8

        return min(score, 1.0)

    def _validate_basic_structure(
        self, line: str, fields: List[str]
    ) -> Tuple[bool, str]:
        """Validaci√≥n b√°sica PRE-Lark para filtrado r√°pido."""
        if not line or not isinstance(line, str):
            self.validation_stats.failed_basic_fields += 1
            return (False, "L√≠nea vac√≠a o tipo inv√°lido")

        if not fields or not isinstance(fields, list):
            self.validation_stats.failed_basic_fields += 1
            return (False, "Campos vac√≠os o tipo inv√°lido")

        if len(fields) < self._MIN_FIELDS_FOR_INSUMO:
            self.validation_stats.failed_basic_fields += 1
            return (
                False,
                f"Insuficientes campos: {len(fields)} < {self._MIN_FIELDS_FOR_INSUMO}",
            )

        first_field = fields[0] if fields else ""
        if not first_field or not first_field.strip():
            self.validation_stats.failed_basic_fields += 1
            return (False, "Campo de descripci√≥n vac√≠o")

        if len(first_field.strip()) < 2:
            self.validation_stats.failed_basic_fields += 1
            return (False, f"Descripci√≥n demasiado corta: '{first_field}'")

        line_upper = line.upper()
        subtotal_keywords = frozenset(
            {
                "SUBTOTAL",
                "TOTAL",
                "SUMA",
                "SUMATORIA",
                "COSTO DIRECTO",
                "COSTO TOTAL",
                "PRECIO TOTAL",
                "VALOR TOTAL",
                "GRAN TOTAL",
            }
        )

        for keyword in subtotal_keywords:
            if keyword in line_upper:
                self.validation_stats.failed_basic_subtotal += 1
                return (False, f"L√≠nea de subtotal/total: contiene '{keyword}'")

        if self._is_junk_line(line_upper):
            self.validation_stats.failed_basic_junk += 1
            return (False, "L√≠nea decorativa/separador")

        has_numeric = False
        for f in fields[1:]:
            if f and self._NUMERIC_PATTERN.search(f.strip()):
                has_numeric = True
                break

        if not has_numeric:
            self.validation_stats.failed_basic_numeric += 1
            return (False, "Sin campos num√©ricos detectables")

        for i, f in enumerate(fields):
            if len(f) > 500:
                self.validation_stats.failed_basic_fields += 1
                return (
                    False,
                    f"Campo {i} excesivamente largo: {len(f)} caracteres",
                )

        self.validation_stats.passed_basic += 1
        return (True, "")

    def _validate_insumo_line(
        self, line: str, fields: List[str]
    ) -> LineValidationResult:
        """Validaci√≥n topol√≥gica unificada con an√°lisis de invariantes homeom√≥rficos."""
        self.validation_stats.total_evaluated += 1

        # === CAPA 0: HOMEOMORFISMO DE TIPO ===
        if not isinstance(line, str) or not line:
            return LineValidationResult(
                is_valid=False,
                reason="Entrada no homeomorfa a string v√°lido",
                fields_count=0,
                validation_layer="type_check_failed",
            )

        if not isinstance(fields, list):
            return LineValidationResult(
                is_valid=False,
                reason="Campos no homeomorfos a lista",
                fields_count=0,
                validation_layer="type_check_failed",
            )

        fields_count = len(fields)

        # === CAPA 1: INVARIANTES ESTRUCTURALES B√ÅSICOS ===
        basic_valid, basic_reason = self._validate_basic_structure(line, fields)

        if not basic_valid:
            error_group = self._classify_basic_error_group(basic_reason)
            return LineValidationResult(
                is_valid=False,
                reason=f"[{error_group}] {basic_reason}",
                fields_count=fields_count,
                has_numeric_fields=False,
                validation_layer="basic_invariant_failed",
            )

        # === CAPA 2: HOMEOMORFISMO LARK ===
        lark_valid, lark_tree, lark_reason = self._validate_with_lark(line)

        has_numeric = any(
            self._NUMERIC_PATTERN.search(f.strip()) for f in fields[1:] if f
        )

        if not lark_valid:
            self._record_failed_sample(line, fields, lark_reason)
            error_class = self._classify_lark_error_topology(lark_reason)
            return LineValidationResult(
                is_valid=False,
                reason=f"[{error_class}] {lark_reason}",
                fields_count=fields_count,
                has_numeric_fields=has_numeric,
                validation_layer="lark_validation_failed",
            )

        self.validation_stats.passed_lark += 1
        self.validation_stats.passed_both += 1

        # === CAPA 3: HOMEOMORFISMO APU ===
        if lark_tree and not self._is_apu_homeomorphic(lark_tree):
            return LineValidationResult(
                is_valid=False,
                reason="√Årbol sint√°ctico no mapeable a esquema APU",
                fields_count=fields_count,
                has_numeric_fields=has_numeric,
                validation_layer="apu_schema_mismatch",
                lark_tree=lark_tree,
            )

        return LineValidationResult(
            is_valid=True,
            reason="Validaci√≥n completa exitosa",
            fields_count=fields_count,
            has_numeric_fields=has_numeric,
            validation_layer="full_homeomorphism",
            lark_tree=lark_tree,
        )

    def _classify_basic_error_group(self, reason: str) -> str:
        """Clasifica errores b√°sicos en grupos topol√≥gicos."""
        error_groups = {
            "campos": "Grupo Cardinalidad G‚Çê",
            "num√©ricos": "Grupo Medida G‚Çò",
            "subtotal": "Grupo Agregaci√≥n G‚Çê",
            "decorativa": "Grupo Trivial G‚ÇÄ",
        }

        for key, group in error_groups.items():
            if key in reason.lower():
                return group
        return "Grupo Desconocido G‚Çì"

    def _classify_lark_error_topology(self, reason: str) -> str:
        """Clasifica errores Lark en tipos topol√≥gicos."""
        if "UnexpectedCharacters" in reason:
            return "Espacio Discontinuo ùìì"
        elif "UnexpectedToken" in reason:
            return "Mapeo Incorrecto ùìú"
        elif "UnexpectedEOF" in reason:
            return "Borde Prematuro ùìë"
        elif "UnexpectedInput" in reason:
            return "Entrada Singular ùì¢"
        else:
            return "Anomal√≠a ùìê"

    def _is_apu_homeomorphic(self, tree: Any) -> bool:
        """
        Verifica que el √°rbol Lark sea homeomorfo (preserva estructura)
        a un registro de insumo APU v√°lido.
        """
        if not self._is_valid_tree(tree):
            return False

        # Un registro APU debe tener al menos estos componentes esenciales
        essential_components = {
            "descripcion": False,
            "valor_numerico": False,
            "separador": False,
        }

        from lark import Token

        def analyze_node(node):
            if isinstance(node, Token):
                if node.type == "SEP":
                    essential_components["separador"] = True
                elif node.type == "FIELD_VALUE":
                    val = str(node.value).strip()
                    if re.search(r"\d", val):  # Heur√≠stica simple: tiene n√∫meros
                        essential_components["valor_numerico"] = True
                    if re.search(
                        r"[a-zA-Z]{3,}", val
                    ):  # Heur√≠stica: tiene palabras
                        essential_components["descripcion"] = True
            elif hasattr(node, "children"):
                for child in node.children:
                    analyze_node(child)
            elif hasattr(node, "data") and node.data == "field_with_value":
                # Caso espec√≠fico de la gram√°tica
                pass  # Se procesar√° en children

        analyze_node(tree)

        # Relajaci√≥n: Si tiene descripcion y numero, asumimos estructura v√°lida
        # El separador es impl√≠cito en la gram√°tica (line: field (SEP field)*)
        return (
            essential_components["descripcion"]
            and essential_components["valor_numerico"]
        )

    def _record_failed_sample(
        self, line: str, fields: List[str], reason: str
    ) -> None:
        """Registra una muestra de l√≠nea fallida para an√°lisis posterior."""
        max_samples = self.config.get(
            "max_failed_samples", self._MAX_FAILED_SAMPLES
        )

        if len(self.validation_stats.failed_samples) >= max_samples:
            return

        safe_line = line[:200] if isinstance(line, str) else str(line)[:200]
        safe_fields = []
        empty_positions = []

        if isinstance(fields, list):
            for i, f in enumerate(fields):
                if isinstance(f, str):
                    safe_fields.append(f[:100] if len(f) > 100 else f)
                    if not f.strip():
                        empty_positions.append(i)
                else:
                    safe_fields.append(str(f)[:100])

        safe_reason = (
            reason[:300] if isinstance(reason, str) else str(reason)[:300]
        )

        sample = {
            "line": safe_line,
            "fields": safe_fields,
            "fields_count": len(fields) if isinstance(fields, list) else 0,
            "reason": safe_reason,
            "has_empty_fields": bool(empty_positions),
            "empty_field_positions": empty_positions,
            "line_length": len(line) if isinstance(line, str) else 0,
            "first_field_preview": safe_fields[0][:50] if safe_fields else "",
        }

        self.validation_stats.failed_samples.append(sample)

    def _log_validation_summary(self):
        """Registra un resumen detallado de la validaci√≥n."""
        total = self.validation_stats.total_evaluated
        valid = self.stats.get("insumos_extracted", 0)

        logger.info("=" * 80)
        logger.info("üìä RESUMEN DE VALIDACI√ìN CON LARK")
        logger.info("=" * 80)
        logger.info(f"Total l√≠neas evaluadas: {total}")
        if total > 0:
            valid_percent = f"({valid / total * 100:.1f}%)"
            logger.info(
                f"‚úì Insumos v√°lidos (ambas capas): {valid} {valid_percent}"
            )
        else:
            logger.info("‚úì Insumos v√°lidos (ambas capas): 0 (0.0%)")

        logger.info(
            f"  - Pasaron validaci√≥n b√°sica: {self.validation_stats.passed_basic}"
        )
        logger.info(
            f"  - Pasaron validaci√≥n Lark: {self.validation_stats.passed_lark}"
        )
        logger.info(f"  - Cache hits: {self.validation_stats.cached_parses}")
        logger.info("")
        logger.info("Rechazos por validaci√≥n b√°sica:")
        logger.info(
            f"  - Campos insuficientes/vac√≠os: {self.validation_stats.failed_basic_fields}"
        )
        logger.info(
            f"  - Sin datos num√©ricos: {self.validation_stats.failed_basic_numeric}"
        )
        logger.info(
            f"  - Subtotales: {self.validation_stats.failed_basic_subtotal}"
        )
        logger.info(
            f"  - L√≠neas decorativas: {self.validation_stats.failed_basic_junk}"
        )
        logger.info("")
        logger.info("Rechazos por validaci√≥n Lark:")
        logger.info(
            f"  - Parse error gen√©rico: {self.validation_stats.failed_lark_parse}"
        )
        logger.info(
            f"  - Unexpected input: {self.validation_stats.failed_lark_unexpected_input}"
        )
        logger.info(
            "  - Unexpected characters: "
            f"{self.validation_stats.failed_lark_unexpected_chars}"
        )
        logger.info("=" * 80)

        # Mostrar muestras de fallos
        if self.validation_stats.failed_samples:
            logger.info("")
            logger.info("üîç MUESTRAS DE L√çNEAS RECHAZADAS POR LARK:")
            logger.info("-" * 80)

            for idx, sample in enumerate(
                self.validation_stats.failed_samples, 1
            ):
                logger.info(f"\nMuestra #{idx}:")
                logger.info(f"  Raz√≥n: {sample['reason']}")
                logger.info(f"  Campos: {sample['fields_count']}")
                logger.info(f"  Campos vac√≠os: {sample['has_empty_fields']}")
                if sample["has_empty_fields"]:
                    logger.info(
                        f"  Posiciones vac√≠as: {sample['empty_field_positions']}"
                    )
                logger.info(f"  Contenido: {sample['line']}")
                logger.info(f"  Campos: {sample['fields']}")

            logger.info("-" * 80)

        if valid == 0 and total > 0:
            logger.error("üö® CR√çTICO: 0 insumos v√°lidos con validaci√≥n Lark.")
        elif total > 0 and valid < total * 0.5:
            logger.warning(
                f"‚ö†Ô∏è  Tasa de validaci√≥n baja: {valid / total * 100:.1f}%"
            )

    def get_parse_cache(self) -> Dict[str, Any]:
        """Retorna el cache de parsing para reutilizaci√≥n en APUProcessor."""
        valid_cache = {}
        invalid_count = 0

        for line, cached_value in self._parse_cache.items():
            if not isinstance(cached_value, tuple) or len(cached_value) != 2:
                invalid_count += 1
                continue

            is_valid, tree = cached_value

            if not is_valid or tree is None:
                continue

            if not self._is_valid_tree(tree):
                invalid_count += 1
                continue

            normalized_key = self._compute_semantic_cache_key(line)
            valid_cache[normalized_key] = tree

        if invalid_count > 0:
            logger.debug(f"Cache: {invalid_count} entradas inv√°lidas filtradas")

        logger.info(
            f"Cache de parsing exportado: {len(valid_cache)} √°rboles v√°lidos"
        )

        return valid_cache

    def _validate_file_path(self) -> None:
        """Valida que la ruta del archivo sea un archivo v√°lido y no vac√≠o."""
        if not self.file_path.exists():
            raise FileNotFoundError(f"Archivo no encontrado: {self.file_path}")
        if not self.file_path.is_file():
            raise ValueError(f"La ruta no es un archivo: {self.file_path}")
        if self.file_path.stat().st_size == 0:
            raise ValueError(f"El archivo est√° vac√≠o: {self.file_path}")

    def parse_to_raw(self) -> List[Dict[str, Any]]:
        """Punto de entrada principal para parsear el archivo."""
        if self._parsed:
            return self.raw_records

        logger.info(f"Iniciando parseo l√≠nea por l√≠nea de: {self.file_path.name}")

        try:
            content = self._read_file_safely()
            lines = content.split("\n")
            self.stats["total_lines"] = len(lines)

            # Inicializar handlers y contexto
            handlers = self._initialize_handlers()
            context = ParserContext()

            logger.info(
                f"üöÄ Iniciando procesamiento de {len(lines)} l√≠neas con L√≥gica Piramidal."
            )

            i = 0
            while i < len(lines):
                line = lines[i]
                context.current_line_number = i + 1
                line = line.strip()

                if not line:
                    i += 1
                    continue

                next_line = (
                    lines[i + 1].strip() if i + 1 < len(lines) else None
                )
                handled = False

                for handler in handlers:
                    if handler.can_handle(line, next_line):
                        should_advance_extra = handler.handle(
                            line, context, next_line
                        )
                        if should_advance_extra:
                            i += 1  # Saltar la siguiente l√≠nea tambi√©n (ej. ITEM)
                        handled = True
                        break

                if not handled:
                    logger.debug(
                        f"L√≠nea {i+1} no reconocida por ning√∫n handler."
                    )

                i += 1

            # Actualizar estado del objeto principal
            self.stats.update(context.stats)
            self.raw_records = context.raw_records
            self._parsed = True

            logger.info(
                f"Parseo completo. Extra√≠dos {self.stats['insumos_extracted']} "
                "registros crudos."
            )

            self._log_validation_summary()

        except Exception as e:
            logger.error(f"Error cr√≠tico de parseo: {e}", exc_info=True)
            raise ParseStrategyError(
                f"Fall√≥ el parseo con estrategia Chain of Responsibility: {e}"
            ) from e

        return self.raw_records

    def _read_file_safely(self) -> str:
        """Lee el contenido del archivo intentando m√∫ltiples codificaciones."""
        default_encodings = self.config.get(
            "encodings", ["utf-8", "latin1", "cp1252", "iso-8859-1"]
        )
        encodings_to_try = [self.profile.get("encoding")] + default_encodings

        for encoding in filter(None, encodings_to_try):
            try:
                with open(
                    self.file_path, "r", encoding=encoding, errors="strict"
                ) as f:
                    content = f.read()
                self.stats["encoding_used"] = encoding
                logger.info(
                    f"Archivo le√≠do exitosamente con codificaci√≥n: {encoding}"
                )
                return content
            except (UnicodeDecodeError, TypeError, LookupError):
                continue
        raise FileReadError(
            f"No se pudo leer el archivo {self.file_path} con ninguna de las "
            "codificaciones especificadas."
        )

    def _detect_category(self, line_upper: str) -> Optional[str]:
        """
        Detecci√≥n topol√≥gica de categor√≠as usando teor√≠a de ret√≠culos.

        Refuerzo: Identifica la categor√≠a como el √≠nfimo del conjunto de keywords.
        """
        if len(line_upper) > 50 or sum(c.isdigit() for c in line_upper) > 3:
            return None

        # Construir ret√≠culo de categor√≠as: cada keyword define un conjunto
        category_membership = {}

        for canonical, variations in self.CATEGORY_KEYWORDS.items():
            for variation in variations:
                # Usar l√≠mites superiores en el ret√≠culo (sup)
                if self._is_supremum_match(variation, line_upper):
                    category_membership[canonical] = (
                        category_membership.get(canonical, 0)
                        + self._calculate_match_strength(variation, line_upper)
                    )

        if not category_membership:
            return None

        # Encontrar el √≠nfimo (mejor categor√≠a) por fuerza de match
        best_category = max(category_membership.items(), key=lambda x: x[1])

        # Umbral topol√≥gico: debe superar un m√≠nimo
        if best_category[1] > 0.15:
            return best_category[0]

        return None

    def _is_supremum_match(self, pattern: str, text: str) -> bool:
        """
        Verifica si pattern es un supremo (l√≠mite superior) en el ret√≠culo de matches.

        Considera matches parciales, prefijos y sufijos.
        """
        # Normalizar para matching topol√≥gico
        pattern_norm = pattern.replace(".", "\\.").replace(" ", "\\s*")

        # Buscar como palabra completa o como prefijo/sufijo significativo
        if "." in pattern:
            # Patr√≥n con abreviatura: match exacto
            # No usar \b al final si termina en punto, ya que el punto no es word char
            regex = rf"\b{pattern_norm}"
            if not pattern.endswith("."):
                regex += r"\b"
            return bool(re.search(regex, text))
        else:
            # Palabra completa: puede ser parte de una frase
            return bool(re.search(rf"\b{pattern_norm}\b", text, re.IGNORECASE))

    def _calculate_match_strength(self, pattern: str, text: str) -> float:
        """
        Calcula la fuerza del match en [0,1] usando m√©trica topol√≥gica.

        Considera posici√≥n, completitud y contexto.
        """
        # Peso por posici√≥n: matches al inicio son m√°s fuertes
        position_weight = 1.0
        match_pos = text.find(pattern)
        if match_pos >= 0:
            position_weight = 1.0 - (match_pos / len(text))

        # Peso por completitud: palabras completas vs parciales
        completeness_weight = 1.0 if f" {pattern} " in f" {text} " else 0.7

        # Peso contextual: l√≠nea corta sugiere categor√≠a, larga sugiere contenido
        context_weight = 2.0 if len(text) < 30 else 1.0

        return position_weight * completeness_weight * context_weight

    def _is_junk_line(self, line_upper: str) -> bool:
        """Determina si una l√≠nea debe ser ignorada por ser "ruido"."""
        if not line_upper or not isinstance(line_upper, str):
            return True

        stripped = line_upper.strip()

        if len(stripped) < self._MIN_LINE_LENGTH:
            return True

        for keyword in self.JUNK_KEYWORDS:
            if keyword in line_upper:
                return True

        if self._DECORATIVE_PATTERN.search(stripped):
            return True

        return False

    def _extract_apu_header(
        self, header_line: str, item_line: str, line_number: int
    ) -> Optional[APUContext]:
        """Extrae informaci√≥n del encabezado APU de forma segura."""
        try:
            parts = header_line.split(";")
            apu_desc = parts[0].strip() if parts else ""

            unit_match = self._UNIT_PATTERN.search(header_line)
            default_unit = self.config.get("default_unit", "UND")
            apu_unit = (
                unit_match.group(1).strip() if unit_match else default_unit
            )

            item_match = self._ITEM_PATTERN.search(item_line)
            if item_match:
                apu_code_raw = item_match.group(1)
            else:
                apu_code_raw = f"UNKNOWN_APU_{line_number}"

            apu_code = clean_apu_code(apu_code_raw)

            if not apu_code or len(apu_code) < 2:
                logger.warning(f"C√≥digo APU inv√°lido extra√≠do: '{apu_code}'")
                return None

            return APUContext(
                apu_code=apu_code,
                apu_desc=apu_desc,
                apu_unit=apu_unit,
                source_line=line_number,
            )

        except ValueError as ve:
            logger.debug(f"Validaci√≥n de APUContext fall√≥: {ve}")
            return None
        except Exception as e:
            logger.warning(f"Error extrayendo encabezado APU: {e}")
            return None

    def _build_insumo_record(
        self,
        context: APUContext,
        category: str,
        line: str,
        line_number: int,
        validation_result: LineValidationResult,
        fields: Optional[List[str]] = None,
    ) -> Dict[str, Any]:
        """
        Construye registro con m√©tricas topol√≥gicas adicionales.

        Refuerzo: A√±ade invariantes y medidas de calidad estructural.
        """
        # Calcular m√©tricas topol√≥gicas
        if fields is None:
            fields = [f.strip() for f in line.split(";")]

        topological_metrics = {
            "field_entropy": self._calculate_field_entropy(fields),
            "structural_density": self._calculate_structural_density(line),
            "numeric_cohesion": self._calculate_numeric_cohesion(fields),
            "homogeneity_index": self._calculate_homogeneity_index(fields),
        }

        # Determinar clase de homeomorfismo
        homeomorphism_class = self._determine_homeomorphism_class(
            validation_result.validation_layer, topological_metrics
        )

        record = {
            "apu_code": context.apu_code,
            "apu_desc": context.apu_desc,
            "apu_unit": context.apu_unit,
            "category": category,
            "insumo_line": line,
            "source_line": line_number,
            "fields_count": validation_result.fields_count,
            "validation_layer": validation_result.validation_layer,
            "homeomorphism_class": homeomorphism_class,
            "topological_metrics": topological_metrics,
            "_lark_tree": validation_result.lark_tree,
            "_structural_signature": self._compute_structural_signature(line),
        }

        return record

    def _calculate_field_entropy(self, fields: List[str]) -> float:
        """Calcula la entrop√≠a topol√≥gica de los campos."""
        if not fields:
            return 0.0

        # Distribuci√≥n de tipos por campo
        type_counts = {"alpha": 0, "numeric": 0, "mixed": 0, "empty": 0}

        for field in fields:
            field = str(field).strip()
            if not field:
                type_counts["empty"] += 1
            elif field.replace(".", "").replace(",", "").isdigit():
                type_counts["numeric"] += 1
            elif any(c.isalpha() for c in field):
                if any(c.isdigit() for c in field):
                    type_counts["mixed"] += 1
                else:
                    type_counts["alpha"] += 1

        # Entrop√≠a de Shannon normalizada
        from math import log2

        total = len(fields)
        entropy = 0.0

        for count in type_counts.values():
            if count > 0:
                p = count / total
                entropy -= p * log2(p)

        # Normalizar a [0,1]
        max_entropy = log2(min(len(type_counts), total))
        return entropy / max_entropy if max_entropy > 0 else 0.0

    def _calculate_structural_density(self, line: str) -> float:
        """Calcula la densidad estructural (informaci√≥n por car√°cter)."""
        # Informaci√≥n sem√°ntica aproximada
        words = re.findall(r"\b[A-Za-z]{3,}\b", line)
        numbers = re.findall(r"\d+(?:[.,]\d+)?", line)

        semantic_units = len(words) + len(numbers)
        total_chars = len(line)

        return semantic_units / total_chars if total_chars > 0 else 0.0

    def _calculate_numeric_cohesion(self, fields: List[str]) -> float:
        """Calcula la cohesi√≥n num√©rica (qu√© tan juntos est√°n los n√∫meros)."""
        numeric_positions = [
            i for i, f in enumerate(fields) if any(c.isdigit() for c in str(f))
        ]

        if len(numeric_positions) < 2:
            return 1.0 if numeric_positions else 0.0

        # Distancia promedio entre n√∫meros
        distances = [
            abs(numeric_positions[i] - numeric_positions[i - 1])
            for i in range(1, len(numeric_positions))
        ]

        avg_distance = sum(distances) / len(distances)

        # Cohesi√≥n inversa a la distancia promedio.
        # Si avg_distance es 1 (contiguos), cohesi√≥n es 1.0.
        # Si avg_distance aumenta, cohesi√≥n baja.
        return 1.0 / avg_distance if avg_distance > 0 else 0.0

    def _calculate_homogeneity_index(self, fields: List[str]) -> float:
        """√çndice de homogeneidad (qu√© tan similares son los campos)."""
        if len(fields) < 2:
            return 1.0

        # Similaridad por tipo de datos
        field_types = []
        for field in fields:
            field_str = str(field).strip()
            if not field_str:
                field_types.append("empty")
            elif field_str.replace(".", "").replace(",", "").isdigit():
                field_types.append("numeric")
            elif any(c.isalpha() for c in field_str):
                if any(c.isdigit() for c in field_str):
                    field_types.append("mixed")
                else:
                    field_types.append("alpha")
            else:
                field_types.append("other")

        # Porcentaje del tipo m√°s com√∫n
        from collections import Counter

        type_counts = Counter(field_types)
        most_common_count = max(type_counts.values())

        return most_common_count / len(fields)

    def _determine_homeomorphism_class(
        self, validation_layer: str, metrics: Dict[str, float]
    ) -> str:
        """Determina la clase de homeomorfismo del registro."""
        if validation_layer != "full_homeomorphism":
            # Mapeo a las clases de error esperadas por los tests
            if "lark" in validation_layer:
                return "CLASE_E_IRREGULAR"
            return f"DEFECTIVO_{validation_layer.upper()}"

        entropy = metrics.get("field_entropy", 0)
        density = metrics.get("structural_density", 0)
        cohesion = metrics.get("numeric_cohesion", 0)
        homogeneity = metrics.get("homogeneity_index", 0)

        # Clasificaci√≥n jer√°rquica alineada con los tests (CLASE_X)
        if entropy > 0.6 and density > 0.08 and cohesion > 0.7:
            return "CLASE_A_COMPLETO"
        elif cohesion > 0.85 and homogeneity > 0.5:
            return "CLASE_B_NUMERICO"
        elif homogeneity > 0.7:
            return "CLASE_C_HOMOGENEO"
        elif entropy > 0.4 or density > 0.05:
            return "CLASE_D_MIXTO"
        else:
            return "CLASE_E_IRREGULAR"

    def _compute_structural_signature(self, line: str) -> str:
        """Computa una firma estructural √∫nica para la l√≠nea."""
        import hashlib

        # Extraer caracter√≠sticas estructurales invariantes
        features = [
            str(len(re.findall(r"[A-Z]", line))),
            str(len(re.findall(r"[a-z]", line))),
            str(len(re.findall(r"\d", line))),
            str(len(re.findall(r"[.;,]", line))),
            str(len(line.split())),
            str(len(line)),
        ]

        feature_string = "|".join(features)
        return hashlib.sha256(feature_string.encode()).hexdigest()[:16]
