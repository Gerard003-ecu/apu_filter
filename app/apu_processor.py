"""
Procesador APU con arquitectura modular de especialistas.

Este m√≥dulo implementa un sistema avanzado para el procesamiento de datos de
An√°lisis de Precios Unitarios (APU). Utiliza una arquitectura modular donde
componentes "especialistas", cada uno con una responsabilidad √∫nica, colaboran
para interpretar y estructurar l√≠neas de texto con formatos variables.

El `APUProcessor` principal mantiene la compatibilidad con la interfaz esperada
por el `LoadDataStep` del pipeline, orquestando internamente a estos especialistas
para lograr un procesamiento robusto y flexible.
"""

import logging
import re
from collections import defaultdict
from dataclasses import dataclass, field
from enum import Enum
from functools import lru_cache
from typing import Any, Dict, List, Optional, Set, Tuple

import pandas as pd
from lark import Token, Transformer, v_args
from lark.exceptions import LarkError

from .schemas import Equipo, InsumoProcesado, ManoDeObra, Otro, Suministro, Transporte
from .utils import parse_number

logger = logging.getLogger(__name__)


# =-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-
# ENUMS Y DATACLASSES
# -=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=


@dataclass
class ParsingStats:
    """Estad√≠sticas detalladas del proceso de parsing."""

    total_lines: int = 0
    successful_parses: int = 0
    lark_parse_errors: int = 0
    lark_unexpected_input: int = 0
    lark_unexpected_chars: int = 0
    transformer_errors: int = 0
    empty_results: int = 0
    fallback_attempts: int = 0
    fallback_successes: int = 0
    cache_hits: int = 0
    failed_lines: List[Dict[str, Any]] = field(default_factory=list)


class TipoInsumo(Enum):
    """Enumeraci√≥n de tipos de insumo v√°lidos."""

    MANO_DE_OBRA = "MANO_DE_OBRA"
    EQUIPO = "EQUIPO"
    TRANSPORTE = "TRANSPORTE"
    SUMINISTRO = "SUMINISTRO"
    OTRO = "OTRO"


class FormatoLinea(Enum):
    """Enumeraci√≥n de formatos de l√≠nea detectados."""

    MO_COMPLETA = "MO_COMPLETA"
    INSUMO_BASICO = "INSUMO_BASICO"
    DESCONOCIDO = "DESCONOCIDO"


@dataclass
class ValidationThresholds:
    """Umbrales de validaci√≥n para diferentes tipos de insumos."""

    min_jornal: float = 50000
    max_jornal: float = 10000000
    min_rendimiento: float = 0.001
    max_rendimiento: float = 1000
    max_rendimiento_tipico: float = 100
    min_cantidad: float = 0.001
    max_cantidad: float = 1000000
    min_precio: float = 0.01
    max_precio: float = 1e9


# -=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=
# GRAM√ÅTICA LARK
# -=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=

APU_GRAMMAR = r"""
    ?start: line

    // CAMBIO CLAVE: Se elimin√≥ la opcionalidad externa.
    // Una l√≠nea ahora DEBE tener al menos un 'field'.
    line: field (SEP field)*

    field: FIELD_VALUE?  // El campo en s√≠ puede estar vac√≠o (ej. 'dato1;;dato3')

    FIELD_VALUE: /[^;\r\n]+/ // El contenido del campo (si existe)
    SEP: /\s*;\s*/          // Separador flexible

    NEWLINE: /[\r\n]+/

    %import common.WS
    %ignore WS
"""


# -=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=
# COMPONENTES ESPECIALISTAS
# -=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=


class PatternMatcher:
    """
    Especialista en detecci√≥n de patrones y clasificaci√≥n de l√≠neas de texto.

    Esta clase encapsula la l√≥gica para identificar si una l√≠nea de texto
    corresponde a un encabezado, un resumen, una categor√≠a o si contiene
    ciertos tipos de contenido (num√©rico, porcentajes), bas√°ndose en un
    conjunto de palabras clave y expresiones regulares predefinidas.
    """

    # Palabras clave de encabezado de tabla
    HEADER_KEYWORDS = [
        "DESCRIPCION",
        "DESCRIPCI√ìN",
        "DESC",
        "UND",
        "UNID",
        "UNIDAD",
        "CANT",
        "CANTIDAD",
        "PRECIO",
        "VALOR",
        "TOTAL",
        "DESP",
        "DESPERDICIO",
        "REND",
        "RENDIMIENTO",
        "JORNAL",
        "ITEM",
        "CODIGO",
        "C√ìDIGO",
    ]

    # Palabras clave de resumen/totalizaci√≥n
    SUMMARY_KEYWORDS = [
        "SUBTOTAL",
        "TOTAL",
        "RESUMEN",
        "SUMA",
        "TOTALES",
        "ACUMULADO",
        "GRAN TOTAL",
        "COSTO DIRECTO",
    ]

    # Categor√≠as t√≠picas (exactas)
    CATEGORY_PATTERNS = [
        r"^MATERIALES?$",
        r"^MANO\s+DE\s+OBRA$",
        r"^EQUIPO$",
        r"^TRANSPORTE$",
        r"^OTROS?$",
        r"^SERVICIOS?$",
        r"^HERRAMIENTAS?$",
        r"^SUMINISTROS?$",
    ]

    def __init__(self):
        """Inicializa el PatternMatcher y pre-compila los patrones regex."""
        self._pattern_cache: Dict[str, re.Pattern] = {}
        self._compile_patterns()

    def _compile_patterns(self) -> None:
        """Pre-compila todos los patrones regex para optimizar el rendimiento."""
        summary_pattern = "|".join(self.SUMMARY_KEYWORDS)
        self._pattern_cache["summary"] = re.compile(summary_pattern, re.IGNORECASE)

        category_pattern = "|".join(self.CATEGORY_PATTERNS)
        self._pattern_cache["category"] = re.compile(category_pattern, re.IGNORECASE)

        self._pattern_cache["numeric"] = re.compile(r"[\d,.]")
        self._pattern_cache["text"] = re.compile(r"[a-zA-Z]{3,}")
        self._pattern_cache["percentage"] = re.compile(r"\d+\s*%")

    def count_header_keywords(self, text: str) -> int:
        """
        Cuenta cu√°ntas palabras clave de encabezado est√°n presentes en el texto.

        Args:
            text: El texto a analizar.

        Returns:
            El n√∫mero de palabras clave de encabezado encontradas.
        """
        text_upper = text.upper()
        return sum(1 for keyword in self.HEADER_KEYWORDS if keyword in text_upper)

    def is_likely_header(self, text: str, field_count: int) -> bool:
        """Determina si una l√≠nea es probablemente un encabezado de tabla."""
        keyword_count = self.count_header_keywords(text)

        if field_count <= 2 and keyword_count >= 3:
            return True

        words = text.upper().split()
        if words and len(words) > 2:
            header_word_ratio = sum(1 for w in words if w in self.HEADER_KEYWORDS) / len(
                words
            )
            if header_word_ratio > 0.6:
                return True

        return False

    def is_likely_summary(self, text: str, field_count: int) -> bool:
        """Determina si una l√≠nea es probablemente un subtotal o resumen."""
        if field_count <= 2 and self._pattern_cache["summary"].search(text):
            return True

        text_stripped = text.strip()
        for keyword in self.SUMMARY_KEYWORDS:
            if text_stripped.upper().startswith(keyword):
                return True

        return False

    def is_likely_category(self, text: str, field_count: int) -> bool:
        """Determina si una l√≠nea es probablemente una l√≠nea de categor√≠a."""
        if field_count <= 2:
            return bool(self._pattern_cache["category"].match(text.strip()))
        return False

    def has_numeric_content(self, text: str) -> bool:
        """Verifica si el texto contiene cualquier car√°cter num√©rico."""
        return bool(self._pattern_cache["numeric"].search(text))

    def has_percentage(self, text: str) -> bool:
        """Verifica si el texto contiene un s√≠mbolo de porcentaje."""
        return bool(self._pattern_cache["percentage"].search(text))


class UnitsValidator:
    """
    Especialista en la validaci√≥n y normalizaci√≥n de unidades de medida.

    Esta clase centraliza el conocimiento sobre las unidades de medida
    aceptadas, proporcionando m√©todos para verificar la validez de una unidad
    y para convertirla a un formato can√≥nico estandarizado.
    """

    VALID_UNITS: Set[str] = {
        "UND",
        "UN",
        "UNID",
        "UNIDAD",
        "UNIDADES",
        "M",
        "MT",
        "MTS",
        "MTR",
        "MTRS",
        "METRO",
        "METROS",
        "ML",
        "KM",
        "M2",
        "MT2",
        "MTS2",
        "MTRS2",
        "METROSCUAD",
        "METROSCUADRADOS",
        "M3",
        "MT3",
        "MTS3",
        "MTRS3",
        "METROSCUB",
        "METROSCUBICOS",
        "HR",
        "HRS",
        "HORA",
        "HORAS",
        "MIN",
        "MINUTO",
        "MINUTOS",
        "DIA",
        "DIAS",
        "SEM",
        "SEMANA",
        "SEMANAS",
        "MES",
        "MESES",
        "JOR",
        "JORN",
        "JORNAL",
        "JORNALES",
        "G",
        "GR",
        "GRAMO",
        "GRAMOS",
        "KG",
        "KGS",
        "KILO",
        "KILOS",
        "KILOGRAMO",
        "KILOGRAMOS",
        "TON",
        "TONS",
        "TONELADA",
        "TONELADAS",
        "LB",
        "LIBRA",
        "LIBRAS",
        "GAL",
        "GLN",
        "GALON",
        "GALONES",
        "LT",
        "LTS",
        "LITRO",
        "LITROS",
        "ML",
        "MILILITRO",
        "MILILITROS",
        "VIAJE",
        "VIAJES",
        "VJE",
        "VJ",
        "BULTO",
        "BULTOS",
        "SACO",
        "SACOS",
        "PAQ",
        "PAQUETE",
        "PAQUETES",
        "GLOBAL",
        "GLB",
        "GB",
    }

    @classmethod
    @lru_cache(maxsize=256)
    def normalize_unit(cls, unit: str) -> str:
        """
        Normaliza una unidad a su forma can√≥nica (ej. "Metro" -> "M").

        Args:
            unit: La cadena de texto de la unidad a normalizar.

        Returns:
            La unidad normalizada. Devuelve "UND" si la unidad es vac√≠a o
            no reconocida.
        """
        if not unit:
            return "UND"

        unit_clean = re.sub(r"[^A-Z0-9]", "", unit.upper().strip())

        unit_mappings = {
            "UNID": "UND",
            "UN": "UND",
            "UNIDAD": "UND",
            "MT": "M",
            "MTS": "M",
            "MTR": "M",
            "MTRS": "M",
            "JORN": "JOR",
            "JORNAL": "JOR",
            "JORNALES": "JOR",
            # Agregar m√°s mapeos seg√∫n sea necesario
        }

        return unit_mappings.get(
            unit_clean, unit_clean if unit_clean in cls.VALID_UNITS else "UND"
        )

    @classmethod
    def is_valid(cls, unit: str) -> bool:
        """
        Verifica si una cadena de texto representa una unidad v√°lida.

        Args:
            unit: La unidad a validar.

        Returns:
            True si la unidad es reconocida, False en caso contrario.
        """
        if not unit:
            return False
        unit_clean = re.sub(r"[^A-Z0-9]", "", unit.upper().strip())
        return unit_clean in cls.VALID_UNITS or len(unit_clean) <= 4


class NumericFieldExtractor:
    """
    Especialista en la extracci√≥n e identificaci√≥n de campos num√©ricos.

    Esta clase es responsable de parsear valores num√©ricos de cadenas de texto,
    manejando diferentes separadores decimales. Su funci√≥n m√°s importante es
    la identificaci√≥n inteligente de valores de "rendimiento" y "jornal"
    para insumos de Mano de Obra, utilizando heur√≠sticas basadas en umbrales
    y magnitud relativa.
    """

    def __init__(
        self,
        config: Dict[str, Any],
        profile: Optional[Dict[str, Any]] = None,
        thresholds: Optional[ValidationThresholds] = None,
    ):
        """
        Inicializa el extractor.

        Args:
            config: El diccionario de configuraci√≥n global.
            profile: El perfil de configuraci√≥n espec√≠fico del archivo.
            thresholds: Un objeto `ValidationThresholds` con los umbrales.
        """
        self.config = config or {}
        self.profile = profile or {}
        self.pattern_matcher = PatternMatcher()
        self.thresholds = thresholds or ValidationThresholds()
        # CAMBIO: Leer el separador decimal desde el profile
        number_format = self.profile.get("number_format", {})
        self.decimal_separator = number_format.get("decimal_separator")

    def extract_all_numeric_values(
        self, fields: List[str], skip_first: bool = True
    ) -> List[float]:
        """
        Extrae todos los valores num√©ricos v√°lidos de una lista de campos.

        Args:
            fields: La lista de cadenas de texto (campos) a procesar.
            skip_first: Si es True, ignora el primer campo (usualmente la
                        descripci√≥n).

        Returns:
            Una lista de los valores num√©ricos encontrados.
        """
        start_idx = 1 if skip_first else 0
        numeric_values = []

        for field in fields[start_idx:]:
            if not field:
                continue

            value = self.parse_number_safe(field)
            if value is not None and value >= 0:
                numeric_values.append(value)

        return numeric_values

    def parse_number_safe(self, value: str) -> Optional[float]:
        """
        Parsea un n√∫mero de forma segura, utilizando el separador decimal
        configurado.

        Args:
            value: La cadena de texto que contiene el n√∫mero.

        Returns:
            El n√∫mero como flotante, o None si el parseo falla.
        """
        if not value or not isinstance(value, str):
            return None
        try:
            # CAMBIO: Pasar el separador decimal a la funci√≥n de parseo
            return parse_number(value, decimal_separator=self.decimal_separator)
        except (ValueError, TypeError, AttributeError):
            return None

    def identify_mo_values(
        self, numeric_values: List[float]
    ) -> Optional[Tuple[float, float]]:
        """
        Identifica rendimiento y jornal de una lista de valores num√©ricos.

        Utiliza heur√≠sticas basadas en rangos t√≠picos y magnitud para
        distinguir entre el valor del jornal (generalmente un n√∫mero grande)
        y el rendimiento (un n√∫mero m√°s peque√±o).

        Args:
            numeric_values: Lista de valores num√©ricos extra√≠dos de una l√≠nea
                            de Mano de Obra.

        Returns:
            Una tupla (rendimiento, jornal) si se identifican ambos, o None.
        """
        if len(numeric_values) < 2:
            return None

        # Heur√≠stica 1: Buscar por rangos t√≠picos
        jornal_candidates = [
            v
            for v in numeric_values
            if self.thresholds.min_jornal <= v <= self.thresholds.max_jornal
        ]

        rendimiento_candidates = [
            v
            for v in numeric_values
            if (
                self.thresholds.min_rendimiento
                <= v
                <= self.thresholds.max_rendimiento_tipico
                and v not in jornal_candidates
            )
        ]

        if jornal_candidates and rendimiento_candidates:
            # Tomar el jornal m√°s grande y el rendimiento m√°s peque√±o
            jornal = max(jornal_candidates)
            rendimiento = min(rendimiento_candidates)
            return rendimiento, jornal

        # Heur√≠stica 2: Si no encontramos con rangos, usar posici√≥n relativa
        if len(numeric_values) >= 2:
            sorted_values = sorted(numeric_values, reverse=True)

            # El valor m√°s grande que sea >= min_jornal es probablemente el jornal
            for val in sorted_values:
                if val >= self.thresholds.min_jornal:
                    jornal = val
                    # Buscar rendimiento entre los valores restantes
                    for other_val in numeric_values:
                        if (
                            other_val != jornal
                            and other_val <= self.thresholds.max_rendimiento_tipico
                        ):
                            return other_val, jornal
                    break

        return None

    def extract_insumo_values(self, fields: List[str], start_from: int = 2) -> List[float]:
        """
        Extrae valores num√©ricos para insumos b√°sicos (no Mano de Obra).

        Args:
            fields: Lista de campos de la l√≠nea.
            start_from: √çndice desde el cual empezar a buscar valores.

        Returns:
            Lista de valores num√©ricos (cantidad, precio, total).
        """
        valores = []
        for i in range(start_from, len(fields)):
            if fields[i] and "%" not in fields[i]:  # Ignorar desperdicio
                val = self.parse_number_safe(fields[i])
                if val is not None and val >= 0:
                    valores.append(val)
        return valores


# -=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=
# TRANSFORMER ORQUESTADOR
# -=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=


@v_args(inline=False)
class APUTransformer(Transformer):
    """
    Orquestador que coordina a los especialistas para transformar una l√≠nea.

    Esta clase act√∫a como un `Transformer` para la librer√≠a Lark. Recibe el
    √°rbol de parseo de una l√≠nea, extrae los campos y utiliza a los
    especialistas (`PatternMatcher`, `NumericFieldExtractor`, etc.) para
    detectar el formato de la l√≠nea y despacharla al m√©todo constructor
    apropiado (`_build_mo_completa`, `_build_insumo_basico`).
    """

    def __init__(
        self,
        apu_context: Dict[str, Any],
        config: Dict[str, Any],
        profile: Dict[str, Any],
        keyword_cache: Any,
    ):
        """
        Inicializa el Transformer.

        Args:
            apu_context: Diccionario con el contexto del APU actual (c√≥digo,
                         descripci√≥n, etc.).
            config: Diccionario de configuraci√≥n de la aplicaci√≥n.
            profile: Perfil de configuraci√≥n espec√≠fico para el archivo.
            keyword_cache: Cache de palabras clave (actualmente no usado).
        """
        self.apu_context = apu_context or {}
        self.config = config or {}
        self.profile = profile or {}
        self.keyword_cache = keyword_cache

        # Inicializar especialistas
        self.pattern_matcher = PatternMatcher()
        self.units_validator = UnitsValidator()
        self.thresholds = self._load_validation_thresholds()
        # CAMBIO: Pasar el profile al NumericFieldExtractor
        self.numeric_extractor = NumericFieldExtractor(
            self.config, self.profile, self.thresholds
        )
        super().__init__()

    def _load_validation_thresholds(self) -> ValidationThresholds:
        """Carga los umbrales de validaci√≥n desde la configuraci√≥n."""
        mo_config = self.config.get("validation_thresholds", {}).get("MANO_DE_OBRA", {})
        return ValidationThresholds(
            min_jornal=mo_config.get("min_jornal", 50000),
            max_jornal=mo_config.get("max_jornal", 10000000),
            min_rendimiento=mo_config.get("min_rendimiento", 0.001),
            max_rendimiento=mo_config.get("max_rendimiento", 1000),
            max_rendimiento_tipico=mo_config.get("max_rendimiento_tipico", 100),
        )

    def _extract_value(self, item) -> str:
        """Extrae el valor de string de un token o string de forma segura."""
        if item is None:
            return ""
        if isinstance(item, Token):
            return str(item.value).strip() if item.value else ""
        if isinstance(item, (str, bytes)):
            value = item.decode("utf-8") if isinstance(item, bytes) else item
            return value.strip()
        try:
            return str(item).strip()
        except Exception:
            return ""

    def line(self, args):
        """
        Procesa una l√≠nea parseada por Lark.

        Args:
            args: Argumentos proporcionados por Lark (campos de la l√≠nea).

        Returns:
            Un objeto `InsumoProcesado` si la l√≠nea es v√°lida y procesable,
            o None en caso contrario.
        """
        fields = []
        # CORRECCI√ìN: Filtrar los tokens SEP ('\;') que Lark incluye en `args`.
        # Lark pasa una lista plana [field, SEP, field, SEP, ...].
        # Un SEP es un Token, un field no.
        filtered_args = [arg for arg in args if not isinstance(arg, Token) or arg.type != 'SEP']

        for arg in filtered_args:
            if isinstance(arg, list):
                fields.extend([self._extract_value(f) for f in arg])
            else:
                fields.append(self._extract_value(arg))

        clean_fields = self._filter_trailing_empty(fields)

        if not clean_fields or not clean_fields[0]:
            return None

        formato = self._detect_format(clean_fields)

        if formato == FormatoLinea.DESCONOCIDO:
            return None

        return self._dispatch_builder(formato, clean_fields)

    def field(self, args):
        """Procesa un campo individual parseado por Lark."""
        if not args:
            return ""
        return self._extract_value(args[0]) if args else ""

    def _filter_trailing_empty(self, tokens: List[str]) -> List[str]:
        """Elimina campos vac√≠os al final de una lista de campos."""
        if not tokens:
            return []

        last_idx = -1
        for i in range(len(tokens) - 1, -1, -1):
            if tokens[i]:
                last_idx = i
                break

        return tokens[: last_idx + 1] if last_idx >= 0 else []

    def _detect_format(self, fields: List[str]) -> FormatoLinea:
        """
        Detecta el formato de la l√≠nea usando los especialistas.

        Coordina al `PatternMatcher` para filtrar ruido (res√∫menes,
        encabezados) y al `NumericFieldExtractor` para determinar si
        la l√≠nea tiene la estructura de un insumo de Mano de Obra o
        de un insumo b√°sico.

        Args:
            fields: La lista de campos de la l√≠nea.

        Returns:
            El `FormatoLinea` detectado.
        """
        if not fields or not fields[0]:
            return FormatoLinea.DESCONOCIDO

        descripcion = fields[0].strip()
        num_fields = len(fields)

        # Usar PatternMatcher para filtrar ruido contextualmente
        if self._is_noise_line(descripcion, num_fields):
            return FormatoLinea.DESCONOCIDO

        if num_fields < 3:
            return FormatoLinea.DESCONOCIDO

        # Clasificar tipo de insumo
        tipo_probable = self._classify_insumo(descripcion)

        # Detectar MO_COMPLETA si es mano de obra y tiene formato v√°lido
        if tipo_probable == TipoInsumo.MANO_DE_OBRA and num_fields >= 5:
            if self._validate_mo_format(fields):
                logger.debug(f"MO_COMPLETA detectado: {descripcion[:30]}...")
                return FormatoLinea.MO_COMPLETA

        # Detectar INSUMO_BASICO si tiene suficientes campos num√©ricos
        if num_fields >= 4:
            numeric_values = self.numeric_extractor.extract_all_numeric_values(fields)
            if len(numeric_values) >= 2:
                logger.debug(f"INSUMO_BASICO detectado: {descripcion[:30]}...")
                return FormatoLinea.INSUMO_BASICO

        return FormatoLinea.DESCONOCIDO

    def _is_noise_line(self, descripcion: str, num_fields: int) -> bool:
        """Detecta si una l√≠nea es ruido (encabezado, resumen, etc.)."""
        if self.pattern_matcher.is_likely_summary(descripcion, num_fields):
            logger.debug(f"L√≠nea de resumen ignorada: {descripcion[:30]}...")
            return True

        if self.pattern_matcher.is_likely_header(descripcion, num_fields):
            logger.debug(f"L√≠nea de encabezado ignorada: {descripcion[:30]}...")
            return True

        if self.pattern_matcher.is_likely_category(descripcion, num_fields):
            logger.debug(f"L√≠nea de categor√≠a ignorada: {descripcion[:30]}...")
            return True

        return False

    def _validate_mo_format(self, fields: List[str]) -> bool:
        """Valida el formato de Mano de Obra usando el NumericFieldExtractor."""
        if len(fields) < 5:
            return False

        numeric_values = self.numeric_extractor.extract_all_numeric_values(fields)
        mo_values = self.numeric_extractor.identify_mo_values(numeric_values)

        return mo_values is not None

    def _dispatch_builder(
        self, formato: FormatoLinea, tokens: List[str]
    ) -> Optional[InsumoProcesado]:
        """
        Llama al m√©todo constructor adecuado seg√∫n el formato detectado.

        Args:
            formato: El `FormatoLinea` detectado.
            tokens: La lista de campos de la l√≠nea.

        Returns:
            Un objeto `InsumoProcesado` o None si la construcci√≥n falla.
        """
        try:
            if formato == FormatoLinea.MO_COMPLETA:
                return self._build_mo_completa(tokens)
            elif formato == FormatoLinea.INSUMO_BASICO:
                return self._build_insumo_basico(tokens)
            return None
        except Exception as e:
            logger.error(f"Error construyendo {formato.value}: {e}")
            return None

    def _build_mo_completa(self, tokens: List[str]) -> Optional[ManoDeObra]:
        """
        Construye un objeto `ManoDeObra` a partir de una l√≠nea de formato completo.

        Utiliza el `NumericFieldExtractor` para encontrar el rendimiento y el
        jornal, y luego calcula los dem√°s valores.

        Args:
            tokens: Lista de campos de la l√≠nea.

        Returns:
            Un objeto `ManoDeObra` o None.
        """
        try:
            descripcion = tokens[0]
            unidad = (
                self.units_validator.normalize_unit(tokens[1]) if len(tokens) > 1 else "JOR"
            )

            # Usar NumericFieldExtractor para identificar valores
            numeric_values = self.numeric_extractor.extract_all_numeric_values(tokens)
            mo_values = self.numeric_extractor.identify_mo_values(numeric_values)

            if not mo_values:
                logger.debug("No se pudieron identificar jornal y rendimiento")
                return None

            rendimiento, jornal = mo_values

            # C√°lculos
            cantidad = 1.0 / rendimiento if rendimiento > 0 else 0
            valor_total = cantidad * jornal

            if cantidad <= 0 or valor_total <= 0:
                return None

            context = self.apu_context.copy()
            context.pop("cantidad_apu", None)
            context.pop("precio_unitario_apu", None)
            return ManoDeObra(
                descripcion_insumo=descripcion,
                unidad_insumo=unidad,
                cantidad=round(cantidad, 6),
                precio_unitario=round(jornal, 2),
                valor_total=round(valor_total, 2),
                rendimiento=round(rendimiento, 6),
                formato_origen="MO_COMPLETA",
                tipo_insumo="MANO_DE_OBRA",
                **context,
            )

        except Exception as e:
            logger.error(f"Error construyendo MO_COMPLETA: {e}")
            return None

    def _build_insumo_basico(self, tokens: List[str]) -> Optional[InsumoProcesado]:
        """
        Construye un objeto de insumo a partir de una l√≠nea de formato b√°sico.

        Clasifica el tipo de insumo bas√°ndose en la descripci√≥n y luego extrae
        los valores num√©ricos (cantidad, precio, total).

        Args:
            tokens: Lista de campos de la l√≠nea.

        Returns:
            Un objeto `InsumoProcesado` (o una de sus subclases) o None.
        """
        try:
            if len(tokens) < 4:
                return None

            descripcion = tokens[0]
            unidad = (
                self.units_validator.normalize_unit(tokens[1]) if len(tokens) > 1 else "UND"
            )

            # Usar NumericFieldExtractor para valores
            valores = self.numeric_extractor.extract_insumo_values(tokens)

            if len(valores) < 2:
                return None

            # Interpretar valores
            cantidad = valores[0] if len(valores) > 0 else 1.0
            precio = valores[1] if len(valores) > 1 else 0.0
            total = valores[2] if len(valores) > 2 else cantidad * precio

            # Corregir si es necesario
            if total == 0 and cantidad > 0 and precio > 0:
                total = cantidad * precio
            elif precio == 0 and cantidad > 0 and total > 0:
                precio = total / cantidad

            if total <= 0:
                return None

            tipo_insumo = self._classify_insumo(descripcion)
            InsumoClass = self._get_insumo_class(tipo_insumo)

            context = self.apu_context.copy()
            context.pop("cantidad_apu", None)
            context.pop("precio_unitario_apu", None)
            return InsumoClass(
                descripcion_insumo=descripcion,
                unidad_insumo=unidad,
                cantidad=round(cantidad, 6),
                precio_unitario=round(precio, 2),
                valor_total=round(total, 2),
                rendimiento=round(cantidad, 6),
                formato_origen="INSUMO_BASICO",
                tipo_insumo=tipo_insumo.value,
                **context,
            )

        except Exception as e:
            logger.error(f"Error construyendo INSUMO_BASICO: {e}")
            return None

    @lru_cache(maxsize=2048)
    def _classify_insumo(self, descripcion: str) -> TipoInsumo:
        """
        Clasifica el tipo de insumo bas√°ndose en palabras clave en la descripci√≥n.

        Args:
            descripcion: La descripci√≥n del insumo.

        Returns:
            El `TipoInsumo` m√°s probable.
        """
        if not descripcion:
            return TipoInsumo.OTRO

        desc_upper = descripcion.upper()

        # CAMBIO: Leer reglas desde la config
        rules = self.config.get("apu_processor_rules", {})
        special_cases = rules.get("special_cases", {})
        mo_keywords = rules.get("mo_keywords", [])
        equipo_keywords = rules.get("equipo_keywords", [])

        for case, tipo_str in special_cases.items():
            if case in desc_upper:
                return TipoInsumo(tipo_str)

        if any(kw in desc_upper for kw in mo_keywords):
            return TipoInsumo.MANO_DE_OBRA
        if any(kw in desc_upper for kw in equipo_keywords):
            return TipoInsumo.EQUIPO

        return TipoInsumo.SUMINISTRO

    def _get_insumo_class(self, tipo_insumo: TipoInsumo):
        """Obtiene la clase de `schemas` correspondiente a un `TipoInsumo`."""
        class_mapping = {
            TipoInsumo.MANO_DE_OBRA: ManoDeObra,
            TipoInsumo.EQUIPO: Equipo,
            TipoInsumo.TRANSPORTE: Transporte,
            TipoInsumo.SUMINISTRO: Suministro,
            TipoInsumo.OTRO: Otro,
        }
        return class_mapping.get(tipo_insumo, Suministro)


# -=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=
# PROCESADOR PRINCIPAL - COMPATIBLE CON LoadDataStep
# -=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=

class APUProcessor:
    """
    Procesador de APUs con soporte para m√∫ltiples formatos de entrada.

    Soporta dos formatos:
    1. Formato agrupado (legacy): [{"codigo_apu": "X", "lines": [...]}]
    2. Formato plano (nuevo): [{"apu_code": "X", "insumo_line": "...", "_lark_tree": ...}]
    """

    def __init__(
        self,
        config,
        profile: Optional[Dict[str, Any]] = None,
        parse_cache: Optional[Dict[str, Any]] = None
    ):
        """
        Inicializa el procesador con cache opcional de parsing.

        Args:
            config: Configuraci√≥n del sistema.
            profile: Perfil de parsing.
            parse_cache: Cache de √°rboles Lark pre-parseados.
        """
        self.config = config
        self.profile = profile or {}
        self.parser = self._initialize_parser()
        self.keyword_cache = {}

        # Cache de parsing (optimizaci√≥n)
        self.parse_cache = parse_cache or {}

        # Estad√≠sticas globales
        self.global_stats = {
            "total_apus": 0,
            "total_insumos": 0,
            "format_detected": None,
        }

        self.parsing_stats = ParsingStats()
        self.debug_mode = self.config.get("debug_mode", False)

        # Registros crudos (se establecer√°n externamente)
        self.raw_records = []

        if self.parse_cache:
            logger.info(
                f"‚úì APUProcessor inicializado con cache de {len(self.parse_cache)} "
                f"l√≠neas pre-parseadas"
            )

    def _detect_record_format(
        self,
        records: List[Dict[str, Any]]
    ) -> Tuple[str, str]:
        """
        Detecta autom√°ticamente el formato de los registros de entrada.

        Args:
            records: Lista de registros a analizar.

        Returns:
            Tupla (formato, descripci√≥n) donde formato es "grouped" o "flat".
        """
        if not records:
            return ("unknown", "No hay registros para analizar")

        first_record = records[0]

        # Formato agrupado (legacy): tiene clave "lines"
        if "lines" in first_record:
            return (
                "grouped",
                "Formato agrupado (legacy): cada registro es un APU con lista de l√≠neas"
            )

        # Formato plano (nuevo): tiene claves "insumo_line" y "apu_code"
        if "insumo_line" in first_record and "apu_code" in first_record:
            return (
                "flat",
                "Formato plano (nuevo): cada registro es un insumo individual"
            )

        # Formato desconocido
        logger.warning(
            f"Formato de registro desconocido. Claves encontradas: {first_record.keys()}"
        )
        return ("unknown", "Formato no reconocido")

    def _group_flat_records(
        self,
        flat_records: List[Dict[str, Any]]
    ) -> List[Dict[str, Any]]:
        """
        Agrupa registros planos por APU.

        Convierte el formato plano (nuevo) al formato agrupado que el resto
        del procesador puede manejar, pero preservando optimizaciones como
        el √°rbol Lark pre-parseado.

        Args:
            flat_records: Lista de registros en formato plano.

        Returns:
            Lista de registros en formato agrupado.
        """
        logger.info(f"Agrupando {len(flat_records)} registros planos por APU...")

        # Agrupar por apu_code
        grouped = defaultdict(lambda: {
            "lines": [],
            "_lark_trees": [],  # Preservar √°rboles pre-parseados
            "metadata": {}
        })

        for record in flat_records:
            apu_code = record.get("apu_code", "UNKNOWN")

            # Agregar l√≠nea de insumo
            insumo_line = record.get("insumo_line", "")
            if insumo_line:
                grouped[apu_code]["lines"].append(insumo_line)

                # Preservar √°rbol Lark si existe
                lark_tree = record.get("_lark_tree")
                grouped[apu_code]["_lark_trees"].append(lark_tree)

            # Preservar metadata del APU (solo la primera vez)
            if not grouped[apu_code]["metadata"]:
                grouped[apu_code]["metadata"] = {
                    "apu_code": apu_code,
                    "apu_desc": record.get("apu_desc", ""),
                    "apu_unit": record.get("apu_unit", ""),
                    "category": record.get("category", "INDEFINIDO"),
                    "source_line": record.get("source_line", 0),
                }

        # Convertir a lista de registros agrupados
        result = []
        for apu_code, data in grouped.items():
            record = {
                "codigo_apu": apu_code,  # Usar nombre legacy para compatibilidad
                "descripcion_apu": data["metadata"].get("apu_desc", ""),
                "unidad_apu": data["metadata"].get("apu_unit", ""),
                "lines": data["lines"],
                "_lark_trees": data["_lark_trees"],  # Nueva clave para optimizaci√≥n
                "category": data["metadata"].get("category", "INDEFINIDO"),
                "source_line": data["metadata"].get("source_line", 0),
            }
            result.append(record)

        logger.info(f"‚úì Agrupados en {len(result)} APUs distintos")

        return result

    def process_all(self) -> pd.DataFrame:
        """
        Procesa todos los registros de APU crudos y devuelve un DataFrame.

        Este m√©todo ahora es ADAPTATIVO:
        - Detecta autom√°ticamente el formato de entrada
        - Convierte formato plano a agrupado si es necesario
        - Reutiliza √°rboles Lark pre-parseados cuando est√°n disponibles
        - Mantiene compatibilidad con formato legacy

        Returns:
            DataFrame con todos los insumos procesados y estructurados.
        """
        if not self.raw_records:
            logger.warning("No hay registros crudos para procesar")
            return pd.DataFrame()

        logger.info(f"Iniciando procesamiento de {len(self.raw_records)} registros")

        # üî• PASO 1: Detectar formato de entrada
        format_type, format_desc = self._detect_record_format(self.raw_records)
        self.global_stats["format_detected"] = format_type

        logger.info(f"üìã Formato detectado: {format_desc}")

        # üî• PASO 2: Normalizar a formato agrupado si es necesario
        if format_type == "flat":
            processed_records = self._group_flat_records(self.raw_records)
        elif format_type == "grouped":
            processed_records = self.raw_records
            logger.info("‚úì Formato ya est√° agrupado, no se requiere conversi√≥n")
        else:
            logger.error(
                "‚ùå Formato de entrada no reconocido. "
                "No se puede procesar sin formato conocido."
            )
            return pd.DataFrame()

        # üî• PASO 3: Procesar cada APU
        all_results = []
        self.global_stats["total_apus"] = len(processed_records)

        for i, record in enumerate(processed_records):
            try:
                apu_context = self._extract_apu_context(record)

                if "lines" in record and record["lines"]:
                    # Preparar cache espec√≠fico para este APU
                    apu_cache = self._prepare_apu_cache(record)

                    insumos = self._process_apu_lines(
                        record["lines"],
                        apu_context,
                        apu_cache
                    )

                    if insumos:
                        all_results.extend(insumos)
                else:
                    logger.debug(
                        f"APU {apu_context.get('codigo_apu')} no tiene l√≠neas para procesar"
                    )

                # Log de progreso
                if (i + 1) % 50 == 0:
                    logger.info(
                        f"Progreso: {i + 1}/{len(processed_records)} APUs procesados "
                        f"({len(all_results)} insumos extra√≠dos hasta ahora)"
                    )

            except Exception as e:
                logger.error(
                    f"Error procesando APU {i} "
                    f"[{record.get('codigo_apu', 'UNKNOWN')}]: {e}"
                )
                if self.debug_mode:
                    import traceback
                    logger.debug(f"Traceback:\n{traceback.format_exc()}")
                continue

        # üî• PASO 4: Log de resultados finales
        self.global_stats["total_insumos"] = len(all_results)
        self._log_global_stats()

        # üî• PASO 5: Convertir a DataFrame
        if all_results:
            return self._convert_to_dataframe(all_results)
        else:
            logger.warning("‚ö†Ô∏è  No se encontraron insumos v√°lidos en ning√∫n APU")
            return pd.DataFrame()

    def _prepare_apu_cache(self, record: Dict[str, Any]) -> Dict[str, Any]:
        """
        Prepara el cache de parsing espec√≠fico para un APU.

        Si el registro tiene √°rboles Lark pre-parseados (_lark_trees),
        crea un mapeo l√≠nea -> √°rbol para ese APU espec√≠fico.

        Args:
            record: Registro del APU con posibles √°rboles pre-parseados.

        Returns:
            Diccionario de cache l√≠nea -> √°rbol para este APU.
        """
        apu_cache = {}

        # Si el registro tiene √°rboles pre-parseados, mapearlos
        if "_lark_trees" in record and record["_lark_trees"]:
            lines = record.get("lines", [])
            trees = record["_lark_trees"]

            # Crear mapeo l√≠nea -> √°rbol
            for line, tree in zip(lines, trees):
                if tree is not None:
                    apu_cache[line.strip()] = tree

            if apu_cache:
                logger.debug(
                    f"‚úì Cache espec√≠fico de APU preparado: {len(apu_cache)} √°rboles"
                )

        # Combinar con cache global
        combined_cache = {**self.parse_cache, **apu_cache}

        return combined_cache

    def _extract_apu_context(self, record: Dict[str, Any]) -> Dict[str, Any]:
        """
        Extrae el contexto relevante de un registro de APU.

        Soporta tanto nombres de claves legacy como nuevos.

        Args:
            record: Registro de APU (formato agrupado).

        Returns:
            Diccionario con contexto del APU normalizado.
        """
        # Intentar claves nuevas primero, luego legacy
        return {
            "codigo_apu": record.get("codigo_apu") or record.get("apu_code", ""),
            "descripcion_apu": (
                record.get("descripcion_apu") or
                record.get("apu_desc", "")
            ),
            "unidad_apu": record.get("unidad_apu") or record.get("apu_unit", ""),
            "cantidad_apu": record.get("cantidad_apu", 1.0),
            "precio_unitario_apu": record.get("precio_unitario_apu", 0.0),
            "categoria": record.get("category", "INDEFINIDO"),
        }

    def _process_apu_lines(
        self,
        lines: List[str],
        apu_context: Dict[str, Any],
        line_cache: Optional[Dict[str, Any]] = None
    ) -> List['InsumoProcesado']:
        """
        Procesa l√≠neas de APU con reutilizaci√≥n de cache de parsing.

        Args:
            lines: Lista de l√≠neas a procesar.
            apu_context: Contexto del APU.
            line_cache: Cache de √°rboles Lark para estas l√≠neas espec√≠ficas.

        Returns:
            Lista de insumos procesados.
        """
        if not lines:
            return []

        results = []
        stats = ParsingStats()

        # Usar cache combinado (espec√≠fico del APU + global)
        active_cache = line_cache if line_cache is not None else self.parse_cache

        apu_code = apu_context.get("codigo_apu", "UNKNOWN")

        logger.debug(
            f"Procesando {len(lines)} l√≠neas para APU: {apu_code} "
            f"(cache: {len(active_cache)} entradas)"
        )

        for line_num, line in enumerate(lines, start=1):
            if not line or not line.strip():
                continue

            stats.total_lines += 1
            line_clean = line.strip()
            insumo = None

            try:
                if self.parser:
                    # üî• OPTIMIZACI√ìN: Usar cache si est√° disponible
                    tree = None
                    used_cache = False

                    if line_clean in active_cache:
                        tree = active_cache[line_clean]
                        used_cache = True
                        stats.cache_hits += 1
                        logger.debug(
                            f"  ‚ö° L√≠nea {line_num}: Usando √°rbol Lark del cache"
                        )

                    if tree is None:
                        # Parsear normalmente
                        try:
                            tree = self.parser.parse(line_clean)
                        except LarkError as lark_error:
                            # Si falla aqu√≠ con validaci√≥n unificada, es inesperado
                            logger.warning(
                                f"  ‚ö†Ô∏è  L√≠nea {line_num}: Fall√≥ Lark pero pas√≥ validaci√≥n previa\n"
                                f"      Error: {lark_error}\n"
                                f"      L√≠nea: {line_clean[:100]}"
                            )
                            stats.lark_parse_errors += 1
                            continue

                    # Transformar √°rbol a insumo
                    try:
                        transformer = APUTransformer(
                            apu_context,
                            self.config,
                            self.profile,
                            self.keyword_cache
                        )
                        insumo = transformer.transform(tree)

                        if isinstance(insumo, list):
                            if insumo:
                                insumo = insumo[0]
                                stats.successful_parses += 1
                            else:
                                stats.empty_results += 1
                                logger.debug(
                                    f"  ‚ö†Ô∏è  L√≠nea {line_num}: Transformer devolvi√≥ lista vac√≠a"
                                )
                                insumo = None
                        else:
                            stats.successful_parses += 1

                    except Exception as transform_error:
                        stats.transformer_errors += 1
                        logger.error(
                            f"  ‚úó L√≠nea {line_num}: Error en transformer\n"
                            f"    Error: {type(transform_error).__name__}: {transform_error}\n"
                            f"    L√≠nea: {line_clean[:100]}"
                        )

                        if self.debug_mode:
                            import traceback
                            logger.debug(f"Traceback:\n{traceback.format_exc()}")

                        continue

                # Agregar resultado si es v√°lido
                if insumo:
                    insumo.line_number = line_num
                    results.append(insumo)
                else:
                    stats.failed_lines.append({
                        "line_number": line_num,
                        "content": line_clean,
                        "apu_code": apu_code
                    })

            except Exception as unexpected_error:
                logger.error(
                    f"  üö® L√≠nea {line_num}: Error inesperado\n"
                    f"    Tipo: {type(unexpected_error).__name__}\n"
                    f"    Error: {unexpected_error}\n"
                    f"    L√≠nea: {line_clean}"
                )

                if self.debug_mode:
                    import traceback
                    logger.debug(f"Traceback completo:\n{traceback.format_exc()}")

                stats.failed_lines.append({
                    "line_number": line_num,
                    "content": line_clean,
                    "error": str(unexpected_error),
                    "apu_code": apu_code
                })
                continue

        # Log de estad√≠sticas del APU
        self._log_parsing_stats(apu_code, stats)

        # Actualizar estad√≠sticas globales
        self._merge_stats(stats)

        return results

    def _merge_stats(self, apu_stats: ParsingStats):
        """Combina estad√≠sticas de un APU con las globales."""
        self.parsing_stats.total_lines += apu_stats.total_lines
        self.parsing_stats.successful_parses += apu_stats.successful_parses
        self.parsing_stats.lark_parse_errors += apu_stats.lark_parse_errors
        self.parsing_stats.transformer_errors += apu_stats.transformer_errors
        self.parsing_stats.empty_results += apu_stats.empty_results
        self.parsing_stats.cache_hits += apu_stats.cache_hits
        self.parsing_stats.failed_lines.extend(apu_stats.failed_lines)

    def _log_parsing_stats(self, apu_code: str, stats: ParsingStats):
        """
        Registra estad√≠sticas detalladas del parsing de un APU.

        Args:
            apu_code: C√≥digo del APU procesado.
            stats: Estad√≠sticas del procesamiento.
        """
        if stats.total_lines == 0:
            return

        success_rate = (
            (stats.successful_parses / stats.total_lines * 100)
            if stats.total_lines > 0 else 0
        )
        cache_rate = (
            (stats.cache_hits / stats.total_lines * 100)
            if stats.total_lines > 0 else 0
        )

        # Solo mostrar detalles si hay problemas o en modo debug
        if success_rate < 100 or self.debug_mode:
            logger.info("-" * 70)
            logger.info(f"üìà APU: {apu_code}")
            logger.info(f"   L√≠neas procesadas:  {stats.total_lines}")
            logger.info(f"   ‚úì Exitosos:         {stats.successful_parses} ({success_rate:.1f}%)")
            logger.info(f"   ‚ö° Cache hits:       {stats.cache_hits} ({cache_rate:.1f}%)")

            if stats.lark_parse_errors > 0:
                logger.info(f"   ‚úó Errores Lark:     {stats.lark_parse_errors}")
            if stats.transformer_errors > 0:
                logger.info(f"   ‚úó Errores Trans.:   {stats.transformer_errors}")
            if stats.empty_results > 0:
                logger.info(f"   ‚ö†Ô∏è  Resultados vac√≠os: {stats.empty_results}")

            logger.info("-" * 70)

    def _log_global_stats(self):
        """Registra estad√≠sticas globales del procesamiento."""
        logger.info("=" * 80)
        logger.info("üìä RESUMEN GLOBAL DE PROCESAMIENTO")
        logger.info("=" * 80)
        logger.info(f"Formato detectado:           {self.global_stats['format_detected']}")
        logger.info(f"Total APUs procesados:       {self.global_stats['total_apus']}")
        logger.info(f"Total insumos extra√≠dos:     {self.global_stats['total_insumos']}")
        logger.info(f"Total l√≠neas procesadas:     {self.parsing_stats.total_lines}")
        logger.info("")
        logger.info("Resultados de parsing:")
        logger.info(f"  ‚úì Exitosos:                {self.parsing_stats.successful_parses}")
        logger.info(f"  ‚ö° Cache hits:              {self.parsing_stats.cache_hits}")
        logger.info(f"  ‚úó Errores Lark:            {self.parsing_stats.lark_parse_errors}")
        logger.info(f"  ‚úó Errores Transformer:     {self.parsing_stats.transformer_errors}")
        logger.info(f"  ‚ö†Ô∏è  Resultados vac√≠os:      {self.parsing_stats.empty_results}")
        logger.info("")

        if self.parsing_stats.total_lines > 0:
            success_rate = (
                self.parsing_stats.successful_parses /
                self.parsing_stats.total_lines * 100
            )
            cache_efficiency = (
                self.parsing_stats.cache_hits /
                self.parsing_stats.total_lines * 100
            )

            logger.info(f"Tasa de √©xito:               {success_rate:.2f}%")
            logger.info(f"Eficiencia de cache:         {cache_efficiency:.2f}%")

        logger.info("=" * 80)

        # Alertas
        if self.global_stats['total_insumos'] == 0:
            logger.error(
                "üö® CR√çTICO: 0 insumos extra√≠dos.\n"
                "   Posibles causas:\n"
                "   1. Formato de datos incompatible con gram√°tica\n"
                "   2. Errores en el transformer\n"
                "   3. Configuraci√≥n de perfil incorrecta\n"
                "   ‚Üí Revise los logs detallados arriba"
            )
        elif success_rate < 50:
            logger.warning(
                f"‚ö†Ô∏è  Tasa de √©xito baja ({success_rate:.1f}%).\n"
                f"   Considere revisar la gram√°tica o el formato de datos."
            )

    def _initialize_parser(self):
        """Inicializa el parser Lark (implementar seg√∫n tu c√≥digo existente)."""
        # Placeholder - implementar seg√∫n tu l√≥gica
        try:
            from lark import Lark

            return Lark(
                APU_GRAMMAR,
                start='line',
                parser='lalr',
                maybe_placeholders=False,
                cache=True,
            )
        except Exception as e:
            logger.error(f"Error inicializando parser Lark: {e}")
            return None

    def _convert_to_dataframe(self, insumos: List[InsumoProcesado]) -> pd.DataFrame:
        """
        Convierte una lista de objetos `InsumoProcesado` a un DataFrame.
        """
        records = []
        for insumo in insumos:
            record = {
                "CODIGO_APU": getattr(insumo, "codigo_apu", ""),
                "DESCRIPCION_APU": getattr(insumo, "descripcion_apu", ""),
                "UNIDAD_APU": getattr(insumo, "unidad_apu", ""),
                "DESCRIPCION_INSUMO": getattr(insumo, "descripcion_insumo", ""),
                "UNIDAD_INSUMO": getattr(insumo, "unidad_insumo", ""),
                "CANTIDAD_APU": getattr(insumo, "cantidad", 0.0),
                "PRECIO_UNIT_APU": getattr(insumo, "precio_unitario", 0.0),
                "VALOR_TOTAL_APU": getattr(insumo, "valor_total", 0.0),
                "RENDIMIENTO": getattr(insumo, "rendimiento", 0.0),
                "TIPO_INSUMO": getattr(insumo, "tipo_insumo", "OTRO"),
                "FORMATO_ORIGEN": getattr(insumo, "formato_origen", ""),
                "CATEGORIA": getattr(insumo, "categoria", ""),
                "NORMALIZED_DESC": getattr(insumo, "normalized_desc", "")
            }
            records.append(record)

        df = pd.DataFrame(records)
        logger.info(f"‚úì DataFrame creado: {len(df)} filas, {len(df.columns)} columnas")
        return df
