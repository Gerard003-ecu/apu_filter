"""
Funciones Auxiliares de Topología Algebraica y Análisis Estructural.

Marco Matemático:
-----------------
1. Homología Simplicial: H_k(X) calculada via complejos de cadenas
2. Persistencia: Filtración F_0 ⊆ F_1 ⊆ ... ⊆ F_n con tracking de nacimiento/muerte
3. Entropía de Shannon: H(X) = -Σ p(x)log(p(x)) con regularización
4. Métricas de Riesgo: Basadas en teoría de portafolios de Markowitz
"""

import math
from collections import Counter
from dataclasses import dataclass
from pathlib import Path
from typing import (
    Any,
    Callable,
    Dict,
    Final,
    FrozenSet,
    List,
    Optional,
    Sequence,
    Set,
    Tuple,
    Type,
    Union,
)

import numpy as np
import pandas as pd

# ══════════════════════════════════════════════════════════════════════════════
# CONSTANTES MATEMÁTICAS
# ══════════════════════════════════════════════════════════════════════════════

_EPSILON: Final[float] = 1e-10  # Para estabilidad numérica
_DEFAULT_RANDOM_SEED: Final[int] = 42
_MAX_SAMPLE_ROWS: Final[int] = 1000  # Límite para análisis topológico
_PERSISTENCE_THRESHOLD: Final[float] = 0.01  # Filtrar características efímeras


# ══════════════════════════════════════════════════════════════════════════════
# ESTRUCTURAS DE DATOS TOPOLÓGICAS
# ══════════════════════════════════════════════════════════════════════════════

@dataclass(frozen=True)
class PersistenceInterval:
    """
    Intervalo de persistencia [birth, death) en el diagrama.
    
    Representa el tiempo de vida de una característica topológica
    durante la filtración del complejo simplicial.
    """
    birth: float
    death: float
    dimension: int = 0  # 0 = componentes, 1 = ciclos, 2 = cavidades
    
    def __post_init__(self):
        if self.birth < 0:
            raise ValueError(f"Birth time must be non-negative, got {self.birth}")
        if self.death < self.birth:
            raise ValueError(f"Death ({self.death}) must be >= birth ({self.birth})")
        if self.dimension < 0:
            raise ValueError(f"Dimension must be non-negative, got {self.dimension}")
    
    @property
    def persistence(self) -> float:
        """Tiempo de vida de la característica."""
        return self.death - self.birth
    
    @property
    def is_essential(self) -> bool:
        """True si la característica nunca muere (death = inf)."""
        return math.isinf(self.death)
    
    def __lt__(self, other: 'PersistenceInterval') -> bool:
        """Ordenamiento por persistencia descendente."""
        return self.persistence > other.persistence


@dataclass(frozen=True)
class TopologicalSummary:
    """
    Resumen de características topológicas de un dataset.
    
    Encapsula números de Betti, entropía estructural y métricas derivadas.
    """
    betti_0: int  # Componentes conexos
    betti_1: int  # Ciclos independientes
    betti_2: int  # Cavidades
    euler_characteristic: int
    structural_entropy: float
    persistence_entropy: float
    
    @classmethod
    def empty(cls) -> 'TopologicalSummary':
        """Crea un resumen vacío para casos de error."""
        return cls(
            betti_0=0, betti_1=0, betti_2=0,
            euler_characteristic=0,
            structural_entropy=0.0,
            persistence_entropy=0.0
        )
    
    def to_dict(self) -> Dict[str, Any]:
        """Serializa a diccionario."""
        return {
            "betti_numbers": [self.betti_0, self.betti_1, self.betti_2],
            "euler_characteristic": self.euler_characteristic,
            "structural_entropy": round(self.structural_entropy, 6),
            "persistence_entropy": round(self.persistence_entropy, 6),
        }


# ══════════════════════════════════════════════════════════════════════════════
# FUNCIONES DE ENTROPÍA Y PROBABILIDAD
# ══════════════════════════════════════════════════════════════════════════════

def _compute_shannon_entropy(
    probabilities: Sequence[float],
    base: float = 2.0
) -> float:
    """
    Calcula la entropía de Shannon con estabilidad numérica.
    
    H(X) = -Σ p(x) * log_b(p(x))
    
    Args:
        probabilities: Distribución de probabilidad (debe sumar ~1)
        base: Base del logaritmo (2 = bits, e = nats)
        
    Returns:
        Entropía en la base especificada
        
    Raises:
        ValueError: Si las probabilidades son inválidas
    """
    if not probabilities:
        return 0.0
    
    probs = np.asarray(probabilities, dtype=np.float64)
    
    # Validación
    if np.any(probs < 0):
        raise ValueError("Probabilities cannot be negative")
    
    total = np.sum(probs)
    if total < _EPSILON:
        return 0.0  # Distribución degenerada
    
    # Normalizar si no suma exactamente 1
    if not np.isclose(total, 1.0, rtol=1e-5):
        probs = probs / total
    
    # Filtrar ceros para evitar log(0)
    nonzero_probs = probs[probs > _EPSILON]
    
    if len(nonzero_probs) == 0:
        return 0.0
    
    # H(X) = -Σ p * log(p)
    log_probs = np.log(nonzero_probs) / np.log(base)
    entropy = -np.sum(nonzero_probs * log_probs)
    
    return float(max(0.0, entropy))  # Asegurar no-negatividad por errores numéricos


def _compute_distribution_from_counts(
    counts: Union[Dict[Any, int], Counter]
) -> List[float]:
    """
    Convierte conteos a distribución de probabilidad.
    
    Args:
        counts: Mapeo valor -> frecuencia
        
    Returns:
        Lista de probabilidades normalizadas
    """
    if not counts:
        return []
    
    values = list(counts.values())
    total = sum(values)
    
    if total == 0:
        return []
    
    return [v / total for v in values]


def _compute_persistence_entropy(
    intervals: Sequence[PersistenceInterval]
) -> float:
    """
    Calcula la entropía del diagrama de persistencia.
    
    Mide la distribución de "importancia" entre características topológicas.
    Alta entropía = muchas características igualmente importantes.
    Baja entropía = pocas características dominantes.
    
    Args:
        intervals: Lista de intervalos de persistencia
        
    Returns:
        Entropía de persistencia normalizada [0, 1]
    """
    if not intervals:
        return 0.0
    
    # Filtrar intervalos esenciales y calcular persistencias finitas
    finite_intervals = [iv for iv in intervals if not iv.is_essential]
    
    if not finite_intervals:
        return 0.0
    
    persistences = [iv.persistence for iv in finite_intervals]
    total_persistence = sum(persistences)
    
    if total_persistence < _EPSILON:
        return 0.0
    
    # Normalizar a distribución de probabilidad
    probs = [p / total_persistence for p in persistences]
    
    # Entropía normalizada por máximo teórico
    raw_entropy = _compute_shannon_entropy(probs)
    max_entropy = math.log2(len(probs)) if len(probs) > 1 else 1.0
    
    return raw_entropy / max_entropy if max_entropy > 0 else 0.0


# ══════════════════════════════════════════════════════════════════════════════
# FUNCIONES DE TOPOLOGÍA ALGEBRAICA
# ══════════════════════════════════════════════════════════════════════════════

def _analyze_topological_features(file_path: Path) -> Dict[str, Any]:
    """
    Analiza características topológicas de un archivo.
    
    Modelo: Construimos un grafo de similitud donde:
    - Nodos = líneas del archivo
    - Aristas = conexiones por contenido compartido
    
    Los números de Betti se estiman via análisis de componentes:
    - β₀ = componentes conexos (clusters de líneas similares)
    - β₁ = ciclos independientes (patrones repetitivos)
    
    Args:
        file_path: Ruta al archivo a analizar
        
    Returns:
        Diccionario con métricas topológicas
    """
    try:
        with open(file_path, 'r', encoding='utf-8', errors='replace') as f:
            lines = [line.rstrip('\n\r') for line in f.readlines()[:_MAX_SAMPLE_ROWS]]
        
        if not lines:
            return TopologicalSummary.empty().to_dict()
        
        # ─── Análisis de Componentes Conexos (β₀) ───
        # Usamos clustering por contenido: líneas idénticas forman componentes
        line_counts = Counter(lines)
        num_unique_patterns = len(line_counts)
        
        # β₀ real: número de clusters de líneas distintas
        # En topología de datos, esto corresponde a componentes conexos
        # del complejo de Vietoris-Rips a radio 0
        beta_0 = num_unique_patterns
        
        # ─── Detección de Ciclos (β₁) ───
        # Ciclo = patrón que se repite después de un gap
        # Detectamos via análisis de periodicidad
        beta_1 = _detect_cyclic_patterns(lines)
        
        # ─── Dimensión Intrínseca ───
        # Estimamos la dimensión del espacio de datos
        dimension = _estimate_intrinsic_dimension(lines)
        
        # ─── Característica de Euler ───
        # χ = β₀ - β₁ + β₂ (asumimos β₂ = 0 para datos 1D)
        euler_char = beta_0 - beta_1
        
        # ─── Entropía Estructural ───
        distribution = _compute_distribution_from_counts(line_counts)
        structural_entropy = _compute_shannon_entropy(distribution)
        
        return {
            "beta_0": beta_0,
            "beta_1": beta_1,
            "beta_2": 0,  # Cavidades no aplican a datos 1D
            "euler_characteristic": euler_char,
            "intrinsic_dimension": dimension,
            "structural_entropy": round(structural_entropy, 6),
            "num_lines": len(lines),
            "num_unique_patterns": num_unique_patterns,
            "repetition_ratio": 1.0 - (num_unique_patterns / len(lines)) if lines else 0.0
        }
        
    except Exception as e:
        logger.warning(f"Topological analysis failed: {e}")
        return {
            **TopologicalSummary.empty().to_dict(),
            "analysis_error": str(e)
        }


def _detect_cyclic_patterns(lines: List[str], max_period: int = 50) -> int:
    """
    Detecta patrones cíclicos en una secuencia de líneas.
    
    Un ciclo de período p existe si line[i] ≈ line[i+p] para muchos i.
    
    Args:
        lines: Secuencia de líneas
        max_period: Período máximo a buscar
        
    Returns:
        Número de ciclos independientes detectados (β₁)
    """
    if len(lines) < 3:
        return 0
    
    cycles_found = 0
    
    for period in range(1, min(max_period, len(lines) // 2)):
        matches = 0
        comparisons = 0
        
        for i in range(len(lines) - period):
            comparisons += 1
            if lines[i] == lines[i + period]:
                matches += 1
        
        if comparisons > 0:
            match_ratio = matches / comparisons
            # Si más del 80% coincide, consideramos un ciclo
            if match_ratio > 0.8:
                cycles_found += 1
    
    return cycles_found


def _estimate_intrinsic_dimension(lines: List[str]) -> int:
    """
    Estima la dimensión intrínseca del espacio de datos.
    
    Basado en la estructura columnar del contenido (para CSVs).
    
    Args:
        lines: Líneas del archivo
        
    Returns:
        Dimensión estimada (número de columnas efectivas)
    """
    if not lines:
        return 0
    
    # Intentar detectar delimitador
    sample = lines[0] if lines else ""
    
    for delimiter in [',', ';', '\t', '|']:
        if delimiter in sample:
            # Contar columnas consistentes
            col_counts = [len(line.split(delimiter)) for line in lines[:100]]
            if col_counts:
                # Usar la moda como dimensión
                return max(set(col_counts), key=col_counts.count)
    
    # Sin estructura columnar detectada
    return 1


def _compute_homology_groups(diagnostic_data: Dict[str, Any]) -> Dict[str, Any]:
    """
    Calcula grupos de homología a partir de datos diagnósticos.
    
    Modelo Algebraico:
    - H₀(X) ≅ ℤ^β₀ donde β₀ = componentes conexos (tipos de issues únicos)
    - H₁(X) ≅ ℤ^β₁ donde β₁ = ciclos (dependencias circulares)
    
    Args:
        diagnostic_data: Resultado del diagnóstico
        
    Returns:
        Diccionario con grupos de homología
    """
    issues = diagnostic_data.get("issues", [])
    warnings = diagnostic_data.get("warnings", [])
    errors = diagnostic_data.get("errors", [])
    
    # ─── H₀: Componentes Conexos ───
    # Categorizar issues por tipo para encontrar componentes independientes
    issue_types: Set[str] = set()
    for issue in issues:
        if isinstance(issue, dict):
            issue_types.add(issue.get("type", issue.get("code", "unknown")))
        else:
            issue_types.add(str(type(issue).__name__))
    
    beta_0 = max(1, len(issue_types))  # Mínimo 1 componente (el espacio mismo)
    
    # ─── H₁: Ciclos ───
    # Detectar dependencias circulares en warnings/issues
    circular_keywords = {"circular", "cycle", "loop", "recursive", "dependency"}
    
    def has_circular_reference(item: Any) -> bool:
        text = str(item).lower()
        return any(kw in text for kw in circular_keywords)
    
    circular_in_warnings = sum(1 for w in warnings if has_circular_reference(w))
    circular_in_issues = sum(1 for i in issues if has_circular_reference(i))
    
    beta_1 = circular_in_warnings + circular_in_issues
    
    # ─── Métricas Derivadas ───
    euler_char = beta_0 - beta_1
    
    # Rango del grupo de homología total
    total_rank = beta_0 + beta_1
    
    return {
        "H_0": f"ℤ^{beta_0}",  # Notación algebraica
        "H_1": f"ℤ^{beta_1}",
        "beta_0": beta_0,
        "beta_1": beta_1,
        "betti_numbers": [beta_0, beta_1, 0],
        "euler_characteristic": euler_char,
        "total_rank": total_rank,
        "is_connected": beta_0 == 1,
        "has_cycles": beta_1 > 0
    }


def _compute_persistence_diagram(
    diagnostic_data: Dict[str, Any]
) -> List[Dict[str, Any]]:
    """
    Calcula diagrama de persistencia para características topológicas.
    
    Cada issue genera un punto (birth, death) donde:
    - birth = momento de aparición en la filtración
    - death = birth + severidad (tiempo de vida de la característica)
    
    Características más severas tienen mayor persistencia.
    
    Args:
        diagnostic_data: Datos del diagnóstico
        
    Returns:
        Lista de intervalos de persistencia serializados
    """
    issues = diagnostic_data.get("issues", [])
    
    if not issues:
        return []
    
    # Mapeo de severidad a valores numéricos
    severity_map: Dict[str, float] = {
        "CRITICAL": 1.0,
        "HIGH": 0.8,
        "MEDIUM": 0.5,
        "LOW": 0.2,
        "INFO": 0.1,
    }
    
    intervals: List[PersistenceInterval] = []
    
    for idx, issue in enumerate(issues):
        # Determinar severidad
        if isinstance(issue, dict):
            raw_severity = issue.get("severity", "MEDIUM")
            if isinstance(raw_severity, str):
                severity = severity_map.get(raw_severity.upper(), 0.5)
            elif isinstance(raw_severity, (int, float)):
                severity = float(raw_severity)
            else:
                severity = 0.5
        else:
            severity = 0.5
        
        # Calcular birth y death
        birth = idx * 0.1  # Espaciado uniforme en la filtración
        death = birth + severity
        
        try:
            interval = PersistenceInterval(
                birth=birth,
                death=death,
                dimension=0  # Issues son características 0-dimensionales
            )
            intervals.append(interval)
        except ValueError:
            continue  # Ignorar intervalos inválidos
    
    # Filtrar características efímeras
    significant_intervals = [
        iv for iv in intervals 
        if iv.persistence >= _PERSISTENCE_THRESHOLD
    ]
    
    # Ordenar por persistencia (más persistente primero)
    significant_intervals.sort()
    
    # Calcular entropía del diagrama
    persistence_entropy = _compute_persistence_entropy(significant_intervals)
    
    # Serializar
    return [
        {
            "birth": round(iv.birth, 4),
            "death": round(iv.death, 4),
            "persistence": round(iv.persistence, 4),
            "dimension": iv.dimension,
            "is_significant": iv.persistence >= 0.5
        }
        for iv in significant_intervals
    ]


def _compute_diagnostic_magnitude(diagnostic_data: Dict[str, Any]) -> float:
    """
    Calcula la magnitud normalizada del vector diagnóstico.
    
    Usa norma L2 ponderada:
    ||d|| = √(w₁·issues² + w₂·warnings² + w₃·errors²)
    
    Pesos reflejan severidad relativa (errores > issues > warnings).
    
    Args:
        diagnostic_data: Datos del diagnóstico
        
    Returns:
        Magnitud normalizada en [0, 1]
    """
    # Conteos
    n_issues = len(diagnostic_data.get("issues", []))
    n_warnings = len(diagnostic_data.get("warnings", []))
    n_errors = len(diagnostic_data.get("errors", []))
    
    # Pesos de severidad
    w_errors = 3.0
    w_issues = 2.0
    w_warnings = 1.0
    
    # Norma L2 ponderada
    raw_magnitude = math.sqrt(
        w_errors * (n_errors ** 2) +
        w_issues * (n_issues ** 2) +
        w_warnings * (n_warnings ** 2)
    )
    
    # Normalización sigmoidal para acotar en [0, 1]
    # Usamos tanh para suavidad: magnitude → tanh(magnitude/k)
    # k = 10 significa que ~10 problemas ponderados → 0.76
    normalization_factor = 10.0
    normalized = math.tanh(raw_magnitude / normalization_factor)
    
    return round(normalized, 4)


# ══════════════════════════════════════════════════════════════════════════════
# FUNCIONES DE ANÁLISIS CSV
# ══════════════════════════════════════════════════════════════════════════════

def _analyze_csv_topology(
    path: Path,
    delimiter: str,
    encoding: str
) -> Dict[str, Any]:
    """
    Analiza la topología estructural de un archivo CSV.
    
    Métricas:
    - Dimensionalidad: número de columnas (dimensión del espacio de features)
    - Densidad: ratio de celdas no-nulas
    - Entropía de nulos: incertidumbre en patrón de valores faltantes
    - Rango estimado: dimensión efectiva del espacio (via correlaciones)
    
    Args:
        path: Ruta al archivo CSV
        delimiter: Delimitador de columnas
        encoding: Encoding del archivo
        
    Returns:
        Diccionario con métricas topológicas del CSV
    """
    try:
        # Leer muestra representativa
        df = pd.read_csv(
            path,
            sep=delimiter,
            encoding=encoding,
            nrows=_MAX_SAMPLE_ROWS,
            on_bad_lines='skip'  # Tolerancia a errores
        )
        
        if df.empty:
            return {
                "rows": 0,
                "columns": 0,
                "density": 0.0,
                "null_entropy": 0.0,
                "effective_rank": 0,
                "is_empty": True
            }
        
        n_rows, n_cols = df.shape
        
        # ─── Densidad ───
        total_cells = n_rows * n_cols
        non_null_cells = df.notna().sum().sum()
        density = non_null_cells / total_cells if total_cells > 0 else 0.0
        
        # ─── Entropía de Nulos ───
        # Calcula la entropía del patrón de valores nulos por columna
        null_ratios = df.isnull().mean().values
        # Añadir epsilon para estabilidad numérica
        null_ratios_safe = np.clip(null_ratios, _EPSILON, 1.0 - _EPSILON)
        # Entropía binaria promedio por columna
        binary_entropies = -(
            null_ratios_safe * np.log2(null_ratios_safe) +
            (1 - null_ratios_safe) * np.log2(1 - null_ratios_safe)
        )
        null_entropy = float(np.mean(binary_entropies))
        
        # ─── Rango Efectivo (Dimensión Intrínseca) ───
        # Estimamos usando la matriz numérica
        effective_rank = _estimate_effective_rank(df)
        
        # ─── Métricas de Distribución ───
        # Entropía de tipos de datos
        dtype_counts = Counter(str(dtype) for dtype in df.dtypes)
        dtype_probs = _compute_distribution_from_counts(dtype_counts)
        type_entropy = _compute_shannon_entropy(dtype_probs)
        
        return {
            "rows": n_rows,
            "columns": n_cols,
            "density": round(density, 4),
            "null_entropy": round(null_entropy, 4),
            "effective_rank": effective_rank,
            "type_entropy": round(type_entropy, 4),
            "dimensionality": n_cols,
            "sparsity": round(1.0 - density, 4),
            "is_empty": False
        }
        
    except pd.errors.EmptyDataError:
        return {"rows": 0, "columns": 0, "is_empty": True, "error": "Empty file"}
    except Exception as e:
        logger.warning(f"CSV topology analysis failed: {e}")
        return {"error": str(e), "is_empty": None}


def _estimate_effective_rank(df: pd.DataFrame, threshold: float = 0.95) -> int:
    """
    Estima el rango efectivo de un DataFrame usando SVD.
    
    El rango efectivo es el número mínimo de componentes principales
    que explican al menos `threshold` de la varianza total.
    
    Args:
        df: DataFrame a analizar
        threshold: Proporción de varianza a explicar
        
    Returns:
        Rango efectivo (dimensión intrínseca)
    """
    # Seleccionar solo columnas numéricas
    numeric_df = df.select_dtypes(include=[np.number])
    
    if numeric_df.empty or numeric_df.shape[1] < 2:
        return numeric_df.shape[1] if not numeric_df.empty else 0
    
    try:
        # Imputar valores faltantes con la media
        filled = numeric_df.fillna(numeric_df.mean())
        
        # Estandarizar
        std = filled.std()
        std[std == 0] = 1  # Evitar división por cero
        normalized = (filled - filled.mean()) / std
        
        # SVD
        _, singular_values, _ = np.linalg.svd(normalized.values, full_matrices=False)
        
        # Calcular varianza explicada
        variance_explained = singular_values ** 2
        total_variance = variance_explained.sum()
        
        if total_variance < _EPSILON:
            return 0
        
        cumulative_variance = np.cumsum(variance_explained) / total_variance
        
        # Encontrar número de componentes para threshold
        effective_rank = int(np.searchsorted(cumulative_variance, threshold) + 1)
        
        return min(effective_rank, len(singular_values))
        
    except Exception:
        return numeric_df.shape[1]


def _compute_topological_preservation(
    initial: Dict[str, Any],
    final: Dict[str, Any]
) -> Dict[str, Any]:
    """
    Compara topologías inicial y final para medir preservación.
    
    Métricas de preservación:
    - Preservación dimensional: |cols_final / cols_initial|
    - Preservación de filas: |rows_final / rows_initial|
    - Delta de densidad: cambio en ratio de no-nulos
    - Delta de entropía: cambio en complejidad estructural
    
    Args:
        initial: Topología antes de la operación
        final: Topología después de la operación
        
    Returns:
        Métricas de preservación topológica
    """
    # Manejar errores
    if initial.get("error") or final.get("error"):
        return {
            "preservation_rate": 0.0,
            "is_valid": False,
            "error": initial.get("error") or final.get("error")
        }
    
    if initial.get("is_empty") or final.get("is_empty"):
        return {
            "preservation_rate": 0.0 if final.get("is_empty") else 1.0,
            "is_valid": True,
            "note": "Empty file involved"
        }
    
    # ─── Preservación de Filas ───
    init_rows = initial.get("rows", 0)
    final_rows = final.get("rows", 0)
    
    if init_rows == 0:
        row_preservation = 1.0 if final_rows == 0 else 0.0
    else:
        row_preservation = min(final_rows / init_rows, 1.0)
    
    # ─── Preservación de Columnas ───
    init_cols = initial.get("columns", 0)
    final_cols = final.get("columns", 0)
    
    if init_cols == 0:
        col_preservation = 1.0 if final_cols == 0 else 0.0
    else:
        col_preservation = min(final_cols / init_cols, 1.0)
    
    # ─── Delta de Densidad ───
    init_density = initial.get("density", 0.0)
    final_density = final.get("density", 0.0)
    density_delta = final_density - init_density
    
    # ─── Delta de Entropía ───
    init_entropy = initial.get("null_entropy", 0.0)
    final_entropy = final.get("null_entropy", 0.0)
    entropy_delta = final_entropy - init_entropy
    
    # ─── Tasa de Preservación Compuesta ───
    # Media ponderada: filas importan más que columnas
    preservation_rate = 0.6 * row_preservation + 0.4 * col_preservation
    
    # Bonus por mejora de densidad (limpieza efectiva)
    if density_delta > 0:
        preservation_rate = min(1.0, preservation_rate + 0.1 * density_delta)
    
    return {
        "preservation_rate": round(preservation_rate, 4),
        "row_preservation": round(row_preservation, 4),
        "column_preservation": round(col_preservation, 4),
        "density_delta": round(density_delta, 4),
        "entropy_delta": round(entropy_delta, 4),
        "is_valid": True,
        "improved_density": density_delta > 0,
        "reduced_entropy": entropy_delta < 0
    }


# ══════════════════════════════════════════════════════════════════════════════
# FUNCIONES FINANCIERAS CON ANÁLISIS TOPOLÓGICO
# ══════════════════════════════════════════════════════════════════════════════

def _generate_topological_cash_flows(
    amount: float,
    time_years: int,
    std_dev: float,
    *,
    random_seed: Optional[int] = None,
    decay_rate: float = 0.02
) -> List[float]:
    """
    Genera flujos de caja con dinámica temporal realista.
    
    Modelo: dC/dt = μC + σC·dW - λC
    Donde:
    - μ = tasa de crecimiento base (20% anual)
    - σ = volatilidad (std_dev)
    - λ = tasa de decaimiento (decay_rate)
    - dW = incremento de Wiener (ruido browniano)
    
    Args:
        amount: Inversión inicial
        time_years: Horizonte temporal
        std_dev: Desviación estándar del ruido
        random_seed: Semilla para reproducibilidad
        decay_rate: Tasa de decaimiento temporal
        
    Returns:
        Lista de flujos de caja proyectados
    """
    # Establecer semilla para reproducibilidad
    rng = np.random.default_rng(random_seed or _DEFAULT_RANDOM_SEED)
    
    if amount <= 0 or time_years <= 0:
        return []
    
    # Parámetros del modelo
    base_return = 0.20  # 20% anual
    
    flows: List[float] = []
    
    for t in range(time_years):
        # Tasa de retorno con decaimiento temporal
        effective_rate = base_return * math.exp(-decay_rate * t)
        
        # Componente determinista
        base_flow = amount * effective_rate
        
        # Ruido estocástico (distribución log-normal para positividad)
        # Se usa log-normal porque los retornos financieros son multiplicativos
        if std_dev > 0:
            noise_factor = rng.lognormal(mean=0, sigma=std_dev / amount)
            noise_factor = np.clip(noise_factor, 0.5, 2.0)  # Limitar extremos
        else:
            noise_factor = 1.0
        
        flow = base_flow * noise_factor
        flows.append(float(flow))
    
    return flows


def _analyze_risk_manifold(
    amount: float,
    std_dev: float,
    time_years: int,
    flows: List[float]
) -> Dict[str, Any]:
    """
    Construye una variedad de riesgo para análisis topológico.
    
    Modelo: El espacio de riesgo es una variedad 2D (Tiempo × Rendimiento)
    con métrica dada por la matriz de covarianza de flujos.
    
    Args:
        amount: Inversión inicial
        std_dev: Volatilidad esperada
        time_years: Horizonte temporal
        flows: Flujos de caja proyectados
        
    Returns:
        Características de la variedad de riesgo
    """
    if not flows or amount <= 0:
        return {
            "dimension": 0,
            "curvature": 0.0,
            "volatility_surface": 0.0,
            "flow_stability": 0.0,
            "is_degenerate": True
        }
    
    flows_array = np.array(flows)
    
    # ─── Volatilidad de Superficie ───
    # Ratio de desviación estándar a inversión
    realized_std = np.std(flows_array)
    volatility_surface = realized_std / amount if amount > 0 else 0.0
    
    # ─── Estabilidad de Flujos ───
    # Coeficiente de variación (inverso = estabilidad)
    mean_flow = np.mean(flows_array)
    if mean_flow > _EPSILON:
        cv = realized_std / mean_flow  # Coeficiente de variación
        stability = 1.0 / (1.0 + cv)  # Mapear a [0, 1]
    else:
        stability = 0.0
    
    # ─── Curvatura (Cambio en Tendencia) ───
    # Segunda derivada discreta de flujos
    if len(flows) >= 3:
        second_diff = np.diff(flows_array, n=2)
        curvature = float(np.mean(np.abs(second_diff))) / (amount + _EPSILON)
    else:
        curvature = 0.0
    
    # ─── Dimensión Efectiva ───
    # Para una variedad 2D (tiempo, dinero), la dimensión es 2
    # Pero si los flujos son constantes, la dimensión efectiva es 1
    if len(set(flows)) == 1:
        effective_dimension = 1
    else:
        effective_dimension = 2
    
    # ─── Número de Máximos/Mínimos Locales ───
    # Indica la "rugosidad" de la variedad
    local_extrema = 0
    for i in range(1, len(flows) - 1):
        if (flows[i] > flows[i-1] and flows[i] > flows[i+1]) or \
           (flows[i] < flows[i-1] and flows[i] < flows[i+1]):
            local_extrema += 1
    
    return {
        "dimension": effective_dimension,
        "curvature": round(curvature, 6),
        "volatility_surface": round(volatility_surface, 6),
        "flow_stability": round(stability, 4),
        "local_extrema": local_extrema,
        "is_degenerate": effective_dimension < 2,
        "mean_flow": round(mean_flow, 2),
        "realized_volatility": round(realized_std, 2)
    }


def _compute_risk_homology(manifold: Dict[str, Any]) -> Dict[str, Any]:
    """
    Calcula invariantes homológicos del espacio de riesgo.
    
    En la variedad de riesgo:
    - β₀ = regímenes de riesgo distintos
    - β₁ = "agujeros" = escenarios de pérdida potencial
    
    Args:
        manifold: Características de la variedad de riesgo
        
    Returns:
        Invariantes homológicos con interpretación financiera
    """
    if manifold.get("is_degenerate", True):
        return {
            "risk_holes_beta_1": 0,
            "risk_regimes_beta_0": 1,
            "euler_characteristic": 1,
            "interpretation": "Degenerate risk space - insufficient data"
        }
    
    stability = manifold.get("flow_stability", 0.5)
    local_extrema = manifold.get("local_extrema", 0)
    volatility = manifold.get("volatility_surface", 0.0)
    
    # ─── β₁: Agujeros de Riesgo ───
    # Más inestabilidad y más extremos locales = más "agujeros" en seguridad
    instability = 1.0 - stability
    risk_holes = int(instability * 5 + local_extrema * 0.5)
    
    # ─── β₀: Regímenes de Riesgo ───
    # Alta volatilidad puede indicar múltiples regímenes
    risk_regimes = 1 + int(volatility * 10)
    risk_regimes = min(risk_regimes, 5)  # Acotar a máximo 5 regímenes
    
    # ─── Característica de Euler ───
    euler = risk_regimes - risk_holes
    
    # ─── Interpretación ───
    if risk_holes == 0:
        interpretation = "Solid risk surface - well-protected"
    elif risk_holes <= 2:
        interpretation = "Minor risk exposures - manageable"
    elif risk_holes <= 4:
        interpretation = "Significant risk gaps - requires hedging"
    else:
        interpretation = "Critical risk topology - high exposure"
    
    return {
        "risk_holes_beta_1": risk_holes,
        "risk_regimes_beta_0": risk_regimes,
        "euler_characteristic": euler,
        "interpretation": interpretation,
        "raw_instability": round(instability, 4)
    }


def _compute_opportunity_persistence(manifold: Dict[str, Any]) -> List[Dict[str, Any]]:
    """
    Calcula la persistencia de oportunidades en el espacio financiero.
    
    Modela cómo las oportunidades de ganancia aparecen y desaparecen
    a lo largo del horizonte temporal.
    
    Args:
        manifold: Variedad de riesgo analizada
        
    Returns:
        Diagrama de persistencia de oportunidades
    """
    mean_flow = manifold.get("mean_flow", 0.0)
    stability = manifold.get("flow_stability", 0.5)
    volatility = manifold.get("volatility_surface", 0.1)
    local_extrema = manifold.get("local_extrema", 0)
    
    intervals = []
    
    # Oportunidad principal: flujo medio positivo
    if mean_flow > 0:
        # Persistencia proporcional a estabilidad
        death_time = 1.0 / (1.0 + volatility) if volatility > 0 else float('inf')
        intervals.append({
            "birth": 0.0,
            "death": round(death_time, 4) if not math.isinf(death_time) else "inf",
            "persistence": round(death_time, 4) if not math.isinf(death_time) else "inf",
            "type": "primary_opportunity",
            "strength": round(mean_flow, 2)
        })
    
    # Oportunidades secundarias por extremos locales
    for i in range(local_extrema):
        birth = 0.1 * (i + 1)
        # Menor persistencia que la principal
        death = birth + 0.3 * stability
        intervals.append({
            "birth": round(birth, 4),
            "death": round(death, 4),
            "persistence": round(death - birth, 4),
            "type": "secondary_opportunity",
            "index": i
        })
    
    # Ordenar por persistencia descendente
    intervals.sort(key=lambda x: -(x["persistence"] if isinstance(x["persistence"], (int, float)) else float('inf')))
    
    return intervals


def _compute_risk_adjusted_return(
    analysis: Dict[str, Any],
    risk_tolerance: float
) -> float:
    """
    Calcula el retorno ajustado al riesgo usando Sharpe Ratio modificado.
    
    RAR = (NPV - Rf) / (σ * √T * (1 + risk_aversion))
    
    Donde:
    - NPV = Valor Presente Neto
    - Rf = Tasa libre de riesgo (asumida 3%)
    - σ = Volatilidad implícita
    - T = Horizonte temporal
    - risk_aversion = 1 - risk_tolerance
    
    Args:
        analysis: Resultado del análisis financiero
        risk_tolerance: Tolerancia al riesgo [0, 1]
        
    Returns:
        Retorno ajustado al riesgo
    """
    npv = analysis.get("npv", 0.0)
    
    if npv is None:
        return 0.0
    
    # Parámetros
    risk_free_rate = 0.03  # 3% anual
    
    # Extraer volatilidad del análisis o usar default
    volatility = analysis.get("volatility", 0.30)
    time_years = analysis.get("project_life_years", 5)
    
    # Evitar división por cero
    risk_aversion = max(0.01, 1.0 - risk_tolerance)
    denominator = volatility * math.sqrt(time_years) * (1 + risk_aversion)
    
    if denominator < _EPSILON:
        return float(npv)  # Sin ajuste si no hay riesgo
    
    excess_return = npv - (risk_free_rate * npv)  # Exceso sobre libre de riesgo
    
    risk_adjusted = excess_return / denominator
    
    return round(risk_adjusted, 4)


def _compute_topological_efficiency(
    analysis: Dict[str, Any],
    manifold: Dict[str, Any]
) -> float:
    """
    Calcula la eficiencia topológica del proyecto.
    
    Mide qué tan "suave" es el camino hacia el retorno,
    penalizando variedad de riesgo irregular.
    
    Eficiencia = NPV / (1 + curvatura + inestabilidad)
    
    Args:
        analysis: Resultados del análisis financiero
        manifold: Variedad de riesgo
        
    Returns:
        Eficiencia topológica normalizada
    """
    npv = analysis.get("npv", 0.0) or 0.0
    curvature = manifold.get("curvature", 0.0)
    stability = manifold.get("flow_stability", 0.5)
    
    instability = 1.0 - stability
    roughness = curvature + instability
    
    # Normalizar NPV a [0, 1] con sigmoide
    normalized_npv = 1.0 / (1.0 + math.exp(-npv / 10000)) if npv != 0 else 0.5
    
    # Penalizar por rugosidad
    efficiency = normalized_npv / (1.0 + roughness)
    
    return round(efficiency, 4)


# ══════════════════════════════════════════════════════════════════════════════
# HANDLERS DE LA MIC REFINADOS
# ══════════════════════════════════════════════════════════════════════════════

def diagnose_file(
    file_path: Union[str, Path],
    file_type: Union[str, FileType],
    *,
    validate_extension: bool = True,
    max_file_size: Optional[int] = None,
    topological_analysis: bool = False
) -> Dict[str, Any]:
    """
    Vector de Diagnóstico con análisis topológico opcional (Nivel 3 - PHYSICS).
    
    Ejecuta diagnóstico estructural sobre archivos CSV según su tipo,
    con opción de análisis topológico avanzado que incluye:
    - Números de Betti (componentes y ciclos)
    - Diagrama de persistencia
    - Entropía estructural
    
    Args:
        file_path: Ruta al archivo a diagnosticar
        file_type: Tipo de archivo (apus, insumos, presupuesto)
        validate_extension: Si validar extensión del archivo
        max_file_size: Tamaño máximo permitido en bytes
        topological_analysis: Si ejecutar análisis topológico avanzado
        
    Returns:
        Respuesta estandarizada con resultados del diagnóstico
    """
    path_str = str(file_path)
    
    try:
        # ─── Fase 1: Validación ───
        path = _normalize_path(file_path)
        normalized_type = _normalize_file_type(file_type)
        
        logger.info(f"Initiating diagnosis: path={path}, type={normalized_type.value}")
        
        _validate_file_exists(path)
        
        if validate_extension:
            _validate_file_extension(path)
        
        effective_max = max_file_size or MAX_FILE_SIZE_BYTES
        size, is_empty = _validate_file_size(path, effective_max)
        
        if is_empty:
            logger.warning(f"File is empty: {path}")
            return _create_success_response(
                {"diagnostic_completed": True, "is_empty": True},
                file_type=normalized_type.value,
                file_path=str(path),
                file_size_bytes=0,
                warning="File is empty"
            )
        
        # ─── Fase 2: Diagnóstico Base ───
        diagnostic_class = _get_diagnostic_class(normalized_type)
        diagnostic = diagnostic_class(str(path))
        diagnostic.diagnose()
        
        result_data = _extract_diagnostic_result(diagnostic)
        
        # ─── Fase 3: Análisis Topológico Opcional ───
        if topological_analysis:
            logger.debug("Executing topological analysis")
            
            # Características topológicas del archivo
            topo_features = _analyze_topological_features(path)
            result_data["topological_features"] = topo_features
            
            # Homología basada en issues encontrados
            homology = _compute_homology_groups(result_data)
            result_data["homology"] = homology
            
            # Diagrama de persistencia
            persistence = _compute_persistence_diagram(result_data)
            result_data["persistence_diagram"] = persistence
            
            # Entropía de persistencia
            if persistence:
                intervals = [
                    PersistenceInterval(
                        birth=p["birth"],
                        death=p["death"],
                        dimension=p.get("dimension", 0)
                    )
                    for p in persistence
                ]
                result_data["persistence_entropy"] = _compute_persistence_entropy(intervals)
        
        # ─── Fase 4: Métricas Globales ───
        magnitude = _compute_diagnostic_magnitude(result_data)
        
        logger.info(f"Diagnosis completed: path={path}, magnitude={magnitude}")
        
        return _create_success_response(
            result_data,
            file_type=normalized_type.value,
            file_path=str(path),
            file_size_bytes=size,
            diagnostic_magnitude=magnitude,
            has_topological_analysis=topological_analysis
        )
        
    except FileNotFoundDiagnosticError as e:
        logger.warning(f"File not found: {path_str}")
        return _create_error_response(e, error_category="validation")
        
    except (UnsupportedFileTypeError, FileValidationError) as e:
        logger.warning(f"Validation error: {e}")
        return _create_error_response(e, error_category="validation")
        
    except (ValueError, TypeError) as e:
        logger.warning(f"Parameter error: {e}")
        return _create_error_response(e, error_category="validation")
        
    except IOError as e:
        logger.error(f"IO error: {e}")
        return _create_error_response(e, error_category="io_error")
        
    except Exception as e:
        logger.exception(f"Unexpected error in diagnosis: {e}")
        return _create_error_response(e, error_category="unexpected")


def clean_file(
    input_path: Union[str, Path],
    output_path: Optional[Union[str, Path]] = None,
    *,
    delimiter: str = ";",
    encoding: str = "utf-8",
    overwrite: bool = True,
    validate_extension: bool = True,
    max_file_size: Optional[int] = None,
    preserve_topology: bool = True,
    compression_ratio: Optional[float] = None
) -> Dict[str, Any]:
    """
    Vector de Limpieza con preservación topológica (Nivel 3 - PHYSICS).
    
    Limpia y normaliza archivos CSV, opcionalmente verificando
    que la estructura topológica se preserve durante la transformación.
    
    Args:
        input_path: Ruta al archivo de entrada
        output_path: Ruta de salida (auto-generada si None)
        delimiter: Delimitador CSV
        encoding: Encoding del archivo
        overwrite: Si sobrescribir archivo existente
        validate_extension: Si validar extensión
        max_file_size: Tamaño máximo permitido
        preserve_topology: Si verificar preservación topológica
        compression_ratio: Ratio de compresión objetivo (no implementado)
        
    Returns:
        Respuesta estandarizada con estadísticas de limpieza
    """
    input_str = str(input_path)
    
    try:
        # ─── Fase 1: Validación de Entrada ───
        input_p = _normalize_path(input_path)
        
        logger.info(f"Initiating cleaning: {input_p}")
        
        _validate_file_exists(input_p)
        
        if validate_extension:
            _validate_file_extension(input_p)
        
        validated_delimiter, validated_encoding = _validate_csv_parameters(
            delimiter, encoding
        )
        
        # ─── Fase 2: Análisis Topológico Pre-Limpieza ───
        initial_topology: Dict[str, Any] = {}
        if preserve_topology:
            initial_topology = _analyze_csv_topology(
                input_p, validated_delimiter, validated_encoding
            )
            logger.debug(f"Initial topology: {initial_topology}")
        
        # ─── Fase 3: Configuración de Salida ───
        if output_path is None:
            output_p = _generate_output_path(input_p)
        else:
            output_p = _normalize_path(output_path)
        
        # Validar que input != output
        try:
            if input_p.resolve() == output_p.resolve():
                raise ValueError(
                    "Output path cannot be the same as input path. "
                    "Use a different filename or directory."
                )
        except OSError:
            # Si resolve() falla, comparar strings
            if str(input_p) == str(output_p):
                raise ValueError("Output path cannot be the same as input path")
        
        # Verificar sobrescritura
        if output_p.exists() and not overwrite:
            raise ValueError(
                f"Output file exists and overwrite=False: {output_p}"
            )
        
        # Crear directorio de salida si no existe
        output_p.parent.mkdir(parents=True, exist_ok=True)
        
        # ─── Fase 4: Ejecución de Limpieza ───
        cleaner = CSVCleaner(
            input_path=str(input_p),
            output_path=str(output_p),
            delimiter=validated_delimiter,
            encoding=validated_encoding,
            overwrite=overwrite
        )
        
        stats = cleaner.clean()
        
        # Normalizar estadísticas
        if isinstance(stats, dict):
            stats_dict = stats
        elif hasattr(stats, "to_dict") and callable(stats.to_dict):
            stats_dict = stats.to_dict()
        else:
            stats_dict = {"raw_result": str(stats)}
        
        # ─── Fase 5: Análisis Topológico Post-Limpieza ───
        if preserve_topology and not initial_topology.get("error"):
            final_topology = _analyze_csv_topology(
                output_p, validated_delimiter, validated_encoding
            )
            
            preservation = _compute_topological_preservation(
                initial_topology, final_topology
            )
            
            stats_dict["topological_preservation"] = preservation
            stats_dict["initial_topology"] = {
                "rows": initial_topology.get("rows"),
                "columns": initial_topology.get("columns"),
                "density": initial_topology.get("density")
            }
            stats_dict["final_topology"] = {
                "rows": final_topology.get("rows"),
                "columns": final_topology.get("columns"),
                "density": final_topology.get("density")
            }
            
            # Advertir si la preservación es baja
            pres_rate = preservation.get("preservation_rate", 1.0)
            if pres_rate < 0.8:
                logger.warning(
                    f"Low topological preservation ({pres_rate:.2%}). "
                    f"Significant data loss may have occurred."
                )
        
        logger.info(f"Cleaning completed: {output_p}")
        
        return _create_success_response(
            stats_dict,
            input_path=str(input_p),
            output_path=str(output_p),
            preserved_topology=preserve_topology
        )
        
    except FileNotFoundDiagnosticError as e:
        logger.warning(f"File not found: {input_str}")
        return _create_error_response(e, error_category="validation")
        
    except (FileValidationError, ValueError) as e:
        logger.warning(f"Validation error: {e}")
        return _create_error_response(e, error_category="validation")
        
    except CleaningError as e:
        logger.error(f"Cleaning error: {e}")
        return _create_error_response(e, error_category="cleaning")
        
    except IOError as e:
        logger.error(f"IO error during cleaning: {e}")
        wrapped = CleaningError(
            f"IO error during cleaning: {e}",
            details={"original_error": str(e), "input_path": input_str}
        )
        return _create_error_response(wrapped, error_category="cleaning")
        
    except Exception as e:
        logger.exception(f"Unexpected error in cleaning: {e}")
        return _create_error_response(e, error_category="unexpected")


def analyze_financial_viability(
    amount: float,
    std_dev: float,
    time_years: int,
    *,
    risk_tolerance: float = 0.05,
    market_volatility: Optional[float] = None,
    topological_risk_analysis: bool = False,
    random_seed: Optional[int] = None
) -> Dict[str, Any]:
    """
    Vector de Análisis Financiero con riesgo topológico (Nivel 1 - STRATEGY).
    
    Evalúa la viabilidad financiera de un proyecto usando:
    - Simulación de flujos de caja con dinámica estocástica
    - Análisis de variedad de riesgo (manifold)
    - Homología del espacio de riesgo
    - Persistencia de oportunidades
    
    Args:
        amount: Inversión inicial (debe ser positivo)
        std_dev: Desviación estándar de los costos
        time_years: Horizonte temporal en años
        risk_tolerance: Tolerancia al riesgo [0, 1] (default 0.05)
        market_volatility: Volatilidad de mercado (opcional)
        topological_risk_analysis: Si ejecutar análisis topológico
        random_seed: Semilla para reproducibilidad
        
    Returns:
        Análisis financiero con métricas y riesgo topológico
    """
    try:
        # ─── Fase 1: Validación de Parámetros ───
        if not isinstance(amount, (int, float)):
            raise TypeError(f"amount must be numeric, got {type(amount).__name__}")
        if amount <= 0:
            raise ValueError("Amount must be positive")
        
        if not isinstance(std_dev, (int, float)):
            raise TypeError(f"std_dev must be numeric, got {type(std_dev).__name__}")
        if std_dev < 0:
            raise ValueError("Standard deviation cannot be negative")
        
        if not isinstance(time_years, int):
            raise TypeError(f"time_years must be integer, got {type(time_years).__name__}")
        if time_years <= 0:
            raise ValueError("Time horizon must be positive")
        
        if not 0 <= risk_tolerance <= 1:
            raise ValueError("risk_tolerance must be in [0, 1]")
        
        logger.info(
            f"Analyzing financial viability: amount={amount}, "
            f"std_dev={std_dev}, years={time_years}"
        )
        
        # ─── Fase 2: Generación de Flujos ───
        cash_flows = _generate_topological_cash_flows(
            amount=amount,
            time_years=time_years,
            std_dev=std_dev,
            random_seed=random_seed
        )
        
        # ─── Fase 3: Análisis Base ───
        config = FinancialConfig(project_life_years=time_years)
        engine = FinancialEngine(config)
        
        analysis = engine.analyze_project(
            initial_investment=amount,
            expected_cash_flows=cash_flows,
            cost_std_dev=std_dev,
            project_volatility=market_volatility or 0.30
        )
        
        # ─── Fase 4: Extracción de Resultados ───
        performance = analysis.get("performance", {})
        
        results: Dict[str, Any] = {
            "wacc": analysis.get("wacc"),
            "npv": analysis.get("npv"),
            "irr": analysis.get("irr"),
            "payback_years": analysis.get("payback_years"),
            "recommendation": performance.get("recommendation"),
            "risk_level": performance.get("risk_level"),
            "cash_flows_summary": {
                "mean": round(np.mean(cash_flows), 2) if cash_flows else 0,
                "std": round(np.std(cash_flows), 2) if cash_flows else 0,
                "min": round(min(cash_flows), 2) if cash_flows else 0,
                "max": round(max(cash_flows), 2) if cash_flows else 0,
            }
        }
        
        # ─── Fase 5: Métricas de Riesgo Ajustado ───
        results["risk_adjusted_return"] = _compute_risk_adjusted_return(
            analysis, risk_tolerance
        )
        
        # ─── Fase 6: Análisis Topológico Opcional ───
        if topological_risk_analysis:
            logger.debug("Executing topological risk analysis")
            
            # Variedad de riesgo
            risk_manifold = _analyze_risk_manifold(
                amount, std_dev, time_years, cash_flows
            )
            results["risk_manifold"] = risk_manifold
            
            # Homología del riesgo
            risk_homology = _compute_risk_homology(risk_manifold)
            results["topological_risk"] = risk_homology
            
            # Persistencia de oportunidades
            opportunity_persistence = _compute_opportunity_persistence(risk_manifold)
            results["opportunity_persistence"] = opportunity_persistence
            
            # Eficiencia topológica
            results["topological_efficiency"] = _compute_topological_efficiency(
                analysis, risk_manifold
            )
        
        logger.info(
            f"Financial analysis complete: NPV={results.get('npv')}, "
            f"recommendation={results.get('recommendation')}"
        )
        
        return {
            "success": True,
            "results": results,
            "parameters": {
                "initial_investment": amount,
                "std_dev": std_dev,
                "time_years": time_years,
                "risk_tolerance": risk_tolerance,
                "random_seed": random_seed or _DEFAULT_RANDOM_SEED
            }
        }
        
    except (ValueError, TypeError) as e:
        logger.warning(f"Validation error in financial analysis: {e}")
        return _create_error_response(e, error_category="validation")
        
    except Exception as e:
        logger.exception(f"Error in financial analysis: {e}")
        return _create_error_response(e, error_category="financial_analysis")


def get_telemetry_status(
    telemetry_context: Optional[TelemetryContextProtocol] = None
) -> Dict[str, Any]:
    """
    Vector de Telemetría del Sistema (Nivel 3 - PHYSICS/Observability).
    
    Obtiene el estado actual del sistema, métricas de salud y
    reporte de actividad del contexto de procesamiento.
    
    Args:
        telemetry_context: Contexto de telemetría activo (opcional)
        
    Returns:
        Estado del sistema con estructura consistente:
        - success: bool
        - status: "IDLE" | "ACTIVE" | "ERROR"
        - system_health: "HEALTHY" | "DEGRADED" | "UNKNOWN"
        - has_active_context: bool
        - (datos adicionales del reporte)
    """
    # ─── Caso 1: Sin Contexto Activo ───
    if telemetry_context is None:
        return {
            "success": True,
            "status": "IDLE",
            "system_health": "UNKNOWN",
            "message": "No active processing context",
            "has_active_context": False
        }
    
    # ─── Caso 2: Contexto Inválido ───
    if not isinstance(telemetry_context, TelemetryContextProtocol):
        context_type = type(telemetry_context).__name__
        logger.warning(f"Invalid telemetry context type: {context_type}")
        return {
            "success": False,
            "status": "ERROR",
            "system_health": "DEGRADED",
            "error": f"Invalid context type: {context_type}",
            "message": (
                f"Telemetry context must implement TelemetryContextProtocol, "
                f"got {context_type}"
            ),
            "has_active_context": False
        }
    
    # ─── Caso 3: Contexto Válido ───
    try:
        report = telemetry_context.get_business_report()
        
        # Manejar reporte nulo
        if report is None:
            return {
                "success": True,
                "status": "ACTIVE",
                "system_health": "UNKNOWN",
                "message": "Context returned null report",
                "has_active_context": True
            }
        
        # Manejar reporte no-diccionario
        if not isinstance(report, dict):
            return {
                "success": True,
                "status": "ACTIVE",
                "system_health": "UNKNOWN",
                "has_active_context": True,
                "raw_report": report
            }
        
        # Construir respuesta con valores del reporte
        return {
            "success": True,
            "status": report.get("status", "ACTIVE"),
            "system_health": report.get("system_health", "HEALTHY"),
            "has_active_context": True,
            **{k: v for k, v in report.items() if k not in ("status", "system_health")}
        }
        
    except Exception as e:
        logger.exception(f"Telemetry error: {e}")
        return {
            "success": False,
            "status": "ERROR",
            "system_health": "DEGRADED",
            "error": str(e),
            "message": f"Error retrieving telemetry report: {e}",
            "has_active_context": True
        }