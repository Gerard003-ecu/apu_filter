### Propuesta 1

def _explode_pyramid(self, graph: nx.DiGraph) -> List[Dict[str, Any]]:
    """
    Recorre el grafo implementando un funtor F: G ‚Üí Mat desde la categor√≠a 
    del DAG hacia la categor√≠a de materiales.
    
    Utiliza propagaci√≥n de cantidades mediante composici√≥n de morfismos,
    donde cada arista representa un morfismo en el monoide multiplicativo (‚Ñù‚Å∫, √ó, 1).
    
    La detecci√≥n de ciclos usa hashing O(1) con frozenset para el camino actual.
    """
    materials: List[Dict[str, Any]] = []
    
    if graph.number_of_nodes() == 0:
        self.logger.warning("‚ö†Ô∏è Grafo vac√≠o proporcionado")
        return materials
    
    # Identificaci√≥n de fuentes (objetos iniciales en la categor√≠a)
    root_nodes = [node for node, in_degree in graph.in_degree() if in_degree == 0]
    
    if not root_nodes:
        self.logger.error("‚ùå Grafo sin ra√≠ces - violaci√≥n de estructura DAG")
        return materials
    
    self.logger.debug(f"üå≥ Ra√≠ces identificadas: {len(root_nodes)}")
    
    # DFS iterativo con detecci√≥n de ciclos O(1)
    # Tupla: (nodo, cantidad_acumulada, camino_set, camino_lista, padre_apu)
    stack: List[tuple] = [
        (root, 1.0, frozenset(), [], None) for root in root_nodes
    ]
    
    iteration_count = 0
    # Cota superior basada en expansi√≥n m√°xima del DAG
    max_iterations = graph.number_of_nodes() * max(graph.number_of_edges(), 1) + 1
    
    while stack:
        iteration_count += 1
        
        if iteration_count > max_iterations:
            raise RuntimeError(
                f"L√≠mite de iteraciones ({max_iterations}) excedido. "
                "Posible inconsistencia estructural."
            )
        
        current_node, current_qty, path_set, path_list, parent_apu = stack.pop()
        
        # Detecci√≥n de ciclo O(1) mediante pertenencia a conjunto
        if current_node in path_set:
            self.logger.warning(
                f"‚ö†Ô∏è Ciclo detectado: {'‚Üí'.join(path_list[-3:])}‚Üí{current_node}"
            )
            continue
        
        node_data = graph.nodes.get(current_node, {})
        node_type = node_data.get("type", "UNDEFINED")
        
        # Extensi√≥n del camino (inmutable para backtracking correcto)
        new_path_set = path_set | {current_node}
        new_path_list = path_list + [current_node]
        
        # Objeto terminal: registramos en la fibra de materiales
        if node_type == "INSUMO":
            description = (
                node_data.get("description") or 
                node_data.get("name") or 
                str(current_node)
            )
            
            # Validaci√≥n de costo unitario
            unit_cost = node_data.get("unit_cost", 0.0)
            try:
                unit_cost = float(unit_cost)
                if not math.isfinite(unit_cost) or unit_cost < 0:
                    self.logger.warning(f"‚ö†Ô∏è Costo inv√°lido en {current_node}")
                    unit_cost = 0.0
            except (TypeError, ValueError):
                unit_cost = 0.0
            
            materials.append({
                "id": current_node,
                "description": description,
                "base_qty": current_qty,
                "unit_cost": unit_cost,
                "source_apu": parent_apu or "ROOT",
                "unit": node_data.get("unit", "UND"),
                "node_data": node_data,
                "composition_path": new_path_list,
                "fiber_depth": len(new_path_list)
            })
            continue
        
        # Determinaci√≥n del APU ancestro para trazabilidad
        next_parent_apu = current_node if node_type == "APU" else parent_apu
        
        # Expansi√≥n de sucesores con composici√≥n de morfismos
        for successor in graph.successors(current_node):
            edge_data = graph.edges.get((current_node, successor), {})
            
            # Extracci√≥n y validaci√≥n del morfismo (cantidad)
            edge_qty = edge_data.get("quantity", 1.0)
            try:
                edge_qty = float(edge_qty)
                if not math.isfinite(edge_qty) or edge_qty <= 0:
                    self.logger.warning(
                        f"‚ö†Ô∏è Morfismo inv√°lido ({current_node}‚Üí{successor}): {edge_qty}"
                    )
                    edge_qty = 1.0
            except (TypeError, ValueError):
                edge_qty = 1.0
            
            # Composici√≥n: q_nuevo = q_actual ‚àò q_arista
            new_qty = current_qty * edge_qty
            
            # Verificaci√≥n de estabilidad num√©rica
            if not math.isfinite(new_qty) or new_qty > 1e12:
                self.logger.error(f"‚ùå Overflow en {successor}: {new_qty:.2e}")
                continue
            
            stack.append((
                successor, new_qty, new_path_set, 
                new_path_list, next_parent_apu
            ))
    
    self.logger.debug(
        f"üìä Exploraci√≥n completada: {iteration_count} iteraciones, "
        f"{len(materials)} materiales"
    )
    return materials


def _apply_entropy_factors(
    self,
    raw_materials: List[Dict[str, Any]],
    flux_metrics: Optional[Dict[str, Any]],
    risk_profile: Optional[Dict[str, Any]] = None
) -> List[Dict[str, Any]]:
    """
    Aplica factores de entrop√≠a mediante acci√≥n del grupo multiplicativo (‚Ñù‚Å∫, √ó, 1).
    
    La composici√≥n de factores es asociativa y conmutativa, formando un
    homomorfismo de grupos: œÜ(f‚ÇÅ ¬∑ f‚ÇÇ) = œÜ(f‚ÇÅ) √ó œÜ(f‚ÇÇ).
    
    El factor de desperdicio Œµ se define como Œµ = Œº - 1, donde Œº es el
    multiplicador total, garantizando Œµ ‚â• 0 para factores Œº ‚â• 1.
    """
    # Constantes del grupo de factores (configurables)
    SATURATION_THRESHOLD = 0.8
    FACTOR_SATURATION = 1.05
    FACTOR_INSTABILITY = 1.03
    
    RISK_FACTORS = {
        "LOW": 1.01,
        "MEDIUM": 1.03,
        "HIGH": 1.07,
        "CRITICAL": 1.15
    }
    
    MATERIAL_FACTORS = {
        "FRAGILE": 1.02,
        "PERISHABLE": 1.04,
        "HAZARDOUS": 1.06
    }
    
    # Elemento identidad del grupo multiplicativo
    base_factor = 1.0
    factor_trace = {"identity": 1.0}
    
    # Composici√≥n de factores de flujo
    if flux_metrics and isinstance(flux_metrics, dict):
        saturation = flux_metrics.get("avg_saturation", 0.0)
        if isinstance(saturation, (int, float)) and saturation > SATURATION_THRESHOLD:
            base_factor *= FACTOR_SATURATION
            factor_trace["saturation"] = FACTOR_SATURATION
        
        stability = flux_metrics.get("pyramid_stability", 1.0)
        if isinstance(stability, (int, float)) and 0 < stability < 1.0:
            base_factor *= FACTOR_INSTABILITY
            factor_trace["instability"] = FACTOR_INSTABILITY
    
    # Composici√≥n de factores de riesgo
    if risk_profile and isinstance(risk_profile, dict):
        risk_level = str(risk_profile.get("level", "MEDIUM")).upper()
        risk_mult = RISK_FACTORS.get(risk_level, RISK_FACTORS["MEDIUM"])
        base_factor *= risk_mult
        factor_trace["risk"] = risk_mult
    
    processed_materials: List[Dict[str, Any]] = []
    
    for mat in raw_materials:
        processed = mat.copy()
        
        # Factor espec√≠fico por categor√≠a de material
        material_category = mat.get("node_data", {}).get("material_category", "GENERIC")
        specific_factor = MATERIAL_FACTORS.get(material_category, 1.0)
        
        # Composici√≥n total: Œº_total = Œº_base √ó Œº_espec√≠fico
        total_multiplier = base_factor * specific_factor
        
        # Invariante: Œº ‚â• 1 (no reducimos cantidades)
        if not math.isfinite(total_multiplier) or total_multiplier < 1.0:
            self.logger.warning(
                f"‚ö†Ô∏è Multiplicador inv√°lido para {mat.get('id')}: {total_multiplier}"
            )
            total_multiplier = 1.0
        
        base_qty = mat.get("base_qty", 0.0)
        
        # Factor de desperdicio: Œµ = Œº - 1
        processed["waste_factor"] = total_multiplier - 1.0
        processed["total_qty"] = base_qty * total_multiplier
        
        # Trazabilidad de transformaciones aplicadas
        processed["applied_factors"] = {
            **factor_trace,
            "material_specific": specific_factor,
            "material_category": material_category,
            "total_multiplier": round(total_multiplier, 6)
        }
        
        processed_materials.append(processed)
    
    return processed_materials


def _cluster_semantically(
    self, 
    materials: List[Dict[str, Any]]
) -> List[MaterialRequirement]:
    """
    Agrupa materiales mediante relaci√≥n de equivalencia categ√≥rica.
    
    Define el funtor cociente Q: Mat ‚Üí Mat/‚àº donde la relaci√≥n ‚àº est√°
    inducida por el kernel de la proyecci√≥n œÄ(m) = (id(m), unit(m)).
    
    Las cantidades se agregan mediante la estructura de monoide aditivo (‚Ñù, +, 0),
    y los costos se estiman mediante el estimador robusto de mediana.
    """
    if not materials:
        return []
    
    # Clases de equivalencia indexadas por clave can√≥nica
    equivalence_classes: Dict[tuple, Dict[str, Any]] = {}
    
    for mat in materials:
        mat_id = mat.get("id", "UNKNOWN")
        mat_unit = mat.get("unit", "UND")
        canonical_key = (mat_id, mat_unit)
        
        if canonical_key not in equivalence_classes:
            equivalence_classes[canonical_key] = {
                "id": mat_id,
                "description": mat.get("description", str(mat_id)),
                "unit": mat_unit,
                "quantity_base": 0.0,
                "quantity_total": 0.0,
                "source_apus": set(),
                "cost_samples": [],
                "instance_count": 0,
                "max_fiber_depth": 0
            }
        
        data = equivalence_classes[canonical_key]
        
        # Acumulaci√≥n monoidal aditiva
        base_qty = float(mat.get("base_qty", 0.0))
        total_qty = float(mat.get("total_qty", base_qty))
        
        data["quantity_base"] += base_qty
        data["quantity_total"] += total_qty
        data["instance_count"] += 1
        
        # Registro de fuente APU
        source_apu = mat.get("source_apu")
        if source_apu:
            data["source_apus"].add(source_apu)
        
        # Profundidad m√°xima en la fibra
        fiber_depth = mat.get("fiber_depth", 0)
        data["max_fiber_depth"] = max(data["max_fiber_depth"], fiber_depth)
        
        # Muestreo de costos para estimaci√≥n robusta
        unit_cost = mat.get("unit_cost", 0.0)
        if isinstance(unit_cost, (int, float)) and math.isfinite(unit_cost) and unit_cost >= 0:
            data["cost_samples"].append(unit_cost)
    
    # Colapso al espacio cociente
    requirements: List[MaterialRequirement] = []
    
    for canonical_key, data in equivalence_classes.items():
        quantity_base = data["quantity_base"]
        quantity_total = data["quantity_total"]
        
        # Validaci√≥n de positividad
        if quantity_base <= 0:
            self.logger.warning(f"‚ö†Ô∏è Cantidad base ‚â§ 0 para {data['id']}")
            continue
        
        # C√°lculo de waste_factor desde cantidades agregadas
        waste_factor = (quantity_total / quantity_base) - 1.0
        
        # Estimador robusto de costo: mediana
        cost_samples = [c for c in data["cost_samples"] if c > 0]
        
        if cost_samples:
            sorted_costs = sorted(cost_samples)
            n = len(sorted_costs)
            mid = n // 2
            representative_cost = (
                sorted_costs[mid] if n % 2 == 1 
                else (sorted_costs[mid - 1] + sorted_costs[mid]) / 2.0
            )
        else:
            representative_cost = 0.0
            self.logger.warning(f"‚ö†Ô∏è Sin muestras de costo para {data['id']}")
        
        # C√°lculo de costo total
        total_cost = quantity_total * representative_cost
        
        if not math.isfinite(total_cost):
            self.logger.error(f"‚ùå Costo no finito para {data['id']}")
            total_cost = 0.0
        
        req = MaterialRequirement(
            id=data["id"],
            description=data["description"],
            quantity_base=round(quantity_base, 6),
            unit=data["unit"],
            waste_factor=round(waste_factor, 6),
            quantity_total=round(quantity_total, 6),
            unit_cost=round(representative_cost, 4),
            total_cost=round(total_cost, 2),
            source_apus=sorted(data["source_apus"])
        )
        
        requirements.append(req)
    
    # Ordenamiento por dominancia de Pareto: (-costo, -complejidad, id)
    requirements.sort(key=lambda r: (-r.total_cost, -len(r.source_apus), r.id))
    
    return requirements


def _compute_total_cost(self, requirements: List[MaterialRequirement]) -> float:
    """
    Calcula el costo total usando el algoritmo de Kahan para suma compensada.
    
    Kahan summation reduce el error de redondeo de O(nŒµ) a O(Œµ), donde Œµ
    es el √©psilon de m√°quina, proporcionando estabilidad num√©rica superior.
    
    Complejidad: O(n) tiempo, O(1) espacio auxiliar.
    """
    if not requirements:
        return 0.0
    
    # Variables de Kahan summation
    total = 0.0
    compensation = 0.0
    
    for req in requirements:
        cost = req.total_cost
        
        # Verificaci√≥n de finitud
        if not math.isfinite(cost):
            self.logger.error(f"‚ùå Costo no finito en {req.id}: {cost}")
            continue
        
        # Paso de Kahan: compensaci√≥n del error de redondeo
        y = cost - compensation
        t = total + y
        compensation = (t - total) - y
        total = t
        
        # Detecci√≥n de overflow
        if math.isinf(total):
            raise OverflowError(
                f"Desbordamiento aritm√©tico. √öltimo elemento: {req.id}"
            )
    
    return round(total, 2)


def _generate_metadata(
    self,
    graph: nx.DiGraph,
    risk_profile: Optional[Dict[str, Any]],
    flux_metrics: Optional[Dict[str, Any]],
    requirements: List[MaterialRequirement]
) -> Dict[str, Any]:
    """
    Genera metadata con invariantes topol√≥gicos del grafo.
    
    Incluye an√°logos discretos de invariantes topol√≥gicos:
    - Œ≤‚ÇÄ (componentes conexas) ‚âà n√∫mero de Betti 0
    - œá = V - E (caracter√≠stica de Euler para grafos)
    - An√°lisis de Pareto con √≠ndice de Gini
    """
    node_count = graph.number_of_nodes()
    edge_count = graph.number_of_edges()
    
    # Componentes d√©bilmente conexas (an√°logo a Œ≤‚ÇÄ)
    weakly_connected = (
        nx.number_weakly_connected_components(graph) 
        if node_count > 0 else 0
    )
    
    # Invariantes de DAG
    is_dag = nx.is_directed_acyclic_graph(graph)
    longest_path_length = None
    layer_count = None
    
    if is_dag and node_count > 0:
        try:
            longest_path_length = nx.dag_longest_path_length(graph)
            topological_generations = list(nx.topological_generations(graph))
            layer_count = len(topological_generations)
        except nx.NetworkXError as e:
            self.logger.warning(f"‚ö†Ô∏è Error calculando invariantes DAG: {e}")
    
    # Fuentes y sumideros
    root_count = sum(1 for _, d in graph.in_degree() if d == 0)
    leaf_count = sum(1 for _, d in graph.out_degree() if d == 0)
    
    # Caracter√≠stica de Euler (simplificada para grafos)
    euler_characteristic = node_count - edge_count
    
    # An√°lisis de distribuci√≥n de Pareto
    pareto_analysis = self._compute_pareto_distribution(requirements)
    
    return {
        "graph_invariants": {
            "node_count": node_count,
            "edge_count": edge_count,
            "is_dag": is_dag,
            "weakly_connected_components": weakly_connected,
            "longest_path_length": longest_path_length,
            "topological_layer_count": layer_count,
            "source_count": root_count,
            "sink_count": leaf_count,
            "euler_characteristic": euler_characteristic
        },
        "cost_distribution": pareto_analysis,
        "input_parameters": {
            "risk_profile": risk_profile,
            "flux_metrics": flux_metrics
        },
        "generation_info": {
            "timestamp": datetime.now().isoformat(),
            "version": "2.1-topological",
            "algorithm": "categorical-materialization-functor"
        }
    }


def _compute_pareto_distribution(
    self,
    requirements: List[MaterialRequirement]
) -> Dict[str, Any]:
    """
    Calcula m√©tricas de distribuci√≥n de Pareto y concentraci√≥n de Gini.
    
    Principio de Pareto: Determina qu√© fracci√≥n de items genera el 80% del costo.
    √çndice de Gini: Mide la desigualdad en la distribuci√≥n (0 = uniforme, 1 = concentrada).
    """
    if not requirements:
        return {
            "item_count": 0,
            "total_cost": 0.0,
            "pareto_80_items_ratio": 0.0,
            "gini_index": 0.0,
            "unique_apus": 0
        }
    
    costs = [r.total_cost for r in requirements]
    total_cost = sum(costs)
    n = len(costs)
    
    if total_cost <= 0:
        all_apus = set().union(*(set(r.source_apus) for r in requirements))
        return {
            "item_count": n,
            "total_cost": 0.0,
            "pareto_80_items_ratio": 0.0,
            "gini_index": 0.0,
            "unique_apus": len(all_apus)
        }
    
    # Pareto: ¬øcu√°ntos items acumulan el 80% del costo?
    sorted_costs_desc = sorted(costs, reverse=True)
    threshold_80 = total_cost * 0.8
    cumulative = 0.0
    items_for_80 = 0
    
    for cost in sorted_costs_desc:
        cumulative += cost
        items_for_80 += 1
        if cumulative >= threshold_80:
            break
    
    pareto_ratio = items_for_80 / n
    
    # √çndice de Gini
    sorted_costs_asc = sorted(costs)
    cumulative_weighted = sum((i + 1) * c for i, c in enumerate(sorted_costs_asc))
    gini = (2.0 * cumulative_weighted) / (n * total_cost) - (n + 1.0) / n
    gini = max(0.0, min(1.0, gini))  # Acotado a [0, 1]
    
    # APUs √∫nicos
    all_apus = set().union(*(set(r.source_apus) for r in requirements))
    
    return {
        "item_count": n,
        "total_cost": round(total_cost, 2),
        "pareto_80_items_ratio": round(pareto_ratio, 4),
        "gini_index": round(gini, 4),
        "unique_apus": len(all_apus)
    }

### Propuesta 2

