def materialize_project(
    self,
    graph: nx.DiGraph,
    risk_profile: Optional[Dict[str, Any]] = None,
    flux_metrics: Optional[Dict[str, Any]] = None,
    telemetry: Optional[TelemetryContext] = None,
) -> BillOfMaterials:
    """
    Orquesta la transformaci√≥n del Grafo en BOM con validaciones mejoradas.
    """
    self.logger.info("üåå Iniciando materializaci√≥n h√≠brida del proyecto...")

    if telemetry:
        telemetry.start_step("materialize_project")

    try:
        # Validaci√≥n de complejidad mejorada (considerando densidad)
        node_count = graph.number_of_nodes()
        edge_count = graph.number_of_edges()
        
        if node_count == 0:
            raise ValueError("Grafo vac√≠o: no hay nodos para procesar")
            
        density = edge_count / (node_count * (node_count - 1)) if node_count > 1 else 0
        complexity = node_count * max(edge_count, 1) * (1 + density)
        
        if complexity > self.max_graph_complexity:
            raise OverflowError(
                f"Complejidad del grafo ({complexity:.0f}) excede el l√≠mite ({self.max_graph_complexity}). "
                f"Nodos: {node_count}, Aristas: {edge_count}, Densidad: {density:.3f}"
            )

        # Validaci√≥n estructural mejorada (DAG con detecci√≥n de ciclos)
        if not nx.is_directed_acyclic_graph(graph):
            # Identificar ciclos espec√≠ficos para debugging
            try:
                cycles = list(nx.simple_cycles(graph))
                cycle_info = "; ".join([f"Ciclo {i+1}: {c}" for i, c in enumerate(cycles[:3])])
                if len(cycles) > 3:
                    cycle_info += f" ... y {len(cycles)-3} ciclos m√°s"
            except:
                cycle_info = "Ciclos detectados pero no enumerables"
                
            raise ValueError(
                f"El grafo contiene ciclos (no es DAG). {cycle_info}"
            )

        # 1. Colapso de Onda con DFS mejorado
        materials = []
        
        # Identificaci√≥n de ra√≠ces con validaci√≥n de conectividad
        root_nodes = [node for node, in_degree in graph.in_degree() if in_degree == 0]
        
        if not root_nodes:
            # Grafo sin ra√≠ces pero con nodos: posible grafo desconectado o autocontenido
            if node_count > 0:
                self.logger.warning("Grafo sin ra√≠ces formales - usando todos los nodos como ra√≠ces")
                root_nodes = list(graph.nodes())
            else:
                return BillOfMaterials([], 0.0, {"error": "Grafo vac√≠o"})

        # DFS iterativo con l√≠mites de profundidad y validaci√≥n de cantidades
        stack = [(root, 1.0, frozenset(), [], None, 0) for root in root_nodes]
        max_depth = node_count * 2  # L√≠mite para prevenir recursi√≥n infinita
        
        visited_edges = set()
        iteration_count = 0
        max_iterations = self.max_graph_complexity * 2

        while stack:
            iteration_count += 1
            if iteration_count > max_iterations:
                raise RuntimeError(
                    f"L√≠mite de iteraciones excedido ({max_iterations}). "
                    f"Posible ciclo no detectado o grafo demasiado complejo."
                )

            current_node, current_qty, path_set, path_list, parent_apu, depth = stack.pop()
            
            # Validaci√≥n de profundidad
            if depth > max_depth:
                self.logger.warning(
                    f"Profundidad excesiva ({depth}) en nodo {current_node}. "
                    f"Camino: {' -> '.join(map(str, path_list[-10:]))}"
                )
                continue

            # Detecci√≥n de ciclos en el camino actual
            if current_node in path_set:
                self.logger.error(
                    f"‚ö†Ô∏è Ciclo detectado en camino DFS: {' -> '.join(map(str, path_list))} -> {current_node}"
                )
                # Registrar m√©trica pero continuar para no detener todo el proceso
                if telemetry:
                    telemetry.record_metric("validation", "cycle_detected", 1)
                continue

            node_data = graph.nodes.get(current_node, {})
            node_type = node_data.get("type", "UNDEFINED")

            new_path_set = path_set | {current_node}
            new_path_list = path_list + [current_node]

            # Objeto terminal (Insumo)
            if node_type == "INSUMO":
                description = (
                    node_data.get("description") or 
                    node_data.get("name") or 
                    node_data.get("label") or 
                    str(current_node)
                )

                # Validaci√≥n robusta de costo unitario
                unit_cost = node_data.get("unit_cost", 0.0)
                try:
                    unit_cost = float(unit_cost)
                    if not math.isfinite(unit_cost):
                        self.logger.warning(
                            f"Costo no finito en {current_node}: {unit_cost}. Usando 0.0"
                        )
                        unit_cost = 0.0
                    elif unit_cost < 0:
                        self.logger.warning(
                            f"Costo negativo en {current_node}: {unit_cost}"
                        )
                except (TypeError, ValueError):
                    unit_cost = 0.0
                    self.logger.debug(f"Costo inv√°lido en {current_node}, usando 0.0")

                # Validaci√≥n de cantidad acumulada
                if not math.isfinite(current_qty) or current_qty <= 0:
                    self.logger.warning(
                        f"Cantidad inv√°lida en {current_node}: {current_qty}. Usando 1.0"
                    )
                    current_qty = 1.0

                materials.append({
                    "id": str(current_node),
                    "description": description,
                    "base_qty": current_qty,
                    "unit_cost": unit_cost,
                    "source_apu": parent_apu or "ROOT",
                    "unit": node_data.get("unit", "UND"),
                    "node_data": node_data,
                    "composition_path": new_path_list,
                    "fiber_depth": len(new_path_list),
                    "topological_order": depth,
                })
                continue

            # Determinaci√≥n de padre APU para trazabilidad
            next_parent_apu = current_node if node_type == "APU" else parent_apu

            # Expansi√≥n de sucesores con validaci√≥n de aristas
            for successor in graph.successors(current_node):
                edge_key = (current_node, successor)
                
                # Detecci√≥n de aristas duplicadas (aunque raro en DAG)
                if edge_key in visited_edges:
                    self.logger.debug(f"Arista duplicada detectada: {edge_key}")
                visited_edges.add(edge_key)
                
                edge_data = graph.edges.get(edge_key, {})
                edge_qty = edge_data.get("quantity", 1.0)

                # Validaci√≥n robusta de cantidad en arista
                try:
                    edge_qty = float(edge_qty)
                    if not math.isfinite(edge_qty) or edge_qty <= 0:
                        self.logger.warning(
                            f"Cantidad inv√°lida en arista {edge_key}: {edge_qty}. Usando 1.0"
                        )
                        edge_qty = 1.0
                except (TypeError, ValueError):
                    edge_qty = 1.0
                    self.logger.debug(f"Cantidad inv√°lida en arista {edge_key}, usando 1.0")

                # Propagaci√≥n con l√≠mites num√©ricos
                new_qty = current_qty * edge_qty
                
                # L√≠mites num√©ricos para prevenir overflow/underflow
                if not math.isfinite(new_qty):
                    self.logger.error(
                        f"Cantidad no finita al propagar {current_node} -> {successor}: "
                        f"{current_qty} * {edge_qty} = {new_qty}"
                    )
                    continue
                    
                if new_qty > 1e12 or new_qty < 1e-12:
                    self.logger.warning(
                        f"Cantidad extrema al propagar {current_node} -> {successor}: {new_qty}"
                    )

                stack.append(
                    (successor, new_qty, new_path_set, new_path_list, next_parent_apu, depth + 1)
                )

        raw_materials = materials
        self.logger.info(
            f"üß± Materiales brutos extra√≠dos: {len(raw_materials)} "
            f"(nodos: {node_count}, aristas: {edge_count})"
        )

        if not raw_materials:
            self.logger.warning("‚ö†Ô∏è No se encontraron materiales tipo INSUMO en el grafo")
            # Retornar BOM vac√≠o pero con metadata informativa
            metadata = self._generate_metadata(graph, risk_profile, flux_metrics, [])
            return BillOfMaterials([], 0.0, metadata)

        # 2. Aplicaci√≥n de Entrop√≠a Trazable
        adjusted_materials = self._apply_entropy_factors(
            raw_materials, flux_metrics, risk_profile
        )

        # 3. Clustering Sem√°ntico mejorado
        final_requirements = self._cluster_semantically(adjusted_materials)
        self.logger.info(f"üõí Requerimientos consolidados: {len(final_requirements)}")

        # 4. C√°lculo de Totales con Kahan mejorado
        total_cost = self._compute_total_cost(final_requirements)

        # 5. Metadata Estrat√©gica
        metadata = self._generate_metadata(
            graph, risk_profile, flux_metrics, final_requirements
        )

        # M√©tricas de calidad
        if telemetry:
            telemetry.record_metric("materialization", "total_material_cost", total_cost)
            telemetry.record_metric("materialization", "item_count", len(final_requirements))
            telemetry.record_metric("materialization", "complexity_processed", complexity)
            telemetry.record_metric("materialization", "graph_density", density)
            telemetry.record_metric("materialization", "max_path_depth", 
                                  max([m.get('fiber_depth', 0) for m in raw_materials], default=0))
            telemetry.end_step("materialize_project", "success")

        return BillOfMaterials(
            requirements=final_requirements,
            total_material_cost=total_cost,
            metadata=metadata,
        )

    except Exception as e:
        self.logger.error(f"‚ùå Error en materializaci√≥n: {str(e)}", exc_info=True)
        if telemetry:
            telemetry.record_error("materialize_project", str(e))
            telemetry.end_step("materialize_project", "error")
        raise


def _cluster_semantically(
    self, materials: List[Dict[str, Any]]
) -> List[MaterialRequirement]:
    """
    Agrupa materiales sem√°nticamente con estad√≠sticas robustas.
    """
    # Primera pasada: agrupaci√≥n por clave compuesta
    clustered = {}
    unit_normalization = {
        "M": 1.0, "M2": 1.0, "M3": 1.0, "KG": 1.0, "TON": 1000.0,
        "UND": 1.0, "UNIT": 1.0, "L": 1.0, "GL": 3.78541
    }
    
    for mat in materials:
        # Clave de agrupaci√≥n con normalizaci√≥n de unidades
        unit = mat.get("unit", "UND").upper()
        norm_unit = unit_normalization.get(unit, 1.0)
        
        # Clave considera ID y unidad base (sin normalizaci√≥n para trazabilidad)
        key = (mat["id"], unit)
        
        if key not in clustered:
            clustered[key] = {
                "id": mat["id"],
                "description": mat["description"],
                "unit": unit,
                "quantity_base": 0.0,
                "quantity_total": 0.0,
                "source_apus": set(),
                "cost_samples": [],
                "waste_factors": [],
                "paths": [],  # Para trazabilidad
                "normalization_factor": norm_unit,
            }
        
        data = clustered[key]
        base_qty = mat["base_qty"]
        total_qty = mat.get("total_qty", base_qty)
        
        # Convertir a unidades base si es necesario
        if norm_unit != 1.0 and unit in unit_normalization:
            base_qty *= norm_unit
            total_qty *= norm_unit
        
        data["quantity_base"] += base_qty
        data["quantity_total"] += total_qty
        data["source_apus"].add(mat["source_apu"])
        
        cost = mat["unit_cost"]
        if math.isfinite(cost):
            data["cost_samples"].append(cost)
        
        waste = mat.get("waste_factor", 0.0)
        if math.isfinite(waste):
            data["waste_factors"].append(waste)
        
        # Guardar camino para debugging
        if len(data["paths"]) < 3:  # Limitar para no usar mucha memoria
            data["paths"].append(mat.get("composition_path", [])[-3:])  # √öltimos 3 nodos

    # Segunda pasada: crear objetos MaterialRequirement
    requirements = []
    
    for (mid, unit), data in clustered.items():
        if data["quantity_base"] <= 0:
            self.logger.debug(f"Material {mid} con cantidad base <= 0, omitiendo")
            continue
        
        # C√°lculo de factor de desperdicio promedio ponderado
        total_waste = 0.0
        if data["waste_factors"]:
            # Promedio ponderado por cantidad
            weighted_sum = 0.0
            total_weight = 0.0
            for waste in data["waste_factors"]:
                weight = 1.0  # Podr√≠a ser la cantidad si est√° disponible
                weighted_sum += waste * weight
                total_weight += weight
            total_waste = weighted_sum / total_weight if total_weight > 0 else 0.0
        else:
            total_waste = (data["quantity_total"] / data["quantity_base"]) - 1.0
        
        # Estimaci√≥n robusta de costo (mediana con validaci√≥n)
        costs = sorted([c for c in data["cost_samples"] if math.isfinite(c)])
        rep_cost = 0.0
        
        if costs:
            # Usar mediana robusta
            mid_idx = len(costs) // 2
            if len(costs) % 2 == 1:
                rep_cost = costs[mid_idx]
            else:
                rep_cost = (costs[mid_idx - 1] + costs[mid_idx]) / 2.0
            
            # Detectar outliers (m√°s de 2 desviaciones est√°ndar)
            if len(costs) >= 3:
                import statistics
                try:
                    stdev = statistics.stdev(costs)
                    mean = statistics.mean(costs)
                    # Filtrar outliers extremos
                    filtered_costs = [c for c in costs if abs(c - mean) <= 3 * stdev]
                    if filtered_costs:
                        mid_idx = len(filtered_costs) // 2
                        if len(filtered_costs) % 2 == 1:
                            rep_cost = filtered_costs[mid_idx]
                        else:
                            rep_cost = (filtered_costs[mid_idx - 1] + filtered_costs[mid_idx]) / 2.0
                except statistics.StatisticsError:
                    pass  # Mantener mediana original
        
        # Validaci√≥n final de costo
        if not math.isfinite(rep_cost) or rep_cost < 0:
            self.logger.warning(f"Costo inv√°lido para {mid}: {rep_cost}. Usando 0.0")
            rep_cost = 0.0
        
        # C√°lculo de costo total con validaci√≥n
        total_cost = data["quantity_total"] * rep_cost
        if not math.isfinite(total_cost):
            self.logger.error(f"Costo total no finito para {mid}")
            total_cost = 0.0
        
        # Crear requerimiento
        req = MaterialRequirement(
            id=data["id"],
            description=data["description"],
            quantity_base=round(data["quantity_base"], 6),
            unit=data["unit"],
            waste_factor=round(max(0.0, total_waste), 6),  # No permitir negativo
            quantity_total=round(data["quantity_total"], 6),
            unit_cost=round(rep_cost, 4),
            total_cost=round(total_cost, 2),
            source_apus=sorted([str(x) for x in data["source_apus"]]),
        )
        requirements.append(req)
    
    # Ordenar por m√∫ltiples criterios: costo descendente, luego descripci√≥n
    requirements.sort(key=lambda x: (-x.total_cost, x.description or "", x.id))
    
    # Estad√≠sticas de agrupaci√≥n
    if requirements:
        self.logger.debug(
            f"Clustering: {len(materials)} materiales brutos -> {len(requirements)} requerimientos "
            f"(ratio: {len(materials)/len(requirements):.1f})"
        )
    
    return requirements


def _compute_total_cost(self, requirements: List[MaterialRequirement]) -> float:
    """
    Calcula costo total usando compensaci√≥n de Kahan mejorada.
    """
    total = 0.0
    c = 0.0  # Compensaci√≥n
    
    # Primera pasada: suma est√°ndar para casos normales
    simple_sum = sum(req.total_cost for req in requirements if math.isfinite(req.total_cost))
    
    # Si la suma simple es finita y no demasiado grande, usarla
    if math.isfinite(simple_sum) and abs(simple_sum) < 1e12:
        return round(simple_sum, 2)
    
    # Segunda pasada: Kahan para precisi√≥n extrema
    sorted_costs = sorted(
        [req.total_cost for req in requirements if math.isfinite(req.total_cost)],
        key=abs  # Ordenar por valor absoluto para mejor estabilidad
    )
    
    for cost in sorted_costs:
        y = cost - c
        t = total + y
        # Compensaci√≥n de Kahan: (t - total) - y
        c = (t - total) - y
        total = t
        
        # Validaci√≥n de overflow
        if not math.isfinite(total):
            raise OverflowError(
                f"Overflow en c√°lculo de Kahan. Parcial: {total}, Costo: {cost}"
            )
    
    # Redondear con validaci√≥n
    result = round(total, 2)
    if not math.isfinite(result):
        raise ValueError(f"Resultado no finito despu√©s de redondeo: {result}")
    
    return result


def _generate_metadata(
    self,
    graph: nx.DiGraph,
    risk_profile: Dict[str, Any],
    flux_metrics: Dict[str, Any],
    requirements: List[MaterialRequirement],
) -> Dict[str, Any]:
    """
    Genera metadata estrat√©gica con m√©tricas avanzadas.
    """
    # M√©tricas topol√≥gicas avanzadas
    node_count = graph.number_of_nodes()
    edge_count = graph.number_of_edges()
    
    # Caracter√≠stica de Euler para grafos dirigidos (simplificada)
    euler = node_count - edge_count
    
    # Distribuci√≥n de grados
    in_degrees = [d for _, d in graph.in_degree()]
    out_degrees = [d for _, d in graph.out_degree()]
    
    avg_in_degree = sum(in_degrees) / max(1, node_count)
    avg_out_degree = sum(out_degrees) / max(1, node_count)
    
    # An√°lisis de Pareto mejorado
    costs = [r.total_cost for r in requirements]
    total_cost = sum(costs)
    n = len(costs)
    
    pareto_metrics = {
        "pareto_80_items_ratio": 0.0,
        "pareto_20_cost_percentage": 0.0,
        "top_10_items_cost": 0.0,
        "cost_concentration_index": 0.0,
    }
    
    gini = 0.0
    theil_index = 0.0  # √çndice de Theil para desigualdad
    
    if total_cost > 0 and n > 0:
        # 1. Pareto tradicional
        sorted_desc = sorted(costs, reverse=True)
        accum = 0.0
        items_80 = 0
        
        for c in sorted_desc:
            accum += c
            items_80 += 1
            if accum >= total_cost * 0.8:
                break
        
        pareto_metrics["pareto_80_items_ratio"] = items_80 / n
        
        # 2. Costo del 20% superior
        top_20_count = max(1, int(math.ceil(n * 0.2)))
        top_20_cost = sum(sorted_desc[:top_20_count])
        pareto_metrics["pareto_20_cost_percentage"] = (top_20_cost / total_cost) * 100.0
        
        # 3. Costo del 10% superior
        top_10_count = max(1, int(math.ceil(n * 0.1)))
        top_10_cost = sum(sorted_desc[:top_10_count])
        pareto_metrics["top_10_items_cost"] = top_10_cost
        
        # 4. √çndice de concentraci√≥n (Herfindahl-Hirschman simplificado)
        cost_shares = [c / total_cost for c in costs]
        hhi = sum(share * share for share in cost_shares) * 10000
        pareto_metrics["cost_concentration_index"] = hhi
        
        # 5. Gini mejorado
        sorted_asc = sorted(costs)
        cum_weighted = sum((i + 1) * c for i, c in enumerate(sorted_asc))
        gini = (2.0 * cum_weighted) / (n * total_cost) - (n + 1.0) / n
        gini = max(0.0, min(1.0, gini))
        
        # 6. √çndice de Theil (entrop√≠a de la desigualdad)
        mean_cost = total_cost / n
        theil_sum = 0.0
        for c in costs:
            if c > 0:
                ratio = c / mean_cost
                theil_sum += c * math.log(ratio)
        theil_index = theil_sum / (n * mean_cost) if mean_cost > 0 else 0.0
    
    # An√°lisis de componentes conectados (para DAGs)
    try:
        # Para grafos dirigidos, usar componentes d√©bilmente conectados
        wcc = list(nx.weakly_connected_components(graph))
        largest_wcc = max(wcc, key=len) if wcc else set()
        wcc_count = len(wcc)
    except:
        wcc_count = 1
        largest_wcc = set(graph.nodes())
    
    # Metadatos de generaci√≥n
    from datetime import datetime
    import platform
    import sys
    
    return {
        "topological_analysis": {
            "is_dag": nx.is_directed_acyclic_graph(graph),
            "euler_characteristic": euler,
            "weakly_connected_components": wcc_count,
            "largest_component_size": len(largest_wcc),
            "avg_in_degree": round(avg_in_degree, 3),
            "avg_out_degree": round(avg_out_degree, 3),
            "max_in_degree": max(in_degrees) if in_degrees else 0,
            "max_out_degree": max(out_degrees) if out_degrees else 0,
        },
        "graph_metrics": {
            "node_count": node_count,
            "edge_count": edge_count,
            "density": edge_count / max(1, node_count * (node_count - 1)),
            "root_count": len([n for n, d in graph.in_degree() if d == 0]),
            "sink_count": len([n for n, d in graph.out_degree() if d == 0]),
        },
        "cost_analysis": {
            "pareto_analysis": pareto_metrics,
            "inequality_metrics": {
                "gini_index": round(gini, 4),
                "theil_index": round(theil_index, 4),
                "coefficient_of_variation": round(
                    math.sqrt(sum((c - total_cost/n)**2 for c in costs) / n) / (total_cost/n) 
                    if total_cost > 0 else 0.0, 4
                ),
            },
            "summary": {
                "item_count": n,
                "total_cost": round(total_cost, 2),
                "average_item_cost": round(total_cost / n, 2) if n > 0 else 0.0,
                "median_item_cost": round(sorted(costs)[n//2] if n > 0 else 0.0, 2),
                "cost_range": round(max(costs) - min(costs), 2) if costs else 0.0,
            },
        },
        "risk_analysis": {
            "profile": risk_profile or {},
            "flux_metrics": flux_metrics or {},
            "risk_adjusted": bool(risk_profile and risk_profile.get("level") != "LOW"),
        },
        "thermodynamics": self.analyze_budget_exergy(requirements),
        "material_distribution": {
            "by_unit": self._analyze_unit_distribution(requirements),
            "by_waste_factor": self._analyze_waste_distribution(requirements),
        },
        "generation_info": {
            "timestamp": datetime.now().isoformat(),
            "algorithm": "Hybrid-Topological-Algebraic-v2.1",
            "generator_version": "2.1.0",
            "platform": platform.platform(),
            "python_version": sys.version.split()[0],
            "networkx_version": nx.__version__,
        },
    }


def _analyze_unit_distribution(self, requirements: List[MaterialRequirement]) -> Dict[str, Any]:
    """Analiza distribuci√≥n por unidad de medida."""
    unit_groups = {}
    for req in requirements:
        unit = req.unit or "UND"
        if unit not in unit_groups:
            unit_groups[unit] = {"count": 0, "total_cost": 0.0, "total_qty": 0.0}
        
        unit_groups[unit]["count"] += 1
        unit_groups[unit]["total_cost"] += req.total_cost
        unit_groups[unit]["total_qty"] += req.quantity_total
    
    # Ordenar por costo total descendente
    sorted_units = sorted(
        unit_groups.items(),
        key=lambda x: x[1]["total_cost"],
        reverse=True
    )
    
    return {
        unit: {
            "count": data["count"],
            "total_cost": round(data["total_cost"], 2),
            "total_quantity": round(data["total_qty"], 2),
            "percentage_of_cost": round(data["total_cost"] / sum(g["total_cost"] for g in unit_groups.values()) * 100, 1)
            if sum(g["total_cost"] for g in unit_groups.values()) > 0 else 0.0
        }
        for unit, data in sorted_units
    }


def _analyze_waste_distribution(self, requirements: List[MaterialRequirement]) -> Dict[str, Any]:
    """Analiza distribuci√≥n de factores de desperdicio."""
    if not requirements:
        return {}
    
    waste_factors = [req.waste_factor for req in requirements]
    
    # Categorizaci√≥n de desperdicio
    categories = {
        "low": [w for w in waste_factors if w <= 0.05],
        "medium": [w for w in waste_factors if 0.05 < w <= 0.10],
        "high": [w for w in waste_factors if 0.10 < w <= 0.20],
        "very_high": [w for w in waste_factors if w > 0.20],
    }
    
    return {
        "statistics": {
            "mean": round(sum(waste_factors) / len(waste_factors), 4),
            "median": round(sorted(waste_factors)[len(waste_factors)//2], 4),
            "max": round(max(waste_factors), 4),
            "min": round(min(waste_factors), 4),
            "std_dev": round(
                math.sqrt(sum((w - sum(waste_factors)/len(waste_factors))**2 for w in waste_factors) / len(waste_factors)), 
                4
            ) if len(waste_factors) > 1 else 0.0,
        },
        "distribution": {
            category: {
                "count": len(values),
                "percentage": round(len(values) / len(waste_factors) * 100, 1),
                "avg_waste": round(sum(values) / len(values), 4) if values else 0.0,
            }
            for category, values in categories.items()
        }
    }