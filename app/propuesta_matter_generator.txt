### propuesta 1

def materialize_project(
    self,
    graph: nx.DiGraph,
    risk_profile: Optional[Dict[str, Any]] = None,
    flux_metrics: Optional[Dict[str, Any]] = None
) -> BillOfMaterials:
    """
    Orquesta la transformaci√≥n del Grafo en BOM mediante colapso topol√≥gico.
    
    Invariante Topol√≥gico: El grafo es un espacio discreto donde la materializaci√≥n
    act√∫a como un functor que preserva la estructura multiplicativa de cantidades
    mientras colapsa la jerarqu√≠a a una lista plana de requerimientos.
    """
    self.logger.info("üåå Iniciando materializaci√≥n del proyecto...")

    # 0. Validaci√≥n topol√≥gica: verificar aciclicidad (propiedad DAG)
    if not nx.is_directed_acyclic_graph(graph):
        cycles = list(nx.simple_cycles(graph))[:3]
        self.logger.error(f"üî• Grafo contiene ciclos. Imposible materializar: {cycles}")
        raise ValueError(f"El grafo contiene ciclos, violando la estructura DAG requerida")

    # 1. Colapso de Onda (Propagaci√≥n en orden topol√≥gico)
    raw_materials = self._explode_pyramid(graph)
    self.logger.info(f"üß± Materiales brutos extra√≠dos: {len(raw_materials)}")

    if not raw_materials:
        self.logger.warning("‚ö†Ô∏è No se extrajeron materiales del grafo.")
        return BillOfMaterials(
            requirements=[],
            total_material_cost=0.0,
            metadata={"warning": "empty_graph", "node_count": graph.number_of_nodes()}
        )

    # 2. Aplicaci√≥n de Entrop√≠a (Transformaci√≥n continua de factores)
    adjusted_materials = self._apply_entropy_factors(raw_materials, flux_metrics)

    # 3. Clustering Cohomol√≥gico (Agrupaci√≥n con preservaci√≥n de invariantes)
    final_requirements = self._cluster_semantically(adjusted_materials)
    self.logger.info(f"üõí Requerimientos consolidados: {len(final_requirements)}")

    # 4. C√°lculo de invariantes finales
    total_cost = sum(req.total_cost for req in final_requirements)
    concentration = self._compute_pareto_concentration(final_requirements, total_cost)

    return BillOfMaterials(
        requirements=final_requirements,
        total_material_cost=round(total_cost, 2),
        metadata={
            "risk_profile": risk_profile,
            "flux_metrics": flux_metrics,
            "node_count": graph.number_of_nodes(),
            "edge_count": graph.number_of_edges(),
            "unique_materials": len(final_requirements),
            "pareto_concentration": concentration,
            "topological_depth": self._compute_dag_depth(graph)
        }
    )


def _explode_pyramid(self, graph: nx.DiGraph) -> List[Dict[str, Any]]:
    """
    Recorre el grafo usando orden topol√≥gico para propagaci√≥n correcta de cantidades.
    
    Principio: Usamos la secuencia de Hasse (orden topol√≥gico) para garantizar
    que cada nodo se procese despu√©s de TODOS sus predecesores. Esto resuelve
    correctamente estructuras tipo diamante donde un insumo es alcanzable
    desde m√∫ltiples APUs, acumulando todas las contribuciones.
    
    Complejidad: O(V + E) donde V=nodos, E=aristas
    """
    materials = []

    # Identificar fuentes (ra√≠ces del DAG: nodos sin predecesores)
    root_nodes = [n for n, deg in graph.in_degree() if deg == 0]

    if not root_nodes:
        self.logger.error("üî• Grafo sin nodos ra√≠z (in_degree=0). Estructura inv√°lida.")
        return materials

    # Estructura de propagaci√≥n: nodo -> {cantidad acumulada, ancestros APU}
    # Permite sumar contribuciones de m√∫ltiples caminos (diamantes)
    propagation: Dict[str, Dict[str, Any]] = {}

    # Inicializar ra√≠ces con cantidad unitaria
    for root in root_nodes:
        propagation[root] = {
            "quantity": 1.0,
            "ancestor_apus": frozenset()
        }

    # Procesar en orden topol√≥gico (garantiza predecesores ya calculados)
    topo_order = list(nx.topological_sort(graph))

    for node in topo_order:
        node_data = graph.nodes[node]
        node_type = node_data.get("type", "UNKNOWN")

        # Si no es ra√≠z, calcular cantidad desde todos los predecesores
        if node not in propagation:
            total_qty = 0.0
            ancestor_apus = set()

            for predecessor in graph.predecessors(node):
                if predecessor not in propagation:
                    continue  # Predecesor no alcanzable desde ra√≠ces

                edge_data = graph[predecessor][node]
                edge_qty = float(edge_data.get("quantity", 1.0))
                pred_data = propagation[predecessor]

                # Acumular cantidad: producto de camino
                total_qty += pred_data["quantity"] * edge_qty

                # Heredar ancestros APU
                ancestor_apus.update(pred_data["ancestor_apus"])

                # Si predecesor es APU, registrarlo como ancestro directo
                if graph.nodes[predecessor].get("type") == "APU":
                    ancestor_apus.add(predecessor)

            propagation[node] = {
                "quantity": total_qty,
                "ancestor_apus": frozenset(ancestor_apus)
            }

        # Extraer INSUMOs (hojas funcionales del grafo)
        if node_type == "INSUMO":
            prop = propagation[node]
            
            # Validar cantidad positiva
            if prop["quantity"] <= 0:
                self.logger.warning(f"‚ö†Ô∏è Insumo '{node}' con cantidad ‚â§ 0, omitido.")
                continue

            materials.append({
                "id": node,
                "description": node_data.get("description", node),
                "base_qty": prop["quantity"],
                "unit_cost": float(node_data.get("unit_cost", 0.0)),
                "source_apus": set(prop["ancestor_apus"]),
                "unit": node_data.get("unit", "UND"),
                "category": node_data.get("category", "GENERAL")
            })

    return materials


def _apply_entropy_factors(
    self,
    raw_materials: List[Dict[str, Any]],
    flux_metrics: Optional[Dict[str, Any]]
) -> List[Dict[str, Any]]:
    """
    Aplica factores de desperdicio usando funciones de transferencia continuas.
    
    Principio: La entrop√≠a del sistema se modela como una transformaci√≥n suave
    (sigmoide/hiperb√≥lica) evitando discontinuidades en los umbrales. Esto
    produce estimaciones m√°s estables ante peque√±as variaciones en las m√©tricas.
    
    Modelo: waste_total = waste_global ‚äï waste_categor√≠a
    Donde ‚äï es uni√≥n probabil√≠stica: P(A‚à™B) = P(A) + P(B) - P(A)¬∑P(B)
    """
    import math

    # Extraer m√©tricas con defaults seguros
    saturation = 0.0
    stability = 10.0
    friction = 0.0

    if flux_metrics:
        saturation = float(flux_metrics.get("avg_saturation", 0.0))
        stability = float(flux_metrics.get("pyramid_stability", 10.0))
        friction = float(flux_metrics.get("avg_friction", 0.0))

    def sigmoid(x: float, center: float, steepness: float) -> float:
        """Sigmoide generalizada para transiciones suaves."""
        exponent = -steepness * (x - center)
        exponent = max(-500, min(500, exponent))  # Prevenir overflow
        return 1.0 / (1.0 + math.exp(exponent))

    def compute_global_waste() -> float:
        """Calcula factor de desperdicio global basado en m√©tricas de flujo."""
        waste = 0.0

        # Componente saturaci√≥n: sigmoide centrada en 0.65, max ~5%
        # Alta saturaci√≥n indica datos complejos/sucios
        waste += 0.05 * sigmoid(saturation, center=0.65, steepness=12)

        # Componente estabilidad: funci√≥n hiperb√≥lica inversa
        # Baja estabilidad (pir√°mide invertida) indica riesgo estructural
        if stability > 0:
            waste += 0.06 * (1.0 / (1.0 + stability))
        else:
            waste += 0.06  # Estabilidad cero = m√°ximo riesgo

        # Componente fricci√≥n: lineal acotada
        waste += min(0.03, friction * 0.015)

        return min(waste, 0.12)  # Cap conservador del 12%

    # Factores intr√≠nsecos por categor√≠a de material
    CATEGORY_FACTORS: Dict[str, float] = {
        "AGREGADO": 0.05,      # P√©rdidas en transporte/manipulaci√≥n
        "CEMENTICIO": 0.02,    # Material controlado
        "ACERO": 0.03,         # Despuntes de corte
        "MADERA": 0.08,        # Alta variabilidad dimensional
        "CERAMICO": 0.05,      # Roturas en instalaci√≥n
        "ELECTRICO": 0.03,     # Recortes de cableado
        "HIDRAULICO": 0.04,    # Ajustes en sitio
        "PINTURA": 0.10,       # Mermas significativas
        "GENERAL": 0.02        # Baseline conservador
    }

    global_waste = compute_global_waste()

    processed_materials = []
    for mat in raw_materials:
        category = mat.get("category", "GENERAL").upper()
        category_waste = CATEGORY_FACTORS.get(category, CATEGORY_FACTORS["GENERAL"])

        # Uni√≥n probabil√≠stica: evita sobreestimaci√≥n por suma directa
        combined_waste = global_waste + category_waste - (global_waste * category_waste)

        mat["waste_factor"] = round(combined_waste, 4)
        mat["waste_components"] = {
            "global": round(global_waste, 4),
            "category": round(category_waste, 4),
            "combined_method": "probabilistic_union"
        }
        mat["total_qty"] = mat["base_qty"] * (1.0 + combined_waste)

        processed_materials.append(mat)

    return processed_materials


def _cluster_semantically(
    self,
    materials: List[Dict[str, Any]]
) -> List[MaterialRequirement]:
    """
    Agrupa materiales por identidad sem√°ntica preservando invariantes num√©ricos.
    
    Principio Cohomol√≥gico: La agrupaci√≥n define una relaci√≥n de equivalencia
    donde el cociente colapsa elementos iguales. Los invariantes (costo total,
    cantidad total) se preservan bajo la proyecci√≥n can√≥nica œÄ: M ‚Üí M/~
    
    Invariantes preservados:
    - Œ£(quantity_base) se conserva
    - Œ£(total_cost) se conserva  
    - Costo unitario: promedio ponderado por cantidad
    - Waste factor: promedio ponderado por cantidad
    """
    clustered: Dict[str, Dict[str, Any]] = {}

    for mat in materials:
        key = mat["id"]

        # Normalizar source_apus a set
        source_apus = mat.get("source_apus", set())
        if isinstance(source_apus, str):
            source_apus = {source_apus}
        elif not isinstance(source_apus, set):
            source_apus = set(source_apus) if source_apus else set()

        if key not in clustered:
            clustered[key] = {
                "id": key,
                "description": mat["description"],
                "quantity_base": 0.0,
                "quantity_total": 0.0,
                "weighted_cost_sum": 0.0,      # Œ£(unit_cost √ó base_qty)
                "weighted_waste_sum": 0.0,      # Œ£(waste_factor √ó base_qty)
                "source_apus": set(),
                "unit": mat.get("unit", "UND"),
                "units_observed": set(),        # Para validaci√≥n
                "costs_observed": []            # Para validaci√≥n
            }

        cluster = clustered[key]
        base_qty = float(mat["base_qty"])
        total_qty = float(mat["total_qty"])
        unit_cost = float(mat["unit_cost"])
        waste_factor = float(mat.get("waste_factor", 0.0))

        # Acumular cantidades (preservaci√≥n de invariante)
        cluster["quantity_base"] += base_qty
        cluster["quantity_total"] += total_qty

        # Acumuladores para promedios ponderados
        cluster["weighted_cost_sum"] += unit_cost * base_qty
        cluster["weighted_waste_sum"] += waste_factor * base_qty

        # Unir fuentes APU
        cluster["source_apus"].update(source_apus)

        # Tracking para validaci√≥n de consistencia
        cluster["units_observed"].add(mat.get("unit", "UND"))
        cluster["costs_observed"].append(unit_cost)

    # Construir MaterialRequirements con validaciones
    requirements = []

    for key, data in clustered.items():
        qty_base = data["quantity_base"]

        # Calcular promedio ponderado del costo unitario
        if qty_base > 0:
            weighted_unit_cost = data["weighted_cost_sum"] / qty_base
            weighted_waste = data["weighted_waste_sum"] / qty_base
        else:
            weighted_unit_cost = 0.0
            weighted_waste = 0.0

        # Validaci√≥n: consistencia de unidades
        if len(data["units_observed"]) > 1:
            self.logger.warning(
                f"‚ö†Ô∏è Material '{key}' tiene unidades mixtas: {data['units_observed']}. "
                f"Usando '{data['unit']}'"
            )

        # Validaci√≥n: dispersi√≥n de costos unitarios (coeficiente de variaci√≥n)
        costs = data["costs_observed"]
        if len(costs) > 1:
            mean_cost = sum(costs) / len(costs)
            if mean_cost > 0:
                variance = sum((c - mean_cost) ** 2 for c in costs) / len(costs)
                cv = (variance ** 0.5) / mean_cost
                if cv > 0.10:  # CV > 10% indica inconsistencia
                    self.logger.warning(
                        f"‚ö†Ô∏è Material '{key}' tiene costos dispersos (CV={cv:.1%}): "
                        f"[{min(costs):.2f} - {max(costs):.2f}]"
                    )

        # Calcular costo total con promedio ponderado
        total_cost = data["quantity_total"] * weighted_unit_cost

        req = MaterialRequirement(
            id=data["id"],
            description=data["description"],
            quantity_base=round(qty_base, 4),
            unit=data["unit"],
            waste_factor=round(weighted_waste, 4),
            quantity_total=round(data["quantity_total"], 4),
            unit_cost=round(weighted_unit_cost, 2),
            total_cost=round(total_cost, 2),
            source_apus=sorted(data["source_apus"])
        )
        requirements.append(req)

    # Ordenar por costo total descendente (Principio de Pareto: 80/20)
    requirements.sort(key=lambda x: x.total_cost, reverse=True)

    return requirements


def _compute_pareto_concentration(
    self,
    requirements: List[MaterialRequirement],
    total_cost: float
) -> Dict[str, float]:
    """
    Calcula m√©tricas de concentraci√≥n de costos para an√°lisis de Pareto.
    
    Retorna el coeficiente de Gini y el porcentaje del costo en el top 20% de items.
    """
    if not requirements or total_cost <= 0:
        return {"gini_coefficient": 0.0, "top_20_percent_cost": 0.0, "items_for_80_percent": 0}

    costs = sorted([r.total_cost for r in requirements], reverse=True)
    n = len(costs)

    # Coeficiente de Gini: mide desigualdad en distribuci√≥n
    cumsum = sum(costs)
    if cumsum > 0:
        weighted_sum = sum((n - i) * cost for i, cost in enumerate(costs))
        gini = (2 * weighted_sum) / (n * cumsum) - (n + 1) / n
        gini = max(0, min(1, gini))  # Acotar a [0, 1]
    else:
        gini = 0.0

    # Porcentaje del costo en top 20% de items
    top_n = max(1, int(n * 0.2))
    top_cost = sum(costs[:top_n])
    top_20_pct = top_cost / total_cost

    # Cantidad de items para alcanzar 80% del costo
    cumulative = 0.0
    items_for_80 = 0
    threshold = total_cost * 0.8
    for cost in costs:
        cumulative += cost
        items_for_80 += 1
        if cumulative >= threshold:
            break

    return {
        "gini_coefficient": round(gini, 4),
        "top_20_percent_cost": round(top_20_pct, 4),
        "items_for_80_percent": items_for_80
    }


def _compute_dag_depth(self, graph: nx.DiGraph) -> int:
    """
    Calcula la profundidad m√°xima del DAG (longitud del camino cr√≠tico).
    """
    if graph.number_of_nodes() == 0:
        return 0
    
    try:
        return nx.dag_longest_path_length(graph)
    except nx.NetworkXError:
        return -1  # Indica error (posible ciclo)

### propuesta 2

class MatterGenerator:
    """
    Motor de Materializaci√≥n con fundamentos topol√≥gicos robustos.
    Transforma la estructura del grafo en BOM usando un colapso de onda deterministico.
    """

    def __init__(self):
        self.logger = logging.getLogger(self.__class__.__name__)

    def materialize_project(
        self,
        graph: nx.DiGraph,
        risk_profile: Optional[Dict[str, Any]] = None,
        flux_metrics: Optional[Dict[str, Any]] = None
    ) -> BillOfMaterials:
        """
        Orquesta la transformaci√≥n del Grafo en BOM con validaciones robustas.

        Args:
            graph: Grafo topol√≥gico (presumiblemente un DAG de composici√≥n)
            risk_profile: Perfil de riesgo para ajuste de factores
            flux_metrics: M√©tricas de estabilidad/fricci√≥n

        Returns:
            BillOfMaterials: La lista de compras consolidada con metadata completa

        Raises:
            ValueError: Si el grafo no es un DAG o contiene ciclos prohibidos
        """
        self.logger.info("üåå Iniciando materializaci√≥n del proyecto...")

        # Validaci√≥n de la estructura del grafo (debe ser DAG para composici√≥n)
        if not nx.is_directed_acyclic_graph(graph):
            raise ValueError("El grafo debe ser un DAG para garantizar consistencia composicional")

        # 1. Colapso de Onda (Recorrido topol√≥gico del Grafo)
        raw_materials = self._explode_pyramid(graph)
        self.logger.info(f"üß± Materiales brutos extra√≠dos: {len(raw_materials)}")

        if not raw_materials:
            self.logger.warning("‚ö†Ô∏è No se encontraron materiales en el grafo")

        # 2. Aplicaci√≥n de Entrop√≠a (Factores de Seguridad con monoides)
        adjusted_materials = self._apply_entropy_factors(
            raw_materials, 
            flux_metrics,
            risk_profile
        )

        # 3. Clustering Sem√°ntico (Agrupaci√≥n con kernel categ√≥rico)
        final_requirements = self._cluster_semantically(adjusted_materials)
        self.logger.info(f"üõí Requerimientos consolidados: {len(final_requirements)}")

        # 4. C√°lculo de Totales con validaci√≥n num√©rica
        total_cost = self._compute_total_cost(final_requirements)

        # 5. Generaci√≥n de metadata con invariantes topol√≥gicos
        metadata = self._generate_metadata(
            graph, risk_profile, flux_metrics, final_requirements
        )

        return BillOfMaterials(
            requirements=final_requirements,
            total_material_cost=total_cost,
            metadata=metadata
        )

    def _explode_pyramid(self, graph: nx.DiGraph) -> List[Dict[str, Any]]:
        """
        Recorre el grafo desde ra√≠ces usando teor√≠a de categor√≠as para propagaci√≥n.
        
        Implementa un funtor desde la categor√≠a del grafo a la categor√≠a de materiales.
        Cada camino √∫nico corresponde a una composici√≥n de monoides.
        """
        materials = []
        
        # Identificaci√≥n de ra√≠ces: nodos con degree de entrada 0
        root_nodes = [node for node, in_degree in graph.in_degree() if in_degree == 0]
        
        if not root_nodes:
            self.logger.error("‚ùå Grafo no tiene ra√≠ces (nodos sin predecesores)")
            return materials
        
        self.logger.debug(f"Encontradas {len(root_nodes)} ra√≠ces: {root_nodes}")
        
        # Usamos DFS con stack para preservar el orden topol√≥gico inverso
        # Estructura: (nodo, cantidad_acumulada, camino, padre_apu)
        stack = [(root, 1.0, [], None) for root in root_nodes]
        
        # Track de ciclos (aunque deber√≠a ser DAG, verificamos defensivamente)
        visited_edges = set()
        steps = 0
        MAX_STEPS = len(graph.nodes()) * 10  # L√≠mite basado en complejidad
        
        while stack:
            current_node, current_qty, current_path, parent_apu = stack.pop()
            steps += 1
            
            if steps > MAX_STEPS:
                raise RuntimeError("Posible ciclo detectado o grafo demasiado complejo")
            
            # Verificaci√≥n de ciclo en el camino actual
            if current_node in current_path:
                self.logger.warning(f"Ciclo detectado en camino: {current_path + [current_node]}")
                continue
            
            node_data = graph.nodes[current_node]
            node_type = node_data.get("type", "UNDEFINED")
            new_path = current_path + [current_node]
            
            # Si es nodo terminal (INSUMO), registramos material
            if node_type == "INSUMO":
                # Validamos que tenemos los atributos necesarios
                description = node_data.get("description", "")
                if not description:
                    description = node_data.get("name", current_node)
                
                materials.append({
                    "id": current_node,
                    "description": description,
                    "base_qty": current_qty,
                    "unit_cost": float(node_data.get("unit_cost", 0.0)),
                    "source_apu": parent_apu or "ROOT",
                    "unit": node_data.get("unit", "UND"),
                    "node_data": node_data,  # Preservamos metadata completa
                    "composition_path": new_path  # Para debugging/trazabilidad
                })
                continue
            
            # Expandimos hijos con propagaci√≥n de cantidades
            for successor in graph.successors(current_node):
                edge_key = (current_node, successor)
                
                if edge_key in visited_edges:
                    self.logger.debug(f"Arista duplicada en recorrido: {edge_key}")
                    continue
                
                visited_edges.add(edge_key)
                edge_data = graph.edges[edge_key]
                edge_qty = float(edge_data.get("quantity", 1.0))
                
                # Validaci√≥n: cantidad debe ser positiva
                if edge_qty <= 0:
                    self.logger.warning(f"Cantidad no positiva en arista {edge_key}: {edge_qty}")
                    edge_qty = 1.0
                
                # Si el nodo actual es APU, lo registramos como padre para materiales hijos
                next_parent = current_node if node_type == "APU" else parent_apu
                
                # Calculamos nueva cantidad (composici√≥n de morfismos)
                new_qty = current_qty * edge_qty
                
                # Verificaci√≥n de desbordamiento num√©rico
                if new_qty > 1e10:  # L√≠mite razonable
                    self.logger.warning(f"Cantidad muy grande en nodo {successor}: {new_qty}")
                
                stack.append((successor, new_qty, new_path, next_parent))
        
        return materials

    def _apply_entropy_factors(
        self,
        raw_materials: List[Dict[str, Any]],
        flux_metrics: Optional[Dict[str, Any]],
        risk_profile: Optional[Dict[str, Any]] = None
    ) -> List[Dict[str, Any]]:
        """
        Aplica factores de entrop√≠a usando un monoide de transformaci√≥n.
        
        El espacio de factores forma un grupo conmutativo bajo multiplicaci√≥n,
        permitiendo composici√≥n jer√°rquica de factores de riesgo.
        """
        # Grupo de factores base con elemento identidad 1.0
        base_factor = 1.0  # Elemento neutro del grupo
        
        # Aplicamos factores de flux_metrics (si existen)
        if flux_metrics:
            # Factor de saturaci√≥n (fricci√≥n del sistema)
            saturation = flux_metrics.get("avg_saturation", 0.0)
            if saturation > 0.8:
                base_factor *= 1.05  # +5% por saturaci√≥n
            
            # Factor de estabilidad estructural
            stability = flux_metrics.get("pyramid_stability", 1.0)
            if stability < 1.0:
                base_factor *= 1.03  # +3% por inestabilidad
        
        # Aplicamos factores de riesgo (si existen)
        if risk_profile:
            risk_level = risk_profile.get("level", "MEDIUM")
            risk_multipliers = {
                "LOW": 1.01,    # +1%
                "MEDIUM": 1.03,  # +3%
                "HIGH": 1.07,    # +7%
                "CRITICAL": 1.15 # +15%
            }
            base_factor *= risk_multipliers.get(risk_level, 1.03)
        
        # Factor de material espec√≠fico (podr√≠a basarse en tipo de material)
        processed_materials = []
        
        for mat in raw_materials:
            # Clonamos el material para no modificar el original
            processed = mat.copy()
            
            # Calculamos factor de desperdicio (waste_factor = base_factor - 1)
            waste_factor = base_factor - 1.0
            
            # Aplicamos factor espec√≠fico si el material tiene metadata
            material_type = mat.get("node_data", {}).get("material_category", "GENERIC")
            type_multipliers = {
                "FRAGILE": 1.02,
                "PERISHABLE": 1.04,
                "HAZARDOUS": 1.06
            }
            specific_factor = type_multipliers.get(material_type, 1.0)
            
            # Composici√≥n de factores: f_total = f_base * f_especifico
            total_waste_factor = (base_factor * specific_factor) - 1.0
            
            processed["waste_factor"] = total_waste_factor
            processed["total_qty"] = mat["base_qty"] * (1 + total_waste_factor)
            
            # Preservamos los factores aplicados para trazabilidad
            processed["applied_factors"] = {
                "base": base_factor,
                "specific": specific_factor,
                "material_type": material_type
            }
            
            processed_materials.append(processed)
        
        return processed_materials

    def _cluster_semantically(self, materials: List[Dict[str, Any]]) -> List[MaterialRequirement]:
        """
        Agrupa materiales usando un kernel de equivalencia categ√≥rica.
        
        Define una relaci√≥n de equivalencia basada en (id, unit) y
        realiza el colapso al cociente con operaciones de monoide aditivo.
        """
        # Diccionario de agrupaci√≥n: clave -> datos agregados
        clustered = {}
        
        for mat in materials:
            # Clave can√≥nica: tupla (id, unidad) para agrupaci√≥n fuerte
            canonical_key = (mat["id"], mat.get("unit", "UND"))
            
            if canonical_key not in clustered:
                clustered[canonical_key] = {
                    "id": mat["id"],
                    "description": mat["description"],
                    "quantity_base": 0.0,
                    "quantity_total": 0.0,
                    "unit_cost": mat["unit_cost"],
                    "waste_factor": 0.0,  # Ser√° recalculado
                    "source_apus": set(),
                    "unit": mat.get("unit", "UND"),
                    "unit_cost_samples": [],  # Para an√°lisis estad√≠stico
                    "composition_paths": []   # Para trazabilidad
                }
            
            data = clustered[canonical_key]
            
            # Acumulaci√≥n de cantidades (monoide aditivo)
            data["quantity_base"] += mat["base_qty"]
            data["quantity_total"] += mat["total_qty"]
            
            # Acumulaci√≥n de APUs de origen
            data["source_apus"].add(mat["source_apu"])
            
            # Muestras de costo unitario para an√°lisis
            data["unit_cost_samples"].append(mat["unit_cost"])
            
            # Preservamos algunos paths para debugging
            if len(data["composition_paths"]) < 3:  # Limitamos para no sobrecargar
                data["composition_paths"].append(mat.get("composition_path", []))
        
        # Procesamiento post-agrupaci√≥n
        requirements = []
        
        for (mat_id, unit), data in clustered.items():
            # Validaci√≥n: cantidad base debe ser positiva
            if data["quantity_base"] <= 0:
                self.logger.warning(f"Material {mat_id} tiene cantidad base no positiva")
                continue
            
            # Recalculamos waste_factor a partir de cantidades agregadas
            # Esto es m√°s preciso que promediar factores individuales
            total_waste = (data["quantity_total"] / data["quantity_base"]) - 1.0
            
            # Calculamos costo unitario representativo
            # Podr√≠amos usar mediana para ser robustos a outliers
            cost_samples = data["unit_cost_samples"]
            if cost_samples:
                # Usamos la mediana para evitar influencia de outliers
                sorted_costs = sorted(cost_samples)
                mid = len(sorted_costs) // 2
                if len(sorted_costs) % 2 == 0:
                    representative_cost = (sorted_costs[mid-1] + sorted_costs[mid]) / 2
                else:
                    representative_cost = sorted_costs[mid]
            else:
                representative_cost = data["unit_cost"]
            
            # Validamos el costo unitario
            if representative_cost <= 0:
                self.logger.warning(f"Material {mat_id} tiene costo unitario no positivo")
                representative_cost = 0.0
            
            # Calculamos costo total
            total_cost = data["quantity_total"] * representative_cost
            
            # Creamos el requerimiento
            req = MaterialRequirement(
                id=data["id"],
                description=data["description"],
                quantity_base=data["quantity_base"],
                unit=data["unit"],
                waste_factor=total_waste,
                quantity_total=data["quantity_total"],
                unit_cost=representative_cost,
                total_cost=total_cost,
                source_apus=list(data["source_apus"])
            )
            
            requirements.append(req)
        
        # Ordenamiento por criterio de Pareto (costo total descendente)
        requirements.sort(key=lambda x: (x.total_cost, -len(x.source_apus)), reverse=True)
        
        return requirements

    def _compute_total_cost(self, requirements: List[MaterialRequirement]) -> float:
        """
        Calcula el costo total con validaci√≥n num√©rica robusta.
        
        Implementa suma en doble precisi√≥n con verificaci√≥n de overflow.
        """
        total = 0.0
        
        for req in requirements:
            # Validaci√≥n: costo debe ser finito
            if not math.isfinite(req.total_cost):
                self.logger.error(f"Costo no finito en material {req.id}: {req.total_cost}")
                continue
            
            # Suma con verificaci√≥n de overflow
            new_total = total + req.total_cost
            if math.isinf(new_total):
                raise OverflowError("Overflow en c√°lculo de costo total")
            
            total = new_total
        
        # Redondeo a 2 decimales para representaci√≥n monetaria
        return round(total, 2)

    def _generate_metadata(
        self,
        graph: nx.DiGraph,
        risk_profile: Optional[Dict[str, Any]],
        flux_metrics: Optional[Dict[str, Any]],
        requirements: List[MaterialRequirement]
    ) -> Dict[str, Any]:
        """
        Genera metadata con invariantes topol√≥gicos y m√©tricas de calidad.
        """
        # Calcular invariantes topol√≥gicos del grafo
        try:
            is_dag = nx.is_directed_acyclic_graph(graph)
            longest_path = nx.dag_longest_path_length(graph) if is_dag else None
        except Exception:
            is_dag = False
            longest_path = None
        
        # M√©tricas de distribuci√≥n de costos
        if requirements:
            costs = [r.total_cost for r in requirements]
            total_cost = sum(costs)
            if total_cost > 0:
                pareto_80_idx = int(len(costs) * 0.2)  # √çndice para 80% del costo
                sorted_costs = sorted(costs, reverse=True)
                pareto_cost = sum(sorted_costs[:pareto_80_idx])
                pareto_percentage = (pareto_cost / total_cost) * 100
            else:
                pareto_percentage = 0.0
        else:
            pareto_percentage = 0.0
        
        metadata = {
            "graph_metrics": {
                "node_count": graph.number_of_nodes(),
                "edge_count": graph.number_of_edges(),
                "is_dag": is_dag,
                "longest_path": longest_path,
                "root_count": len([n for n, d in graph.in_degree() if d == 0])
            },
            "cost_distribution": {
                "item_count": len(requirements),
                "pareto_80_percentage": round(pareto_percentage, 2),
                "unique_apus": len(set().union(*[r.source_apus for r in requirements]))
            },
            "risk_profile": risk_profile,
            "flux_metrics": flux_metrics,
            "generation_timestamp": datetime.now().isoformat(),
            "version": "2.0-topology"
        }
        
        return metadata