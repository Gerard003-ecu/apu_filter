### Propuesta 1

def _explode_pyramid(self, graph: nx.DiGraph) -> List[Dict[str, Any]]:
    """
    Recorre el grafo implementando un funtor F: G ‚Üí Mat desde la categor√≠a 
    del DAG hacia la categor√≠a de materiales.
    
    Utiliza propagaci√≥n de cantidades mediante composici√≥n de morfismos,
    donde cada arista representa un morfismo en el monoide multiplicativo (‚Ñù‚Å∫, √ó, 1).
    
    La detecci√≥n de ciclos usa hashing O(1) con frozenset para el camino actual.
    """
    materials: List[Dict[str, Any]] = []
    
    if graph.number_of_nodes() == 0:
        self.logger.warning("‚ö†Ô∏è Grafo vac√≠o proporcionado")
        return materials
    
    # Identificaci√≥n de fuentes (objetos iniciales en la categor√≠a)
    root_nodes = [node for node, in_degree in graph.in_degree() if in_degree == 0]
    
    if not root_nodes:
        self.logger.error("‚ùå Grafo sin ra√≠ces - violaci√≥n de estructura DAG")
        return materials
    
    self.logger.debug(f"üå≥ Ra√≠ces identificadas: {len(root_nodes)}")
    
    # DFS iterativo con detecci√≥n de ciclos O(1)
    # Tupla: (nodo, cantidad_acumulada, camino_set, camino_lista, padre_apu)
    stack: List[tuple] = [
        (root, 1.0, frozenset(), [], None) for root in root_nodes
    ]
    
    iteration_count = 0
    # Cota superior basada en expansi√≥n m√°xima del DAG
    max_iterations = graph.number_of_nodes() * max(graph.number_of_edges(), 1) + 1
    
    while stack:
        iteration_count += 1
        
        if iteration_count > max_iterations:
            raise RuntimeError(
                f"L√≠mite de iteraciones ({max_iterations}) excedido. "
                "Posible inconsistencia estructural."
            )
        
        current_node, current_qty, path_set, path_list, parent_apu = stack.pop()
        
        # Detecci√≥n de ciclo O(1) mediante pertenencia a conjunto
        if current_node in path_set:
            self.logger.warning(
                f"‚ö†Ô∏è Ciclo detectado: {'‚Üí'.join(path_list[-3:])}‚Üí{current_node}"
            )
            continue
        
        node_data = graph.nodes.get(current_node, {})
        node_type = node_data.get("type", "UNDEFINED")
        
        # Extensi√≥n del camino (inmutable para backtracking correcto)
        new_path_set = path_set | {current_node}
        new_path_list = path_list + [current_node]
        
        # Objeto terminal: registramos en la fibra de materiales
        if node_type == "INSUMO":
            description = (
                node_data.get("description") or 
                node_data.get("name") or 
                str(current_node)
            )
            
            # Validaci√≥n de costo unitario
            unit_cost = node_data.get("unit_cost", 0.0)
            try:
                unit_cost = float(unit_cost)
                if not math.isfinite(unit_cost) or unit_cost < 0:
                    self.logger.warning(f"‚ö†Ô∏è Costo inv√°lido en {current_node}")
                    unit_cost = 0.0
            except (TypeError, ValueError):
                unit_cost = 0.0
            
            materials.append({
                "id": current_node,
                "description": description,
                "base_qty": current_qty,
                "unit_cost": unit_cost,
                "source_apu": parent_apu or "ROOT",
                "unit": node_data.get("unit", "UND"),
                "node_data": node_data,
                "composition_path": new_path_list,
                "fiber_depth": len(new_path_list)
            })
            continue
        
        # Determinaci√≥n del APU ancestro para trazabilidad
        next_parent_apu = current_node if node_type == "APU" else parent_apu
        
        # Expansi√≥n de sucesores con composici√≥n de morfismos
        for successor in graph.successors(current_node):
            edge_data = graph.edges.get((current_node, successor), {})
            
            # Extracci√≥n y validaci√≥n del morfismo (cantidad)
            edge_qty = edge_data.get("quantity", 1.0)
            try:
                edge_qty = float(edge_qty)
                if not math.isfinite(edge_qty) or edge_qty <= 0:
                    self.logger.warning(
                        f"‚ö†Ô∏è Morfismo inv√°lido ({current_node}‚Üí{successor}): {edge_qty}"
                    )
                    edge_qty = 1.0
            except (TypeError, ValueError):
                edge_qty = 1.0
            
            # Composici√≥n: q_nuevo = q_actual ‚àò q_arista
            new_qty = current_qty * edge_qty
            
            # Verificaci√≥n de estabilidad num√©rica
            if not math.isfinite(new_qty) or new_qty > 1e12:
                self.logger.error(f"‚ùå Overflow en {successor}: {new_qty:.2e}")
                continue
            
            stack.append((
                successor, new_qty, new_path_set, 
                new_path_list, next_parent_apu
            ))
    
    self.logger.debug(
        f"üìä Exploraci√≥n completada: {iteration_count} iteraciones, "
        f"{len(materials)} materiales"
    )
    return materials


def _apply_entropy_factors(
    self,
    raw_materials: List[Dict[str, Any]],
    flux_metrics: Optional[Dict[str, Any]],
    risk_profile: Optional[Dict[str, Any]] = None
) -> List[Dict[str, Any]]:
    """
    Aplica factores de entrop√≠a mediante acci√≥n del grupo multiplicativo (‚Ñù‚Å∫, √ó, 1).
    
    La composici√≥n de factores es asociativa y conmutativa, formando un
    homomorfismo de grupos: œÜ(f‚ÇÅ ¬∑ f‚ÇÇ) = œÜ(f‚ÇÅ) √ó œÜ(f‚ÇÇ).
    
    El factor de desperdicio Œµ se define como Œµ = Œº - 1, donde Œº es el
    multiplicador total, garantizando Œµ ‚â• 0 para factores Œº ‚â• 1.
    """
    # Constantes del grupo de factores (configurables)
    SATURATION_THRESHOLD = 0.8
    FACTOR_SATURATION = 1.05
    FACTOR_INSTABILITY = 1.03
    
    RISK_FACTORS = {
        "LOW": 1.01,
        "MEDIUM": 1.03,
        "HIGH": 1.07,
        "CRITICAL": 1.15
    }
    
    MATERIAL_FACTORS = {
        "FRAGILE": 1.02,
        "PERISHABLE": 1.04,
        "HAZARDOUS": 1.06
    }
    
    # Elemento identidad del grupo multiplicativo
    base_factor = 1.0
    factor_trace = {"identity": 1.0}
    
    # Composici√≥n de factores de flujo
    if flux_metrics and isinstance(flux_metrics, dict):
        saturation = flux_metrics.get("avg_saturation", 0.0)
        if isinstance(saturation, (int, float)) and saturation > SATURATION_THRESHOLD:
            base_factor *= FACTOR_SATURATION
            factor_trace["saturation"] = FACTOR_SATURATION
        
        stability = flux_metrics.get("pyramid_stability", 1.0)
        if isinstance(stability, (int, float)) and 0 < stability < 1.0:
            base_factor *= FACTOR_INSTABILITY
            factor_trace["instability"] = FACTOR_INSTABILITY
    
    # Composici√≥n de factores de riesgo
    if risk_profile and isinstance(risk_profile, dict):
        risk_level = str(risk_profile.get("level", "MEDIUM")).upper()
        risk_mult = RISK_FACTORS.get(risk_level, RISK_FACTORS["MEDIUM"])
        base_factor *= risk_mult
        factor_trace["risk"] = risk_mult
    
    processed_materials: List[Dict[str, Any]] = []
    
    for mat in raw_materials:
        processed = mat.copy()
        
        # Factor espec√≠fico por categor√≠a de material
        material_category = mat.get("node_data", {}).get("material_category", "GENERIC")
        specific_factor = MATERIAL_FACTORS.get(material_category, 1.0)
        
        # Composici√≥n total: Œº_total = Œº_base √ó Œº_espec√≠fico
        total_multiplier = base_factor * specific_factor
        
        # Invariante: Œº ‚â• 1 (no reducimos cantidades)
        if not math.isfinite(total_multiplier) or total_multiplier < 1.0:
            self.logger.warning(
                f"‚ö†Ô∏è Multiplicador inv√°lido para {mat.get('id')}: {total_multiplier}"
            )
            total_multiplier = 1.0
        
        base_qty = mat.get("base_qty", 0.0)
        
        # Factor de desperdicio: Œµ = Œº - 1
        processed["waste_factor"] = total_multiplier - 1.0
        processed["total_qty"] = base_qty * total_multiplier
        
        # Trazabilidad de transformaciones aplicadas
        processed["applied_factors"] = {
            **factor_trace,
            "material_specific": specific_factor,
            "material_category": material_category,
            "total_multiplier": round(total_multiplier, 6)
        }
        
        processed_materials.append(processed)
    
    return processed_materials


def _cluster_semantically(
    self, 
    materials: List[Dict[str, Any]]
) -> List[MaterialRequirement]:
    """
    Agrupa materiales mediante relaci√≥n de equivalencia categ√≥rica.
    
    Define el funtor cociente Q: Mat ‚Üí Mat/‚àº donde la relaci√≥n ‚àº est√°
    inducida por el kernel de la proyecci√≥n œÄ(m) = (id(m), unit(m)).
    
    Las cantidades se agregan mediante la estructura de monoide aditivo (‚Ñù, +, 0),
    y los costos se estiman mediante el estimador robusto de mediana.
    """
    if not materials:
        return []
    
    # Clases de equivalencia indexadas por clave can√≥nica
    equivalence_classes: Dict[tuple, Dict[str, Any]] = {}
    
    for mat in materials:
        mat_id = mat.get("id", "UNKNOWN")
        mat_unit = mat.get("unit", "UND")
        canonical_key = (mat_id, mat_unit)
        
        if canonical_key not in equivalence_classes:
            equivalence_classes[canonical_key] = {
                "id": mat_id,
                "description": mat.get("description", str(mat_id)),
                "unit": mat_unit,
                "quantity_base": 0.0,
                "quantity_total": 0.0,
                "source_apus": set(),
                "cost_samples": [],
                "instance_count": 0,
                "max_fiber_depth": 0
            }
        
        data = equivalence_classes[canonical_key]
        
        # Acumulaci√≥n monoidal aditiva
        base_qty = float(mat.get("base_qty", 0.0))
        total_qty = float(mat.get("total_qty", base_qty))
        
        data["quantity_base"] += base_qty
        data["quantity_total"] += total_qty
        data["instance_count"] += 1
        
        # Registro de fuente APU
        source_apu = mat.get("source_apu")
        if source_apu:
            data["source_apus"].add(source_apu)
        
        # Profundidad m√°xima en la fibra
        fiber_depth = mat.get("fiber_depth", 0)
        data["max_fiber_depth"] = max(data["max_fiber_depth"], fiber_depth)
        
        # Muestreo de costos para estimaci√≥n robusta
        unit_cost = mat.get("unit_cost", 0.0)
        if isinstance(unit_cost, (int, float)) and math.isfinite(unit_cost) and unit_cost >= 0:
            data["cost_samples"].append(unit_cost)
    
    # Colapso al espacio cociente
    requirements: List[MaterialRequirement] = []
    
    for canonical_key, data in equivalence_classes.items():
        quantity_base = data["quantity_base"]
        quantity_total = data["quantity_total"]
        
        # Validaci√≥n de positividad
        if quantity_base <= 0:
            self.logger.warning(f"‚ö†Ô∏è Cantidad base ‚â§ 0 para {data['id']}")
            continue
        
        # C√°lculo de waste_factor desde cantidades agregadas
        waste_factor = (quantity_total / quantity_base) - 1.0
        
        # Estimador robusto de costo: mediana
        cost_samples = [c for c in data["cost_samples"] if c > 0]
        
        if cost_samples:
            sorted_costs = sorted(cost_samples)
            n = len(sorted_costs)
            mid = n // 2
            representative_cost = (
                sorted_costs[mid] if n % 2 == 1 
                else (sorted_costs[mid - 1] + sorted_costs[mid]) / 2.0
            )
        else:
            representative_cost = 0.0
            self.logger.warning(f"‚ö†Ô∏è Sin muestras de costo para {data['id']}")
        
        # C√°lculo de costo total
        total_cost = quantity_total * representative_cost
        
        if not math.isfinite(total_cost):
            self.logger.error(f"‚ùå Costo no finito para {data['id']}")
            total_cost = 0.0
        
        req = MaterialRequirement(
            id=data["id"],
            description=data["description"],
            quantity_base=round(quantity_base, 6),
            unit=data["unit"],
            waste_factor=round(waste_factor, 6),
            quantity_total=round(quantity_total, 6),
            unit_cost=round(representative_cost, 4),
            total_cost=round(total_cost, 2),
            source_apus=sorted(data["source_apus"])
        )
        
        requirements.append(req)
    
    # Ordenamiento por dominancia de Pareto: (-costo, -complejidad, id)
    requirements.sort(key=lambda r: (-r.total_cost, -len(r.source_apus), r.id))
    
    return requirements


def _compute_total_cost(self, requirements: List[MaterialRequirement]) -> float:
    """
    Calcula el costo total usando el algoritmo de Kahan para suma compensada.
    
    Kahan summation reduce el error de redondeo de O(nŒµ) a O(Œµ), donde Œµ
    es el √©psilon de m√°quina, proporcionando estabilidad num√©rica superior.
    
    Complejidad: O(n) tiempo, O(1) espacio auxiliar.
    """
    if not requirements:
        return 0.0
    
    # Variables de Kahan summation
    total = 0.0
    compensation = 0.0
    
    for req in requirements:
        cost = req.total_cost
        
        # Verificaci√≥n de finitud
        if not math.isfinite(cost):
            self.logger.error(f"‚ùå Costo no finito en {req.id}: {cost}")
            continue
        
        # Paso de Kahan: compensaci√≥n del error de redondeo
        y = cost - compensation
        t = total + y
        compensation = (t - total) - y
        total = t
        
        # Detecci√≥n de overflow
        if math.isinf(total):
            raise OverflowError(
                f"Desbordamiento aritm√©tico. √öltimo elemento: {req.id}"
            )
    
    return round(total, 2)


def _generate_metadata(
    self,
    graph: nx.DiGraph,
    risk_profile: Optional[Dict[str, Any]],
    flux_metrics: Optional[Dict[str, Any]],
    requirements: List[MaterialRequirement]
) -> Dict[str, Any]:
    """
    Genera metadata con invariantes topol√≥gicos del grafo.
    
    Incluye an√°logos discretos de invariantes topol√≥gicos:
    - Œ≤‚ÇÄ (componentes conexas) ‚âà n√∫mero de Betti 0
    - œá = V - E (caracter√≠stica de Euler para grafos)
    - An√°lisis de Pareto con √≠ndice de Gini
    """
    node_count = graph.number_of_nodes()
    edge_count = graph.number_of_edges()
    
    # Componentes d√©bilmente conexas (an√°logo a Œ≤‚ÇÄ)
    weakly_connected = (
        nx.number_weakly_connected_components(graph) 
        if node_count > 0 else 0
    )
    
    # Invariantes de DAG
    is_dag = nx.is_directed_acyclic_graph(graph)
    longest_path_length = None
    layer_count = None
    
    if is_dag and node_count > 0:
        try:
            longest_path_length = nx.dag_longest_path_length(graph)
            topological_generations = list(nx.topological_generations(graph))
            layer_count = len(topological_generations)
        except nx.NetworkXError as e:
            self.logger.warning(f"‚ö†Ô∏è Error calculando invariantes DAG: {e}")
    
    # Fuentes y sumideros
    root_count = sum(1 for _, d in graph.in_degree() if d == 0)
    leaf_count = sum(1 for _, d in graph.out_degree() if d == 0)
    
    # Caracter√≠stica de Euler (simplificada para grafos)
    euler_characteristic = node_count - edge_count
    
    # An√°lisis de distribuci√≥n de Pareto
    pareto_analysis = self._compute_pareto_distribution(requirements)
    
    return {
        "graph_invariants": {
            "node_count": node_count,
            "edge_count": edge_count,
            "is_dag": is_dag,
            "weakly_connected_components": weakly_connected,
            "longest_path_length": longest_path_length,
            "topological_layer_count": layer_count,
            "source_count": root_count,
            "sink_count": leaf_count,
            "euler_characteristic": euler_characteristic
        },
        "cost_distribution": pareto_analysis,
        "input_parameters": {
            "risk_profile": risk_profile,
            "flux_metrics": flux_metrics
        },
        "generation_info": {
            "timestamp": datetime.now().isoformat(),
            "version": "2.1-topological",
            "algorithm": "categorical-materialization-functor"
        }
    }


def _compute_pareto_distribution(
    self,
    requirements: List[MaterialRequirement]
) -> Dict[str, Any]:
    """
    Calcula m√©tricas de distribuci√≥n de Pareto y concentraci√≥n de Gini.
    
    Principio de Pareto: Determina qu√© fracci√≥n de items genera el 80% del costo.
    √çndice de Gini: Mide la desigualdad en la distribuci√≥n (0 = uniforme, 1 = concentrada).
    """
    if not requirements:
        return {
            "item_count": 0,
            "total_cost": 0.0,
            "pareto_80_items_ratio": 0.0,
            "gini_index": 0.0,
            "unique_apus": 0
        }
    
    costs = [r.total_cost for r in requirements]
    total_cost = sum(costs)
    n = len(costs)
    
    if total_cost <= 0:
        all_apus = set().union(*(set(r.source_apus) for r in requirements))
        return {
            "item_count": n,
            "total_cost": 0.0,
            "pareto_80_items_ratio": 0.0,
            "gini_index": 0.0,
            "unique_apus": len(all_apus)
        }
    
    # Pareto: ¬øcu√°ntos items acumulan el 80% del costo?
    sorted_costs_desc = sorted(costs, reverse=True)
    threshold_80 = total_cost * 0.8
    cumulative = 0.0
    items_for_80 = 0
    
    for cost in sorted_costs_desc:
        cumulative += cost
        items_for_80 += 1
        if cumulative >= threshold_80:
            break
    
    pareto_ratio = items_for_80 / n
    
    # √çndice de Gini
    sorted_costs_asc = sorted(costs)
    cumulative_weighted = sum((i + 1) * c for i, c in enumerate(sorted_costs_asc))
    gini = (2.0 * cumulative_weighted) / (n * total_cost) - (n + 1.0) / n
    gini = max(0.0, min(1.0, gini))  # Acotado a [0, 1]
    
    # APUs √∫nicos
    all_apus = set().union(*(set(r.source_apus) for r in requirements))
    
    return {
        "item_count": n,
        "total_cost": round(total_cost, 2),
        "pareto_80_items_ratio": round(pareto_ratio, 4),
        "gini_index": round(gini, 4),
        "unique_apus": len(all_apus)
    }

### Propuesta 2

import networkx as nx
import logging
import math
from collections import defaultdict
from typing import Dict, List, Any, Optional, Set, Tuple
from dataclasses import dataclass, field
from datetime import datetime

@dataclass
class MaterialRequirement:
    """
    Representa un requerimiento de material consolidado con validaci√≥n de invariantes.
    """
    id: str
    description: str
    quantity_base: float
    unit: str
    waste_factor: float
    quantity_total: float
    unit_cost: float
    total_cost: float
    source_apus: List[str] = field(default_factory=list)
    
    def __post_init__(self):
        """Validaci√≥n de invariantes despu√©s de la inicializaci√≥n."""
        if self.quantity_base <= 0:
            raise ValueError(f"Cantidad base no positiva para material {self.id}")
        if self.waste_factor < 0:
            raise ValueError(f"Factor de desperdicio negativo para material {self.id}")
        if not math.isfinite(self.total_cost):
            raise ValueError(f"Costo total no finito para material {self.id}")

@dataclass
class BillOfMaterials:
    """
    Lista de materiales final con metadata de generaci√≥n y validaci√≥n topol√≥gica.
    """
    requirements: List[MaterialRequirement]
    total_material_cost: float
    metadata: Dict[str, Any]
    
    def __post_init__(self):
        """Validaci√≥n de coherencia interna."""
        computed_total = sum(req.total_cost for req in self.requirements)
        if not math.isclose(self.total_material_cost, computed_total, rel_tol=1e-9):
            raise ValueError(
                f"Costo total inconsistente: declarado={self.total_material_cost}, "
                f"calculado={computed_total}"
            )

class MatterGenerator:
    """
    Motor de Materializaci√≥n con fundamentos topol√≥gicos robustos.
    Transforma la estructura del grafo en BOM usando un colapso de onda determin√≠stico.
    Implementa un funtor F: Graph ‚Üí Set(Material) preservando la estructura composicional.
    """

    def __init__(self, max_graph_complexity: int = 10000):
        """
        Args:
            max_graph_complexity: L√≠mite de nodos*aristas para prevenir explosi√≥n combinatoria
        """
        self.logger = logging.getLogger(self.__class__.__name__)
        self.max_graph_complexity = max_graph_complexity

    def materialize_project(
        self,
        graph: nx.DiGraph,
        risk_profile: Optional[Dict[str, Any]] = None,
        flux_metrics: Optional[Dict[str, Any]] = None
    ) -> BillOfMaterials:
        """
        Orquesta la transformaci√≥n del Grafo en BOM con validaciones robustas.
        
        Teorema: Para un DAG G, existe un √∫nico morfismo F: G ‚Üí BOM que preserva
        la estructura monoidal de composici√≥n de materiales.
        
        Args:
            graph: Grafo topol√≥gico (presumiblemente un DAG de composici√≥n)
            risk_profile: Perfil de riesgo para ajuste de factores
            flux_metrics: M√©tricas de estabilidad/fricci√≥n
            
        Returns:
            BillOfMaterials: La lista de compras consolidada con metadata completa
            
        Raises:
            ValueError: Si el grafo no es un DAG o contiene ciclos prohibidos
            OverflowError: Si la complejidad del grafo excede l√≠mites seguros
        """
        self.logger.info("üåå Iniciando materializaci√≥n del proyecto...")

        # Validaci√≥n de complejidad del grafo
        complexity = graph.number_of_nodes() * graph.number_of_edges()
        if complexity > self.max_graph_complexity:
            raise OverflowError(
                f"Complejidad del grafo ({complexity}) excede el l√≠mite "
                f"({self.max_graph_complexity})"
            )

        # Validaci√≥n de la estructura del grafo (debe ser DAG para composici√≥n)
        if not nx.is_directed_acyclic_graph(graph):
            # Intentamos detectar ciclos espec√≠ficos para mejor diagn√≥stico
            try:
                cycles = list(nx.simple_cycles(graph))
                raise ValueError(
                    f"El grafo contiene {len(cycles)} ciclos. "
                    f"Primer ciclo: {cycles[0] if cycles else 'N/A'}"
                )
            except (nx.NetworkXNoCycle, ValueError):
                raise ValueError(
                    "El grafo debe ser un DAG para garantizar consistencia composicional"
                )

        # 1. Colapso de Onda (Recorrido topol√≥gico del Grafo)
        raw_materials = self._explode_pyramid(graph)
        self.logger.info(f"üß± Materiales brutos extra√≠dos: {len(raw_materials)}")

        if not raw_materials:
            self.logger.warning("‚ö†Ô∏è No se encontraron materiales en el grafo")
            # Retornamos BOM vac√≠o pero v√°lido
            return self._empty_bom(graph, risk_profile, flux_metrics)

        # 2. Aplicaci√≥n de Entrop√≠a (Factores de Seguridad con monoides)
        adjusted_materials = self._apply_entropy_factors(
            raw_materials,
            flux_metrics,
            risk_profile
        )

        # 3. Clustering Sem√°ntico (Agrupaci√≥n con kernel categ√≥rico)
        final_requirements = self._cluster_semantically(adjusted_materials)
        self.logger.info(f"üõí Requerimientos consolidados: {len(final_requirements)}")

        # 4. C√°lculo de Totales con validaci√≥n num√©rica
        total_cost = self._compute_total_cost(final_requirements)

        # 5. Generaci√≥n de metadata con invariantes topol√≥gicos
        metadata = self._generate_metadata(
            graph, risk_profile, flux_metrics, final_requirements
        )

        return BillOfMaterials(
            requirements=final_requirements,
            total_material_cost=total_cost,
            metadata=metadata
        )

    def _explode_pyramid(self, graph: nx.DiGraph) -> List[Dict[str, Any]]:
        """
        Recorre el grafo desde ra√≠ces usando teor√≠a de categor√≠as para propagaci√≥n.
        
        Implementa un funtor F: Graph ‚Üí Set(Material) donde:
        - Objetos: Nodos del grafo
        - Morfismos: Aristas con cantidades
        - F(nodo): Material terminal (si es INSUMO) o composici√≥n recursiva
        
        Teorema: Para cada nodo ra√≠z r, el camino √∫nico a un nodo terminal t
        define una composici√≥n monoidal de cantidades.
        
        Returns:
            Lista de materiales brutos con trazabilidad completa
        """
        materials = []

        # Identificaci√≥n de ra√≠ces: nodos con degree de entrada 0
        root_nodes = [node for node, in_degree in graph.in_degree() if in_degree == 0]

        if not root_nodes:
            self.logger.error("‚ùå Grafo no tiene ra√≠ces (nodos sin predecesores)")
            return materials

        self.logger.debug(f"Encontradas {len(root_nodes)} ra√≠ces: {root_nodes}")

        # Usamos BFS con queue para procesamiento en niveles (topol√≥gico natural)
        # Estructura: (nodo, cantidad_acumulada, camino, padre_apu, profundidad)
        from collections import deque
        queue = deque()
        for root in root_nodes:
            queue.append((root, 1.0, [], None, 0))

        # Track de visitas por camino para prevenir ciclos en DAGs malformados
        visited_in_path = set()
        MAX_DEPTH = len(graph.nodes())  # Profundidad m√°xima razonable

        while queue:
            current_node, current_qty, current_path, parent_apu, depth = queue.popleft()
            
            # Validaci√≥n de profundidad para prevenir recursi√≥n infinita
            if depth > MAX_DEPTH:
                self.logger.error(f"Profundidad excesiva en nodo {current_node}")
                continue

            # Verificaci√≥n de ciclo en el camino actual
            if current_node in current_path:
                path_key = tuple(current_path + [current_node])
                if path_key not in visited_in_path:
                    self.logger.warning(f"Ciclo detectado en camino: {path_key}")
                    visited_in_path.add(path_key)
                continue

            node_data = graph.nodes[current_node]
            node_type = node_data.get("type", "UNDEFINED")
            new_path = current_path + [current_node]
            new_depth = depth + 1

            # Si es nodo terminal (INSUMO), registramos material
            if node_type == "INSUMO":
                # Validaci√≥n exhaustiva de atributos requeridos
                description = node_data.get("description", "")
                if not description:
                    description = node_data.get("name", str(current_node))

                unit = node_data.get("unit", "UND")
                unit_cost = float(node_data.get("unit_cost", 0.0))
                
                # Validaci√≥n de valores num√©ricos
                if unit_cost < 0:
                    self.logger.warning(f"Costo unitario negativo en {current_node}: {unit_cost}")
                    unit_cost = 0.0

                materials.append({
                    "id": str(current_node),
                    "description": description,
                    "base_qty": current_qty,
                    "unit_cost": unit_cost,
                    "source_apu": parent_apu or "ROOT",
                    "unit": unit,
                    "node_data": node_data.copy(),
                    "composition_path": new_path,
                    "depth": depth
                })
                continue

            # Para nodos no terminales, expandimos hijos
            successors = list(graph.successors(current_node))
            
            # Si no hay sucesores y no es INSUMO, es un nodo intermedio inv√°lido
            if not successors and node_type != "INSUMO":
                self.logger.warning(
                    f"Nodo no-terminal sin sucesores: {current_node} (tipo: {node_type})"
                )
                continue

            # Expandimos hijos con propagaci√≥n de cantidades
            for successor in successors:
                edge_key = (current_node, successor)
                
                if not graph.has_edge(current_node, successor):
                    self.logger.error(f"Arista no encontrada: {edge_key}")
                    continue

                edge_data = graph.edges[edge_key]
                edge_qty = float(edge_data.get("quantity", 1.0))

                # Validaci√≥n: cantidad debe ser positiva
                if edge_qty <= 0:
                    self.logger.warning(
                        f"Cantidad no positiva en arista {edge_key}: {edge_qty}. "
                        f"Usando 1.0 como valor por defecto."
                    )
                    edge_qty = 1.0

                # Si el nodo actual es APU, lo registramos como padre para materiales hijos
                next_parent = current_node if node_type == "APU" else parent_apu

                # Calculamos nueva cantidad (composici√≥n de morfismos)
                # Usamos multiplicaci√≥n precisa para evitar drift num√©rico
                new_qty = current_qty * edge_qty

                # Validaci√≥n de overflow num√©rico
                if new_qty > 1e100 or not math.isfinite(new_qty):
                    self.logger.error(
                        f"Desbordamiento num√©rico en nodo {successor}: {new_qty}"
                    )
                    new_qty = current_qty  # Fallback al valor anterior

                queue.append((successor, new_qty, new_path, next_parent, new_depth))

        return materials

    def _apply_entropy_factors(
        self,
        raw_materials: List[Dict[str, Any]],
        flux_metrics: Optional[Dict[str, Any]],
        risk_profile: Optional[Dict[str, Any]] = None
    ) -> List[Dict[str, Any]]:
        """
        Aplica factores de entrop√≠a usando un monoide de transformaci√≥n.
        
        Definimos el monoide (M, √ó) donde:
        - M = ‚Ñù‚Å∫ (n√∫meros reales positivos)
        - Operaci√≥n: multiplicaci√≥n
        - Elemento identidad: 1.0
        
        Cada factor es un morfismo f: Material ‚Üí Material que escala la cantidad.
        La composici√≥n de factores es conmutativa: f ‚àò g = g ‚àò f.
        
        Returns:
            Materiales con factores aplicados y trazabilidad de transformaciones
        """
        # Grupo de factores base con elemento identidad 1.0
        base_factor = 1.0  # Elemento neutro del grupo
        
        # Registro de factores aplicados para trazabilidad
        factor_log = {
            "base_factor": base_factor,
            "flux_adjustments": [],
            "risk_adjustment": None,
            "material_adjustments": {}
        }

        # Aplicamos factores de flux_metrics (si existen)
        if flux_metrics:
            # Factor de saturaci√≥n (fricci√≥n del sistema)
            saturation = flux_metrics.get("avg_saturation", 0.0)
            if saturation > 0.8:
                adjustment = 1.05  # +5% por saturaci√≥n
                base_factor *= adjustment
                factor_log["flux_adjustments"].append(
                    f"saturation_{saturation:.2f}:x{adjustment}"
                )

            # Factor de estabilidad estructural
            stability = flux_metrics.get("pyramid_stability", 1.0)
            if stability < 1.0:
                adjustment = 1.03  # +3% por inestabilidad
                base_factor *= adjustment
                factor_log["flux_adjustments"].append(
                    f"stability_{stability:.2f}:x{adjustment}"
                )

        # Aplicamos factores de riesgo (si existen)
        if risk_profile:
            risk_level = risk_profile.get("level", "MEDIUM")
            risk_multipliers = {
                "LOW": 1.01,    # +1%
                "MEDIUM": 1.03,  # +3%
                "HIGH": 1.07,    # +7%
                "CRITICAL": 1.15 # +15%
            }
            adjustment = risk_multipliers.get(risk_level, 1.03)
            base_factor *= adjustment
            factor_log["risk_adjustment"] = f"{risk_level}:x{adjustment}"

        # Factor de material espec√≠fico (basado en propiedades categ√≥ricas del material)
        processed_materials = []
        material_categories = set()

        for mat in raw_materials:
            # Clonamos el material para no modificar el original
            processed = mat.copy()
            
            # Determinamos la categor√≠a del material
            material_type = mat.get("node_data", {}).get("material_category", "GENERIC")
            material_categories.add(material_type)
            
            # Mapeo categ√≥rico de factores espec√≠ficos
            type_multipliers = {
                "FRAGILE": 1.02,      # +2% por fragilidad
                "PERISHABLE": 1.04,    # +4% por caducidad
                "HAZARDOUS": 1.06,     # +6% por manejo especial
                "PRECISION": 1.03,     # +3% por tolerancias estrechas
                "BULKY": 1.02,         # +2% por volumen
                "GENERIC": 1.0         # Sin ajuste
            }
            
            specific_factor = type_multipliers.get(material_type, 1.0)
            
            # Registramos el ajuste espec√≠fico
            if material_type not in factor_log["material_adjustments"]:
                factor_log["material_adjustments"][material_type] = specific_factor

            # Composici√≥n de factores: f_total = f_base * f_espec√≠fico
            # Esto forma un morfismo compuesto en la categor√≠a de transformaciones
            total_multiplier = base_factor * specific_factor
            
            # Validaci√≥n: el multiplicador debe ser positivo
            if total_multiplier <= 0:
                self.logger.error(
                    f"Multiplicador no positivo para material {mat['id']}: "
                    f"{total_multiplier}. Usando 1.0"
                )
                total_multiplier = 1.0

            total_waste_factor = total_multiplier - 1.0
            
            # C√°lculo de cantidad total con validaci√≥n
            base_qty = mat["base_qty"]
            total_qty = base_qty * total_multiplier
            
            if not math.isfinite(total_qty):
                self.logger.error(
                    f"Cantidad total no finita para material {mat['id']}. "
                    f"Usando cantidad base."
                )
                total_qty = base_qty
                total_waste_factor = 0.0

            processed["waste_factor"] = total_waste_factor
            processed["total_qty"] = total_qty
            
            # Preservamos los factores aplicados para trazabilidad
            processed["applied_factors"] = {
                "base": base_factor,
                "specific": specific_factor,
                "material_type": material_type,
                "total_multiplier": total_multiplier,
                "factor_log": factor_log.copy()
            }

            processed_materials.append(processed)

        self.logger.debug(
            f"Aplicados factores a {len(processed_materials)} materiales. "
            f"Categor√≠as encontradas: {material_categories}"
        )

        return processed_materials

    def _cluster_semantically(
        self, 
        materials: List[Dict[str, Any]]
    ) -> List[MaterialRequirement]:
        """
        Agrupa materiales usando un kernel de equivalencia categ√≥rica.
        
        Definimos la relaci√≥n de equivalencia ‚àº donde:
        m1 ‚àº m2 ‚áî (id(m1) = id(m2)) ‚àß (unit(m1) = unit(m2))
        
        El espacio cociente Material/‚àº forma una categor√≠a donde los objetos
        son clases de equivalencia y los morfismos son sumas de cantidades.
        
        Returns:
            Lista de requerimientos de material consolidados
        """
        # Diccionario de agrupaci√≥n: clave -> datos agregados
        clustered = defaultdict(lambda: {
            "id": None,
            "description": None,
            "quantity_base": 0.0,
            "quantity_total": 0.0,
            "unit_cost_samples": [],
            "source_apus": set(),
            "unit": None,
            "composition_paths": [],
            "applied_factors": []
        })
        
        # Contadores para estad√≠sticas
        merge_count = 0
        
        for mat in materials:
            # Clave can√≥nica: tupla (id, unidad) para agrupaci√≥n fuerte
            canonical_key = (mat["id"], mat.get("unit", "UND"))
            data = clustered[canonical_key]
            
            # Inicializaci√≥n en primera ocurrencia
            if data["id"] is None:
                data["id"] = mat["id"]
                data["description"] = mat["description"]
                data["unit"] = mat.get("unit", "UND")
            
            # Validaci√≥n de consistencia de descripci√≥n
            elif data["description"] != mat["description"]:
                self.logger.warning(
                    f"Descripci√≥n inconsistente para material {mat['id']}: "
                    f"'{data['description']}' vs '{mat['description']}'. "
                    f"Manteniendo la primera."
                )
            
            # Acumulaci√≥n de cantidades (operaci√≥n del monoide aditivo)
            # Validamos que las cantidades sean finitas
            base_qty = mat["base_qty"]
            total_qty = mat["total_qty"]
            
            if not math.isfinite(base_qty):
                self.logger.error(f"Cantidad base no finita para material {mat['id']}")
                base_qty = 0.0
            if not math.isfinite(total_qty):
                self.logger.error(f"Cantidad total no finita para material {mat['id']}")
                total_qty = 0.0
            
            data["quantity_base"] += base_qty
            data["quantity_total"] += total_qty
            
            # Validaci√≥n de overflow en acumulaci√≥n
            if data["quantity_base"] > 1e100 or data["quantity_total"] > 1e100:
                raise OverflowError(
                    f"Overflow en acumulaci√≥n de cantidades para material {mat['id']}"
                )
            
            # Acumulaci√≥n de APUs de origen
            data["source_apus"].add(mat["source_apu"])
            
            # Muestras de costo unitario para an√°lisis estad√≠stico
            unit_cost = mat["unit_cost"]
            if math.isfinite(unit_cost) and unit_cost >= 0:
                data["unit_cost_samples"].append(unit_cost)
            else:
                self.logger.warning(
                    f"Costo unitario inv√°lido para material {mat['id']}: {unit_cost}"
                )
            
            # Preservamos algunos paths para debugging (limitado)
            if len(data["composition_paths"]) < 5:
                data["composition_paths"].append(mat.get("composition_path", []))
            
            # Preservamos factores aplicados para trazabilidad
            data["applied_factors"].append(mat.get("applied_factors", {}))
            
            # Estad√≠sticas de merging
            if len(data["unit_cost_samples"]) > 1:
                merge_count += 1
        
        self.logger.debug(
            f"Clusterizaci√≥n completada: {len(clustered)} grupos √∫nicos, "
            f"{merge_count} merges realizados"
        )
        
        # Procesamiento post-agrupaci√≥n
        requirements = []
        skipped_materials = 0
        
        for (mat_id, unit), data in clustered.items():
            # Validaci√≥n: cantidad base debe ser positiva
            if data["quantity_base"] <= 0:
                self.logger.warning(
                    f"Material {mat_id} tiene cantidad base no positiva: "
                    f"{data['quantity_base']}. Saltando."
                )
                skipped_materials += 1
                continue
            
            # Recalculamos waste_factor a partir de cantidades agregadas
            # Esto es m√°s preciso que promediar factores individuales
            try:
                total_waste = (data["quantity_total"] / data["quantity_base"]) - 1.0
                
                # Validaci√≥n de waste_factor
                if total_waste < -0.5 or total_waste > 10.0:  # L√≠mites razonables
                    self.logger.warning(
                        f"Factor de desperdicio an√≥malo para {mat_id}: {total_waste:.3f}"
                    )
                    # Ajustamos a un valor razonable
                    total_waste = max(0.0, min(total_waste, 2.0))
            except ZeroDivisionError:
                self.logger.error(f"Divisi√≥n por cero al calcular waste_factor para {mat_id}")
                total_waste = 0.0
            
            # Calculamos costo unitario representativo
            # Usamos la mediana para ser robustos a outliers
            cost_samples = data["unit_cost_samples"]
            representative_cost = 0.0
            
            if cost_samples:
                try:
                    # Filtramos valores no finitos
                    valid_costs = [c for c in cost_samples if math.isfinite(c) and c >= 0]
                    
                    if not valid_costs:
                        self.logger.warning(f"Sin costos v√°lidos para {mat_id}")
                        representative_cost = 0.0
                    else:
                        # Calculamos mediana de costos v√°lidos
                        sorted_costs = sorted(valid_costs)
                        mid = len(sorted_costs) // 2
                        if len(sorted_costs) % 2 == 0:
                            representative_cost = (sorted_costs[mid-1] + sorted_costs[mid]) / 2
                        else:
                            representative_cost = sorted_costs[mid]
                        
                        # Tambi√©n calculamos desviaci√≥n para logging
                        if len(valid_costs) > 1:
                            mean_cost = sum(valid_costs) / len(valid_costs)
                            variance = sum((c - mean_cost) ** 2 for c in valid_costs) / len(valid_costs)
                            std_dev = math.sqrt(variance)
                            
                            if std_dev / mean_cost > 0.1:  # >10% de variaci√≥n relativa
                                self.logger.info(
                                    f"Alta variaci√≥n en costos para {mat_id}: "
                                    f"media={mean_cost:.2f}, œÉ={std_dev:.2f} "
                                    f"(CV={(std_dev/mean_cost)*100:.1f}%)"
                                )
                except Exception as e:
                    self.logger.error(f"Error calculando costo para {mat_id}: {e}")
                    representative_cost = 0.0
            else:
                self.logger.warning(f"Sin muestras de costo para {mat_id}")
                representative_cost = 0.0
            
            # Validamos el costo unitario
            if representative_cost < 0 or not math.isfinite(representative_cost):
                self.logger.warning(
                    f"Costo unitario inv√°lido para {mat_id}: {representative_cost}. "
                    f"Ajustando a 0.0"
                )
                representative_cost = 0.0
            
            # Calculamos costo total con validaci√≥n
            total_cost = data["quantity_total"] * representative_cost
            if not math.isfinite(total_cost) or total_cost < 0:
                self.logger.error(
                    f"Costo total inv√°lido para {mat_id}: {total_cost}. "
                    f"Ajustando a 0.0"
                )
                total_cost = 0.0
            
            # Creamos el requerimiento con validaci√≥n autom√°tica v√≠a __post_init__
            try:
                req = MaterialRequirement(
                    id=data["id"],
                    description=data["description"] or f"Material {data['id']}",
                    quantity_base=data["quantity_base"],
                    unit=data["unit"],
                    waste_factor=total_waste,
                    quantity_total=data["quantity_total"],
                    unit_cost=representative_cost,
                    total_cost=total_cost,
                    source_apus=sorted(data["source_apus"])  # Ordenamos para consistencia
                )
                requirements.append(req)
            except ValueError as e:
                self.logger.error(f"Error creando requerimiento para {mat_id}: {e}")
                skipped_materials += 1
        
        # Ordenamiento por criterio de Pareto (costo total descendente)
        # Tambi√©n consideramos n√∫mero de APUs como desempate (m√°s APUs = m√°s cr√≠tico)
        requirements.sort(
            key=lambda x: (x.total_cost, -len(x.source_apus), x.id),
            reverse=True
        )
        
        if skipped_materials > 0:
            self.logger.warning(f"Se omitieron {skipped_materials} materiales inv√°lidos")
        
        return requirements

    def _compute_total_cost(self, requirements: List[MaterialRequirement]) -> float:
        """
        Calcula el costo total con validaci√≥n num√©rica robusta.
        
        Implementa suma en doble precisi√≥n con verificaci√≥n de overflow
        y compensaci√≥n de Kahan para minimizar errores de punto flotante.
        
        Teorema: La suma de costos finitos es conmutativa y asociativa.
        
        Returns:
            Costo total redondeado a 2 decimales para representaci√≥n monetaria
        """
        # Algoritmo de suma de Kahan para alta precisi√≥n
        total = 0.0
        compensation = 0.0
        
        for req in requirements:
            # Validaci√≥n: costo debe ser finito y no negativo
            if not math.isfinite(req.total_cost):
                self.logger.error(f"Costo no finito en material {req.id}: {req.total_cost}")
                continue
            
            if req.total_cost < 0:
                self.logger.warning(f"Costo negativo en material {req.id}: {req.total_cost}")
                # Consideramos el valor absoluto para el total
                y = -req.total_cost - compensation
            else:
                y = req.total_cost - compensation
            
            t = total + y
            compensation = (t - total) - y
            total = t
            
            # Verificaci√≥n de overflow
            if math.isinf(total):
                raise OverflowError(
                    f"Overflow en c√°lculo de costo total al procesar {req.id}"
                )
        
        # Redondeo a 2 decimales para representaci√≥n monetaria
        # Usamos round-half-even (banker's rounding) para minimizar sesgo
        total_rounded = round(total, 2)
        
        # Validaci√≥n de que el redondeo no introduce error significativo
        rounding_error = abs(total - total_rounded)
        if rounding_error > 0.01:  # M√°s de 1 c√©ntimo de error
            self.logger.warning(
                f"Error de redondeo significativo: {rounding_error:.6f}. "
                f"Original: {total:.6f}, Redondeado: {total_rounded:.6f}"
            )
        
        return total_rounded

    def _generate_metadata(
        self,
        graph: nx.DiGraph,
        risk_profile: Optional[Dict[str, Any]],
        flux_metrics: Optional[Dict[str, Any]],
        requirements: List[MaterialRequirement]
    ) -> Dict[str, Any]:
        """
        Genera metadata con invariantes topol√≥gicos y m√©tricas de calidad.
        
        Calcula invariantes homol√≥gicos del grafo (n√∫mero de Betti 0 = componentes conexas)
        y m√©tricas de distribuci√≥n de Pareto para an√°lisis de costos.
        
        Returns:
            Diccionario de metadata con estructura jer√°rquica
        """
        metadata = {
            "topological_invariants": {},
            "graph_metrics": {},
            "cost_analysis": {},
            "risk_analysis": {},
            "generation_info": {}
        }
        
        # 1. Invariantes topol√≥gicos del grafo
        try:
            # N√∫mero de componentes d√©bilmente conexas (Œ≤‚ÇÄ para grafo no dirigido)
            undirected_graph = graph.to_undirected()
            num_components = nx.number_connected_components(undirected_graph)
            
            # Longitud del camino m√°s largo en el DAG
            if nx.is_directed_acyclic_graph(graph):
                try:
                    longest_path_length = nx.dag_longest_path_length(graph)
                    longest_path_nodes = nx.dag_longest_path(graph)
                except (nx.NetworkXError, ValueError):
                    longest_path_length = None
                    longest_path_nodes = []
            else:
                longest_path_length = None
                longest_path_nodes = []
            
            metadata["topological_invariants"].update({
                "weakly_connected_components": num_components,
                "is_dag": nx.is_directed_acyclic_graph(graph),
                "longest_path_length": longest_path_length,
                "longest_path_sample": longest_path_nodes[:5] if longest_path_nodes else []
            })
        except Exception as e:
            self.logger.error(f"Error calculando invariantes topol√≥gicos: {e}")
            metadata["topological_invariants"]["error"] = str(e)
        
        # 2. M√©tricas b√°sicas del grafo
        try:
            root_nodes = [node for node, in_degree in graph.in_degree() if in_degree == 0]
            leaf_nodes = [node for node, out_degree in graph.out_degree() if out_degree == 0]
            
            # Distribuci√≥n de grados
            in_degrees = [d for _, d in graph.in_degree()]
            out_degrees = [d for _, d in graph.out_degree()]
            
            metadata["graph_metrics"].update({
                "node_count": graph.number_of_nodes(),
                "edge_count": graph.number_of_edges(),
                "density": nx.density(graph),
                "root_count": len(root_nodes),
                "leaf_count": len(leaf_nodes),
                "avg_in_degree": sum(in_degrees) / len(in_degrees) if in_degrees else 0,
                "avg_out_degree": sum(out_degrees) / len(out_degrees) if out_degrees else 0,
                "max_in_degree": max(in_degrees) if in_degrees else 0,
                "max_out_degree": max(out_degrees) if out_degrees else 0,
                "root_sample": root_nodes[:5] if root_nodes else [],
                "leaf_sample": leaf_nodes[:5] if leaf_nodes else []
            })
        except Exception as e:
            self.logger.error(f"Error calculando m√©tricas de grafo: {e}")
            metadata["graph_metrics"]["error"] = str(e)
        
        # 3. An√°lisis de costos y distribuci√≥n de Pareto
        if requirements:
            try:
                costs = [r.total_cost for r in requirements]
                base_quantities = [r.quantity_base for r in requirements]
                total_quantities = [r.quantity_total for r in requirements]
                
                total_cost = sum(costs)
                total_base_qty = sum(base_quantities)
                total_qty = sum(total_quantities)
                
                if total_cost > 0 and costs:
                    # An√°lisis de Pareto (80/20)
                    sorted_costs = sorted(costs, reverse=True)
                    cumulative_cost = 0
                    pareto_index = 0
                    
                    for i, cost in enumerate(sorted_costs):
                        cumulative_cost += cost
                        if cumulative_cost / total_cost >= 0.8:
                            pareto_index = i + 1
                            break
                    
                    pareto_20_percent = pareto_index / len(costs)
                    pareto_items = sorted_costs[:pareto_index] if pareto_index > 0 else []
                    pareto_cost = sum(pareto_items)
                    
                    # M√©tricas de distribuci√≥n
                    if len(costs) > 1:
                        mean_cost = total_cost / len(costs)
                        variance = sum((c - mean_cost) ** 2 for c in costs) / len(costs)
                        std_dev = math.sqrt(variance)
                        cv = (std_dev / mean_cost) * 100 if mean_cost > 0 else 0
                    else:
                        mean_cost = total_cost if costs else 0
                        variance = 0
                        std_dev = 0
                        cv = 0
                    
                    # Waste analysis
                    total_waste = total_qty - total_base_qty
                    avg_waste_factor = (total_waste / total_base_qty) if total_base_qty > 0 else 0
                    
                    metadata["cost_analysis"].update({
                        "total_cost": total_cost,
                        "item_count": len(requirements),
                        "unique_materials": len(set(r.id for r in requirements)),
                        "unique_apus": len(set().union(*[r.source_apus for r in requirements])),
                        "total_base_quantity": total_base_qty,
                        "total_quantity": total_qty,
                        "total_waste": total_waste,
                        "avg_waste_factor": avg_waste_factor,
                        "pareto_analysis": {
                            "pareto_20_percent": pareto_20_percent,
                            "pareto_items_count": pareto_index,
                            "pareto_cost_percentage": (pareto_cost / total_cost) * 100,
                            "pareto_cost_absolute": pareto_cost
                        },
                        "distribution": {
                            "mean_cost": mean_cost,
                            "median_cost": sorted_costs[len(sorted_costs)//2] if sorted_costs else 0,
                            "std_dev_cost": std_dev,
                            "coefficient_variation": cv,
                            "min_cost": min(costs) if costs else 0,
                            "max_cost": max(costs) if costs else 0,
                            "cost_range": max(costs) - min(costs) if costs else 0
                        }
                    })
                else:
                    metadata["cost_analysis"].update({
                        "total_cost": 0.0,
                        "item_count": len(requirements),
                        "error": "Costo total cero o negativo"
                    })
            except Exception as e:
                self.logger.error(f"Error en an√°lisis de costos: {e}")
                metadata["cost_analysis"]["error"] = str(e)
        else:
            metadata["cost_analysis"] = {"empty": True, "item_count": 0}
        
        # 4. An√°lisis de riesgo
        if risk_profile:
            metadata["risk_analysis"] = {
                "profile_applied": True,
                "risk_level": risk_profile.get("level", "UNKNOWN"),
                "adjustment_factor": risk_profile.get("adjustment_factor", 1.0),
                "raw_profile": {k: v for k, v in risk_profile.items() 
                               if not isinstance(v, (dict, list)) or k == "level"}
            }
        else:
            metadata["risk_analysis"] = {"profile_applied": False}
        
        # 5. Informaci√≥n de generaci√≥n
        metadata["generation_info"] = {
            "timestamp": datetime.now().isoformat(),
            "generator_version": "3.0-topology-algebraic",
            "generator_class": self.__class__.__name__,
            "python_version": f"{sys.version_info.major}.{sys.version_info.minor}",
            "networkx_version": nx.__version__
        }
        
        # 6. Flux metrics (si existen)
        if flux_metrics:
            metadata["flux_analysis"] = {
                "metrics_applied": True,
                "key_metrics": {k: v for k, v in flux_metrics.items() 
                               if not isinstance(v, (dict, list)) or k in ["avg_saturation", "pyramid_stability"]}
            }
        else:
            metadata["flux_analysis"] = {"metrics_applied": False}
        
        return metadata

    def _empty_bom(
        self,
        graph: nx.DiGraph,
        risk_profile: Optional[Dict[str, Any]],
        flux_metrics: Optional[Dict[str, Any]]
    ) -> BillOfMaterials:
        """
        Crea un BOM vac√≠o pero v√°lido cuando no hay materiales.
        
        Esto preserva la estructura del contrato incluso en casos degenerados.
        """
        metadata = self._generate_metadata(graph, risk_profile, flux_metrics, [])
        
        return BillOfMaterials(
            requirements=[],
            total_material_cost=0.0,
            metadata=metadata
        )

# Nota: Se requiere importar sys al inicio del m√≥dulo para la metadata
import sys