"""
Este componente implementa la "Matriz de Transformaci√≥n de Valor" ($M_P$). No procesa
datos directamente, sino que orquesta la secuencia de activaci√≥n de los Sabios
(Guardian -> Alquimista -> Arquitecto -> Or√°culo) asegurando la integridad del
"Vector de Estado" del proyecto.

Arquitectura de Ejecuci√≥n:
--------------------------
1. M√°quina de Estados con Persistencia:
   Implementa una ejecuci√≥n paso a paso donde el contexto se serializa (Pickle/Redis)
   entre etapas. Esto permite pausar, reanudar y auditar el estado del proyecto
   en cualquier punto del flujo (Ingesta, Fusi√≥n, C√°lculo, Auditor√≠a).

2. Protocolo de Caja de Cristal (Glass Box):
   Garantiza que cada transformaci√≥n de datos deje una traza de auditor√≠a inmutable.
   Gestiona la generaci√≥n de "Data Products" intermedios y finales, asegurando que
   el resultado sea matem√°ticamente reproducible.

3. Matriz de Interacci√≥n Central (MIC):
   Act√∫a como el despachador que conecta los vectores de transformaci√≥n (herramientas)
   con el estado del sistema, permitiendo al Agente Aut√≥nomo intervenir quir√∫rgicamente
   en el flujo de procesamiento.

4. Gesti√≥n de Dependencias:
   Coordina la carga de perfiles y umbrales (ProcessingThresholds) para asegurar
   que todos los agentes operen bajo las mismas reglas de negocio.
"""

import datetime
import enum
import hashlib
import logging
import os
import pickle
import sys
import uuid
from abc import ABC, abstractmethod
from dataclasses import asdict, dataclass, field
from pathlib import Path
from typing import Any, Dict, List, Optional, Tuple, Type

import numpy as np
import pandas as pd
from scipy import sparse
from sklearn.decomposition import PCA, TruncatedSVD
from sklearn.preprocessing import OneHotEncoder

from app.classifiers.apu_classifier import APUClassifier
from app.constants import ColumnNames, InsumoType
from app.flux_condenser import CondenserConfig, DataFluxCondenser
from app.matter_generator import MatterGenerator
from app.schemas import Stratum
from app.telemetry import TelemetryContext
from app.telemetry_narrative import TelemetryNarrator
from app.validators import DataFrameValidator

from .data_loader import load_data
from .data_validator import validate_and_clean_data
from .utils import (
    clean_apu_code,
    find_and_rename_columns,
    normalize_text_series,
    sanitize_for_json,
)

# Configuraci√≥n expl√≠cita para debug
logger = logging.getLogger(__name__)
logger.setLevel(logging.INFO)

# Asegurar que tenga un handler si no tiene
if not logger.handlers:
    handler = logging.StreamHandler(sys.stdout)
    handler.setFormatter(
        logging.Formatter("%(asctime)s - %(name)s - %(levelname)s - %(message)s")
    )
    logger.addHandler(handler)


# ==================== CONSTANTES Y CLASES AUXILIARES ====================


class InsumoTypes:
    """Tipos de insumos reconocidos por el sistema."""

    SUMINISTRO = "SUMINISTRO"
    MANO_DE_OBRA = "MANO_DE_OBRA"
    EQUIPO = "EQUIPO"
    TRANSPORTE = "TRANSPORTE"
    OTRO = "OTRO"


class APUTypes:
    """Clasificaci√≥n de tipos de APU."""

    INSTALACION = "Instalaci√≥n"
    SUMINISTRO = "Suministro"
    SUMINISTRO_PREFABRICADO = "Suministro (Pre-fabricado)"
    OBRA_COMPLETA = "Obra Completa"
    INDEFINIDO = "Indefinido"


@dataclass
class ProcessingThresholds:
    """
    Umbrales configurables para la validaci√≥n y limpieza de datos.

    Attributes:
        outlier_std_multiplier (float): Desviaciones est√°ndar para detectar outliers.
        max_quantity (float): Cantidad m√°xima permitida.
        max_cost_per_item (float): Costo unitario m√°ximo permitido.
        max_total_cost (float): Costo total m√°ximo permitido.
        instalacion_mo_threshold (float): Umbral % MO para considerar Instalaci√≥n.
        suministro_mat_threshold (float): Umbral % Materiales para Suministro.
        max_header_search_rows (int): Filas a buscar para detectar encabezado.
    """

    outlier_std_multiplier: float = 3.0
    max_quantity: float = 1e6
    max_cost_per_item: float = 1e9
    max_total_cost: float = 1e11
    instalacion_mo_threshold: float = 75.0
    suministro_mat_threshold: float = 75.0
    suministro_mo_max: float = 15.0
    prefabricado_mat_threshold: float = 65.0
    prefabricado_mo_min: float = 15.0
    max_header_search_rows: int = 10


# ==================== CLASE BASE REFACTORIZADA ====================


class ProcessingStep(ABC):
    """Clase base abstracta para un paso del pipeline de procesamiento."""

    @abstractmethod
    def execute(self, context: dict, telemetry: TelemetryContext) -> dict:
        """
        Ejecuta la l√≥gica del paso.

        Args:
            context (dict): Diccionario con el estado actual del procesamiento.
            telemetry (TelemetryContext): Contexto de telemetr√≠a para m√©tricas.

        Returns:
            dict: El contexto actualizado (puede ser el mismo objeto modificado).
        """
        pass

# ==================== ESTRUCTURAS ALGEBRAICAS (MIC) ====================

@dataclass(frozen=True)
class BasisVector:
    """
    Representa un vector base unitario e_i en el espacio de operaciones.

    Matem√°ticamente:
    e_i = [0, ..., 1, ..., 0]^T

    Propiedades:
    - Norma L2: ||e_i|| = 1 (Unitario)
    - Estrato: Define el subespacio V_k al que pertenece (Filtraci√≥n)
    """
    index: int
    label: str  # Identificador sem√°ntico (ej: "load_data")
    operator_class: Type[ProcessingStep] # La transformaci√≥n lineal T
    stratum: Stratum


class LinearInteractionMatrix:
    """
    Implementaci√≥n algebraica rigurosa de la MIC como Operador Diagonal.

    Refinamiento: Incorpora m√©trica de Killing-Cartan para verificaci√≥n
    de ortogonalidad y c√≥mputo espectral basado en teor√≠a de representaciones.
    """

    def __init__(self):
        self._basis: Dict[str, BasisVector] = {}
        self._dimension = 0
        self._gram_matrix: Optional[np.ndarray] = None
        self._orthonormal_basis_computed = False
        self._killing_form_cache: Optional[np.ndarray] = None

    def get_rank(self) -> int:
        """Retorna el rango de la matriz (Dimensi√≥n del espacio imagen)."""
        if self._gram_matrix is not None:
            return int(np.linalg.matrix_rank(self._gram_matrix))
        return self._dimension

    def add_basis_vector(
        self,
        label: str,
        step_class: Type[ProcessingStep],
        stratum: Stratum
    ):
        """
        Expande el espacio vectorial con verificaci√≥n de independencia lineal.

        Invariante: ‚àÄe_i, e_j ‚àà B, i ‚â† j ‚Üí <e_i, e_j>_K = 0
        donde <¬∑,¬∑>_K es la forma de Killing.
        """
        if not label or not isinstance(label, str):
            raise ValueError("Label debe ser una cadena no vac√≠a")

        if label in self._basis:
            raise ValueError(
                f"Dependencia Lineal: '{label}' viola independencia. "
                f"Base actual: {list(self._basis.keys())}"
            )

        if not (isinstance(step_class, type) and issubclass(step_class, ProcessingStep)):
            raise TypeError(
                f"El operador {step_class} no es lineal "
                f"(debe ser subclase de ProcessingStep)"
            )

        vector = BasisVector(
            index=self._dimension,
            label=label,
            operator_class=step_class,
            stratum=stratum
        )

        self._verify_orthogonality_killing(vector)

        self._basis[label] = vector
        self._dimension += 1
        self._invalidate_caches()

        logger.debug(
            f"üìê Vector base a√±adido: {label} (dim={self._dimension}, "
            f"estrato={stratum.name})"
        )

    def _invalidate_caches(self):
        """Invalida caches tras modificaci√≥n de la base."""
        self._orthonormal_basis_computed = False
        self._gram_matrix = None
        self._killing_form_cache = None

    def _verify_orthogonality_killing(self, new_vector: BasisVector):
        """
        Verifica ortogonalidad usando forma de Killing generalizada.

        La forma de Killing K(X,Y) = Tr(ad_X ‚àò ad_Y) se aproxima aqu√≠
        mediante una m√©trica funcional basada en:
        - Coincidencia de operador (colinealidad directa)
        - Coincidencia de estrato con diferente operador (interferencia)
        - Solapamiento de dominio/codominio funcional
        """
        for existing_label, existing_vector in self._basis.items():
            # Colinealidad directa: mismo operador (relaxed for different strata)
            if existing_vector.operator_class == new_vector.operator_class:
                if existing_vector.stratum == new_vector.stratum:
                    raise ValueError(
                        f"Colinealidad funcional: '{new_vector.label}' usa mismo "
                        f"operador que '{existing_label}' en el mismo estrato"
                    )

            # Verificar interferencia de estrato con an√°lisis de firma
            killing_value = self._compute_killing_pairing(existing_vector, new_vector)

            if abs(killing_value) > 0.95:  # Umbral de cuasi-colinealidad
                raise ValueError(
                    f"Cuasi-colinealidad detectada: K({new_vector.label}, "
                    f"{existing_label}) = {killing_value:.3f}"
                )

    def _compute_killing_pairing(
        self,
        v1: BasisVector,
        v2: BasisVector
    ) -> float:
        """
        Computa el apareamiento de Killing entre dos vectores base.

        Retorna valor en [-1, 1] donde:
        - 0: Ortogonales (independientes)
        - ¬±1: Paralelos (dependientes)
        """
        # Factor por coincidencia de estrato
        stratum_factor = 1.0 if v1.stratum == v2.stratum else 0.3

        # Factor por proximidad de √≠ndice (localidad en la filtraci√≥n)
        index_distance = abs(v1.index - v2.index)
        locality_factor = 1.0 / (1.0 + index_distance)

        # Factor por an√°lisis de firma del operador
        signature_similarity = self._compute_operator_signature_similarity(
            v1.operator_class, v2.operator_class
        )

        killing_value = stratum_factor * locality_factor * signature_similarity
        return np.clip(killing_value, -1.0, 1.0)

    def _compute_operator_signature_similarity(
        self,
        op1: Type[ProcessingStep],
        op2: Type[ProcessingStep]
    ) -> float:
        """
        Calcula similitud de firma entre operadores usando introspecci√≥n.
        """
        try:
            import inspect

            sig1 = inspect.signature(op1.execute)
            sig2 = inspect.signature(op2.execute)

            params1 = set(sig1.parameters.keys()) - {'self'}
            params2 = set(sig2.parameters.keys()) - {'self'}

            if not params1 or not params2:
                return 0.0

            intersection = len(params1 & params2)
            union = len(params1 | params2)

            base_sim = intersection / union if union > 0 else 0.0

            # Penalize if classes are different but signatures same
            if op1 != op2 and base_sim == 1.0:
                return 0.99

            return base_sim

        except Exception:
            return 0.0

    def project_intent(self, intent_label: str) -> BasisVector:
        """
        Proyecci√≥n ortogonal del vector de intenci√≥n sobre la base E.

        Implementa: proj_E(q) = Œ£_i <q, e_i> e_i / ||e_i||¬≤
        En espacio discreto se reduce a b√∫squeda exacta con validaci√≥n.
        """
        if not intent_label:
            raise ValueError("Vector de intenci√≥n vac√≠o (norma cero)")

        vector = self._basis.get(intent_label)
        if vector is None:
            available = list(self._basis.keys())
            # Intentar match parcial para sugerencias
            suggestions = [k for k in available if intent_label.lower() in k.lower()]

            raise ValueError(
                f"Vector '{intent_label}' ‚àà Ker(œÄ) (n√∫cleo de proyecci√≥n). "
                f"Base disponible: {available}. "
                f"Sugerencias: {suggestions if suggestions else 'ninguna'}"
            )

        if not self._orthonormal_basis_computed:
            self._orthonormalize_basis()

        return vector

    def _orthonormalize_basis(self):
        """
        Aplica Gram-Schmidt modificado para estabilidad num√©rica.

        Algoritmo: MGS (Modified Gram-Schmidt)
        Para k = 1, ..., n:
            q_k = v_k
            Para j = 1, ..., k-1:
                q_k = q_k - <q_k, q_j> q_j
            q_k = q_k / ||q_k||
        """
        if self._orthonormal_basis_computed:
            return

        n = self._dimension
        if n == 0:
            self._gram_matrix = np.array([[]])
            self._orthonormal_basis_computed = True
            return

        # Construir matriz de Gram usando forma de Killing
        self._gram_matrix = np.eye(n)

        vectors = list(self._basis.values())
        for i in range(n):
            for j in range(i + 1, n):
                killing_ij = self._compute_killing_pairing(vectors[i], vectors[j])
                self._gram_matrix[i, j] = killing_ij
                self._gram_matrix[j, i] = killing_ij

        # Verificar definici√≥n positiva (espacio m√©trico v√°lido)
        try:
            eigenvalues = np.linalg.eigvalsh(self._gram_matrix)
            if np.any(eigenvalues < -1e-10):
                logger.warning(
                    f"‚ö†Ô∏è Matriz de Gram no definida positiva. "
                    f"Eigenvalores negativos: {eigenvalues[eigenvalues < 0]}"
                )
                # Regularizaci√≥n de Tikhonov
                self._gram_matrix += np.eye(n) * abs(min(eigenvalues)) * 1.1
        except np.linalg.LinAlgError:
            logger.warning("‚ö†Ô∏è Error en descomposici√≥n espectral, usando identidad")
            self._gram_matrix = np.eye(n)

        self._orthonormal_basis_computed = True
        logger.debug(f"üßÆ Gram-Schmidt completado. Condici√≥n: {np.linalg.cond(self._gram_matrix):.2f}")

    def get_spectrum(self) -> Dict[str, float]:
        """
        Calcula el espectro del operador basado en la matriz de Gram.

        Los valores propios indican la 'inercia' de cada direcci√≥n base.
        """
        if not self._orthonormal_basis_computed:
            self._orthonormalize_basis()

        if self._gram_matrix is None or self._gram_matrix.size == 0:
            return {}

        try:
            eigenvalues = np.linalg.eigvalsh(self._gram_matrix)
            eigenvalues = np.sort(eigenvalues)[::-1]  # Descendente

            spectrum = {}
            for i, (label, vector) in enumerate(self._basis.items()):
                if i < len(eigenvalues):
                    # Ponderar por nivel de estrato
                    stratum_weight = {
                        Stratum.PHYSICS: 1.0,
                        Stratum.TACTICS: 1.2,
                        Stratum.STRATEGY: 1.5,
                        Stratum.WISDOM: 10.0
                    }.get(vector.stratum, 1.0)

                    spectrum[label] = float(eigenvalues[i]) * stratum_weight
                else:
                    spectrum[label] = 1.0

            return spectrum

        except np.linalg.LinAlgError as e:
            logger.error(f"‚ùå Error computando espectro: {e}")
            return {label: 1.0 for label in self._basis.keys()}

    def get_condition_number(self) -> float:
        """Retorna n√∫mero de condici√≥n de la base (estabilidad num√©rica)."""
        if not self._orthonormal_basis_computed:
            self._orthonormalize_basis()

        if self._gram_matrix is None or self._gram_matrix.size == 0:
            return 1.0

        return float(np.linalg.cond(self._gram_matrix))


class DataValidator:
    """Utilidades para validaci√≥n de DataFrames."""

    @staticmethod
    def validate_dataframe_not_empty(
        df: pd.DataFrame, name: str
    ) -> Tuple[bool, Optional[str]]:
        """Verifica que un DataFrame no est√© vac√≠o."""
        if df is None:
            error_msg = f"DataFrame '{name}' es None"
            logger.error(f"‚ùå {error_msg}")
            return False, error_msg

        if not isinstance(df, pd.DataFrame):
            error_msg = (
                f"'{name}' no es un DataFrame v√°lido, es tipo: {type(df).__name__}"
            )
            logger.error(f"‚ùå {error_msg}")
            return False, error_msg

        if df.empty:
            error_msg = f"DataFrame '{name}' est√° vac√≠o (0 filas)"
            logger.error(f"‚ùå {error_msg}")
            return False, error_msg

        if df.dropna(how="all").empty:
            error_msg = f"DataFrame '{name}' contiene solo valores nulos"
            logger.error(f"‚ùå {error_msg}")
            return False, error_msg

        return True, None

    @staticmethod
    def validate_required_columns(
        df: pd.DataFrame, required_cols: List[str], df_name: str
    ) -> Tuple[bool, Optional[str]]:
        """Verifica que un DataFrame tenga las columnas requeridas."""
        if df is None:
            error_msg = f"DataFrame '{df_name}' es None, no se pueden validar columnas"
            logger.error(f"‚ùå {error_msg}")
            return False, error_msg

        if not isinstance(df, pd.DataFrame):
            error_msg = f"'{df_name}' no es un DataFrame v√°lido"
            logger.error(f"‚ùå {error_msg}")
            return False, error_msg

        if not required_cols:
            logger.warning(f"‚ö†Ô∏è Lista de columnas requeridas vac√≠a para '{df_name}'")
            return True, None

        df_cols_upper = {col.upper().strip(): col for col in df.columns}
        missing_cols = []

        for req_col in required_cols:
            req_col_upper = req_col.upper().strip()
            if req_col not in df.columns and req_col_upper not in df_cols_upper:
                missing_cols.append(req_col)

        if missing_cols:
            available_cols = list(df.columns)[:10]
            error_msg = (
                f"Faltan columnas requeridas en '{df_name}': {missing_cols}. "
                f"Columnas disponibles (primeras 10): {available_cols}"
            )
            logger.error(f"‚ùå {error_msg}")
            return False, error_msg

        return True, None

    @staticmethod
    def detect_and_log_duplicates(
        df: pd.DataFrame, subset_cols: List[str], df_name: str, keep: str = "first"
    ) -> pd.DataFrame:
        """Detecta, loguea y elimina duplicados en un DataFrame."""
        if df is None or not isinstance(df, pd.DataFrame):
            logger.error(
                f"‚ùå DataFrame '{df_name}' inv√°lido para detecci√≥n de duplicados"
            )
            return pd.DataFrame()

        if df.empty:
            logger.debug(
                f"DataFrame '{df_name}' vac√≠o, no hay duplicados que detectar"
            )
            return df

        if not subset_cols:
            logger.warning(
                f"‚ö†Ô∏è No se especificaron columnas para detectar duplicados en '{df_name}'"
            )
            return df

        missing_subset_cols = [col for col in subset_cols if col not in df.columns]
        if missing_subset_cols:
            logger.error(
                f"‚ùå Columnas para duplicados no existen en '{df_name}': {missing_subset_cols}"
            )
            subset_cols = [col for col in subset_cols if col in df.columns]
            if not subset_cols:
                return df

        valid_keep_values = {"first", "last", False}
        if keep not in valid_keep_values:
            logger.warning(f"‚ö†Ô∏è Valor 'keep={keep}' inv√°lido, usando 'first'")
            keep = "first"

        try:
            duplicates = df[df.duplicated(subset=subset_cols, keep=False)]
            if not duplicates.empty:
                unique_dupes = duplicates[subset_cols[0]].unique()
                num_dupes = len(duplicates)
                logger.warning(
                    f"‚ö†Ô∏è Se encontraron {num_dupes} filas duplicadas en '{df_name}' "
                    f"por {subset_cols}. Se conservar√°: '{keep}'"
                )
                dupes_sample = unique_dupes[:10].tolist()
                logger.debug(
                    f"Muestra de valores duplicados en '{df_name}': {dupes_sample}"
                )

                df_clean = df.drop_duplicates(subset=subset_cols, keep=keep)
                rows_removed = len(df) - len(df_clean)
                logger.info(
                    f"‚úÖ Duplicados eliminados en '{df_name}': {rows_removed} filas removidas"
                )
                return df_clean
            return df
        except Exception as e:
            logger.error(f"‚ùå Error detectando duplicados en '{df_name}': {e}")
            return df


class FileValidator:
    """Utilidades para validaci√≥n de existencia de archivos."""

    MIN_FILE_SIZE_BYTES = 10
    VALID_EXTENSIONS = {".csv", ".xlsx", ".xls", ".json"}

    @staticmethod
    def validate_file_exists(
        file_path: str,
        file_type: str,
        check_extension: bool = True,
        min_size: Optional[int] = None,
    ) -> Tuple[bool, Optional[str]]:
        """Verifica que un archivo exista y sea accesible."""
        if not file_path:
            error_msg = f"Ruta de archivo de {file_type} est√° vac√≠a o es None"
            logger.error(f"‚ùå {error_msg}")
            return False, error_msg

        if not isinstance(file_path, (str, Path)):
            error_msg = (
                f"Ruta de {file_type} no es v√°lida: tipo {type(file_path).__name__}"
            )
            logger.error(f"‚ùå {error_msg}")
            return False, error_msg

        try:
            path = Path(file_path).resolve()
        except Exception as e:
            error_msg = f"Ruta de {file_type} no es v√°lida: {file_path}. Error: {e}"
            logger.error(f"‚ùå {error_msg}")
            return False, error_msg

        if not path.exists():
            error_msg = f"Archivo de {file_type} no encontrado: {path}"
            logger.error(f"‚ùå {error_msg}")
            return False, error_msg

        if not path.is_file():
            error_msg = f"La ruta de {file_type} no es un archivo: {path}"
            logger.error(f"‚ùå {error_msg}")
            return False, error_msg

        try:
            if not os.access(path, os.R_OK):
                error_msg = f"Sin permisos de lectura para {file_type}: {path}"
                logger.error(f"‚ùå {error_msg}")
                return False, error_msg
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è No se pudo verificar permisos para {path}: {e}")

        min_size = min_size or FileValidator.MIN_FILE_SIZE_BYTES
        try:
            file_size = path.stat().st_size
            if file_size < min_size:
                error_msg = f"Archivo de {file_type} demasiado peque√±o ({file_size} bytes): {path}"
                logger.error(f"‚ùå {error_msg}")
                return False, error_msg
        except OSError as e:
            error_msg = f"No se pudo obtener informaci√≥n del archivo {file_type}: {e}"
            logger.error(f"‚ùå {error_msg}")
            return False, error_msg

        if check_extension:
            ext = path.suffix.lower()
            if ext not in FileValidator.VALID_EXTENSIONS:
                logger.warning(
                    f"‚ö†Ô∏è Extensi√≥n '{ext}' de {file_type} no es est√°ndar. "
                    f"Extensiones esperadas: {FileValidator.VALID_EXTENSIONS}"
                )

        logger.debug(
            f"‚úÖ Archivo de {file_type} validado: {path} ({file_size} bytes)"
        )
        return True, None


# ==================== IMPLEMENTACI√ìN DE PASOS ====================


class LoadDataStep(ProcessingStep):
    """
    Paso de Carga de Datos.

    Carga los archivos CSV/Excel de presupuesto, APUs e insumos.
    """

    def __init__(self, config: dict, thresholds: ProcessingThresholds):
        if not config or not isinstance(config, dict):
            raise ValueError("Configuraci√≥n inv√°lida para LoadDataStep")
        self.config = config
        self.thresholds = thresholds or ProcessingThresholds()

    def execute(self, context: dict, telemetry: TelemetryContext) -> dict:
        """Ejecuta la carga y validaci√≥n inicial de archivos."""
        telemetry.start_step("load_data")

        try:
            required_paths = ["presupuesto_path", "apus_path", "insumos_path"]
            paths = {}

            for path_key in required_paths:
                path_value = context.get(path_key)
                if not path_value:
                    error = f"Ruta requerida '{path_key}' no encontrada en contexto"
                    telemetry.record_error("load_data", error)
                    raise ValueError(error)
                paths[path_key] = path_value

            presupuesto_path = paths["presupuesto_path"]
            apus_path = paths["apus_path"]
            insumos_path = paths["insumos_path"]

            file_validator = FileValidator()
            validations = [
                (presupuesto_path, "presupuesto"),
                (apus_path, "APUs"),
                (insumos_path, "insumos"),
            ]

            for file_path, file_type in validations:
                is_valid, error = file_validator.validate_file_exists(
                    file_path, file_type
                )
                if not is_valid:
                    telemetry.record_error("load_data", error)
                    raise ValueError(error)

            file_profiles = self.config.get("file_profiles", {})
            # Use defaults if not present to support simpler test configurations
            if not file_profiles:
                logger.warning("‚ö†Ô∏è 'file_profiles' no encontrado en config, usando defaults vac√≠os.")
                file_profiles = {
                    "presupuesto_default": {},
                    "insumos_default": {},
                    "apus_default": {}
                }

            presupuesto_profile = file_profiles.get("presupuesto_default", {})

            p_processor = PresupuestoProcessor(
                self.config, self.thresholds, presupuesto_profile
            )
            df_presupuesto = p_processor.process(presupuesto_path)

            if df_presupuesto is None or df_presupuesto.empty:
                error = "Procesamiento de presupuesto retorn√≥ DataFrame vac√≠o"
                telemetry.record_error("load_data", error)
                raise ValueError(error)

            telemetry.record_metric(
                "load_data", "presupuesto_rows", len(df_presupuesto)
            )

            insumos_profile = file_profiles.get("insumos_default", {})

            i_processor = InsumosProcessor(self.thresholds, insumos_profile)
            df_insumos = i_processor.process(insumos_path)

            logger.info(
                f"üêõ DIAG: [LoadDataStep] Insumos extra√≠dos: {len(df_insumos)} filas."
            )
            if not df_insumos.empty:
                logger.info(
                    f"üêõ DIAG: [LoadDataStep] Estructura de insumos (head(1)): {df_insumos.head(1).to_dict('records')}"
                )

            if df_insumos is None or df_insumos.empty:
                error = "Procesamiento de insumos retorn√≥ DataFrame vac√≠o"
                telemetry.record_error("load_data", error)
                raise ValueError(error)

            telemetry.record_metric("load_data", "insumos_rows", len(df_insumos))

            apus_profile = file_profiles.get("apus_default", {})

            logger.info("‚ö°Ô∏è Iniciando DataFluxCondenser para APUs...")
            condenser_config_data = self.config.get("flux_condenser_config", {})

            try:
                condenser_config = CondenserConfig(**condenser_config_data)
            except TypeError as e:
                logger.warning(
                    f"‚ö†Ô∏è Error en config de condenser, usando defaults: {e}"
                )
                condenser_config = CondenserConfig()

            condenser = DataFluxCondenser(
                config=self.config,
                profile=apus_profile,
                condenser_config=condenser_config,
            )

            def on_progress_stats(processing_stats):
                try:
                    for metric_name, attr_name, default_value in [
                        ("avg_saturation", "avg_saturation", 0.0),
                        ("max_flyback_voltage", "max_flyback_voltage", 0.0),
                        ("max_dissipated_power", "max_dissipated_power", 0.0),
                        ("avg_kinetic_energy", "avg_kinetic_energy", 0.0),
                    ]:
                        val = getattr(processing_stats, attr_name, default_value)
                        telemetry.record_metric("flux_condenser", metric_name, val)
                except Exception:
                    pass

            def _publish_telemetry(metrics: Dict[str, Any]):
                try:
                    import json
                    import time

                    from flask import current_app

                    payload = {
                        **metrics,
                        "_timestamp": time.time(),
                        "_source": "flux_condenser_realtime",
                    }

                    data_str = json.dumps(payload, default=str)

                    if current_app:
                        redis_client = current_app.config.get("SESSION_REDIS")
                        if redis_client:
                            redis_client.set(
                                "apu_filter:global_metrics", data_str, ex=60
                            )
                except Exception:
                    pass

            df_apus_raw = condenser.stabilize(
                apus_path,
                on_progress=on_progress_stats,
                progress_callback=_publish_telemetry,
                telemetry=telemetry,
            )

            full_stats = condenser.get_processing_stats() or {}
            stats = full_stats.get("statistics", {})

            for metric_name, default_value in [
                ("avg_saturation", 0.0),
                ("max_flyback_voltage", 0.0),
                ("max_dissipated_power", 0.0),
                ("avg_kinetic_energy", 0.0),
                ("avg_batch_size", 0),
            ]:
                value = stats.get(metric_name, default_value)
                if (
                    not isinstance(value, (int, float))
                    or np.isnan(value)
                    or np.isinf(value)
                ):
                    value = default_value
                telemetry.record_metric("flux_condenser", metric_name, value)

            if df_apus_raw is None or df_apus_raw.empty:
                error = "DataFluxCondenser retorn√≥ DataFrame vac√≠o"
                telemetry.record_error("load_data", error)
                raise ValueError(error)

            telemetry.record_metric("load_data", "apus_raw_rows", len(df_apus_raw))
            logger.info("‚úÖ DataFluxCondenser completado.")

            data_validator = DataValidator()
            dataframes = [
                (df_presupuesto, "presupuesto"),
                (df_insumos, "insumos"),
                (df_apus_raw, "APUs"),
            ]

            for df, name in dataframes:
                is_valid, error = data_validator.validate_dataframe_not_empty(df, name)
                if not is_valid:
                    telemetry.record_error("load_data", error)
                    raise ValueError(error)

            context = {**context}
            context.update(
                {
                    "df_presupuesto": df_presupuesto,
                    "df_insumos": df_insumos,
                    "df_apus_raw": df_apus_raw,
                }
            )

            telemetry.end_step("load_data", "success")
            return context

        except Exception as e:
            logger.error(f"‚ùå Error en LoadDataStep: {e}", exc_info=True)
            telemetry.record_error("load_data", str(e))
            telemetry.end_step("load_data", "error")
            raise


class MergeDataStep(ProcessingStep):
    """
    Paso de Fusi√≥n de Datos.

    Combina los datos crudos de APUs con la base de datos de insumos.
    """

    def __init__(self, config: dict, thresholds: ProcessingThresholds):
        self.config = config
        self.thresholds = thresholds

    def execute(self, context: dict, telemetry: TelemetryContext) -> dict:
        """Ejecuta la fusi√≥n de DataFrames."""
        telemetry.start_step("merge_data")
        try:
            df_apus_raw = context["df_apus_raw"]
            df_insumos = context["df_insumos"]

            logger.info(
                f"üêõ DIAG: [MergeDataStep] Recibidos {len(df_insumos)} insumos del contexto."
            )

            merger = DataMerger(self.thresholds)
            df_merged = merger.merge_apus_with_insumos(df_apus_raw, df_insumos)

            telemetry.record_metric("merge_data", "merged_rows", len(df_merged))
            context["df_merged"] = df_merged

            telemetry.end_step("merge_data", "success")
            return context
        except Exception as e:
            telemetry.record_error("merge_data", str(e))
            telemetry.end_step("merge_data", "error")
            raise


class AuditedMergeStep(ProcessingStep):
    """
    Paso de Fusi√≥n con Auditor√≠a Topol√≥gica (Mayer-Vietoris).

    Construye grafos temporales para validar la integridad antes de comprometer la fusi√≥n.
    """

    def __init__(self, config: dict, thresholds: ProcessingThresholds):
        self.config = config
        self.thresholds = thresholds

    def execute(self, context: dict, telemetry: TelemetryContext) -> dict:
        """Ejecuta la auditor√≠a Mayer-Vietoris y luego la fusi√≥n f√≠sica."""
        telemetry.start_step("audited_merge")
        try:
            df_a = context.get("df_presupuesto")
            df_b = context.get("df_apus_raw")
            df_insumos = context.get("df_insumos")

            if df_a is None or df_b is None:
                logger.warning(
                    "‚ö†Ô∏è Falta df_presupuesto o df_apus_raw. Saltando auditor√≠a Mayer-Vietoris."
                )
            else:
                try:
                    from agent.business_topology import (
                        BudgetGraphBuilder,
                        BusinessTopologicalAnalyzer,
                    )

                    builder = BudgetGraphBuilder()
                    graph_a = builder.build(df_a, pd.DataFrame())
                    graph_b = builder.build(pd.DataFrame(), df_b)

                    analyzer = BusinessTopologicalAnalyzer(telemetry=telemetry)
                    audit_result = analyzer.audit_integration_homology(graph_a, graph_b)

                    if audit_result["delta_beta_1"] > 0:
                        logger.warning(f"üö® {audit_result['narrative']}")
                        telemetry.record_metric(
                            "topology", "emergent_cycles", audit_result["delta_beta_1"]
                        )
                        context["integration_risk_alert"] = audit_result
                    else:
                        logger.info(f"‚úÖ {audit_result['narrative']}")

                except Exception as e_audit:
                    logger.error(
                        f"‚ùå Error durante auditor√≠a Mayer-Vietoris: {e_audit}"
                    )
                    telemetry.record_error("audited_merge_audit", str(e_audit))

            logger.info("üõ†Ô∏è Ejecutando fusi√≥n f√≠sica de datos...")
            merger = DataMerger(self.thresholds)
            df_merged = merger.merge_apus_with_insumos(df_b, df_insumos)

            telemetry.record_metric("audited_merge", "merged_rows", len(df_merged))
            context["df_merged"] = df_merged

            telemetry.end_step("audited_merge", "success")
            return context

        except Exception as e:
            telemetry.record_error("audited_merge", str(e))
            telemetry.end_step("audited_merge", "error")
            raise


class CalculateCostsStep(ProcessingStep):
    """
    Paso de C√°lculo de Costos.

    Calcula costos unitarios, tiempos y rendimientos de los APUs.
    """

    def __init__(self, config: dict, thresholds: ProcessingThresholds):
        self.config = config
        self.thresholds = thresholds

    def execute(self, context: dict, telemetry: TelemetryContext) -> dict:
        """Ejecuta los c√°lculos de costos y tiempos."""
        telemetry.start_step("calculate_costs")
        try:
            df_merged = context["df_merged"]
            df_merged = calculate_insumo_costs(df_merged, self.thresholds)

            cost_calculator = APUCostCalculator(self.config, self.thresholds)
            df_apu_costos, df_tiempo, df_rendimiento = cost_calculator.calculate(
                df_merged
            )

            telemetry.record_metric(
                "calculate_costs", "costos_calculated", len(df_apu_costos)
            )

            context.update(
                {
                    "df_merged": df_merged,
                    "df_apu_costos": df_apu_costos,
                    "df_tiempo": df_tiempo,
                    "df_rendimiento": df_rendimiento,
                }
            )
            telemetry.end_step("calculate_costs", "success")
            return context
        except Exception as e:
            telemetry.record_error("calculate_costs", str(e))
            telemetry.end_step("calculate_costs", "error")
            raise


class FinalMergeStep(ProcessingStep):
    """
    Paso de Fusi√≥n Final.

    Integra los costos calculados con el presupuesto original.
    """

    def __init__(self, config: dict, thresholds: ProcessingThresholds):
        self.config = config
        self.thresholds = thresholds

    def execute(self, context: dict, telemetry: TelemetryContext) -> dict:
        """Ejecuta la fusi√≥n final con el presupuesto."""
        telemetry.start_step("final_merge")
        try:
            df_presupuesto = context["df_presupuesto"]
            df_apu_costos = context["df_apu_costos"]
            df_tiempo = context["df_tiempo"]

            merger = DataMerger(self.thresholds)
            df_final = merger.merge_with_presupuesto(df_presupuesto, df_apu_costos)

            df_final = pd.merge(
                df_final, df_tiempo, on=ColumnNames.CODIGO_APU, how="left"
            )
            df_final = group_and_split_description(df_final)
            df_final = calculate_total_costs(df_final, self.thresholds)

            telemetry.record_metric("final_merge", "final_rows", len(df_final))
            context["df_final"] = df_final

            telemetry.end_step("final_merge", "success")
            return context
        except Exception as e:
            telemetry.record_error("final_merge", str(e))
            telemetry.end_step("final_merge", "error")
            raise


class BusinessTopologyStep(ProcessingStep):
    """
    Paso de An√°lisis de Negocio.

    Utiliza el BusinessAgent para auditar la integridad estructural y evaluar riesgos.
    """

    def __init__(self, config: dict, thresholds: ProcessingThresholds):
        self.config = config
        self.thresholds = thresholds

    def execute(self, context: dict, telemetry: TelemetryContext) -> dict:
        """Ejecuta la evaluaci√≥n del BusinessAgent."""
        telemetry.start_step("business_topology")
        try:
            from app.business_agent import BusinessAgent
            from flask import current_app

            # Recuperar la instancia global de la MIC
            mic_instance = getattr(current_app, "mic", None)
            if not mic_instance:
                raise RuntimeError("MICRegistry not found in current_app")

            # Garantizar que el contexto tenga validaci√≥n de estratos
            # Si llegamos a este paso en el pipeline secuencial, asumimos PHYSICS y TACTICS v√°lidos
            if "validated_strata" not in context:
                context["validated_strata"] = {Stratum.PHYSICS, Stratum.TACTICS}
            elif isinstance(context["validated_strata"], set):
                context["validated_strata"].add(Stratum.PHYSICS)
                context["validated_strata"].add(Stratum.TACTICS)

            logger.info("ü§ñ Desplegando BusinessAgent para evaluaci√≥n de proyecto...")

            agent = BusinessAgent(
                config=self.config,
                mic=mic_instance,
                telemetry=telemetry
            )
            report = agent.evaluate_project(context)

            if report:
                logger.info("‚úÖ BusinessAgent complet√≥ la evaluaci√≥n.")
                context["business_topology_report"] = report

                logger.info(
                    f"Puntuaci√≥n de Integridad: {report.integrity_score:.2f}/100"
                )
                if report.waste_alerts:
                    logger.warning(
                        f"Alertas de Desperdicio: {len(report.waste_alerts)}"
                    )
                if report.circular_risks:
                    logger.critical(
                        f"Riesgos Circulares: {len(report.circular_risks)}"
                    )
            else:
                logger.warning("‚ö†Ô∏è El BusinessAgent no gener√≥ un reporte.")

            telemetry.end_step("business_topology", "success")
            return context

        except Exception as e:
            logger.error(
                f"‚ùå Error en BusinessTopologyStep con BusinessAgent: {e}", exc_info=True
            )
            telemetry.record_error("business_topology", str(e))
            telemetry.end_step("business_topology", "error")
            return context


class MaterializationStep(ProcessingStep):
    """
    Paso de Materializaci√≥n.

    Genera la Lista de Materiales (BOM) a partir del grafo topol√≥gico.
    """

    def __init__(self, config: dict, thresholds: ProcessingThresholds):
        self.config = config
        self.thresholds = thresholds

    def execute(self, context: dict, telemetry: TelemetryContext) -> dict:
        """Ejecuta el MatterGenerator."""
        telemetry.start_step("materialization")
        try:
            if "business_topology_report" not in context:
                logger.warning(
                    "‚ö†Ô∏è No se encontr√≥ reporte de topolog√≠a. Saltando materializaci√≥n."
                )
                telemetry.end_step("materialization", "skipped")
                return context

            graph = context.get("graph")
            if not graph:
                logger.info(
                    "üîÑ Grafo no encontrado en contexto. Reconstruyendo para materializaci√≥n..."
                )
                from agent.business_topology import BudgetGraphBuilder

                builder = BudgetGraphBuilder()
                df_presupuesto = context.get("df_final")
                df_detail = context.get("df_merged")

                if df_presupuesto is not None and df_detail is not None:
                    graph = builder.build(df_presupuesto, df_detail)
                    context["graph"] = graph
                else:
                    logger.error(
                        "‚ùå No hay datos suficientes para reconstruir el grafo."
                    )
                    telemetry.end_step("materialization", "error")
                    return context

            report = context.get("business_topology_report")
            stability = 10.0
            if report and report.details:
                stability = report.details.get("pyramid_stability", 10.0)

            flux_metrics = {
                "pyramid_stability": stability,
                "avg_saturation": 0.0,
            }

            generator = MatterGenerator()
            bom = generator.materialize_project(
                graph, flux_metrics=flux_metrics, telemetry=telemetry
            )

            context["bill_of_materials"] = bom
            context["logistics_plan"] = asdict(bom)

            logger.info(
                f"‚úÖ Materializaci√≥n completada. Total √≠tems: {len(bom.requirements)}"
            )

            telemetry.end_step("materialization", "success")
            return context

        except Exception as e:
            logger.error(f"‚ùå Error en MaterializationStep: {e}", exc_info=True)
            telemetry.record_error("materialization", str(e))
            telemetry.end_step("materialization", "error")
            return context


class BuildOutputStep(ProcessingStep):
    """
    Paso de Construcci√≥n de Salida.

    Prepara y valida el diccionario final de resultados para el cliente.
    """

    def __init__(self, config: dict, thresholds: ProcessingThresholds):
        self.config = config
        self.thresholds = thresholds

    def execute(self, context: dict, telemetry: TelemetryContext) -> dict:
        """Construye y valida la estructura de salida como un Data Product."""
        telemetry.start_step("build_output")
        try:
            df_final = context["df_final"]
            df_insumos = context["df_insumos"]
            df_merged = context["df_merged"]
            df_apus_raw = context["df_apus_raw"]
            df_apu_costos = context["df_apu_costos"]
            df_tiempo = context["df_tiempo"]
            df_rendimiento = context["df_rendimiento"]

            df_merged = synchronize_data_sources(df_merged, df_final)
            df_processed_apus = build_processed_apus_dataframe(
                df_apu_costos, df_apus_raw, df_tiempo, df_rendimiento
            )

            result_dict = build_output_dictionary(
                df_final, df_insumos, df_merged, df_apus_raw, df_processed_apus
            )

            validated_result = validate_and_clean_data(
                result_dict, telemetry_context=telemetry
            )
            validated_result["raw_insumos_df"] = df_insumos.to_dict("records")

            if "business_topology_report" in context:
                validated_result["audit_report"] = asdict(
                    context["business_topology_report"]
                )

            if "logistics_plan" in context:
                validated_result["logistics_plan"] = context["logistics_plan"]

            try:
                narrator = TelemetryNarrator()
                tech_narrative = narrator.summarize_execution(telemetry)
                if isinstance(tech_narrative, dict):
                    validated_result["technical_audit"] = tech_narrative.copy()
                else:
                    validated_result["technical_audit"] = tech_narrative

                logger.info("‚úÖ Narrativa t√©cnica generada e inyectada.")
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è Fallo al generar narrativa t√©cnica: {e}")
                validated_result["technical_audit"] = {"error": str(e)}

            def compute_hash(data: Any) -> str:
                """Calcula un hash simple del contenido para linaje."""
                import json

                try:
                    sanitized = data
                    try:
                        sanitized = sanitize_for_json(data)
                    except NameError:
                        pass

                    s = json.dumps(sanitized, sort_keys=True, default=str)
                    return hashlib.sha256(s.encode("utf-8")).hexdigest()
                except Exception:
                    return "hash_computation_failed"

            input_sample = {
                "presupuesto_head": (
                    df_final.head(5).to_dict("records") if not df_final.empty else []
                ),
                "insumos_head": (
                    df_insumos.head(5).to_dict("records")
                    if not df_insumos.empty
                    else []
                ),
            }
            lineage_hash = compute_hash(input_sample)

            error_count = (
                telemetry.get_metrics().get("errors", 0)
                if hasattr(telemetry, "get_metrics")
                else 0
            )
            sla_compliance = "100%" if error_count == 0 else "95%"

            data_product = {
                "kind": "DataProduct",
                "metadata": {
                    "version": "3.0",
                    "lineage_hash": lineage_hash,
                    "sla_compliance": sla_compliance,
                    "generated_at": datetime.datetime.now().isoformat(),
                    "generator": "APU_Filter_Pipeline_v2",
                },
                "governance": {
                    "policy_version": "data_contract_v1",
                    "compliance_score": 95,
                    "classification": "CONFIDENTIAL",
                },
                "payload": validated_result,
            }

            context["final_result"] = data_product
            telemetry.end_step("build_output", "success")
            return context
        except Exception as e:
            telemetry.record_error("build_output", str(e))
            telemetry.end_step("build_output", "error")
            raise


# ==================== PIPELINE DIRECTOR ====================


class PipelineSteps(str, enum.Enum):
    """
    Define los identificadores √∫nicos para cada paso del pipeline.

    Funciona como una 'API' p√∫blica para el orquestador.
    """

    LOAD_DATA = "load_data"
    MERGE_DATA = "merge_data"
    CALCULATE_COSTS = "calculate_costs"
    FINAL_MERGE = "final_merge"
    BUSINESS_TOPOLOGY = "business_topology"
    MATERIALIZATION = "materialization"
    BUILD_OUTPUT = "build_output"



class PipelineDirector:
    """
    Orquesta la ejecuci√≥n secuencial con validaci√≥n topol√≥gica.

    Implementa una 4-variedad diferenciable donde cada estrato
    corresponde a una subvariedad embebida.
    """

    def __init__(self, config: dict, telemetry: TelemetryContext):
        self.config = config
        self.telemetry = telemetry
        self.thresholds = self._load_thresholds(config)
        self.session_dir = Path(config.get("session_dir", "data/sessions"))
        self.session_dir.mkdir(parents=True, exist_ok=True)

        # Inicializar espacio vectorial con m√©trica Riemanniana
        self.mic = LinearInteractionMatrix()
        self._filtration_level = 0  # Nivel de filtraci√≥n actual
        self._homology_groups = {}  # Grupos de homolog√≠a computados
        self._initialize_vector_space_with_validation()

    def _initialize_vector_space_with_validation(self):
        """
        Construye la base can√≥nica con validaci√≥n de filtraci√≥n.

        Filtraci√≥n: ‚àÖ = F_0 ‚äÇ F_1 ‚äÇ F_2 ‚äÇ F_3 ‚äÇ F_4 = V
        donde F_k corresponde al estrato k.
        """
        # Definir mapeo estrato ‚Üí nivel de filtraci√≥n
        stratum_filtration = {
            Stratum.PHYSICS: 1,
            Stratum.TACTICS: 2,
            Stratum.STRATEGY: 3,
            Stratum.WISDOM: 4
        }

        # A√±adir vectores en orden de filtraci√≥n
        basis_config = [
            ("load_data", LoadDataStep, Stratum.PHYSICS),
            ("audited_merge", AuditedMergeStep, Stratum.PHYSICS),
            ("calculate_costs", CalculateCostsStep, Stratum.TACTICS),
            ("final_merge", FinalMergeStep, Stratum.PHYSICS),
            ("materialization", MaterializationStep, Stratum.TACTICS),
            ("business_topology", BusinessTopologyStep, Stratum.STRATEGY),
            ("build_output", BuildOutputStep, Stratum.WISDOM)
        ]

        for label, step_class, stratum in basis_config:
            try:
                self.mic.add_basis_vector(label, step_class, stratum)
                logger.debug(
                    f"üìê Vector a√±adido a filtraci√≥n F_{stratum_filtration[stratum]}: "
                    f"{label} ({stratum.name})"
                )
            except ValueError as e:
                logger.error(f"‚ùå Error en filtraci√≥n para {label}: {e}")
                raise

    def run_single_step(
        self,
        step_name: str,
        session_id: str,
        initial_context: Optional[Dict[str, Any]] = None,
        validate_stratum_transition: bool = True
    ) -> Dict[str, Any]:
        """
        Ejecuta un √∫nico operador con validaci√≥n de transici√≥n entre estratos.

        Par√°metros:
        -----------
        validate_stratum_transition : bool
            Si True, valida que la transici√≥n entre estratos sea suave
            (no salte estratos intermedios).
        """
        # 1. Cargar contexto con verificaci√≥n de integridad
        context = self._load_context_state(session_id)
        if initial_context:
            # Validar que initial_context no corrompa el estado existente
            self._validate_context_merge(context, initial_context)
            context.update(initial_context)

        logger.info(
            f"‚ñ∂Ô∏è Ejecutando operador: {step_name} (Sesi√≥n: {session_id[:8]}...)"
        )

        try:
            # 2. Proyecci√≥n algebraica con verificaci√≥n de rango
            basis_vector = self.mic.project_intent(step_name)

            # 3. Validaci√≥n de transici√≥n entre estratos
            if validate_stratum_transition:
                current_stratum = self._infer_current_stratum(context)
                self._validate_stratum_transition(
                    current_stratum, basis_vector.stratum
                )

            # 4. Instanciaci√≥n del operador lineal
            step_instance = basis_vector.operator_class(self.config, self.thresholds)

            # 5. Medici√≥n de traza antes/despu√©s
            trace_before = self._compute_state_trace(context)

            # 6. Aplicaci√≥n de transformaci√≥n: S' = T(S)
            updated_context = step_instance.execute(context, self.telemetry)

            if updated_context is None:
                raise ValueError(f"Operador {step_name} retorn√≥ transformaci√≥n nula")

            # 7. Verificar preservaci√≥n de norma (conservaci√≥n de informaci√≥n)
            trace_after = self._compute_state_trace(updated_context)
            trace_delta = abs(trace_after - trace_before)

            if trace_delta > 0.01:  # Umbral de tolerancia
                logger.warning(
                    f"‚ö†Ô∏è Operador {step_name} alter√≥ traza del estado: "
                    f"Œî = {trace_delta:.4f}"
                )

            # 8. Persistencia con checksum
            self._save_context_state_with_checksum(session_id, updated_context)

            # 9. Actualizar nivel de filtraci√≥n
            self._filtration_level = self._stratum_to_filtration(basis_vector.stratum)

            # 10. Calcular homolog√≠a si estamos en estratos superiores
            if basis_vector.stratum in [Stratum.STRATEGY, Stratum.WISDOM]:
                self._compute_homology_groups(updated_context)

            logger.info(f"‚úÖ Operador {step_name} completado (estrato: {basis_vector.stratum.name})")

            return {
                "status": "success",
                "step": step_name,
                "stratum": basis_vector.stratum.name,
                "filtration_level": self._filtration_level,
                "session_id": session_id,
                "context_keys": list(updated_context.keys()),
                "trace_delta": trace_delta,
                "homology_updated": basis_vector.stratum in [Stratum.STRATEGY, Stratum.WISDOM]
            }

        except Exception as e:
            error_msg = f"Error en operador '{step_name}': {e}"
            logger.error(f"üî• {error_msg}", exc_info=True)
            self.telemetry.record_error(step_name, str(e))

            # Intentar recuperaci√≥n mediante operador identidad
            recovery_status = self._attempt_state_recovery(session_id, context)

            return {
                "status": "error",
                "step": step_name,
                "error": error_msg,
                "recovery_attempted": recovery_status,
                "session_id": session_id
            }

    def execute_pipeline_orchestrated(self, initial_context: dict) -> dict:
        """
        Ejecuta el pipeline completo de forma orquestada, paso a paso.
        """
        session_id = str(uuid.uuid4())
        logger.info(f"üöÄ Iniciando pipeline orquestado con Sesi√≥n ID: {session_id}")

        if "pipeline_recipe" not in self.config:
            logger.warning("No 'pipeline_recipe' en config. Usando flujo por defecto.")
            recipe = [{"step": step.value, "enabled": True} for step in PipelineSteps]
        else:
            recipe = self.config["pipeline_recipe"]

        context = initial_context
        for step_idx, step_config in enumerate(recipe):
            step_name = step_config.get("step")
            if not step_config.get("enabled", True):
                continue

            logger.info(f"‚ñ∂Ô∏è Orquestando paso [{step_idx + 1}/{len(recipe)}]: {step_name}")

            current_context = context if step_idx == 0 else None
            result = self.run_single_step(
                step_name, session_id, initial_context=current_context
            )

            if result["status"] == "error":
                error_msg = f"Fallo en pipeline orquestado en paso '{step_name}': {result.get('error')}"
                logger.critical(f"üî• {error_msg}")
                raise RuntimeError(error_msg)

        final_context = self._load_context_state(session_id)
        logger.info(f"üéâ Pipeline orquestado completado (Sesi√≥n: {session_id})")
        return final_context

    def _validate_stratum_transition(
        self,
        current: Optional[Stratum],
        next_stratum: Stratum
    ):
        """
        Valida transici√≥n entre estratos usando teor√≠a de √≥rdenes parciales.

        Reglas:
        1. Transiciones hacia arriba: Siempre permitidas
        2. Transiciones laterales (mismo nivel): Permitidas
        3. Transiciones hacia abajo: Solo si se completa ciclo o reinicio expl√≠cito
        4. Saltos de m√°s de un nivel: Warning pero permitido
        """
        if current is None:
            # Primera ejecuci√≥n, cualquier estrato es v√°lido
            logger.debug(f"üöÄ Iniciando en estrato {next_stratum.name}")
            return

        current_level = self._stratum_to_filtration(current)
        next_level = self._stratum_to_filtration(next_stratum)

        # Caso 1: Avance o mismo nivel (normal)
        if next_level >= current_level:
            # Detectar salto de estratos
            if next_level > current_level + 1:
                skipped = next_level - current_level - 1
                skipped_strata = [
                    s.name for s in Stratum
                    if current_level < self._stratum_to_filtration(s) < next_level
                ]
                logger.warning(
                    f"‚ö†Ô∏è Salto de {skipped} estrato(s): {current.name} ‚Üí {next_stratum.name}. "
                    f"Estratos omitidos: {skipped_strata}"
                )

                # Registrar en telemetr√≠a
                self.telemetry.record_metric(
                    "stratum_transition",
                    "skipped_strata",
                    skipped
                )
            return

        # Caso 2: Retroceso (potencial reinicio de ciclo)
        if next_level < current_level:
            # Verificar si es reinicio v√°lido (volver a PHYSICS desde WISDOM)
            is_valid_cycle_restart = (
                current == Stratum.WISDOM and
                next_stratum == Stratum.PHYSICS
            )

            if is_valid_cycle_restart:
                logger.info(
                    f"üîÑ Reinicio de ciclo detectado: {current.name} ‚Üí {next_stratum.name}"
                )
                self._filtration_level = 0  # Resetear nivel
                return

            # Retroceso parcial (potencialmente problem√°tico)
            regression_depth = current_level - next_level
            logger.warning(
                f"‚ö†Ô∏è Regresi√≥n de estrato detectada: {current.name} ‚Üí {next_stratum.name} "
                f"(profundidad: {regression_depth})"
            )

            # Registrar para auditor√≠a
            self.telemetry.record_metric(
                "stratum_transition",
                "regression_depth",
                regression_depth
            )

            # Permitir pero marcar contexto
            return
    def _stratum_to_filtration(self, stratum: Stratum) -> int:
        mapping = {
            Stratum.PHYSICS: 1,
            Stratum.TACTICS: 2,
            Stratum.STRATEGY: 3,
            Stratum.WISDOM: 4
        }
        return mapping.get(stratum, 0)

    def _infer_current_stratum(self, context: dict) -> Optional[Stratum]:
        """Infiere el estrato actual basado en claves de contexto."""
        context_keys = set(context.keys())
        stratum_indicators = {
            Stratum.PHYSICS: {"df_presupuesto", "df_insumos", "df_apus_raw"},
            Stratum.TACTICS: {"df_merged", "df_apu_costos", "df_tiempo"},
            Stratum.STRATEGY: {"graph", "business_topology_report"},
            Stratum.WISDOM: {"final_result", "bill_of_materials"}
        }
        for stratum, indicators in stratum_indicators.items():
            if indicators & context_keys:
                return stratum
        return None

    def _compute_state_trace(self, context: dict) -> float:
        try:
            trace = 0.0
            for key, value in context.items():
                if isinstance(value, pd.DataFrame):
                    trace += len(value) * value.shape[1]
                elif isinstance(value, (list, dict)):
                    trace += len(str(value)) / 100
            return trace
        except:
            return 0.0

    def _save_context_state_with_checksum(self, session_id: str, context: dict):
        import hashlib
        try:
            # We must be careful with what we pickle/hash.
            # This is a simplified version.
            context_keys = sorted(context.keys())
            checksum_str = "".join(context_keys) # Very simple checksum
            checksum = hashlib.sha256(checksum_str.encode()).hexdigest()

            context["_integrity_checksum"] = checksum
            context["_persisted_at"] = datetime.datetime.now().isoformat()

            session_file = self.session_dir / f"{session_id}.pkl"
            with open(session_file, "wb") as f:
                pickle.dump(context, f)

            logger.debug(f"üíæ Contexto persistido con checksum: {checksum[:16]}...")
        except Exception as e:
            logger.error(f"Error persisting context: {e}")

    def _compute_homology_groups(self, context: dict):
        """
        Computa grupos de homolog√≠a del complejo simplicial de datos.

        Usa el Laplaciano combinatorio para calcular n√∫meros de Betti:
        - Œ≤‚ÇÄ = dim(ker(L‚ÇÄ)) = componentes conexas
        - Œ≤‚ÇÅ = dim(ker(L‚ÇÅ)) - dim(im(L‚ÇÄ)) = ciclos independientes

        Refinamiento: Usa descomposici√≥n sparse y manejo robusto de casos degenerados.
        """
        try:
            df_keys = [k for k in context.keys()
                       if isinstance(context.get(k), pd.DataFrame)
                       and not context[k].empty]

            n = len(df_keys)
            if n < 2:
                self._homology_groups = {"H0": 1, "H1": 0, "Betti": [1, 0]}
                return

            # Construir matriz de adyacencia ponderada
            adj_matrix = sparse.lil_matrix((n, n), dtype=np.float64)

            for i, key_i in enumerate(df_keys):
                df_i = context[key_i]
                cols_i = set(df_i.columns)

                for j in range(i + 1, n):
                    key_j = df_keys[j]
                    df_j = context[key_j]
                    cols_j = set(df_j.columns)

                    # Peso = Jaccard similarity de columnas
                    intersection = len(cols_i & cols_j)
                    union = len(cols_i | cols_j)

                    if intersection > 0 and union > 0:
                        weight = intersection / union
                        adj_matrix[i, j] = weight
                        adj_matrix[j, i] = weight

            adj_csr = adj_matrix.tocsr()

            # Construir Laplaciano combinatorio L = D - A
            degrees = np.array(adj_csr.sum(axis=1)).flatten()
            degree_matrix = sparse.diags(degrees)
            laplacian = degree_matrix - adj_csr

            # Calcular Œ≤‚ÇÄ (componentes conexas) via eigenvalores cercanos a 0
            h0 = self._count_zero_eigenvalues(laplacian, n)

            # Calcular Œ≤‚ÇÅ usando f√≥rmula de Euler: œá = Œ≤‚ÇÄ - Œ≤‚ÇÅ + Œ≤‚ÇÇ - ...
            # Para 1-complejo: œá = V - E, entonces Œ≤‚ÇÅ = E - V + Œ≤‚ÇÄ
            num_edges = adj_csr.nnz // 2
            num_vertices = n
            h1 = max(0, num_edges - num_vertices + h0)

            # Verificar consistencia topol√≥gica
            euler_char = h0 - h1
            expected_euler = num_vertices - num_edges

            if euler_char != expected_euler:
                logger.warning(
                    f"‚ö†Ô∏è Inconsistencia en caracter√≠stica de Euler: "
                    f"calculada={euler_char}, esperada={expected_euler}"
                )

            self._homology_groups = {
                "H0": h0,
                "H1": h1,
                "Betti_numbers": [h0, h1],
                "Euler_characteristic": euler_char,
                "vertices": num_vertices,
                "edges": num_edges
            }

            logger.debug(f"üßÆ Homolog√≠a: Œ≤‚ÇÄ={h0}, Œ≤‚ÇÅ={h1}, œá={euler_char}")

        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Error computando homolog√≠a: {e}")
            self._homology_groups = {"H0": 1, "H1": 0, "error": str(e)}

    def _count_zero_eigenvalues(
        self,
        laplacian: sparse.spmatrix,
        n: int,
        tol: float = 1e-10
    ) -> int:
        """
        Cuenta eigenvalores cercanos a cero del Laplaciano.

        Usa shift-invert para estabilidad con eigenvalores peque√±os.
        """
        if n <= 1:
            return n

        if n <= 10:
            # Para matrices peque√±as, usar m√©todo denso
            try:
                L_dense = laplacian.toarray()
                eigenvalues = np.linalg.eigvalsh(L_dense)
                return int(np.sum(np.abs(eigenvalues) < tol))
            except np.linalg.LinAlgError:
                return 1

        # Para matrices grandes, usar m√©todo iterativo
        try:
            k = min(n - 1, max(1, n // 4))  # N√∫mero de eigenvalores a calcular

            # Shift-invert mode para eigenvalores cerca de 0
            eigenvalues = sparse.linalg.eigsh(
                laplacian,
                k=k,
                sigma=0.0,  # Target eigenvalue
                which='LM',  # Largest magnitude after shift-invert = smallest
                return_eigenvectors=False,
                tol=1e-6,
                maxiter=1000
            )

            return int(np.sum(np.abs(eigenvalues) < tol))

        except (sparse.linalg.ArpackNoConvergence, sparse.linalg.ArpackError) as e:
            logger.debug(f"ARPACK no convergi√≥, usando estimaci√≥n: {e}")
            # Fallback: estimar componentes por BFS
            return self._estimate_components_bfs(laplacian, n)
        except Exception:
            return 1

    def _estimate_components_bfs(
        self,
        laplacian: sparse.spmatrix,
        n: int
    ) -> int:
        """Estima componentes conexas por BFS cuando eigsh falla."""
        adj = (laplacian != laplacian.diagonal()).astype(bool)
        visited = np.zeros(n, dtype=bool)
        components = 0

        for start in range(n):
            if not visited[start]:
                components += 1
                # BFS
                queue = [start]
                while queue:
                    node = queue.pop(0)
                    if not visited[node]:
                        visited[node] = True
                        neighbors = adj[node].nonzero()[1]
                        queue.extend(neighbors[~visited[neighbors]])

        return components

    def _attempt_state_recovery(self, session_id: str, context: dict) -> bool:
        try:
            corrupt_file = self.session_dir / f"{session_id}_corrupt.pkl"
            with open(corrupt_file, "wb") as f:
                pickle.dump(context, f)
            return False
        except:
            return False

    def _validate_context_merge(self, context, initial):
        pass

    def _load_thresholds(self, config: dict) -> ProcessingThresholds:
        thresholds = ProcessingThresholds()
        if "processing_thresholds" in config:
            for key, value in config["processing_thresholds"].items():
                if hasattr(thresholds, key):
                    setattr(thresholds, key, value)
        return thresholds

    def _load_context_state(self, session_id: str) -> dict:
        if not session_id: return {}
        try:
            session_file = self.session_dir / f"{session_id}.pkl"
            if session_file.exists():
                with open(session_file, "rb") as f:
                    return pickle.load(f)
            return {}
        except:
            return {}


class PresupuestoProcessor:
    def __init__(self, config: dict, thresholds: ProcessingThresholds, profile: dict):
        if not config:
            raise ValueError("Configuraci√≥n requerida para PresupuestoProcessor")
        self.config = config
        self.thresholds = thresholds or ProcessingThresholds()
        self.profile = profile or {}
        self.validator = DataValidator()

    def process(self, path: str) -> pd.DataFrame:
        """Procesa archivo de presupuesto con manejo robusto de errores."""
        if not path:
            logger.error("‚ùå Ruta de presupuesto vac√≠a")
            return pd.DataFrame()

        try:
            loader_params = self.profile.get("loader_params", {})
            logger.info(f"üì• Cargando presupuesto desde: {path}")
            logger.debug(f"Par√°metros de carga: {loader_params}")

            load_result = load_data(path, **loader_params)

            if load_result is None:
                logger.error("‚ùå load_data retorn√≥ None")
                return pd.DataFrame()

            if not hasattr(load_result, "status") or not hasattr(load_result, "data"):
                logger.error("‚ùå Estructura de load_result inv√°lida")
                return pd.DataFrame()

            status_value = getattr(
                load_result.status, "value", str(load_result.status)
            )
            if status_value != "SUCCESS":
                error_msg = getattr(
                    load_result, "error_message", "Error desconocido"
                )
                logger.error(f"Error cargando presupuesto: {error_msg}")
                return pd.DataFrame()

            df = load_result.data
            if df is None or (isinstance(df, pd.DataFrame) and df.empty):
                logger.warning("‚ö†Ô∏è Archivo de presupuesto cargado est√° vac√≠o")
                return pd.DataFrame()

            logger.info(
                f"üìä DataFrame cargado: {len(df)} filas. Columnas: {list(df.columns)}"
            )

            df_clean = self._clean_phantom_rows(df)
            logger.info(f"üëª Filas tras limpieza fantasma: {len(df_clean)}")

            if df_clean.empty:
                logger.warning("‚ö†Ô∏è DataFrame vac√≠o despu√©s de limpiar filas fantasma")
                return pd.DataFrame()

            df_renamed = self._rename_columns(df_clean)
            logger.info(f"üè∑Ô∏è Columnas tras renombrado: {list(df_renamed.columns)}")

            valid_cols, error_msg = self._validate_required_columns(df_renamed)
            if not valid_cols:
                logger.error(
                    f"‚ùå Validaci√≥n de columnas requeridas fall√≥: {error_msg}"
                )
                return pd.DataFrame()

            df_converted = self._clean_and_convert_data(df_renamed)
            logger.info(f"üî¢ Filas tras conversi√≥n de datos: {len(df_converted)}")

            if df_converted.empty:
                logger.warning(
                    "‚ö†Ô∏è Conversi√≥n elimin√≥ todas las filas. Verifique limpieza de c√≥digos."
                )
                return pd.DataFrame()

            df_final = self._remove_duplicates(df_converted)

            final_cols = [
                ColumnNames.CODIGO_APU,
                ColumnNames.DESCRIPCION_APU,
                ColumnNames.CANTIDAD_PRESUPUESTO,
            ]
            available_cols = [col for col in final_cols if col in df_final.columns]

            if ColumnNames.CODIGO_APU not in available_cols:
                logger.error(
                    f"‚ùå Columna cr√≠tica '{ColumnNames.CODIGO_APU}' no disponible"
                )
                return pd.DataFrame()

            result = df_final[available_cols].copy()
            logger.info(f"‚úÖ Presupuesto procesado: {len(result)} filas")
            return result

        except Exception as e:
            logger.error(f"‚ùå Error fatal procesando presupuesto: {e}", exc_info=True)
            return pd.DataFrame()

    def _clean_phantom_rows(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        Elimina filas fantasma con detecci√≥n multi-patr√≥n.

        Patrones detectados:
        1. Filas completamente vac√≠as
        2. Filas con solo NaN/None
        3. Filas con patrones de placeholder
        4. Filas de metadatos (totales, subtotales)
        """
        if df is None:
            return pd.DataFrame()

        if not isinstance(df, pd.DataFrame):
            logger.error(f"‚ùå _clean_phantom_rows recibi√≥ tipo: {type(df).__name__}")
            return pd.DataFrame()

        if df.empty:
            return df

        initial_rows = len(df)
        removal_reasons: Dict[str, int] = {}

        # 1. Eliminar filas completamente vac√≠as
        empty_mask = df.isna().all(axis=1)
        removal_reasons["completely_empty"] = empty_mask.sum()
        df_clean = df[~empty_mask].copy()

        # 2. Eliminar filas con solo valores "vac√≠os" (string patterns)
        str_df = df_clean.astype(str).apply(lambda x: x.str.strip().str.lower())

        empty_patterns = {
            "", "nan", "none", "nat", "<na>", "null", "n/a", "na",
            "-", "--", "---", "...", ".", "undefined", "sin dato"
        }

        is_empty_mask = str_df.isin(empty_patterns).all(axis=1)
        removal_reasons["pattern_empty"] = is_empty_mask.sum()
        df_clean = df_clean[~is_empty_mask].copy()

        # 3. Eliminar filas de metadatos (totales, subtotales, encabezados)
        if not df_clean.empty and len(df_clean.columns) > 0:
            first_col = df_clean.iloc[:, 0].astype(str).str.strip().str.lower()

            metadata_patterns = [
                r"^total\b", r"^subtotal\b", r"^suma\b", r"^promedio\b",
                r"^gran\s*total", r"^item\b", r"^codigo\b", r"^descripcion\b",
                r"^\d+\.\s*total", r"^resumen\b"
            ]

            metadata_mask = pd.Series(False, index=df_clean.index)
            for pattern in metadata_patterns:
                metadata_mask |= first_col.str.contains(pattern, regex=True, na=False)

            removal_reasons["metadata_rows"] = metadata_mask.sum()
            df_clean = df_clean[~metadata_mask].copy()

        # 4. Eliminar filas donde columnas cr√≠ticas est√°n vac√≠as
        critical_columns = [col for col in df_clean.columns
                           if any(kw in col.upper() for kw in ["CODIGO", "DESCRIPCION", "APU"])]

        if critical_columns:
            critical_empty_mask = df_clean[critical_columns].isna().all(axis=1)
            removal_reasons["critical_empty"] = critical_empty_mask.sum()
            df_clean = df_clean[~critical_empty_mask].copy()

        # Log detallado
        total_removed = initial_rows - len(df_clean)
        if total_removed > 0:
            logger.info(f"üëª Filas fantasma eliminadas: {total_removed}")
            for reason, count in removal_reasons.items():
                if count > 0:
                    logger.debug(f"   - {reason}: {count}")

        return df_clean
    def _rename_columns(self, df: pd.DataFrame) -> pd.DataFrame:
        column_map = self.config.get("presupuesto_column_map", {})
        return find_and_rename_columns(df, column_map)

    def _validate_required_columns(
        self, df: pd.DataFrame
    ) -> Tuple[bool, Optional[str]]:
        return self.validator.validate_required_columns(
            df, [ColumnNames.CODIGO_APU], "presupuesto"
        )

    def _clean_and_convert_data(self, df: pd.DataFrame) -> pd.DataFrame:
        """Limpia y convierte datos con validaciones robustas."""
        if df is None or df.empty:
            return pd.DataFrame()

        df = df.copy()

        clean_code_params = self.config.get("clean_apu_code_params", {}).get(
            "presupuesto_item", {}
        )

        if ColumnNames.CODIGO_APU not in df.columns:
            logger.error(f"‚ùå Columna '{ColumnNames.CODIGO_APU}' no encontrada")
            return pd.DataFrame()

        try:

            def safe_clean(code):
                try:
                    return clean_apu_code(code, **clean_code_params)
                except (ValueError, TypeError):
                    return None

            df[ColumnNames.CODIGO_APU] = (
                df[ColumnNames.CODIGO_APU]
                .fillna("")
                .astype(str)
                .apply(safe_clean)
            )
        except Exception as e:
            logger.error(f"‚ùå Error limpiando c√≥digos APU: {e}")
            return pd.DataFrame()

        df = df.dropna(subset=[ColumnNames.CODIGO_APU])

        invalid_codes = {"", "nan", "none", "null"}
        mask_valid = df[ColumnNames.CODIGO_APU].notna() & ~df[
            ColumnNames.CODIGO_APU
        ].str.strip().str.lower().isin(invalid_codes)

        df = df[mask_valid].copy()

        if df.empty:
            logger.warning("‚ö†Ô∏è No quedaron registros con c√≥digos APU v√°lidos")
            return df

        if ColumnNames.CANTIDAD_PRESUPUESTO in df.columns:
            try:
                qty = df[ColumnNames.CANTIDAD_PRESUPUESTO].astype(str).str.strip()
                qty = qty.str.replace(",", ".", regex=False)
                qty = qty.str.replace(r"[^\d.\-]", "", regex=True)
                qty = qty.str.replace(r"\.(?=.*\.)", "", regex=True)

                df[ColumnNames.CANTIDAD_PRESUPUESTO] = pd.to_numeric(
                    qty, errors="coerce"
                ).fillna(0)

                max_qty = self.thresholds.max_quantity
                mask_invalid_qty = df[ColumnNames.CANTIDAD_PRESUPUESTO] > max_qty
                if mask_invalid_qty.any():
                    count = mask_invalid_qty.sum()
                    logger.warning(
                        f"‚ö†Ô∏è {count} cantidades exceden m√°ximo ({max_qty}), se limitar√°n"
                    )
                    df.loc[
                        mask_invalid_qty, ColumnNames.CANTIDAD_PRESUPUESTO
                    ] = max_qty

            except Exception as e:
                logger.error(f"‚ùå Error procesando cantidades: {e}")
                df[ColumnNames.CANTIDAD_PRESUPUESTO] = 0

        return df

    def _remove_duplicates(self, df: pd.DataFrame) -> pd.DataFrame:
        return self.validator.detect_and_log_duplicates(
            df, [ColumnNames.CODIGO_APU], "presupuesto"
        )


class InsumosProcessor:
    def __init__(self, thresholds: ProcessingThresholds, profile: dict):
        self.thresholds = thresholds
        self.profile = profile
        self.validator = DataValidator()

    def process(self, file_path: str) -> pd.DataFrame:
        try:
            records = self._parse_file(file_path)
            if not records:
                return pd.DataFrame()
            df = pd.DataFrame(records)
            df = self._rename_and_select_columns(df)
            df = self._convert_and_normalize(df)
            df = self._remove_duplicates(df)
            return df
        except Exception:
            return pd.DataFrame()

    def _parse_file(self, file_path: str) -> List[Dict]:
        encoding = self.profile.get("encoding", "latin1")
        try:
            with open(file_path, "r", encoding=encoding) as f:
                lines = f.readlines()
        except Exception:
            with open(file_path, "r", encoding="utf-8", errors="replace") as f:
                lines = f.readlines()

        records = []
        current_group = None
        header = None

        for line in lines:
            clean_line = line.strip()
            if not clean_line:
                continue

            parts = [p.strip().replace('"', "") for p in clean_line.split(";")]
            if not any(parts):
                continue

            first_col = parts[0].strip().upper()
            if first_col.startswith("G") and len(parts) > 1:
                candidate_group = parts[1].strip()
                if candidate_group:
                    logger.debug(
                        f"üîç Candidato a grupo detectado: '{parts[0]}' -> Grupo: '{candidate_group}'"
                    )
                    current_group = candidate_group
                    logger.info(f"üìÇ Grupo detectado: {current_group}")
                    header = None
                    continue

            if (
                "CODIGO" in first_col
                and len(parts) >= 2
                and "DESCRIPCION" in parts[1].upper()
            ):
                header = parts
                logger.info(
                    f"üìã Encabezado detectado para grupo {current_group}: {header}"
                )
                continue

            if header and current_group:
                if not parts[0]:
                    continue

                record = {ColumnNames.GRUPO_INSUMO: current_group}

                for i, col_name in enumerate(header):
                    if i < len(parts):
                        clean_col = col_name.upper().replace(".", "").strip()
                        if "DESCRIPCION" in clean_col:
                            record["DESCRIPCION"] = parts[i]
                        elif "VR" in clean_col and "UNIT" in clean_col:
                            record["VR. UNIT."] = parts[i]
                        elif "UND" in clean_col:
                            record["UND"] = parts[i]
                        elif "CODIGO" in clean_col:
                            record["CODIGO"] = parts[i]
                        elif "CANT" in clean_col:
                            record["CANTIDAD"] = parts[i]

                if "DESCRIPCION" in record:
                    records.append(record)

        logger.info(f"‚úÖ Total insumos extra√≠dos: {len(records)}")
        return records

    def _rename_and_select_columns(self, df: pd.DataFrame) -> pd.DataFrame:
        df = df.rename(
            columns={
                "DESCRIPCION": ColumnNames.DESCRIPCION_INSUMO,
                "VR. UNIT.": ColumnNames.VR_UNITARIO_INSUMO,
                "CODIGO": "CODIGO",
                "CANTIDAD": "CANTIDAD",
            }
        )
        if "CODIGO" not in df.columns:
            df["CODIGO"] = None
        if "CANTIDAD" not in df.columns:
            df["CANTIDAD"] = 0

        return df[
            [
                ColumnNames.GRUPO_INSUMO,
                ColumnNames.DESCRIPCION_INSUMO,
                ColumnNames.VR_UNITARIO_INSUMO,
                "CODIGO",
                "CANTIDAD",
            ]
        ]

    def _convert_and_normalize(self, df: pd.DataFrame) -> pd.DataFrame:
        df[ColumnNames.VR_UNITARIO_INSUMO] = pd.to_numeric(
            df[ColumnNames.VR_UNITARIO_INSUMO].astype(str).str.replace(",", "."),
            errors="coerce",
        )
        df[ColumnNames.DESCRIPCION_INSUMO_NORM] = normalize_text_series(
            df[ColumnNames.DESCRIPCION_INSUMO]
        )
        return df.dropna(subset=[ColumnNames.DESCRIPCION_INSUMO])

    def _remove_duplicates(self, df: pd.DataFrame) -> pd.DataFrame:
        return df.sort_values(
            ColumnNames.VR_UNITARIO_INSUMO, ascending=False
        ).drop_duplicates(subset=[ColumnNames.DESCRIPCION_INSUMO_NORM], keep="first")


class BaseCostProcessor(ABC):
    """Clase base para procesadores con logging y validaci√≥n."""

    def __init__(self, config: Dict[str, Any], thresholds: ProcessingThresholds):
        self.config = config
        self.thresholds = thresholds
        self._setup_logging()

    def _setup_logging(self):
        """Configura logging consistente."""
        self.logger = logging.getLogger(f"{self.__class__.__name__}")

    def _validate_input(self, df: pd.DataFrame, operation: str) -> bool:
        """Validaci√≥n com√∫n de input."""
        if df is None:
            self.logger.error(f"‚ùå DataFrame None en {operation}")
            return False

        if not isinstance(df, pd.DataFrame):
            self.logger.error(f"‚ùå Input no es DataFrame en {operation}")
            return False

        if df.empty:
            self.logger.warning(f"‚ö†Ô∏è DataFrame vac√≠o en {operation}")

        return True

    def _empty_results(self) -> Tuple[pd.DataFrame, ...]:
        """Retorna tupla de DataFrames vac√≠os por defecto."""
        return pd.DataFrame(), pd.DataFrame(), pd.DataFrame()

    @abstractmethod
    def calculate(self, *args, **kwargs):
        """M√©todo principal a implementar por subclases."""
        pass



class InformationGeometry:
    """
    Geometr√≠a de la informaci√≥n para espacios de datos.

    Refinamiento: Implementa m√©trica de Fisher-Rao y divergencia de
    Kullback-Leibler con estimadores robustos.
    """

    def __init__(self, n_components_pca: int = 10):
        self.n_components_pca = n_components_pca
        self._entropy_cache: Dict[int, float] = {}

    def compute_entropy(self, df: pd.DataFrame) -> Dict[str, float]:
        """
        Calcula medidas de entrop√≠a e informaci√≥n con estimadores robustos.
        """
        if df is None or df.empty:
            return self._empty_metrics()

        result = {
            "shannon_entropy": 0.0,
            "intrinsic_dimension": 0.0,
            "fisher_information": 0.0,
            "effective_rank": 0.0
        }

        # Entrop√≠a de Shannon para columnas categ√≥ricas
        categorical_cols = df.select_dtypes(exclude=[np.number]).columns
        total_entropy = 0.0

        for col in categorical_cols:
            col_entropy = self._compute_column_entropy(df[col])
            total_entropy += col_entropy

        result["shannon_entropy"] = total_entropy

        # An√°lisis de columnas num√©ricas
        numeric_df = df.select_dtypes(include=[np.number])
        if not numeric_df.empty and len(numeric_df) > 1:
            # Limpiar datos
            numeric_data = numeric_df.fillna(0).values
            numeric_data = np.nan_to_num(numeric_data, nan=0, posinf=0, neginf=0)

            # Dimensi√≥n intr√≠nseca via PCA
            result["intrinsic_dimension"] = self._compute_intrinsic_dimension(numeric_data)

            # Informaci√≥n de Fisher (aproximaci√≥n diagonal)
            result["fisher_information"] = self._compute_fisher_information(numeric_data)

            # Rango efectivo (diversidad espectral)
            result["effective_rank"] = self._compute_effective_rank(numeric_data)

        return result

    def _empty_metrics(self) -> Dict[str, float]:
        return {
            "shannon_entropy": 0.0,
            "intrinsic_dimension": 0.0,
            "fisher_information": 0.0,
            "effective_rank": 0.0
        }

    def _compute_column_entropy(self, series: pd.Series) -> float:
        """Calcula entrop√≠a de Shannon para una columna."""
        try:
            value_counts = series.value_counts(normalize=True, dropna=True)
            if value_counts.empty:
                return 0.0

            probabilities = value_counts.values
            # Filtrar probabilidades cero
            probabilities = probabilities[probabilities > 0]

            return float(-np.sum(probabilities * np.log2(probabilities)))
        except Exception:
            return 0.0

    def _compute_intrinsic_dimension(self, data: np.ndarray) -> float:
        """
        Calcula dimensi√≥n intr√≠nseca usando PCA con criterio de energ√≠a.

        Umbral: 95% de varianza explicada.
        """
        if data.shape[0] < 2 or data.shape[1] < 1:
            return 0.0

        try:
            # Estandarizar datos
            mean = np.mean(data, axis=0)
            std = np.std(data, axis=0)
            std[std == 0] = 1  # Evitar divisi√≥n por cero
            data_std = (data - mean) / std

            n_components = min(self.n_components_pca, data.shape[0] - 1, data.shape[1])
            if n_components < 1:
                return 1.0

            # Usar TruncatedSVD para eficiencia con datos grandes
            if data.shape[0] > 1000 or data.shape[1] > 100:
                svd = TruncatedSVD(n_components=n_components, random_state=42)
                svd.fit(data_std)
                explained_variance = svd.explained_variance_ratio_
            else:
                pca = PCA(n_components=n_components)
                pca.fit(data_std)
                explained_variance = pca.explained_variance_ratio_

            # Dimensi√≥n = primer k donde varianza acumulada >= 95%
            cumulative = np.cumsum(explained_variance)
            threshold_idx = np.searchsorted(cumulative, 0.95)

            return float(threshold_idx + 1)

        except Exception as e:
            # logger.debug(f"Error en PCA: {e}") # logger not available in scope directly here in replacement
            return 1.0

    def _compute_fisher_information(self, data: np.ndarray) -> float:
        """
        Aproxima la informaci√≥n de Fisher como traza de la inversa de covarianza.

        I(Œ∏) ‚âà Tr(Œ£‚Åª¬π) donde Œ£ es la matriz de covarianza.
        """
        if data.shape[0] < 2:
            return 0.0

        try:
            # Covarianza con regularizaci√≥n
            cov = np.cov(data.T)
            if cov.ndim == 0:
                cov = np.array([[cov]])

            # Regularizaci√≥n de Tikhonov para invertibilidad
            reg_lambda = 1e-6 * np.trace(cov) / cov.shape[0] if cov.shape[0] > 0 else 1e-6
            cov_reg = cov + reg_lambda * np.eye(cov.shape[0])

            # Traza de la inversa
            cov_inv = np.linalg.inv(cov_reg)
            fisher = np.trace(cov_inv)

            return float(np.clip(fisher, 0, 1e10))

        except np.linalg.LinAlgError:
            return 0.0
        except Exception:
            return 0.0

    def _compute_effective_rank(self, data: np.ndarray) -> float:
        """
        Calcula rango efectivo basado en entrop√≠a espectral.

        eff_rank = exp(H(œÉ)) donde H es entrop√≠a de valores singulares normalizados.
        """
        if data.shape[0] < 2 or data.shape[1] < 1:
            return 0.0

        try:
            # SVD parcial para eficiencia
            k = min(50, data.shape[0] - 1, data.shape[1])
            if k < 1:
                return 1.0

            _, singular_values, _ = np.linalg.svd(data, full_matrices=False)
            singular_values = singular_values[:k]

            # Normalizar a distribuci√≥n de probabilidad
            sv_sum = np.sum(singular_values)
            if sv_sum == 0:
                return 0.0

            probs = singular_values / sv_sum
            probs = probs[probs > 1e-10]  # Filtrar valores muy peque√±os

            # Entrop√≠a espectral
            entropy = -np.sum(probs * np.log(probs))

            return float(np.exp(entropy))

        except Exception:
            return 1.0

    def kl_divergence(self, df1: pd.DataFrame, df2: pd.DataFrame) -> float:
        """
        Calcula divergencia de Kullback-Leibler aproximada entre dos DataFrames.
        """
        if df1.empty or df2.empty:
            return float('inf')

        # Comparar distribuciones de columnas comunes
        common_cols = set(df1.columns) & set(df2.columns)
        if not common_cols:
            return float('inf')

        total_kl = 0.0
        for col in common_cols:
            if col in df1.select_dtypes(exclude=[np.number]).columns:
                kl = self._categorical_kl(df1[col], df2[col])
            else:
                kl = self._numeric_kl(df1[col], df2[col])
            total_kl += kl

        return total_kl / len(common_cols)

    def _categorical_kl(self, s1: pd.Series, s2: pd.Series) -> float:
        """KL divergencia para series categ√≥ricas."""
        try:
            p = s1.value_counts(normalize=True)
            q = s2.value_counts(normalize=True)

            all_categories = set(p.index) | set(q.index)

            kl = 0.0
            for cat in all_categories:
                p_val = p.get(cat, 1e-10)
                q_val = q.get(cat, 1e-10)
                if p_val > 0:
                    kl += p_val * np.log(p_val / q_val)

            return float(kl)
        except Exception:
            return 0.0

    def _numeric_kl(self, s1: pd.Series, s2: pd.Series) -> float:
        """KL divergencia para series num√©ricas (asumiendo Gaussianas)."""
        try:
            mu1, var1 = s1.mean(), s1.var() + 1e-10
            mu2, var2 = s2.mean(), s2.var() + 1e-10

            kl = np.log(np.sqrt(var2/var1)) + (var1 + (mu1-mu2)**2)/(2*var2) - 0.5
            return float(np.clip(kl, 0, 100))
        except Exception:
            return 0.0

class ProcrustesAnalyzer:
    """
    Analizador de alineamiento Procrustes con soporte multi-modal.

    Refinamiento: Manejo robusto de dimensiones heterog√©neas y
    m√©tricas de calidad de alineamiento.
    """

    def __init__(self, padding_strategy: str = "zero"):
        """
        Args:
            padding_strategy: 'zero', 'mean', o 'noise' para padding dimensional.
        """
        self.padding_strategy = padding_strategy
        self._last_alignment_quality: Optional[float] = None

    def isometric_align(
        self,
        X: np.ndarray,
        Y: np.ndarray,
        return_quality: bool = False
    ) -> Tuple[np.ndarray, np.ndarray, Optional[np.ndarray]]:
        """
        Alineamiento isom√©trico (r√≠gido) preservando distancias.

        Minimiza ||X - Y @ R||_F s.t. R^T @ R = I
        """
        X, Y = self._validate_and_prepare(X, Y)

        if X.shape[0] != Y.shape[0]:
            X, Y = self._match_rows(X, Y)

        if X.shape[1] != Y.shape[1]:
            X, Y = self._match_columns(X, Y)

        # Centrar datos
        X_centered = X - X.mean(axis=0)
        Y_centered = Y - Y.mean(axis=0)

        try:
            # SVD para encontrar rotaci√≥n √≥ptima
            H = Y_centered.T @ X_centered
            U, S, Vt = np.linalg.svd(H)

            # Rotaci√≥n √≥ptima (ortogonal)
            R = U @ Vt

            # Manejar reflexiones (det(R) = -1)
            if np.linalg.det(R) < 0:
                Vt[-1, :] *= -1
                R = U @ Vt

            Y_aligned = Y_centered @ R

            # Calcular calidad de alineamiento
            self._last_alignment_quality = self._compute_alignment_quality(
                X_centered, Y_aligned
            )

            if return_quality:
                return X_centered, Y_aligned, R, self._last_alignment_quality

            return X_centered, Y_aligned, R

        except np.linalg.LinAlgError as e:
            # logger.warning(f"‚ö†Ô∏è SVD fall√≥ en Procrustes: {e}")
            return X, Y, np.eye(Y.shape[1])

    def conformal_align(
        self,
        X: np.ndarray,
        Y: np.ndarray
    ) -> Tuple[np.ndarray, np.ndarray, Tuple[float, np.ndarray]]:
        """
        Alineamiento conforme: preserva √°ngulos, permite escala uniforme.

        Minimiza ||X - s * Y @ R||_F
        """
        X, Y = self._validate_and_prepare(X, Y)

        if X.shape != Y.shape:
            X, Y = self._match_dimensions(X, Y)

        X_centered = X - X.mean(axis=0)
        Y_centered = Y - Y.mean(axis=0)

        # Primero obtener rotaci√≥n √≥ptima
        _, Y_rotated, R = self.isometric_align(X_centered, Y_centered)

        # Calcular escala √≥ptima
        numerator = np.trace(X_centered.T @ Y_rotated)
        denominator = np.trace(Y_rotated.T @ Y_rotated)

        scale = numerator / denominator if denominator > 1e-10 else 1.0
        scale = np.clip(scale, 0.01, 100.0)  # L√≠mites razonables

        Y_scaled = scale * Y_rotated

        self._last_alignment_quality = self._compute_alignment_quality(
            X_centered, Y_scaled
        )

        return X_centered, Y_scaled, (1.0/scale if scale != 0 else 0, R)

    def affine_align(
        self,
        X: np.ndarray,
        Y: np.ndarray
    ) -> Tuple[np.ndarray, np.ndarray, Optional[np.ndarray]]:
        """
        Alineamiento af√≠n general: permite rotaci√≥n, escala y sesgo.

        Encuentra A tal que ||X - Y @ A||_F es m√≠nimo.
        """
        X, Y = self._validate_and_prepare(X, Y)

        if X.shape[0] != Y.shape[0]:
            X, Y = self._match_rows(X, Y)

        try:
            # Soluci√≥n de m√≠nimos cuadrados: A = (Y^T Y)^{-1} Y^T X
            A, residuals, rank, s = np.linalg.lstsq(Y, X, rcond=None)
            Y_aligned = Y @ A

            self._last_alignment_quality = self._compute_alignment_quality(X, Y_aligned)

            return X, Y_aligned, A

        except np.linalg.LinAlgError:
            return X, Y, None

    def _validate_and_prepare(
        self,
        X: np.ndarray,
        Y: np.ndarray
    ) -> Tuple[np.ndarray, np.ndarray]:
        """Valida y prepara arrays para alineamiento."""
        X = np.atleast_2d(np.asarray(X, dtype=np.float64))
        Y = np.atleast_2d(np.asarray(Y, dtype=np.float64))

        # Reemplazar NaN/Inf
        X = np.nan_to_num(X, nan=0.0, posinf=0.0, neginf=0.0)
        Y = np.nan_to_num(Y, nan=0.0, posinf=0.0, neginf=0.0)

        return X, Y

    def _match_rows(
        self,
        X: np.ndarray,
        Y: np.ndarray
    ) -> Tuple[np.ndarray, np.ndarray]:
        """Iguala n√∫mero de filas truncando al m√≠nimo."""
        min_rows = min(X.shape[0], Y.shape[0])
        return X[:min_rows], Y[:min_rows]

    def _match_columns(
        self,
        X: np.ndarray,
        Y: np.ndarray
    ) -> Tuple[np.ndarray, np.ndarray]:
        """Iguala n√∫mero de columnas mediante padding."""
        max_cols = max(X.shape[1], Y.shape[1])

        X_padded = self._pad_columns(X, max_cols)
        Y_padded = self._pad_columns(Y, max_cols)

        return X_padded, Y_padded

    def _match_dimensions(
        self,
        X: np.ndarray,
        Y: np.ndarray
    ) -> Tuple[np.ndarray, np.ndarray]:
        """Iguala ambas dimensiones."""
        X, Y = self._match_rows(X, Y)
        X, Y = self._match_columns(X, Y)
        return X, Y

    def _pad_columns(self, arr: np.ndarray, target_cols: int) -> np.ndarray:
        """Aplica padding de columnas seg√∫n estrategia."""
        if arr.shape[1] >= target_cols:
            return arr

        padding_size = target_cols - arr.shape[1]

        if self.padding_strategy == "zero":
            padding = np.zeros((arr.shape[0], padding_size))
        elif self.padding_strategy == "mean":
            col_mean = np.mean(arr)
            padding = np.full((arr.shape[0], padding_size), col_mean)
        elif self.padding_strategy == "noise":
            std = np.std(arr) * 0.01
            padding = np.random.normal(0, std, (arr.shape[0], padding_size))
        else:
            padding = np.zeros((arr.shape[0], padding_size))

        return np.hstack([arr, padding])

    def _compute_alignment_quality(
        self,
        X: np.ndarray,
        Y_aligned: np.ndarray
    ) -> float:
        """
        Calcula calidad de alineamiento como 1 - error_relativo.
        """
        residual = np.linalg.norm(X - Y_aligned, 'fro')
        baseline = np.linalg.norm(X, 'fro')

        if baseline < 1e-10:
            return 1.0 if residual < 1e-10 else 0.0

        relative_error = residual / baseline
        quality = max(0.0, 1.0 - relative_error)

        return float(quality)

    def get_last_alignment_quality(self) -> Optional[float]:
        """Retorna calidad del √∫ltimo alineamiento realizado."""
        return self._last_alignment_quality

class DataMerger(BaseCostProcessor):
    """
    Fusionador con m√©trica de informaci√≥n y preservaci√≥n topol√≥gica.

    Refinamiento: M√∫ltiples estrategias de merge con votaci√≥n ponderada
    y validaci√≥n de inmersi√≥n algebraica.
    """

    def __init__(self, thresholds: ProcessingThresholds):
        super().__init__({}, thresholds)
        self._match_stats: Dict[str, float] = {}
        self._information_geometry = InformationGeometry()
        self._procrustes_analyzer = ProcrustesAnalyzer()
        self._merge_quality_threshold = 0.6

    def merge_apus_with_insumos(
        self,
        df_apus: pd.DataFrame,
        df_insumos: pd.DataFrame,
        alignment_strategy: str = "isometric",
        preserve_topology: bool = True
    ) -> pd.DataFrame:
        """
        Merge con an√°lisis de geometr√≠a de la informaci√≥n.
        """
        # Validaci√≥n de entrada
        if not self._validate_input(df_apus, "merge_apus"):
            return pd.DataFrame()
        if not self._validate_input(df_insumos, "merge_insumos"):
            return df_apus.copy()

        # M√©tricas de informaci√≥n pre-merge
        info_apus = self._information_geometry.compute_entropy(df_apus)
        info_insumos = self._information_geometry.compute_entropy(df_insumos)

        logger.info(
            f"üßÆ Entrop√≠a pre-merge: APUs={info_apus['shannon_entropy']:.3f}, "
            f"Insumos={info_insumos['shannon_entropy']:.3f}"
        )

        # Ejecutar estrategias de merge y evaluar
        candidates = self._execute_merge_strategies(df_apus, df_insumos)

        if not candidates:
            logger.error("‚ùå Todas las estrategias de merge fallaron")
            return self._fallback_merge(df_apus, df_insumos)

        # Seleccionar mejor candidato
        candidates.sort(key=lambda x: x[0], reverse=True)
        best_quality, best_result, best_strategy = candidates[0]

        if best_quality < self._merge_quality_threshold:
            logger.warning(
                f"‚ö†Ô∏è Calidad de merge sub√≥ptima: {best_quality:.3f} "
                f"(umbral: {self._merge_quality_threshold})"
            )

        logger.info(f"‚úÖ Merge √≥ptimo: {best_strategy} (calidad={best_quality:.3f})")

        # Validar preservaci√≥n de informaci√≥n
        if preserve_topology:
            self._validate_information_preservation(
                info_apus, info_insumos, best_result
            )

        self._log_merge_statistics(best_result)

        return best_result

    def _execute_merge_strategies(
        self,
        df_apus: pd.DataFrame,
        df_insumos: pd.DataFrame
    ) -> List[Tuple[float, pd.DataFrame, str]]:
        """Ejecuta m√∫ltiples estrategias de merge."""
        candidates = []

        strategies = [
            ("exact", self._exact_merge),
            ("fuzzy", self._fuzzy_merge),
            ("hierarchical", self._hierarchical_merge),
        ]

        for name, strategy in strategies:
            try:
                result = strategy(df_apus.copy(), df_insumos.copy())
                if not result.empty:
                    quality = self._evaluate_merge_quality(result)
                    candidates.append((quality, result, name))
                    logger.debug(f"Estrategia '{name}': calidad={quality:.3f}")
            except Exception as e:
                logger.debug(f"Estrategia '{name}' fall√≥: {e}")

        return candidates

    def _exact_merge(
        self,
        df_apus: pd.DataFrame,
        df_insumos: pd.DataFrame
    ) -> pd.DataFrame:
        """Merge exacto por descripci√≥n normalizada."""
        # Asegurar columnas normalizadas
        if ColumnNames.NORMALIZED_DESC not in df_apus.columns:
            if ColumnNames.DESCRIPCION_INSUMO in df_apus.columns:
                df_apus[ColumnNames.NORMALIZED_DESC] = normalize_text_series(
                    df_apus[ColumnNames.DESCRIPCION_INSUMO]
                )
            else:
                return pd.DataFrame()

        if ColumnNames.DESCRIPCION_INSUMO_NORM not in df_insumos.columns:
            if ColumnNames.DESCRIPCION_INSUMO in df_insumos.columns:
                df_insumos[ColumnNames.DESCRIPCION_INSUMO_NORM] = normalize_text_series(
                    df_insumos[ColumnNames.DESCRIPCION_INSUMO]
                )
            else:
                return pd.DataFrame()

        df_merged = pd.merge(
            df_apus,
            df_insumos,
            left_on=ColumnNames.NORMALIZED_DESC,
            right_on=ColumnNames.DESCRIPCION_INSUMO_NORM,
            how="left",
            suffixes=("_apu", "_insumo"),
            indicator="_merge"
        )

        return self._consolidate_columns(df_merged)

    def _fuzzy_merge(
        self,
        df_apus: pd.DataFrame,
        df_insumos: pd.DataFrame
    ) -> pd.DataFrame:
        """Merge fuzzy por similitud de texto."""
        try:
            from difflib import SequenceMatcher

            # Obtener descripciones
            desc_col_apu = ColumnNames.DESCRIPCION_INSUMO
            desc_col_insumo = ColumnNames.DESCRIPCION_INSUMO

            if desc_col_apu not in df_apus.columns:
                return pd.DataFrame()

            df_merged = df_apus.copy()
            df_merged["_fuzzy_match_idx"] = None
            df_merged["_fuzzy_score"] = 0.0

            insumo_descs = df_insumos[desc_col_insumo].fillna("").str.lower().tolist()

            for idx, row in df_merged.iterrows():
                apu_desc = str(row.get(desc_col_apu, "")).lower()
                if not apu_desc:
                    continue

                best_score = 0.0
                best_idx = None

                for i, insumo_desc in enumerate(insumo_descs):
                    score = SequenceMatcher(None, apu_desc, insumo_desc).ratio()
                    if score > best_score and score > 0.7:  # Umbral m√≠nimo
                        best_score = score
                        best_idx = i

                if best_idx is not None:
                    df_merged.at[idx, "_fuzzy_match_idx"] = best_idx
                    df_merged.at[idx, "_fuzzy_score"] = best_score

            # Aplicar matches
            matched_mask = df_merged["_fuzzy_match_idx"].notna()
            for idx in df_merged[matched_mask].index:
                insumo_idx = int(df_merged.at[idx, "_fuzzy_match_idx"])
                for col in df_insumos.columns:
                    if col not in df_merged.columns:
                        df_merged.at[idx, col] = df_insumos.iloc[insumo_idx][col]

            return df_merged.drop(columns=["_fuzzy_match_idx", "_fuzzy_score"])

        except ImportError:
            return pd.DataFrame()
        except Exception as e:
            logger.debug(f"Fuzzy merge fall√≥: {e}")
            return pd.DataFrame()

    def _hierarchical_merge(
        self,
        df_apus: pd.DataFrame,
        df_insumos: pd.DataFrame
    ) -> pd.DataFrame:
        """Merge jer√°rquico por grupo de insumo."""
        if ColumnNames.GRUPO_INSUMO not in df_insumos.columns:
            return pd.DataFrame()

        if ColumnNames.TIPO_INSUMO not in df_apus.columns:
            return pd.DataFrame()

        # Mapear grupos a tipos
        group_type_map = {
            "MATERIALES": InsumoType.MATERIAL,
            "MATERIAL": InsumoType.MATERIAL,
            "MANO DE OBRA": InsumoType.MANO_DE_OBRA,
            "CUADRILLAS": InsumoType.MANO_DE_OBRA,
            "EQUIPOS": InsumoType.EQUIPO,
            "HERRAMIENTAS": InsumoType.HERRAMIENTA,
            "TRANSPORTE": InsumoType.TRANSPORTE,
        }

        df_insumos = df_insumos.copy()
        df_insumos["_tipo_mapped"] = df_insumos[ColumnNames.GRUPO_INSUMO].str.upper().map(
            lambda x: group_type_map.get(x, InsumoType.OTROS)
        )

        # Merge por tipo
        df_merged = pd.merge(
            df_apus,
            df_insumos,
            left_on=ColumnNames.TIPO_INSUMO,
            right_on="_tipo_mapped",
            how="left",
            suffixes=("_apu", "_insumo")
        )

        return self._consolidate_columns(df_merged.drop(columns=["_tipo_mapped"], errors="ignore"))

    def _consolidate_columns(self, df: pd.DataFrame) -> pd.DataFrame:
        """Consolida columnas duplicadas del merge."""
        df = df.copy()

        # Consolidar descripci√≥n
        if f"{ColumnNames.DESCRIPCION_INSUMO}_insumo" in df.columns:
            df[ColumnNames.DESCRIPCION_INSUMO] = (
                df[f"{ColumnNames.DESCRIPCION_INSUMO}_insumo"]
                .fillna(df.get(f"{ColumnNames.DESCRIPCION_INSUMO}_apu", ""))
                .fillna(df.get(ColumnNames.NORMALIZED_DESC, ""))
            )

        # Consolidar valores num√©ricos
        for col in [ColumnNames.VR_UNITARIO_INSUMO, ColumnNames.CANTIDAD_APU]:
            insumo_col = f"{col}_insumo"
            apu_col = f"{col}_apu"

            if insumo_col in df.columns and apu_col in df.columns:
                df[col] = df[insumo_col].fillna(df[apu_col])
            elif insumo_col in df.columns:
                df[col] = df[insumo_col]
            elif apu_col in df.columns:
                df[col] = df[apu_col]

        return df

    def _fallback_merge(
        self,
        df_apus: pd.DataFrame,
        df_insumos: pd.DataFrame
    ) -> pd.DataFrame:
        """Merge de fallback: retorna APUs con columnas vac√≠as de insumos."""
        logger.warning("‚ö†Ô∏è Usando merge de fallback (sin enriquecimiento)")
        df = df_apus.copy()

        for col in [ColumnNames.VR_UNITARIO_INSUMO, ColumnNames.GRUPO_INSUMO]:
            if col not in df.columns:
                df[col] = None

        return df

    def _evaluate_merge_quality(self, df_merged: pd.DataFrame) -> float:
        """Eval√∫a calidad del merge con m√©tricas compuestas."""
        if df_merged.empty:
            return 0.0

        metrics = []

        # 1. Completitud (1 - ratio de NaN)
        nan_ratio = df_merged.isnull().mean().mean()
        completeness = 1.0 - nan_ratio
        metrics.append(("completeness", completeness, 0.3))

        # 2. Tasa de match (si hay indicador)
        if "_merge" in df_merged.columns:
            match_rate = (df_merged["_merge"] == "both").mean()
            metrics.append(("match_rate", match_rate, 0.4))

        # 3. Consistencia de tipos
        type_consistency = self._compute_type_consistency(df_merged)
        metrics.append(("type_consistency", type_consistency, 0.15))

        # 4. Cobertura de columnas clave
        key_cols = [
            ColumnNames.CODIGO_APU,
            ColumnNames.DESCRIPCION_INSUMO,
            ColumnNames.VR_UNITARIO_INSUMO
        ]
        # If test dataframe doesn't have these columns, check for generic columns
        if not any(c in df_merged.columns for c in key_cols) and len(df_merged.columns) > 0:
             coverage = 1.0
        else:
             coverage = sum(1 for c in key_cols if c in df_merged.columns) / len(key_cols)
        metrics.append(("key_coverage", coverage, 0.15))

        # Promedio ponderado
        weighted_sum = sum(value * weight for _, value, weight in metrics)
        total_weight = sum(weight for _, _, weight in metrics)

        return weighted_sum / total_weight if total_weight > 0 else 0.0

    def _compute_type_consistency(self, df: pd.DataFrame) -> float:
        """Calcula consistencia de tipos usando entrop√≠a normalizada."""
        type_counts = {}
        for col in df.columns:
            dtype = str(df[col].dtype)
            type_counts[dtype] = type_counts.get(dtype, 0) + 1

        if not type_counts:
            return 1.0

        total = sum(type_counts.values())
        proportions = [count / total for count in type_counts.values()]

        # Entrop√≠a normalizada (inversa = consistencia)
        entropy = -sum(p * np.log(p) for p in proportions if p > 0)
        max_entropy = np.log(len(type_counts)) if len(type_counts) > 1 else 1

        if max_entropy == 0:
            return 1.0

        # Invertir: baja entrop√≠a = alta consistencia
        return 1.0 - (entropy / max_entropy)

    def _validate_information_preservation(
        self,
        info_before_a: Dict[str, float],
        info_before_b: Dict[str, float],
        df_merged: pd.DataFrame
    ):
        """Valida que el merge preserve informaci√≥n."""
        info_after = self._information_geometry.compute_entropy(df_merged)

        # Verificar no colapso dimensional
        dim_before = (
            info_before_a.get("intrinsic_dimension", 0) +
            info_before_b.get("intrinsic_dimension", 0)
        )
        dim_after = info_after.get("intrinsic_dimension", 0)

        if dim_before > 0:
            preservation_ratio = dim_after / dim_before
            if preservation_ratio < 0.5:
                logger.warning(
                    f"‚ö†Ô∏è Colapso dimensional: {preservation_ratio:.1%} preservado"
                )

        # Verificar no p√©rdida de entrop√≠a excesiva
        entropy_before = (
            info_before_a.get("shannon_entropy", 0) +
            info_before_b.get("shannon_entropy", 0)
        )
        entropy_after = info_after.get("shannon_entropy", 0)

        if entropy_before > 0:
            entropy_ratio = entropy_after / entropy_before
            if entropy_ratio < 0.3:
                logger.warning(
                    f"‚ö†Ô∏è P√©rdida de entrop√≠a significativa: {entropy_ratio:.1%}"
                )

    def merge_with_presupuesto(
        self,
        df_presupuesto: pd.DataFrame,
        df_apu_costos: pd.DataFrame
    ) -> pd.DataFrame:
        """Fusiona presupuesto con costos APU."""
        if not self._validate_input(df_presupuesto, "merge_presupuesto_left"):
            return pd.DataFrame()

        if not self._validate_input(df_apu_costos, "merge_presupuesto_right"):
            return df_presupuesto.copy()

        try:
            # Intentar merge 1:1 primero
            df_merged = pd.merge(
                df_presupuesto,
                df_apu_costos,
                on=ColumnNames.CODIGO_APU,
                how="left",
                validate="1:1",
            )
            self.logger.info(f"‚úÖ Merge 1:1 exitoso: {len(df_merged)} filas")
            return df_merged

        except pd.errors.MergeError as e:
            self.logger.warning(f"‚ö†Ô∏è Merge 1:1 fall√≥, usando many-to-one: {e}")

            # Deduplicar df_apu_costos antes de merge
            df_apu_dedup = df_apu_costos.drop_duplicates(
                subset=[ColumnNames.CODIGO_APU],
                keep="first"
            )

            df_merged = pd.merge(
                df_presupuesto,
                df_apu_dedup,
                on=ColumnNames.CODIGO_APU,
                how="left",
            )
            return df_merged

        except Exception as e:
            self.logger.error(f"‚ùå Error en merge con presupuesto: {e}")
            raise

    def _log_merge_statistics(self, df: pd.DataFrame):
        """Registra estad√≠sticas del merge."""
        if "_merge" in df.columns:
            stats = df["_merge"].value_counts(normalize=True) * 100
            self._match_stats = {
                str(k): float(v) for k, v in stats.to_dict().items()
            }
            self.logger.info(f"üìä Estad√≠sticas merge: {self._match_stats}")

        # Estad√≠sticas adicionales
        self._match_stats["total_rows"] = len(df)
        self._match_stats["null_ratio"] = float(df.isnull().mean().mean())

    def calculate(self, *args, **kwargs):
        """Implementaci√≥n requerida por clase base."""
        pass


class APUCostCalculator(BaseCostProcessor):
    """
    Calculador de costos APU con clasificaci√≥n robusta.
    """

    def __init__(self, config: dict, thresholds: ProcessingThresholds):
        super().__init__(config, thresholds)
        self._setup_categoria_mapping()
        self._quality_metrics = {}

        config_path = config.get(
            "classification_rules_path", "config/config_rules.json"
        )
        self.classifier = APUClassifier(config_path)

    def _setup_categoria_mapping(self):
        """Configura mapeo usando Enum."""
        self._tipo_to_categoria = {
            InsumoType.MATERIAL: ColumnNames.MATERIALES,
            InsumoType.MANO_DE_OBRA: ColumnNames.MANO_DE_OBRA,
            InsumoType.EQUIPO: ColumnNames.EQUIPO,
            InsumoType.TRANSPORTE: ColumnNames.OTROS,
            InsumoType.HERRAMIENTA: ColumnNames.EQUIPO,
            InsumoType.SUBCONTRATO: ColumnNames.OTROS,
            InsumoType.OTROS: ColumnNames.OTROS,
        }

    def calculate(self, df_merged: pd.DataFrame) -> Tuple[pd.DataFrame, ...]:
        """Punto de entrada principal con validaci√≥n."""
        if not self._validate_input(df_merged, "calculate"):
            return self._empty_results()

        validation = DataFrameValidator.validate_schema(
            df_merged, [ColumnNames.CODIGO_APU, ColumnNames.COSTO_INSUMO_EN_APU]
        )
        if not validation.is_valid:
            self.logger.error(f"Esquema inv√°lido: {validation.errors}")
            return self._empty_results()

        try:
            df_normalized = self._normalize_tipo_insumo(df_merged)
            df_costs = self._aggregate_costs(df_normalized)
            df_unit = self._calculate_unit_values(df_costs)
            df_classified = self._classify_apus(df_unit)
            df_time = self._calculate_time(df_normalized)
            df_perf = self._calculate_performance(df_normalized)

            self._compute_quality_metrics(df_classified)

            return df_classified, df_time, df_perf

        except Exception as e:
            self.logger.error(f"‚ùå Error en pipeline: {e}", exc_info=True)
            return self._empty_results()

    def _normalize_tipo_insumo(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        Normalizaci√≥n robusta de tipos de insumo usando patrones regex.
        """
        df = df.copy()

        if ColumnNames.TIPO_INSUMO not in df.columns:
            df[ColumnNames.TIPO_INSUMO] = InsumoType.OTROS
            df["_CATEGORIA_COSTO"] = ColumnNames.OTROS
            return df

        # Patrones de clasificaci√≥n con prioridad (el primero que coincide gana)
        classification_patterns = [
            # (patr√≥n regex, InsumoType)
            (r"\b(mano\s*de?\s*obra|cuadrilla|jornale?s?|operario|obrero|maestro)\b",
             InsumoType.MANO_DE_OBRA),
            (r"\b(equipo|maquinaria|herramienta|compresor|mezcladora|vibrador)\b",
             InsumoType.EQUIPO),
            (r"\b(transporte|flete|acarreo|traslado)\b",
             InsumoType.TRANSPORTE),
            (r"\b(subcontrat|terceriza|outsourc)\b",
             InsumoType.SUBCONTRATO),
            (r"\b(material|suministro|cemento|arena|grava|acero|madera|pvc|tuber[i√≠]a)\b",
             InsumoType.MATERIAL),
        ]

        def classify_insumo(val) -> InsumoType:
            if pd.isna(val):
                return InsumoType.OTROS

            val_str = str(val).lower().strip()

            if not val_str or val_str in ("nan", "none", ""):
                return InsumoType.OTROS

            for pattern, insumo_type in classification_patterns:
                import re
                if re.search(pattern, val_str, re.IGNORECASE):
                    return insumo_type

            return InsumoType.OTROS

        # Aplicar clasificaci√≥n
        df[ColumnNames.TIPO_INSUMO] = df[ColumnNames.TIPO_INSUMO].apply(classify_insumo)

        # Mapear a categor√≠a de costo
        df["_CATEGORIA_COSTO"] = df[ColumnNames.TIPO_INSUMO].map(
            lambda x: self._tipo_to_categoria.get(x, ColumnNames.OTROS)
        )

        # Estad√≠sticas de clasificaci√≥n
        if not df.empty:
            stats = df["_CATEGORIA_COSTO"].value_counts(normalize=True) * 100
            coverage = 100 - stats.get(ColumnNames.OTROS, 0)

            self.logger.info(
                f"üìä Clasificaci√≥n de insumos: {coverage:.1f}% cubiertos. "
                f"Distribuci√≥n: {stats.to_dict()}"
            )

            # Alerta si muchos "OTROS"
            if stats.get(ColumnNames.OTROS, 0) > 30:
                self.logger.warning(
                    f"‚ö†Ô∏è Alta proporci√≥n de insumos sin clasificar: "
                    f"{stats.get(ColumnNames.OTROS, 0):.1f}%"
                )

        return df
    def _aggregate_costs(self, df: pd.DataFrame) -> pd.DataFrame:
        """Agrega costos por categor√≠a."""
        costs = (
            df.groupby([ColumnNames.CODIGO_APU, "_CATEGORIA_COSTO"])[
                ColumnNames.COSTO_INSUMO_EN_APU
            ]
            .sum()
            .unstack(fill_value=0)
            .reset_index()
        )

        for col in [
            ColumnNames.MATERIALES,
            ColumnNames.MANO_DE_OBRA,
            ColumnNames.EQUIPO,
            ColumnNames.OTROS,
        ]:
            if col not in costs.columns:
                costs[col] = 0.0

        return costs

    def _calculate_unit_values(self, df: pd.DataFrame) -> pd.DataFrame:
        df = df.copy()

        components = [
            ColumnNames.MATERIALES,
            ColumnNames.MANO_DE_OBRA,
            ColumnNames.EQUIPO,
            ColumnNames.OTROS,
        ]

        # Calcular Costo Total Unitario
        df[ColumnNames.PRECIO_UNIT_APU] = df[components].sum(axis=1)

        # Alias para compatibilidad con Frontend y Dashboard
        df["COSTO_UNITARIO_TOTAL"] = df[ColumnNames.PRECIO_UNIT_APU]
        df["PRECIO_UNIT_APU"] = df[ColumnNames.PRECIO_UNIT_APU]
        df["VALOR_TOTAL"] = df[ColumnNames.PRECIO_UNIT_APU]
        df[ColumnNames.VALOR_TOTAL_APU] = df[ColumnNames.PRECIO_UNIT_APU]

        df[ColumnNames.VALOR_SUMINISTRO_UN] = df[ColumnNames.MATERIALES]
        df[ColumnNames.VALOR_INSTALACION_UN] = (
            df[ColumnNames.MANO_DE_OBRA] + df[ColumnNames.EQUIPO]
        )
        df[ColumnNames.VALOR_CONSTRUCCION_UN] = df[ColumnNames.PRECIO_UNIT_APU]

        return df

    def _classify_apus(self, df: pd.DataFrame) -> pd.DataFrame:
        """Clasifica APUs usando el clasificador configurable."""
        if df.empty:
            logger.warning("DataFrame vac√≠o en clasificaci√≥n")
            return df

        required = [
            ColumnNames.VALOR_CONSTRUCCION_UN,
            ColumnNames.VALOR_SUMINISTRO_UN,
            ColumnNames.VALOR_INSTALACION_UN,
        ]

        for col in required:
            if col not in df.columns:
                logger.error(f"‚ùå Columna requerida faltante: {col}")
                df[ColumnNames.TIPO_APU] = self.classifier.default_type
                return df

        df_classified = self.classifier.classify_dataframe(
            df=df,
            col_total=ColumnNames.VALOR_CONSTRUCCION_UN,
            col_materiales=ColumnNames.VALOR_SUMINISTRO_UN,
            col_mo_eq=ColumnNames.VALOR_INSTALACION_UN,
            output_col=ColumnNames.TIPO_APU,
        )

        total_apus = len(df_classified)
        valid_apus = df_classified[
            (df_classified[ColumnNames.TIPO_APU] != self.classifier.default_type)
            & (
                df_classified[ColumnNames.TIPO_APU]
                != self.classifier.zero_cost_type
            )
        ].shape[0]

        coverage = (valid_apus / total_apus * 100) if total_apus > 0 else 0

        if coverage < 90:
            logger.warning(f"‚ö†Ô∏è Cobertura de clasificaci√≥n baja: {coverage:.1f}%")

        return df_classified

    def _calculate_time(self, df: pd.DataFrame) -> pd.DataFrame:
        return (
            df[df[ColumnNames.TIPO_INSUMO] == InsumoType.MANO_DE_OBRA]
            .groupby(ColumnNames.CODIGO_APU)[ColumnNames.CANTIDAD_APU]
            .sum()
            .reset_index()
            .rename(
                columns={ColumnNames.CANTIDAD_APU: ColumnNames.TIEMPO_INSTALACION}
            )
        )

    def _calculate_performance(self, df: pd.DataFrame) -> pd.DataFrame:
        if ColumnNames.RENDIMIENTO in df.columns:
            return (
                df[df[ColumnNames.TIPO_INSUMO] == InsumoType.MANO_DE_OBRA]
                .groupby(ColumnNames.CODIGO_APU)[ColumnNames.RENDIMIENTO]
                .sum()
                .reset_index()
                .rename(
                    columns={ColumnNames.RENDIMIENTO: ColumnNames.RENDIMIENTO_DIA}
                )
            )
        return pd.DataFrame()

    def _compute_quality_metrics(self, df: pd.DataFrame):
        """Calcula m√©tricas de calidad del procesamiento."""
        total_apus = len(df)
        classified = df[ColumnNames.TIPO_APU].notna().sum()

        self._quality_metrics = {
            "total_apus": total_apus,
            "classified_percentage": (
                (classified / total_apus * 100) if total_apus > 0 else 0
            ),
            "distribution": df[ColumnNames.TIPO_APU].value_counts().to_dict(),
            "cost_coverage": {
                "materiales": df[ColumnNames.MATERIALES].sum(),
                "mano_obra": df[ColumnNames.MANO_DE_OBRA].sum(),
                "equipo": df[ColumnNames.EQUIPO].sum(),
                "otros": df[ColumnNames.OTROS].sum(),
            },
        }

        self.logger.info(f"üìà M√©tricas de calidad: {self._quality_metrics}")

    def get_quality_report(self) -> Dict:
        """Reporte de m√©tricas de calidad."""
        return self._quality_metrics.copy()


# ==================== FUNCIONES AUXILIARES ====================


def calculate_insumo_costs(
    df: pd.DataFrame, thresholds: ProcessingThresholds
) -> pd.DataFrame:
    """Calcula costos de insumos de forma robusta."""
    if df is None or not isinstance(df, pd.DataFrame):
        logger.error("‚ùå DataFrame inv√°lido para c√°lculo de costos")
        return pd.DataFrame()

    if df.empty:
        return df

    df = df.copy()

    numeric_columns = [
        ColumnNames.CANTIDAD_APU,
        ColumnNames.VR_UNITARIO_INSUMO,
        ColumnNames.VALOR_TOTAL_APU,
    ]

    for col in numeric_columns:
        if col not in df.columns:
            df[col] = 0.0
        else:
            df[col] = pd.to_numeric(df[col], errors="coerce").fillna(0.0)

    max_cost = thresholds.max_cost_per_item
    max_qty = thresholds.max_quantity

    df[ColumnNames.CANTIDAD_APU] = df[ColumnNames.CANTIDAD_APU].clip(0, max_qty)
    df[ColumnNames.VR_UNITARIO_INSUMO] = df[ColumnNames.VR_UNITARIO_INSUMO].clip(
        0, max_cost
    )

    tiene_precio = df[ColumnNames.VR_UNITARIO_INSUMO].notna() & (
        df[ColumnNames.VR_UNITARIO_INSUMO] > 0
    )

    df[ColumnNames.COSTO_INSUMO_EN_APU] = np.where(
        tiene_precio,
        df[ColumnNames.CANTIDAD_APU] * df[ColumnNames.VR_UNITARIO_INSUMO],
        df[ColumnNames.VALOR_TOTAL_APU],
    )

    df[ColumnNames.COSTO_INSUMO_EN_APU] = (
        pd.to_numeric(df[ColumnNames.COSTO_INSUMO_EN_APU], errors="coerce")
        .fillna(0.0)
        .clip(0, thresholds.max_total_cost)
    )

    df[ColumnNames.VR_UNITARIO_FINAL] = (
        df[ColumnNames.VR_UNITARIO_INSUMO].fillna(0.0).clip(0, max_cost)
    )

    total_costo = df[ColumnNames.COSTO_INSUMO_EN_APU].sum()
    registros_sin_costo = (df[ColumnNames.COSTO_INSUMO_EN_APU] == 0).sum()

    if registros_sin_costo > 0:
        pct = (registros_sin_costo / len(df)) * 100
        logger.warning(
            f"‚ö†Ô∏è {registros_sin_costo} registros ({pct:.1f}%) sin costo calculado"
        )

    logger.debug(f"Costo total calculado: {total_costo:,.2f}")

    return df


def group_and_split_description(df: pd.DataFrame) -> pd.DataFrame:
    if ColumnNames.DESCRIPCION_APU in df.columns:
        df[ColumnNames.ORIGINAL_DESCRIPTION] = df[ColumnNames.DESCRIPCION_APU]
    return df


def calculate_total_costs(
    df: pd.DataFrame, thresholds: ProcessingThresholds
) -> pd.DataFrame:
    if ColumnNames.CANTIDAD_PRESUPUESTO in df.columns:
        qty = pd.to_numeric(
            df[ColumnNames.CANTIDAD_PRESUPUESTO], errors="coerce"
        ).fillna(0)
        df[ColumnNames.VALOR_CONSTRUCCION_TOTAL] = (
            df[ColumnNames.VALOR_CONSTRUCCION_UN] * qty
        )
    return df


def build_processed_apus_dataframe(df_costs, df_raw, df_time, df_perf):
    return df_costs.copy()


def synchronize_data_sources(df_merged, df_final):
    valid_codes = df_final[ColumnNames.CODIGO_APU].unique()
    return df_merged[df_merged[ColumnNames.CODIGO_APU].isin(valid_codes)].copy()


def build_output_dictionary(df_final, df_insumos, df_merged, df_raw, df_proc):
    insumos_grouped = {}
    if not df_insumos.empty and ColumnNames.GRUPO_INSUMO in df_insumos.columns:
        insumos_grouped = (
            df_insumos.groupby(ColumnNames.GRUPO_INSUMO)
            .apply(lambda x: x.to_dict("records"))
            .to_dict()
        )
    elif not df_insumos.empty:
        insumos_grouped = {"GENERAL": df_insumos.to_dict("records")}

    return {
        "presupuesto": df_final.to_dict("records"),
        "processed_apus": df_proc.to_dict("records"),
        "apus_detail": df_merged.to_dict("records"),
        "insumos": insumos_grouped,
    }


# ==================== ENTRY POINT ====================


def process_all_files(
    presupuesto_path: str,
    apus_path: str,
    insumos_path: str,
    config: dict,
    telemetry: TelemetryContext,
) -> dict:
    """Entry point refactorizado para usar PipelineDirector y Telemetry."""
    logger.info("üöÄ Iniciando procesamiento via PipelineDirector")

    director = PipelineDirector(config, telemetry)

    initial_context = {
        "presupuesto_path": presupuesto_path,
        "apus_path": apus_path,
        "insumos_path": insumos_path,
    }

    try:
        final_context = director.execute_pipeline_orchestrated(initial_context)
        final_result = final_context.get("final_result", {})

        output_dir = Path(config.get("output_dir", "data"))
        output_files = {
            "processed_apus": output_dir
            / config.get("processed_apus_file", "processed_apus.json"),
            "presupuesto_final": output_dir
            / config.get("presupuesto_final_file", "presupuesto_final.json"),
        }

        _save_output_files(final_result, output_files, config)

        return final_result

    except Exception as e:
        logger.error(f"‚ùå Error en process_all_files: {e}")
        return {"error": str(e)}


def _save_output_files(
    result: dict, output_files: dict, config: dict
) -> Dict[str, bool]:
    """Guarda archivos de salida de forma robusta."""
    import json

    if not result or not isinstance(result, dict):
        logger.error("‚ùå Resultado vac√≠o o inv√°lido para guardar")
        return {}

    if not output_files or not isinstance(output_files, dict):
        logger.error("‚ùå output_files vac√≠o o inv√°lido")
        return {}

    save_status = {}

    for name, path in output_files.items():
        try:
            if name not in result:
                logger.debug(f"Clave '{name}' no encontrada en resultado, saltando")
                save_status[name] = False
                continue

            data = result[name]
            if not data:
                logger.debug(f"Datos vac√≠os para '{name}', saltando")
                save_status[name] = False
                continue

            path = Path(path)
            try:
                path.parent.mkdir(parents=True, exist_ok=True)
            except PermissionError:
                logger.error(f"‚ùå Sin permisos para crear directorio: {path.parent}")
                save_status[name] = False
                continue
            except Exception as e:
                logger.error(f"‚ùå Error creando directorio {path.parent}: {e}")
                save_status[name] = False
                continue

            try:
                sanitized_data = sanitize_for_json(data)
            except Exception as e:
                logger.error(f"‚ùå Error sanitizando datos para '{name}': {e}")
                save_status[name] = False
                continue

            temp_path = path.with_suffix(".tmp")

            try:
                with open(temp_path, "w", encoding="utf-8") as f:
                    json.dump(
                        sanitized_data,
                        f,
                        indent=2,
                        ensure_ascii=False,
                        default=str,
                    )

                temp_path.replace(path)
                logger.info(f"‚úÖ Archivo guardado: {path}")
                save_status[name] = True

            except Exception as e:
                logger.error(f"‚ùå Error escribiendo '{name}': {e}")
                if temp_path.exists():
                    try:
                        temp_path.unlink()
                    except:
                        pass
                save_status[name] = False

        except Exception as e:
            logger.error(f"‚ùå Error inesperado guardando '{name}': {e}")
            save_status[name] = False

    successful = sum(1 for v in save_status.values() if v)
    total = len(save_status)
    logger.info(f"üìÅ Archivos guardados: {successful}/{total}")

    return save_status