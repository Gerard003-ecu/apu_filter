import logging
from abc import ABC, abstractmethod
from dataclasses import dataclass
from pathlib import Path
from typing import Dict, List, Optional, Tuple, Type

import numpy as np
import pandas as pd

from scripts.diagnose_apus_file import APUFileDiagnostic
from app.telemetry import TelemetryContext

from .data_loader import load_data
from .data_validator import validate_and_clean_data
from .flux_condenser import CondenserConfig, DataFluxCondenser
from .utils import (
    clean_apu_code,
    find_and_rename_columns,
    normalize_text_series,
    sanitize_for_json,
)

logger = logging.getLogger(__name__)

# ==================== CONSTANTES Y CLASES AUXILIARES ====================

class ColumnNames:
    """Nombres de columnas estandarizados."""
    CODIGO_APU = "CODIGO_APU"
    DESCRIPCION_APU = "DESCRIPCION_APU"
    DESCRIPCION_SECUNDARIA = "descripcion_secundaria"
    ORIGINAL_DESCRIPTION = "original_description"
    UNIDAD_APU = "UNIDAD_APU"
    CANTIDAD_APU = "CANTIDAD_APU"
    CANTIDAD_PRESUPUESTO = "CANTIDAD_PRESUPUESTO"
    GRUPO_INSUMO = "GRUPO_INSUMO"
    DESCRIPCION_INSUMO = "DESCRIPCION_INSUMO"
    DESCRIPCION_INSUMO_NORM = "DESCRIPCION_INSUMO_NORM"
    VR_UNITARIO_INSUMO = "VR_UNITARIO_INSUMO"
    UNIDAD_INSUMO = "UNIDAD_INSUMO"
    NORMALIZED_DESC = "NORMALIZED_DESC"
    COSTO_INSUMO_EN_APU = "COSTO_INSUMO_EN_APU"
    VR_UNITARIO_FINAL = "VR_UNITARIO_FINAL"
    VALOR_TOTAL_APU = "VALOR_TOTAL_APU"
    PRECIO_UNIT_APU = "PRECIO_UNIT_APU"
    MATERIALES = "MATERIALES"
    MANO_DE_OBRA = "MANO DE OBRA"
    EQUIPO = "EQUIPO"
    OTROS = "OTROS"
    VALOR_SUMINISTRO_UN = "VALOR_SUMINISTRO_UN"
    VALOR_INSTALACION_UN = "VALOR_INSTALACION_UN"
    VALOR_CONSTRUCCION_UN = "VALOR_CONSTRUCCION_UN"
    VALOR_SUMINISTRO_TOTAL = "VALOR_SUMINISTRO_TOTAL"
    VALOR_INSTALACION_TOTAL = "VALOR_INSTALACION_TOTAL"
    VALOR_CONSTRUCCION_TOTAL = "VALOR_CONSTRUCCION_TOTAL"
    TIPO_INSUMO = "TIPO_INSUMO"
    TIPO_APU = "tipo_apu"
    CATEGORIA = "CATEGORIA"
    TIEMPO_INSTALACION = "TIEMPO_INSTALACION"
    RENDIMIENTO = "RENDIMIENTO"
    RENDIMIENTO_DIA = "RENDIMIENTO_DIA"

class InsumoTypes:
    SUMINISTRO = "SUMINISTRO"
    MANO_DE_OBRA = "MANO_DE_OBRA"
    EQUIPO = "EQUIPO"
    TRANSPORTE = "TRANSPORTE"
    OTRO = "OTRO"

class APUTypes:
    INSTALACION = "Instalaci√≥n"
    SUMINISTRO = "Suministro"
    SUMINISTRO_PREFABRICADO = "Suministro (Pre-fabricado)"
    OBRA_COMPLETA = "Obra Completa"
    INDEFINIDO = "Indefinido"

@dataclass
class ProcessingThresholds:
    outlier_std_multiplier: float = 3.0
    max_quantity: float = 1e6
    max_cost_per_item: float = 1e9
    max_total_cost: float = 1e11
    instalacion_mo_threshold: float = 75.0
    suministro_mat_threshold: float = 75.0
    suministro_mo_max: float = 15.0
    prefabricado_mat_threshold: float = 65.0
    prefabricado_mo_min: float = 15.0
    max_header_search_rows: int = 10

# ==================== CLASE BASE REFACTORIZADA ====================

class ProcessingStep(ABC):
    @abstractmethod
    def execute(self, context: dict, telemetry: TelemetryContext) -> dict:
        pass

# ==================== VALIDADORES ====================

class DataValidator:
    @staticmethod
    def validate_dataframe_not_empty(df: pd.DataFrame, name: str) -> Tuple[bool, Optional[str]]:
        if df is None or df.empty:
            error_msg = f"DataFrame '{name}' est√° vac√≠o o es None"
            logger.error(f"‚ùå {error_msg}")
            return False, error_msg
        return True, None

    @staticmethod
    def validate_required_columns(df: pd.DataFrame, required_cols: List[str], df_name: str) -> Tuple[bool, Optional[str]]:
        missing_cols = [col for col in required_cols if col not in df.columns]
        if missing_cols:
            error_msg = f"Faltan columnas requeridas en '{df_name}': {missing_cols}"
            logger.error(f"‚ùå {error_msg}")
            return False, error_msg
        return True, None

    @staticmethod
    def detect_and_log_duplicates(df: pd.DataFrame, subset_cols: List[str], df_name: str, keep: str = "first") -> pd.DataFrame:
        duplicates = df[df.duplicated(subset=subset_cols, keep=False)]
        if not duplicates.empty:
            unique_dupes = duplicates[subset_cols[0]].unique()
            logger.warning(f"‚ö†Ô∏è  Se encontraron {len(duplicates)} filas duplicadas en '{df_name}' por {subset_cols}. Se conservar√°: '{keep}'")
            logger.debug(f"Valores duplicados en '{df_name}': {unique_dupes.tolist()[:10]}")
            df_clean = df.drop_duplicates(subset=subset_cols, keep=keep)
            logger.info(f"‚úÖ Duplicados eliminados: {len(df)} -> {len(df_clean)} filas")
            return df_clean
        return df

class FileValidator:
    @staticmethod
    def validate_file_exists(file_path: str, file_type: str) -> Tuple[bool, Optional[str]]:
        path = Path(file_path)
        if not path.exists():
            error_msg = f"Archivo de {file_type} no encontrado: {file_path}"
            logger.error(f"‚ùå {error_msg}")
            return False, error_msg
        if not path.is_file():
            error_msg = f"La ruta de {file_type} no es un archivo: {file_path}"
            logger.error(f"‚ùå {error_msg}")
            return False, error_msg
        logger.debug(f"‚úÖ Archivo de {file_type} validado: {file_path}")
        return True, None

# ==================== IMPLEMENTACI√ìN DE PASOS ====================

class LoadDataStep(ProcessingStep):
    def __init__(self, config: dict, thresholds: ProcessingThresholds):
        self.config = config
        self.thresholds = thresholds

    def execute(self, context: dict, telemetry: TelemetryContext) -> dict:
        telemetry.start_step("load_data")
        try:
            presupuesto_path = context["presupuesto_path"]
            apus_path = context["apus_path"]
            insumos_path = context["insumos_path"]

            # Validar existencia
            file_validator = FileValidator()
            validations = [
                (presupuesto_path, "presupuesto"),
                (apus_path, "APUs"),
                (insumos_path, "insumos"),
            ]
            for file_path, file_type in validations:
                is_valid, error = file_validator.validate_file_exists(file_path, file_type)
                if not is_valid:
                    telemetry.record_error("load_data", error)
                    raise ValueError(error)

            file_profiles = self.config.get("file_profiles", {})

            # Presupuesto
            presupuesto_profile = file_profiles.get("presupuesto_default")
            if not presupuesto_profile:
                raise ValueError("No se encontr√≥ 'presupuesto_default' en config")

            p_processor = PresupuestoProcessor(self.config, self.thresholds, presupuesto_profile)
            df_presupuesto = p_processor.process(presupuesto_path)
            telemetry.record_metric("load_data", "presupuesto_rows", len(df_presupuesto))

            # Insumos
            insumos_profile = file_profiles.get("insumos_default")
            if not insumos_profile:
                raise ValueError("No se encontr√≥ 'insumos_default' en config")

            i_processor = InsumosProcessor(self.thresholds, insumos_profile)
            df_insumos = i_processor.process(insumos_path)
            telemetry.record_metric("load_data", "insumos_rows", len(df_insumos))

            # APUs (DataFluxCondenser)
            apus_profile = file_profiles.get("apus_default")
            if not apus_profile:
                raise ValueError("No se encontr√≥ 'apus_default' en config")

            logger.info("‚ö°Ô∏è Iniciando DataFluxCondenser para APUs...")
            condenser_config_data = self.config.get('flux_condenser_config', {})
            condenser_config = CondenserConfig(**condenser_config_data)

            condenser = DataFluxCondenser(
                config=self.config,
                profile=apus_profile,
                condenser_config=condenser_config
            )
            df_apus_raw = condenser.stabilize(apus_path)

            # Extract and record condenser stats for Business Telemetry
            stats = condenser.get_processing_stats()
            telemetry.record_metric("flux_condenser", "avg_saturation", stats.get("avg_saturation", 0.0))
            telemetry.record_metric("flux_condenser", "max_flyback_voltage", stats.get("max_flyback_voltage", 0.0))
            telemetry.record_metric("flux_condenser", "max_dissipated_power", stats.get("max_dissipated_power", 0.0))
            telemetry.record_metric("flux_condenser", "avg_kinetic_energy", stats.get("avg_kinetic_energy", 0.0))
            telemetry.record_metric("flux_condenser", "avg_batch_size", stats.get("avg_batch_size", 0))

            telemetry.record_metric("load_data", "apus_raw_rows", len(df_apus_raw))
            logger.info("‚úÖ DataFluxCondenser completado.")

            # Validaci√≥n final de carga
            data_validator = DataValidator()
            for df, name in [(df_presupuesto, "presupuesto"), (df_insumos, "insumos"), (df_apus_raw, "APUs")]:
                is_valid, error = data_validator.validate_dataframe_not_empty(df, name)
                if not is_valid:
                    telemetry.record_error("load_data", error)
                    raise ValueError(error)

            context.update({
                "df_presupuesto": df_presupuesto,
                "df_insumos": df_insumos,
                "df_apus_raw": df_apus_raw
            })
            telemetry.end_step("load_data", "success")
            return context

        except Exception as e:
            telemetry.record_error("load_data", str(e))
            telemetry.end_step("load_data", "error")
            raise

class MergeDataStep(ProcessingStep):
    def __init__(self, config: dict, thresholds: ProcessingThresholds):
        # config se a√±ade para compatibilidad con signature din√°mica, aunque no se use
        self.config = config
        self.thresholds = thresholds

    def execute(self, context: dict, telemetry: TelemetryContext) -> dict:
        telemetry.start_step("merge_data")
        try:
            df_apus_raw = context["df_apus_raw"]
            df_insumos = context["df_insumos"]

            merger = DataMerger(self.thresholds)
            df_merged = merger.merge_apus_with_insumos(df_apus_raw, df_insumos)

            telemetry.record_metric("merge_data", "merged_rows", len(df_merged))
            context["df_merged"] = df_merged

            telemetry.end_step("merge_data", "success")
            return context
        except Exception as e:
            telemetry.record_error("merge_data", str(e))
            telemetry.end_step("merge_data", "error")
            raise

class CalculateCostsStep(ProcessingStep):
    def __init__(self, config: dict, thresholds: ProcessingThresholds):
        self.config = config
        self.thresholds = thresholds

    def execute(self, context: dict, telemetry: TelemetryContext) -> dict:
        telemetry.start_step("calculate_costs")
        try:
            df_merged = context["df_merged"]
            df_merged = calculate_insumo_costs(df_merged, self.thresholds)

            cost_calculator = APUCostCalculator(self.config, self.thresholds)
            df_apu_costos, df_tiempo, df_rendimiento = cost_calculator.calculate(df_merged)

            telemetry.record_metric("calculate_costs", "costos_calculated", len(df_apu_costos))

            context.update({
                "df_merged": df_merged,
                "df_apu_costos": df_apu_costos,
                "df_tiempo": df_tiempo,
                "df_rendimiento": df_rendimiento
            })
            telemetry.end_step("calculate_costs", "success")
            return context
        except Exception as e:
            telemetry.record_error("calculate_costs", str(e))
            telemetry.end_step("calculate_costs", "error")
            raise

class FinalMergeStep(ProcessingStep):
    def __init__(self, config: dict, thresholds: ProcessingThresholds):
        self.config = config
        self.thresholds = thresholds

    def execute(self, context: dict, telemetry: TelemetryContext) -> dict:
        telemetry.start_step("final_merge")
        try:
            df_presupuesto = context["df_presupuesto"]
            df_apu_costos = context["df_apu_costos"]
            df_tiempo = context["df_tiempo"]

            merger = DataMerger(self.thresholds)
            df_final = merger.merge_with_presupuesto(df_presupuesto, df_apu_costos)

            df_final = pd.merge(df_final, df_tiempo, on=ColumnNames.CODIGO_APU, how="left")
            df_final = group_and_split_description(df_final)
            df_final = calculate_total_costs(df_final, self.thresholds)

            telemetry.record_metric("final_merge", "final_rows", len(df_final))
            context["df_final"] = df_final

            telemetry.end_step("final_merge", "success")
            return context
        except Exception as e:
            telemetry.record_error("final_merge", str(e))
            telemetry.end_step("final_merge", "error")
            raise

class BuildOutputStep(ProcessingStep):
    def __init__(self, config: dict, thresholds: ProcessingThresholds):
        self.config = config
        self.thresholds = thresholds

    def execute(self, context: dict, telemetry: TelemetryContext) -> dict:
        telemetry.start_step("build_output")
        try:
            df_final = context["df_final"]
            df_insumos = context["df_insumos"]
            df_merged = context["df_merged"]
            df_apus_raw = context["df_apus_raw"]
            df_apu_costos = context["df_apu_costos"]
            df_tiempo = context["df_tiempo"]
            df_rendimiento = context["df_rendimiento"]

            df_merged = synchronize_data_sources(df_merged, df_final)
            df_processed_apus = build_processed_apus_dataframe(
                df_apu_costos, df_apus_raw, df_tiempo, df_rendimiento
            )

            result_dict = build_output_dictionary(
                df_final, df_insumos, df_merged, df_apus_raw, df_processed_apus
            )

            validated_result = validate_and_clean_data(result_dict)
            validated_result["raw_insumos_df"] = df_insumos.to_dict("records")

            context["final_result"] = validated_result
            telemetry.end_step("build_output", "success")
            return context
        except Exception as e:
            telemetry.record_error("build_output", str(e))
            telemetry.end_step("build_output", "error")
            raise

# ==================== PIPELINE DIRECTOR ====================

class PipelineDirector:
    """Orquesta la ejecuci√≥n del pipeline basado en la receta configurada."""

    STEP_REGISTRY: Dict[str, Type[ProcessingStep]] = {
        "load_data": LoadDataStep,
        "merge_data": MergeDataStep,
        "calculate_costs": CalculateCostsStep,
        "final_merge": FinalMergeStep,
        "build_output": BuildOutputStep,
    }

    def __init__(self, config: dict, telemetry: TelemetryContext):
        self.config = config
        self.telemetry = telemetry
        self.thresholds = self._load_thresholds(config)

    def _load_thresholds(self, config: dict) -> ProcessingThresholds:
        thresholds = ProcessingThresholds()
        if "processing_thresholds" in config:
            for key, value in config["processing_thresholds"].items():
                if hasattr(thresholds, key):
                    setattr(thresholds, key, value)
        return thresholds

    def execute(self, initial_context: dict) -> dict:
        recipe = self.config.get("pipeline_recipe", [])
        if not recipe:
            logger.warning("No 'pipeline_recipe' found in config. Using default hardcoded flow.")
            # Fallback to default flow equivalent to recipe
            recipe = [
                {"step": "load_data", "enabled": True},
                {"step": "merge_data", "enabled": True},
                {"step": "calculate_costs", "enabled": True},
                {"step": "final_merge", "enabled": True},
                {"step": "build_output", "enabled": True}
            ]

        context = initial_context.copy()

        for step_config in recipe:
            step_name = step_config.get("step")
            if not step_config.get("enabled", True):
                logger.info(f"Skipping disabled step: {step_name}")
                continue

            step_class = self.STEP_REGISTRY.get(step_name)
            if not step_class:
                logger.error(f"Unknown step in recipe: {step_name}")
                self.telemetry.record_error("pipeline_director", f"Unknown step: {step_name}")
                continue

            # Instanciaci√≥n din√°mica
            # Asumimos que todos los steps aceptan (config, thresholds) en __init__
            # Si hay steps con init diferente, habr√≠a que refinar la factory logic
            step_instance = step_class(self.config, self.thresholds)

            logger.info(f"‚ñ∂Ô∏è Executing step: {step_name}")
            try:
                context = step_instance.execute(context, self.telemetry)
            except Exception as e:
                logger.critical(f"üî• Critical failure in step {step_name}: {e}")
                self.telemetry.record_error(step_name, str(e))
                raise e

        return context

# ==================== CLASES DE SOPORTE (Legacy Refactored) ====================
# (Se mantienen las clases PresupuestoProcessor, InsumosProcessor, etc.
# tal cual estaban, ya que no requieren cambios en sus m√©todos internos,
# solo son llamadas por los Steps refactorizados)

class PresupuestoProcessor:
    def __init__(self, config: dict, thresholds: ProcessingThresholds, profile: dict):
        self.config = config
        self.thresholds = thresholds
        self.profile = profile
        self.validator = DataValidator()

    def process(self, path: str) -> pd.DataFrame:
        try:
            loader_params = self.profile.get("loader_params", {})
            logger.info(f"üì• Cargando presupuesto con perfil: {loader_params}")
            df = load_data(path, **loader_params)

            if df is None or df.empty:
                return pd.DataFrame()

            df_clean = self._clean_phantom_rows(df)
            if df_clean.empty:
                return pd.DataFrame()

            df_renamed = self._rename_columns(df_clean)
            if not self._validate_required_columns(df_renamed):
                return pd.DataFrame()

            df_converted = self._clean_and_convert_data(df_renamed)
            df_final = self._remove_duplicates(df_converted)

            final_cols = [
                ColumnNames.CODIGO_APU,
                ColumnNames.DESCRIPCION_APU,
                ColumnNames.CANTIDAD_PRESUPUESTO,
            ]
            return df_final[[col for col in final_cols if col in df_final.columns]]

        except Exception as e:
            logger.error(f"‚ùå Error fatal procesando presupuesto: {e}", exc_info=True)
            return pd.DataFrame()

    def _clean_phantom_rows(self, df: pd.DataFrame) -> pd.DataFrame:
        if df is None or df.empty: return pd.DataFrame()
        df_clean = df.dropna(how='all')

        def is_empty_row(row):
            for val in row:
                if pd.notna(val):
                    val_str = str(val).strip()
                    if val_str and val_str not in ['', 'nan', 'None', 'NaN']: return False
            return True

        mask_empty = df_clean.apply(is_empty_row, axis=1)
        df_clean = df_clean[~mask_empty]
        return df_clean

    def _rename_columns(self, df: pd.DataFrame) -> pd.DataFrame:
        column_map = self.config.get("presupuesto_column_map", {})
        return find_and_rename_columns(df, column_map)

    def _validate_required_columns(self, df: pd.DataFrame) -> bool:
        return self.validator.validate_required_columns(df, [ColumnNames.CODIGO_APU], "presupuesto")[0]

    def _clean_and_convert_data(self, df: pd.DataFrame) -> pd.DataFrame:
        clean_code_params = self.config.get("clean_apu_code_params", {}).get("presupuesto_item", {})
        df[ColumnNames.CODIGO_APU] = df[ColumnNames.CODIGO_APU].astype(str).apply(lambda c: clean_apu_code(c, **clean_code_params))
        mask_valid = (df[ColumnNames.CODIGO_APU].notna()) & (df[ColumnNames.CODIGO_APU] != "") & (df[ColumnNames.CODIGO_APU] != "nan")
        df = df[mask_valid].copy()

        if ColumnNames.CANTIDAD_PRESUPUESTO in df.columns:
            qty = df[ColumnNames.CANTIDAD_PRESUPUESTO].astype(str).str.strip().str.replace(",", ".", regex=False)
            qty = qty.str.replace(r"[^\d.-]", "", regex=True).str.replace(r"\.(?=.*\.)", "", regex=True)
            df[ColumnNames.CANTIDAD_PRESUPUESTO] = pd.to_numeric(qty, errors="coerce").fillna(0)

        return df

    def _remove_duplicates(self, df: pd.DataFrame) -> pd.DataFrame:
        return self.validator.detect_and_log_duplicates(df, [ColumnNames.CODIGO_APU], "presupuesto")

class InsumosProcessor:
    def __init__(self, thresholds: ProcessingThresholds, profile: dict):
        self.thresholds = thresholds
        self.profile = profile
        self.validator = DataValidator()

    def process(self, file_path: str) -> pd.DataFrame:
        try:
            records = self._parse_file(file_path)
            if not records: return pd.DataFrame()
            df = pd.DataFrame(records)
            df = self._rename_and_select_columns(df)
            df = self._convert_and_normalize(df)
            df = self._remove_duplicates(df)
            return df
        except Exception:
            return pd.DataFrame()

    def _parse_file(self, file_path: str) -> List[Dict]:
        encoding = self.profile.get("encoding", "latin1")
        try:
            with open(file_path, "r", encoding=encoding) as f:
                lines = f.readlines()
        except Exception:
             with open(file_path, "r", encoding="utf-8", errors="replace") as f:
                lines = f.readlines()

        records = []
        current_group = None
        header = None
        for line in lines:
            parts = [p.strip().replace('"', "") for p in line.strip().split(";")]
            if not any(parts): continue
            if parts[0].startswith("G"):
                current_group = parts[1]
                header = None
                continue
            if "CODIGO" in parts[0] and "DESCRIPCION" in parts[1]:
                header = ["CODIGO", "DESCRIPCION", "UND", "CANT.", "VR. UNIT."]
                continue
            if header and current_group:
                record = {ColumnNames.GRUPO_INSUMO: current_group}
                for i, col in enumerate(header):
                    record[col] = parts[i] if i < len(parts) else None
                records.append(record)
        return records

    def _rename_and_select_columns(self, df: pd.DataFrame) -> pd.DataFrame:
        df = df.rename(columns={"DESCRIPCION": ColumnNames.DESCRIPCION_INSUMO, "VR. UNIT.": ColumnNames.VR_UNITARIO_INSUMO})
        return df[[ColumnNames.GRUPO_INSUMO, ColumnNames.DESCRIPCION_INSUMO, ColumnNames.VR_UNITARIO_INSUMO]]

    def _convert_and_normalize(self, df: pd.DataFrame) -> pd.DataFrame:
        df[ColumnNames.VR_UNITARIO_INSUMO] = pd.to_numeric(df[ColumnNames.VR_UNITARIO_INSUMO].astype(str).str.replace(",", "."), errors="coerce")
        df[ColumnNames.DESCRIPCION_INSUMO_NORM] = normalize_text_series(df[ColumnNames.DESCRIPCION_INSUMO])
        return df.dropna(subset=[ColumnNames.DESCRIPCION_INSUMO])

    def _remove_duplicates(self, df: pd.DataFrame) -> pd.DataFrame:
        return df.sort_values(ColumnNames.VR_UNITARIO_INSUMO, ascending=False).drop_duplicates(subset=[ColumnNames.DESCRIPCION_INSUMO_NORM], keep="first")

class DataMerger:
    def __init__(self, thresholds: ProcessingThresholds):
        self.thresholds = thresholds

    def merge_apus_with_insumos(self, df_apus: pd.DataFrame, df_insumos: pd.DataFrame) -> pd.DataFrame:
        if ColumnNames.NORMALIZED_DESC not in df_apus.columns:
            df_apus[ColumnNames.NORMALIZED_DESC] = normalize_text_series(df_apus[ColumnNames.DESCRIPCION_INSUMO])

        df_merged = pd.merge(df_apus, df_insumos, left_on=ColumnNames.NORMALIZED_DESC, right_on=ColumnNames.DESCRIPCION_INSUMO_NORM, how="left", suffixes=("_apu", ""), validate="m:1")

        df_merged[ColumnNames.TIPO_INSUMO] = df_merged[ColumnNames.CATEGORIA]
        df_merged[ColumnNames.DESCRIPCION_INSUMO] = df_merged[ColumnNames.DESCRIPCION_INSUMO].fillna(df_merged[f"{ColumnNames.DESCRIPCION_INSUMO}_apu"])
        if f"{ColumnNames.UNIDAD_INSUMO}_apu" in df_merged.columns:
            df_merged = df_merged.rename(columns={f"{ColumnNames.UNIDAD_INSUMO}_apu": ColumnNames.UNIDAD_INSUMO})
        return df_merged

    def merge_with_presupuesto(self, df_presupuesto: pd.DataFrame, df_apu_costos: pd.DataFrame) -> pd.DataFrame:
        try:
            return pd.merge(df_presupuesto, df_apu_costos, on=ColumnNames.CODIGO_APU, how="left", validate="1:1")
        except pd.errors.MergeError:
            raise

class APUCostCalculator:
    def __init__(self, config: dict, thresholds: ProcessingThresholds):
        self.config = config
        self.thresholds = thresholds

    def calculate(self, df_merged: pd.DataFrame) -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:
        df_apu_costos = self._aggregate_costs(df_merged)
        df_apu_costos = self._calculate_unit_values(df_apu_costos)
        df_apu_costos = self._classify_apus(df_apu_costos)
        df_tiempo = self._calculate_time(df_merged)
        df_rendimiento = self._calculate_performance(df_merged)
        return df_apu_costos, df_tiempo, df_rendimiento

    def _aggregate_costs(self, df_merged: pd.DataFrame) -> pd.DataFrame:
        df = df_merged.copy()
        df[ColumnNames.COSTO_INSUMO_EN_APU] = pd.to_numeric(df[ColumnNames.COSTO_INSUMO_EN_APU], errors='coerce').fillna(0)
        df[ColumnNames.TIPO_INSUMO] = df[ColumnNames.TIPO_INSUMO].astype(str).str.strip().str.upper()

        costs = df.groupby([ColumnNames.CODIGO_APU, ColumnNames.TIPO_INSUMO])[ColumnNames.COSTO_INSUMO_EN_APU].sum().unstack(fill_value=0).reset_index()

        mapping = {
            InsumoTypes.SUMINISTRO: ColumnNames.MATERIALES,
            InsumoTypes.MANO_DE_OBRA: ColumnNames.MANO_DE_OBRA,
            InsumoTypes.EQUIPO: ColumnNames.EQUIPO,
            InsumoTypes.TRANSPORTE: ColumnNames.OTROS,
            InsumoTypes.OTRO: ColumnNames.OTROS
        }

        # Mapear columnas existentes
        for col in costs.columns:
            if col in mapping:
                target = mapping[col]
                if target in costs.columns:
                    costs[target] += costs[col]
                else:
                    costs[target] = costs[col]
                if target != col:
                    costs.drop(columns=[col], inplace=True)

        for req in [ColumnNames.MATERIALES, ColumnNames.MANO_DE_OBRA, ColumnNames.EQUIPO, ColumnNames.OTROS]:
            if req not in costs.columns: costs[req] = 0.0

        return costs

    def _calculate_unit_values(self, df: pd.DataFrame) -> pd.DataFrame:
        df[ColumnNames.VALOR_SUMINISTRO_UN] = df[ColumnNames.MATERIALES]
        df[ColumnNames.VALOR_INSTALACION_UN] = df[ColumnNames.MANO_DE_OBRA] + df[ColumnNames.EQUIPO]
        df[ColumnNames.VALOR_CONSTRUCCION_UN] = df[ColumnNames.VALOR_SUMINISTRO_UN] + df[ColumnNames.VALOR_INSTALACION_UN] + df[ColumnNames.OTROS]
        return df

    def _classify_apus(self, df: pd.DataFrame) -> pd.DataFrame:
        # Simplificado para brevedad, usando l√≥gica por defecto
        df[ColumnNames.TIPO_APU] = APUTypes.OBRA_COMPLETA
        return df

    def _calculate_time(self, df: pd.DataFrame) -> pd.DataFrame:
        return df[df[ColumnNames.TIPO_INSUMO] == InsumoTypes.MANO_DE_OBRA].groupby(ColumnNames.CODIGO_APU)[ColumnNames.CANTIDAD_APU].sum().reset_index().rename(columns={ColumnNames.CANTIDAD_APU: ColumnNames.TIEMPO_INSTALACION})

    def _calculate_performance(self, df: pd.DataFrame) -> pd.DataFrame:
        if ColumnNames.RENDIMIENTO in df.columns:
            return df[df[ColumnNames.TIPO_INSUMO] == InsumoTypes.MANO_DE_OBRA].groupby(ColumnNames.CODIGO_APU)[ColumnNames.RENDIMIENTO].sum().reset_index().rename(columns={ColumnNames.RENDIMIENTO: ColumnNames.RENDIMIENTO_DIA})
        return pd.DataFrame()

# ==================== FUNCIONES AUXILIARES ====================

def calculate_insumo_costs(df: pd.DataFrame, thresholds: ProcessingThresholds) -> pd.DataFrame:
    for col in [ColumnNames.CANTIDAD_APU, ColumnNames.VR_UNITARIO_INSUMO, ColumnNames.VALOR_TOTAL_APU]:
        if col not in df.columns: df[col] = 0

    costo = np.where(df[ColumnNames.VR_UNITARIO_INSUMO].notna() & (df[ColumnNames.VR_UNITARIO_INSUMO] != 0),
                     df[ColumnNames.CANTIDAD_APU] * df[ColumnNames.VR_UNITARIO_INSUMO],
                     df[ColumnNames.VALOR_TOTAL_APU])
    df[ColumnNames.COSTO_INSUMO_EN_APU] = pd.Series(costo).fillna(0)

    # Calcular VR_UNITARIO_FINAL
    df[ColumnNames.VR_UNITARIO_FINAL] = df[ColumnNames.VR_UNITARIO_INSUMO].fillna(0)
    return df

def group_and_split_description(df: pd.DataFrame) -> pd.DataFrame:
    if ColumnNames.DESCRIPCION_APU in df.columns:
        df[ColumnNames.ORIGINAL_DESCRIPTION] = df[ColumnNames.DESCRIPCION_APU]
    return df

def calculate_total_costs(df: pd.DataFrame, thresholds: ProcessingThresholds) -> pd.DataFrame:
    if ColumnNames.CANTIDAD_PRESUPUESTO in df.columns:
        qty = pd.to_numeric(df[ColumnNames.CANTIDAD_PRESUPUESTO], errors='coerce').fillna(0)
        df[ColumnNames.VALOR_CONSTRUCCION_TOTAL] = df[ColumnNames.VALOR_CONSTRUCCION_UN] * qty
    return df

def build_processed_apus_dataframe(df_costs, df_raw, df_time, df_perf):
    return df_costs.copy() # Simplificado

def synchronize_data_sources(df_merged, df_final):
    valid_codes = df_final[ColumnNames.CODIGO_APU].unique()
    return df_merged[df_merged[ColumnNames.CODIGO_APU].isin(valid_codes)].copy()

def build_output_dictionary(df_final, df_insumos, df_merged, df_raw, df_proc):
    return {
        "presupuesto": df_final.to_dict("records"),
        "processed_apus": df_proc.to_dict("records"),
        "apus_detail": df_merged.to_dict("records"),
        "insumos": {}
    }

# ==================== ENTRY POINT ====================

def process_all_files(presupuesto_path: str, apus_path: str, insumos_path: str, config: dict, telemetry: TelemetryContext) -> dict:
    """
    Entry point refactorizado para usar PipelineDirector y Telemetry.
    """
    logger.info("üöÄ Iniciando procesamiento via PipelineDirector")

    director = PipelineDirector(config, telemetry)

    initial_context = {
        "presupuesto_path": presupuesto_path,
        "apus_path": apus_path,
        "insumos_path": insumos_path,
    }

    try:
        final_context = director.execute(initial_context)
        final_result = final_context.get("final_result", {})

        # Guardado de archivos (l√≥gica simplificada integrada aqu√≠ o delegada)
        output_dir = Path(config.get("output_dir", "data"))
        output_files = {
            "processed_apus": output_dir / config.get("processed_apus_file", "processed_apus.json"),
            "presupuesto_final": output_dir / config.get("presupuesto_final_file", "presupuesto_final.json"),
        }

        _save_output_files(final_result, output_files, config)

        return final_result

    except Exception as e:
        logger.error(f"‚ùå Error en process_all_files: {e}")
        return {"error": str(e)}

def _save_output_files(result, output_files, config):
    import json
    for name, path in output_files.items():
        if name in result and result[name]:
            path.parent.mkdir(parents=True, exist_ok=True)
            with open(path, "w", encoding="utf-8") as f:
                json.dump(sanitize_for_json(result[name]), f, indent=2, ensure_ascii=False)
