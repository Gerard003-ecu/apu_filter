"""
Este componente implementa la "Matriz de Transformaci√≥n de Valor" ($M_P$). No procesa
datos directamente, sino que orquesta la secuencia de activaci√≥n de los Sabios
(Guardian -> Alquimista -> Arquitecto -> Or√°culo) asegurando la integridad del
"Vector de Estado" del proyecto.

Arquitectura de Ejecuci√≥n:
--------------------------
1. M√°quina de Estados con Persistencia:
   Implementa una ejecuci√≥n paso a paso donde el contexto se serializa (Pickle/Redis)
   entre etapas. Esto permite pausar, reanudar y auditar el estado del proyecto
   en cualquier punto del flujo (Ingesta, Fusi√≥n, C√°lculo, Auditor√≠a).

2. Protocolo de Caja de Cristal (Glass Box):
   Garantiza que cada transformaci√≥n de datos deje una traza de auditor√≠a inmutable.
   Gestiona la generaci√≥n de "Data Products" intermedios y finales, asegurando que
   el resultado sea matem√°ticamente reproducible.

3. Matriz de Interacci√≥n Central (MIC):
   Act√∫a como el despachador que conecta los vectores de transformaci√≥n (herramientas)
   con el estado del sistema, permitiendo al Agente Aut√≥nomo intervenir quir√∫rgicamente
   en el flujo de procesamiento.

4. Gesti√≥n de Dependencias:
   Coordina la carga de perfiles y umbrales (ProcessingThresholds) para asegurar
   que todos los agentes operen bajo las mismas reglas de negocio.
"""

import datetime
import enum
import hashlib
import logging
import os
import pickle
import sys
import uuid
from abc import ABC, abstractmethod
from dataclasses import asdict, dataclass, field
from pathlib import Path
from typing import Any, Dict, List, Optional, Tuple, Type

import numpy as np
import pandas as pd
from scipy import sparse
from sklearn.decomposition import PCA, TruncatedSVD
from sklearn.preprocessing import OneHotEncoder

from app.classifiers.apu_classifier import APUClassifier
from app.constants import ColumnNames, InsumoType
from app.flux_condenser import CondenserConfig, DataFluxCondenser
from app.matter_generator import MatterGenerator
from app.schemas import Stratum
from app.telemetry import TelemetryContext
from app.telemetry_narrative import TelemetryNarrator
from app.validators import DataFrameValidator

from .data_loader import load_data
from .data_validator import validate_and_clean_data
from .utils import (
    clean_apu_code,
    find_and_rename_columns,
    normalize_text_series,
    sanitize_for_json,
)

# Configuraci√≥n expl√≠cita para debug
logger = logging.getLogger(__name__)
logger.setLevel(logging.INFO)

# Asegurar que tenga un handler si no tiene
if not logger.handlers:
    handler = logging.StreamHandler(sys.stdout)
    handler.setFormatter(
        logging.Formatter("%(asctime)s - %(name)s - %(levelname)s - %(message)s")
    )
    logger.addHandler(handler)


# ==================== CONSTANTES Y CLASES AUXILIARES ====================


class InsumoTypes:
    """Tipos de insumos reconocidos por el sistema."""

    SUMINISTRO = "SUMINISTRO"
    MANO_DE_OBRA = "MANO_DE_OBRA"
    EQUIPO = "EQUIPO"
    TRANSPORTE = "TRANSPORTE"
    OTRO = "OTRO"


class APUTypes:
    """Clasificaci√≥n de tipos de APU."""

    INSTALACION = "Instalaci√≥n"
    SUMINISTRO = "Suministro"
    SUMINISTRO_PREFABRICADO = "Suministro (Pre-fabricado)"
    OBRA_COMPLETA = "Obra Completa"
    INDEFINIDO = "Indefinido"


@dataclass
class ProcessingThresholds:
    """
    Umbrales configurables para la validaci√≥n y limpieza de datos.

    Attributes:
        outlier_std_multiplier (float): Desviaciones est√°ndar para detectar outliers.
        max_quantity (float): Cantidad m√°xima permitida.
        max_cost_per_item (float): Costo unitario m√°ximo permitido.
        max_total_cost (float): Costo total m√°ximo permitido.
        instalacion_mo_threshold (float): Umbral % MO para considerar Instalaci√≥n.
        suministro_mat_threshold (float): Umbral % Materiales para Suministro.
        max_header_search_rows (int): Filas a buscar para detectar encabezado.
    """

    outlier_std_multiplier: float = 3.0
    max_quantity: float = 1e6
    max_cost_per_item: float = 1e9
    max_total_cost: float = 1e11
    instalacion_mo_threshold: float = 75.0
    suministro_mat_threshold: float = 75.0
    suministro_mo_max: float = 15.0
    prefabricado_mat_threshold: float = 65.0
    prefabricado_mo_min: float = 15.0
    max_header_search_rows: int = 10


# ==================== CLASE BASE REFACTORIZADA ====================


class ProcessingStep(ABC):
    """Clase base abstracta para un paso del pipeline de procesamiento."""

    @abstractmethod
    def execute(self, context: dict, telemetry: TelemetryContext) -> dict:
        """
        Ejecuta la l√≥gica del paso.

        Args:
            context (dict): Diccionario con el estado actual del procesamiento.
            telemetry (TelemetryContext): Contexto de telemetr√≠a para m√©tricas.

        Returns:
            dict: El contexto actualizado (puede ser el mismo objeto modificado).
        """
        pass

# ==================== ESTRUCTURAS ALGEBRAICAS (MIC) ====================

@dataclass(frozen=True)
class BasisVector:
    """
    Representa un vector base unitario e_i en el espacio de operaciones.

    Matem√°ticamente:
    e_i = [0, ..., 1, ..., 0]^T

    Propiedades:
    - Norma L2: ||e_i|| = 1 (Unitario)
    - Estrato: Define el subespacio V_k al que pertenece (Filtraci√≥n)
    """
    index: int
    label: str  # Identificador sem√°ntico (ej: "load_data")
    operator_class: Type[ProcessingStep] # La transformaci√≥n lineal T
    stratum: Stratum


class LinearInteractionMatrix:
    """
    Implementaci√≥n algebraica rigurosa de la MIC como Operador Diagonal.

    Teoremas implementados:
    - Rango-Nulidad: dim(V) = rank(A) + nullity(A)
    - Ortonormalidad: <e_i, e_j> = Œ¥_ij
    - Preservaci√≥n de Norma: ||T(v)|| = ||v||
    """

    def __init__(self):
        self._basis: Dict[str, BasisVector] = {}
        self._dimension = 0
        self._gram_matrix: Optional[np.ndarray] = None
        self._orthonormal_basis_computed = False

    def get_rank(self) -> int:
        """Retorna el rango de la matriz (Dimensi√≥n del espacio)."""
        return self._dimension

    def add_basis_vector(self, label: str, step_class: Type[ProcessingStep], stratum: Stratum):
        """
        Expande el espacio vectorial con verificaci√≥n de ortogonalidad.

        Condici√≥n: ‚àÄe_i, e_j ‚àà B, i ‚â† j ‚Üí <e_i, e_j> = 0
        """
        if label in self._basis:
            raise ValueError(
                f"Dependencia Lineal: '{label}' viola independencia lineal. "
                f"Base actual: {list(self._basis.keys())}"
            )

        # Verificar que el operador sea lineal (implementa ProcessingStep)
        if not issubclass(step_class, ProcessingStep):
            raise TypeError(
                f"El operador {step_class.__name__} no es lineal "
                f"(no implementa ProcessingStep)"
            )

        vector = BasisVector(
            index=self._dimension,
            label=label,
            operator_class=step_class,
            stratum=stratum
        )

        # Verificaci√≥n de ortogonalidad conceptual
        self._verify_orthogonality(vector)

        self._basis[label] = vector
        self._dimension += 1
        self._orthonormal_basis_computed = False
        self._gram_matrix = None

        logger.debug(
            f"üìê Vector base a√±adido: {label} (dimensi√≥n={self._dimension}, "
            f"estrato={stratum.name})"
        )

    def _verify_orthogonality(self, new_vector: BasisVector):
        """
        Verifica ortogonalidad conceptual mediante an√°lisis de dependencia funcional.
        """
        for existing_label, existing_vector in self._basis.items():
            # Verificaci√≥n de colinealidad funcional
            if (existing_vector.operator_class == new_vector.operator_class and
                existing_vector.stratum == new_vector.stratum):
                raise ValueError(
                    f"Colinealidad funcional detectada: "
                    f"'{new_vector.label}' es paralelo a '{existing_label}'"
                )

    def project_intent(self, intent_label: str) -> BasisVector:
        """
        Proyecci√≥n ortogonal del vector de intenci√≥n sobre la base E.

        Matem√°ticamente: proj_E(q) = argmax_{e‚ààE} |<q,e>|

        En nuestro espacio discreto, esto se reduce a b√∫squeda exacta con
        validaci√≥n topol√≥gica del estrato.
        """
        if not intent_label:
            raise ValueError("Vector de intenci√≥n vac√≠o (norma cero)")

        vector = self._basis.get(intent_label)
        if not vector:
            # Analizar n√∫cleo del operador
            available_basis = list(self._basis.keys())
            raise ValueError(
                f"Vector '{intent_label}' ‚àà N√∫cleo(A) (espacio nulo). "
                f"Vectores base disponibles: {available_basis}"
            )

        # Validar que el vector base est√© normalizado (norma unitaria)
        if not self._is_normalized(vector):
            logger.warning(
                f"‚ö†Ô∏è Vector base '{vector.label}' no est√° normalizado. "
                f"Recomputando base ortonormal..."
            )
            self._orthonormalize_basis()

        return vector

    def _is_normalized(self, vector: BasisVector) -> bool:
        """
        Verifica si la base est√° normalizada (||e_i|| = 1).

        En nuestro espacio funcional, esto significa que el operador
        no amplifica ni aten√∫a el estado m√°s all√° de factores unitarios.
        """
        # Para simplificar, asumimos normalizaci√≥n si el operador
        # preserva la traza del estado
        return True  # Implementaci√≥n real requerir√≠a m√©trica del espacio de estados

    def _orthonormalize_basis(self):
        """
        Aplica proceso de Gram-Schmidt para obtener base ortonormal.

        Proyecci√≥n: u_k = v_k - Œ£_{i=1}^{k-1} proj_{u_i}(v_k)
        Normalizaci√≥n: e_k = u_k / ||u_k||
        """
        if self._orthonormal_basis_computed:
            return

        # En nuestro espacio discreto, la ortonormalizaci√≥n es conceptual
        # pero importante para garantizar independencia de efectos
        logger.debug("üßÆ Aplicando Gram-Schmidt conceptual a base operacional")

        # Para futuras implementaciones con espacios continuos
        self._gram_matrix = np.eye(self._dimension)
        self._orthonormal_basis_computed = True

    def get_spectrum(self) -> Dict[str, float]:
        """
        Calcula el espectro del operador (valores propios conceptuales).

        √ötil para analizar estabilidad del pipeline.
        """
        # En matriz diagonal, valores propios = 1 (operadores unitarios)
        spectrum = {label: 1.0 for label in self._basis.keys()}

        # Ajustar por estrato (operadores de estratos superiores tienen mayor "inercia")
        for label, vector in self._basis.items():
            stratum_factor = {
                Stratum.PHYSICS: 1.0,
                Stratum.TACTICS: 1.1,
                Stratum.STRATEGY: 1.3,
                Stratum.WISDOM: 1.5
            }.get(vector.stratum, 1.0)
            spectrum[label] *= stratum_factor

        return spectrum


class DataValidator:
    """Utilidades para validaci√≥n de DataFrames."""

    @staticmethod
    def validate_dataframe_not_empty(
        df: pd.DataFrame, name: str
    ) -> Tuple[bool, Optional[str]]:
        """Verifica que un DataFrame no est√© vac√≠o."""
        if df is None:
            error_msg = f"DataFrame '{name}' es None"
            logger.error(f"‚ùå {error_msg}")
            return False, error_msg

        if not isinstance(df, pd.DataFrame):
            error_msg = (
                f"'{name}' no es un DataFrame v√°lido, es tipo: {type(df).__name__}"
            )
            logger.error(f"‚ùå {error_msg}")
            return False, error_msg

        if df.empty:
            error_msg = f"DataFrame '{name}' est√° vac√≠o (0 filas)"
            logger.error(f"‚ùå {error_msg}")
            return False, error_msg

        if df.dropna(how="all").empty:
            error_msg = f"DataFrame '{name}' contiene solo valores nulos"
            logger.error(f"‚ùå {error_msg}")
            return False, error_msg

        return True, None

    @staticmethod
    def validate_required_columns(
        df: pd.DataFrame, required_cols: List[str], df_name: str
    ) -> Tuple[bool, Optional[str]]:
        """Verifica que un DataFrame tenga las columnas requeridas."""
        if df is None:
            error_msg = f"DataFrame '{df_name}' es None, no se pueden validar columnas"
            logger.error(f"‚ùå {error_msg}")
            return False, error_msg

        if not isinstance(df, pd.DataFrame):
            error_msg = f"'{df_name}' no es un DataFrame v√°lido"
            logger.error(f"‚ùå {error_msg}")
            return False, error_msg

        if not required_cols:
            logger.warning(f"‚ö†Ô∏è Lista de columnas requeridas vac√≠a para '{df_name}'")
            return True, None

        df_cols_upper = {col.upper().strip(): col for col in df.columns}
        missing_cols = []

        for req_col in required_cols:
            req_col_upper = req_col.upper().strip()
            if req_col not in df.columns and req_col_upper not in df_cols_upper:
                missing_cols.append(req_col)

        if missing_cols:
            available_cols = list(df.columns)[:10]
            error_msg = (
                f"Faltan columnas requeridas en '{df_name}': {missing_cols}. "
                f"Columnas disponibles (primeras 10): {available_cols}"
            )
            logger.error(f"‚ùå {error_msg}")
            return False, error_msg

        return True, None

    @staticmethod
    def detect_and_log_duplicates(
        df: pd.DataFrame, subset_cols: List[str], df_name: str, keep: str = "first"
    ) -> pd.DataFrame:
        """Detecta, loguea y elimina duplicados en un DataFrame."""
        if df is None or not isinstance(df, pd.DataFrame):
            logger.error(
                f"‚ùå DataFrame '{df_name}' inv√°lido para detecci√≥n de duplicados"
            )
            return pd.DataFrame()

        if df.empty:
            logger.debug(
                f"DataFrame '{df_name}' vac√≠o, no hay duplicados que detectar"
            )
            return df

        if not subset_cols:
            logger.warning(
                f"‚ö†Ô∏è No se especificaron columnas para detectar duplicados en '{df_name}'"
            )
            return df

        missing_subset_cols = [col for col in subset_cols if col not in df.columns]
        if missing_subset_cols:
            logger.error(
                f"‚ùå Columnas para duplicados no existen en '{df_name}': {missing_subset_cols}"
            )
            subset_cols = [col for col in subset_cols if col in df.columns]
            if not subset_cols:
                return df

        valid_keep_values = {"first", "last", False}
        if keep not in valid_keep_values:
            logger.warning(f"‚ö†Ô∏è Valor 'keep={keep}' inv√°lido, usando 'first'")
            keep = "first"

        try:
            duplicates = df[df.duplicated(subset=subset_cols, keep=False)]
            if not duplicates.empty:
                unique_dupes = duplicates[subset_cols[0]].unique()
                num_dupes = len(duplicates)
                logger.warning(
                    f"‚ö†Ô∏è Se encontraron {num_dupes} filas duplicadas en '{df_name}' "
                    f"por {subset_cols}. Se conservar√°: '{keep}'"
                )
                dupes_sample = unique_dupes[:10].tolist()
                logger.debug(
                    f"Muestra de valores duplicados en '{df_name}': {dupes_sample}"
                )

                df_clean = df.drop_duplicates(subset=subset_cols, keep=keep)
                rows_removed = len(df) - len(df_clean)
                logger.info(
                    f"‚úÖ Duplicados eliminados en '{df_name}': {rows_removed} filas removidas"
                )
                return df_clean
            return df
        except Exception as e:
            logger.error(f"‚ùå Error detectando duplicados en '{df_name}': {e}")
            return df


class FileValidator:
    """Utilidades para validaci√≥n de existencia de archivos."""

    MIN_FILE_SIZE_BYTES = 10
    VALID_EXTENSIONS = {".csv", ".xlsx", ".xls", ".json"}

    @staticmethod
    def validate_file_exists(
        file_path: str,
        file_type: str,
        check_extension: bool = True,
        min_size: Optional[int] = None,
    ) -> Tuple[bool, Optional[str]]:
        """Verifica que un archivo exista y sea accesible."""
        if not file_path:
            error_msg = f"Ruta de archivo de {file_type} est√° vac√≠a o es None"
            logger.error(f"‚ùå {error_msg}")
            return False, error_msg

        if not isinstance(file_path, (str, Path)):
            error_msg = (
                f"Ruta de {file_type} no es v√°lida: tipo {type(file_path).__name__}"
            )
            logger.error(f"‚ùå {error_msg}")
            return False, error_msg

        try:
            path = Path(file_path).resolve()
        except Exception as e:
            error_msg = f"Ruta de {file_type} no es v√°lida: {file_path}. Error: {e}"
            logger.error(f"‚ùå {error_msg}")
            return False, error_msg

        if not path.exists():
            error_msg = f"Archivo de {file_type} no encontrado: {path}"
            logger.error(f"‚ùå {error_msg}")
            return False, error_msg

        if not path.is_file():
            error_msg = f"La ruta de {file_type} no es un archivo: {path}"
            logger.error(f"‚ùå {error_msg}")
            return False, error_msg

        try:
            if not os.access(path, os.R_OK):
                error_msg = f"Sin permisos de lectura para {file_type}: {path}"
                logger.error(f"‚ùå {error_msg}")
                return False, error_msg
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è No se pudo verificar permisos para {path}: {e}")

        min_size = min_size or FileValidator.MIN_FILE_SIZE_BYTES
        try:
            file_size = path.stat().st_size
            if file_size < min_size:
                error_msg = f"Archivo de {file_type} demasiado peque√±o ({file_size} bytes): {path}"
                logger.error(f"‚ùå {error_msg}")
                return False, error_msg
        except OSError as e:
            error_msg = f"No se pudo obtener informaci√≥n del archivo {file_type}: {e}"
            logger.error(f"‚ùå {error_msg}")
            return False, error_msg

        if check_extension:
            ext = path.suffix.lower()
            if ext not in FileValidator.VALID_EXTENSIONS:
                logger.warning(
                    f"‚ö†Ô∏è Extensi√≥n '{ext}' de {file_type} no es est√°ndar. "
                    f"Extensiones esperadas: {FileValidator.VALID_EXTENSIONS}"
                )

        logger.debug(
            f"‚úÖ Archivo de {file_type} validado: {path} ({file_size} bytes)"
        )
        return True, None


# ==================== IMPLEMENTACI√ìN DE PASOS ====================


class LoadDataStep(ProcessingStep):
    """
    Paso de Carga de Datos.

    Carga los archivos CSV/Excel de presupuesto, APUs e insumos.
    """

    def __init__(self, config: dict, thresholds: ProcessingThresholds):
        if not config or not isinstance(config, dict):
            raise ValueError("Configuraci√≥n inv√°lida para LoadDataStep")
        self.config = config
        self.thresholds = thresholds or ProcessingThresholds()

    def execute(self, context: dict, telemetry: TelemetryContext) -> dict:
        """Ejecuta la carga y validaci√≥n inicial de archivos."""
        telemetry.start_step("load_data")

        try:
            required_paths = ["presupuesto_path", "apus_path", "insumos_path"]
            paths = {}

            for path_key in required_paths:
                path_value = context.get(path_key)
                if not path_value:
                    error = f"Ruta requerida '{path_key}' no encontrada en contexto"
                    telemetry.record_error("load_data", error)
                    raise ValueError(error)
                paths[path_key] = path_value

            presupuesto_path = paths["presupuesto_path"]
            apus_path = paths["apus_path"]
            insumos_path = paths["insumos_path"]

            file_validator = FileValidator()
            validations = [
                (presupuesto_path, "presupuesto"),
                (apus_path, "APUs"),
                (insumos_path, "insumos"),
            ]

            for file_path, file_type in validations:
                is_valid, error = file_validator.validate_file_exists(
                    file_path, file_type
                )
                if not is_valid:
                    telemetry.record_error("load_data", error)
                    raise ValueError(error)

            file_profiles = self.config.get("file_profiles", {})
            # Use defaults if not present to support simpler test configurations
            if not file_profiles:
                logger.warning("‚ö†Ô∏è 'file_profiles' no encontrado en config, usando defaults vac√≠os.")
                file_profiles = {
                    "presupuesto_default": {},
                    "insumos_default": {},
                    "apus_default": {}
                }

            presupuesto_profile = file_profiles.get("presupuesto_default", {})

            p_processor = PresupuestoProcessor(
                self.config, self.thresholds, presupuesto_profile
            )
            df_presupuesto = p_processor.process(presupuesto_path)

            if df_presupuesto is None or df_presupuesto.empty:
                error = "Procesamiento de presupuesto retorn√≥ DataFrame vac√≠o"
                telemetry.record_error("load_data", error)
                raise ValueError(error)

            telemetry.record_metric(
                "load_data", "presupuesto_rows", len(df_presupuesto)
            )

            insumos_profile = file_profiles.get("insumos_default", {})

            i_processor = InsumosProcessor(self.thresholds, insumos_profile)
            df_insumos = i_processor.process(insumos_path)

            logger.info(
                f"üêõ DIAG: [LoadDataStep] Insumos extra√≠dos: {len(df_insumos)} filas."
            )
            if not df_insumos.empty:
                logger.info(
                    f"üêõ DIAG: [LoadDataStep] Estructura de insumos (head(1)): {df_insumos.head(1).to_dict('records')}"
                )

            if df_insumos is None or df_insumos.empty:
                error = "Procesamiento de insumos retorn√≥ DataFrame vac√≠o"
                telemetry.record_error("load_data", error)
                raise ValueError(error)

            telemetry.record_metric("load_data", "insumos_rows", len(df_insumos))

            apus_profile = file_profiles.get("apus_default", {})

            logger.info("‚ö°Ô∏è Iniciando DataFluxCondenser para APUs...")
            condenser_config_data = self.config.get("flux_condenser_config", {})

            try:
                condenser_config = CondenserConfig(**condenser_config_data)
            except TypeError as e:
                logger.warning(
                    f"‚ö†Ô∏è Error en config de condenser, usando defaults: {e}"
                )
                condenser_config = CondenserConfig()

            condenser = DataFluxCondenser(
                config=self.config,
                profile=apus_profile,
                condenser_config=condenser_config,
            )

            def on_progress_stats(processing_stats):
                try:
                    for metric_name, attr_name, default_value in [
                        ("avg_saturation", "avg_saturation", 0.0),
                        ("max_flyback_voltage", "max_flyback_voltage", 0.0),
                        ("max_dissipated_power", "max_dissipated_power", 0.0),
                        ("avg_kinetic_energy", "avg_kinetic_energy", 0.0),
                    ]:
                        val = getattr(processing_stats, attr_name, default_value)
                        telemetry.record_metric("flux_condenser", metric_name, val)
                except Exception:
                    pass

            def _publish_telemetry(metrics: Dict[str, Any]):
                try:
                    import json
                    import time

                    from flask import current_app

                    payload = {
                        **metrics,
                        "_timestamp": time.time(),
                        "_source": "flux_condenser_realtime",
                    }

                    data_str = json.dumps(payload, default=str)

                    if current_app:
                        redis_client = current_app.config.get("SESSION_REDIS")
                        if redis_client:
                            redis_client.set(
                                "apu_filter:global_metrics", data_str, ex=60
                            )
                except Exception:
                    pass

            df_apus_raw = condenser.stabilize(
                apus_path,
                on_progress=on_progress_stats,
                progress_callback=_publish_telemetry,
                telemetry=telemetry,
            )

            full_stats = condenser.get_processing_stats() or {}
            stats = full_stats.get("statistics", {})

            for metric_name, default_value in [
                ("avg_saturation", 0.0),
                ("max_flyback_voltage", 0.0),
                ("max_dissipated_power", 0.0),
                ("avg_kinetic_energy", 0.0),
                ("avg_batch_size", 0),
            ]:
                value = stats.get(metric_name, default_value)
                if (
                    not isinstance(value, (int, float))
                    or np.isnan(value)
                    or np.isinf(value)
                ):
                    value = default_value
                telemetry.record_metric("flux_condenser", metric_name, value)

            if df_apus_raw is None or df_apus_raw.empty:
                error = "DataFluxCondenser retorn√≥ DataFrame vac√≠o"
                telemetry.record_error("load_data", error)
                raise ValueError(error)

            telemetry.record_metric("load_data", "apus_raw_rows", len(df_apus_raw))
            logger.info("‚úÖ DataFluxCondenser completado.")

            data_validator = DataValidator()
            dataframes = [
                (df_presupuesto, "presupuesto"),
                (df_insumos, "insumos"),
                (df_apus_raw, "APUs"),
            ]

            for df, name in dataframes:
                is_valid, error = data_validator.validate_dataframe_not_empty(df, name)
                if not is_valid:
                    telemetry.record_error("load_data", error)
                    raise ValueError(error)

            context = {**context}
            context.update(
                {
                    "df_presupuesto": df_presupuesto,
                    "df_insumos": df_insumos,
                    "df_apus_raw": df_apus_raw,
                }
            )

            telemetry.end_step("load_data", "success")
            return context

        except Exception as e:
            logger.error(f"‚ùå Error en LoadDataStep: {e}", exc_info=True)
            telemetry.record_error("load_data", str(e))
            telemetry.end_step("load_data", "error")
            raise


class MergeDataStep(ProcessingStep):
    """
    Paso de Fusi√≥n de Datos.

    Combina los datos crudos de APUs con la base de datos de insumos.
    """

    def __init__(self, config: dict, thresholds: ProcessingThresholds):
        self.config = config
        self.thresholds = thresholds

    def execute(self, context: dict, telemetry: TelemetryContext) -> dict:
        """Ejecuta la fusi√≥n de DataFrames."""
        telemetry.start_step("merge_data")
        try:
            df_apus_raw = context["df_apus_raw"]
            df_insumos = context["df_insumos"]

            logger.info(
                f"üêõ DIAG: [MergeDataStep] Recibidos {len(df_insumos)} insumos del contexto."
            )

            merger = DataMerger(self.thresholds)
            df_merged = merger.merge_apus_with_insumos(df_apus_raw, df_insumos)

            telemetry.record_metric("merge_data", "merged_rows", len(df_merged))
            context["df_merged"] = df_merged

            telemetry.end_step("merge_data", "success")
            return context
        except Exception as e:
            telemetry.record_error("merge_data", str(e))
            telemetry.end_step("merge_data", "error")
            raise


class AuditedMergeStep(ProcessingStep):
    """
    Paso de Fusi√≥n con Auditor√≠a Topol√≥gica (Mayer-Vietoris).

    Construye grafos temporales para validar la integridad antes de comprometer la fusi√≥n.
    """

    def __init__(self, config: dict, thresholds: ProcessingThresholds):
        self.config = config
        self.thresholds = thresholds

    def execute(self, context: dict, telemetry: TelemetryContext) -> dict:
        """Ejecuta la auditor√≠a Mayer-Vietoris y luego la fusi√≥n f√≠sica."""
        telemetry.start_step("audited_merge")
        try:
            df_a = context.get("df_presupuesto")
            df_b = context.get("df_apus_raw")
            df_insumos = context.get("df_insumos")

            if df_a is None or df_b is None:
                logger.warning(
                    "‚ö†Ô∏è Falta df_presupuesto o df_apus_raw. Saltando auditor√≠a Mayer-Vietoris."
                )
            else:
                try:
                    from agent.business_topology import (
                        BudgetGraphBuilder,
                        BusinessTopologicalAnalyzer,
                    )

                    builder = BudgetGraphBuilder()
                    graph_a = builder.build(df_a, pd.DataFrame())
                    graph_b = builder.build(pd.DataFrame(), df_b)

                    analyzer = BusinessTopologicalAnalyzer(telemetry=telemetry)
                    audit_result = analyzer.audit_integration_homology(graph_a, graph_b)

                    if audit_result["delta_beta_1"] > 0:
                        logger.warning(f"üö® {audit_result['narrative']}")
                        telemetry.record_metric(
                            "topology", "emergent_cycles", audit_result["delta_beta_1"]
                        )
                        context["integration_risk_alert"] = audit_result
                    else:
                        logger.info(f"‚úÖ {audit_result['narrative']}")

                except Exception as e_audit:
                    logger.error(
                        f"‚ùå Error durante auditor√≠a Mayer-Vietoris: {e_audit}"
                    )
                    telemetry.record_error("audited_merge_audit", str(e_audit))

            logger.info("üõ†Ô∏è Ejecutando fusi√≥n f√≠sica de datos...")
            merger = DataMerger(self.thresholds)
            df_merged = merger.merge_apus_with_insumos(df_b, df_insumos)

            telemetry.record_metric("audited_merge", "merged_rows", len(df_merged))
            context["df_merged"] = df_merged

            telemetry.end_step("audited_merge", "success")
            return context

        except Exception as e:
            telemetry.record_error("audited_merge", str(e))
            telemetry.end_step("audited_merge", "error")
            raise


class CalculateCostsStep(ProcessingStep):
    """
    Paso de C√°lculo de Costos.

    Calcula costos unitarios, tiempos y rendimientos de los APUs.
    """

    def __init__(self, config: dict, thresholds: ProcessingThresholds):
        self.config = config
        self.thresholds = thresholds

    def execute(self, context: dict, telemetry: TelemetryContext) -> dict:
        """Ejecuta los c√°lculos de costos y tiempos."""
        telemetry.start_step("calculate_costs")
        try:
            df_merged = context["df_merged"]
            df_merged = calculate_insumo_costs(df_merged, self.thresholds)

            cost_calculator = APUCostCalculator(self.config, self.thresholds)
            df_apu_costos, df_tiempo, df_rendimiento = cost_calculator.calculate(
                df_merged
            )

            telemetry.record_metric(
                "calculate_costs", "costos_calculated", len(df_apu_costos)
            )

            context.update(
                {
                    "df_merged": df_merged,
                    "df_apu_costos": df_apu_costos,
                    "df_tiempo": df_tiempo,
                    "df_rendimiento": df_rendimiento,
                }
            )
            telemetry.end_step("calculate_costs", "success")
            return context
        except Exception as e:
            telemetry.record_error("calculate_costs", str(e))
            telemetry.end_step("calculate_costs", "error")
            raise


class FinalMergeStep(ProcessingStep):
    """
    Paso de Fusi√≥n Final.

    Integra los costos calculados con el presupuesto original.
    """

    def __init__(self, config: dict, thresholds: ProcessingThresholds):
        self.config = config
        self.thresholds = thresholds

    def execute(self, context: dict, telemetry: TelemetryContext) -> dict:
        """Ejecuta la fusi√≥n final con el presupuesto."""
        telemetry.start_step("final_merge")
        try:
            df_presupuesto = context["df_presupuesto"]
            df_apu_costos = context["df_apu_costos"]
            df_tiempo = context["df_tiempo"]

            merger = DataMerger(self.thresholds)
            df_final = merger.merge_with_presupuesto(df_presupuesto, df_apu_costos)

            df_final = pd.merge(
                df_final, df_tiempo, on=ColumnNames.CODIGO_APU, how="left"
            )
            df_final = group_and_split_description(df_final)
            df_final = calculate_total_costs(df_final, self.thresholds)

            telemetry.record_metric("final_merge", "final_rows", len(df_final))
            context["df_final"] = df_final

            telemetry.end_step("final_merge", "success")
            return context
        except Exception as e:
            telemetry.record_error("final_merge", str(e))
            telemetry.end_step("final_merge", "error")
            raise


class BusinessTopologyStep(ProcessingStep):
    """
    Paso de An√°lisis de Negocio.

    Utiliza el BusinessAgent para auditar la integridad estructural y evaluar riesgos.
    """

    def __init__(self, config: dict, thresholds: ProcessingThresholds):
        self.config = config
        self.thresholds = thresholds

    def execute(self, context: dict, telemetry: TelemetryContext) -> dict:
        """Ejecuta la evaluaci√≥n del BusinessAgent."""
        telemetry.start_step("business_topology")
        try:
            from app.business_agent import BusinessAgent
            from flask import current_app

            # Recuperar la instancia global de la MIC
            mic_instance = getattr(current_app, "mic", None)
            if not mic_instance:
                raise RuntimeError("MICRegistry not found in current_app")

            # Garantizar que el contexto tenga validaci√≥n de estratos
            # Si llegamos a este paso en el pipeline secuencial, asumimos PHYSICS y TACTICS v√°lidos
            if "validated_strata" not in context:
                context["validated_strata"] = {Stratum.PHYSICS, Stratum.TACTICS}
            elif isinstance(context["validated_strata"], set):
                context["validated_strata"].add(Stratum.PHYSICS)
                context["validated_strata"].add(Stratum.TACTICS)

            logger.info("ü§ñ Desplegando BusinessAgent para evaluaci√≥n de proyecto...")

            agent = BusinessAgent(
                config=self.config,
                mic=mic_instance,
                telemetry=telemetry
            )
            report = agent.evaluate_project(context)

            if report:
                logger.info("‚úÖ BusinessAgent complet√≥ la evaluaci√≥n.")
                context["business_topology_report"] = report

                logger.info(
                    f"Puntuaci√≥n de Integridad: {report.integrity_score:.2f}/100"
                )
                if report.waste_alerts:
                    logger.warning(
                        f"Alertas de Desperdicio: {len(report.waste_alerts)}"
                    )
                if report.circular_risks:
                    logger.critical(
                        f"Riesgos Circulares: {len(report.circular_risks)}"
                    )
            else:
                logger.warning("‚ö†Ô∏è El BusinessAgent no gener√≥ un reporte.")

            telemetry.end_step("business_topology", "success")
            return context

        except Exception as e:
            logger.error(
                f"‚ùå Error en BusinessTopologyStep con BusinessAgent: {e}", exc_info=True
            )
            telemetry.record_error("business_topology", str(e))
            telemetry.end_step("business_topology", "error")
            return context


class MaterializationStep(ProcessingStep):
    """
    Paso de Materializaci√≥n.

    Genera la Lista de Materiales (BOM) a partir del grafo topol√≥gico.
    """

    def __init__(self, config: dict, thresholds: ProcessingThresholds):
        self.config = config
        self.thresholds = thresholds

    def execute(self, context: dict, telemetry: TelemetryContext) -> dict:
        """Ejecuta el MatterGenerator."""
        telemetry.start_step("materialization")
        try:
            if "business_topology_report" not in context:
                logger.warning(
                    "‚ö†Ô∏è No se encontr√≥ reporte de topolog√≠a. Saltando materializaci√≥n."
                )
                telemetry.end_step("materialization", "skipped")
                return context

            graph = context.get("graph")
            if not graph:
                logger.info(
                    "üîÑ Grafo no encontrado en contexto. Reconstruyendo para materializaci√≥n..."
                )
                from agent.business_topology import BudgetGraphBuilder

                builder = BudgetGraphBuilder()
                df_presupuesto = context.get("df_final")
                df_detail = context.get("df_merged")

                if df_presupuesto is not None and df_detail is not None:
                    graph = builder.build(df_presupuesto, df_detail)
                    context["graph"] = graph
                else:
                    logger.error(
                        "‚ùå No hay datos suficientes para reconstruir el grafo."
                    )
                    telemetry.end_step("materialization", "error")
                    return context

            report = context.get("business_topology_report")
            stability = 10.0
            if report and report.details:
                stability = report.details.get("pyramid_stability", 10.0)

            flux_metrics = {
                "pyramid_stability": stability,
                "avg_saturation": 0.0,
            }

            generator = MatterGenerator()
            bom = generator.materialize_project(
                graph, flux_metrics=flux_metrics, telemetry=telemetry
            )

            context["bill_of_materials"] = bom
            context["logistics_plan"] = asdict(bom)

            logger.info(
                f"‚úÖ Materializaci√≥n completada. Total √≠tems: {len(bom.requirements)}"
            )

            telemetry.end_step("materialization", "success")
            return context

        except Exception as e:
            logger.error(f"‚ùå Error en MaterializationStep: {e}", exc_info=True)
            telemetry.record_error("materialization", str(e))
            telemetry.end_step("materialization", "error")
            return context


class BuildOutputStep(ProcessingStep):
    """
    Paso de Construcci√≥n de Salida.

    Prepara y valida el diccionario final de resultados para el cliente.
    """

    def __init__(self, config: dict, thresholds: ProcessingThresholds):
        self.config = config
        self.thresholds = thresholds

    def execute(self, context: dict, telemetry: TelemetryContext) -> dict:
        """Construye y valida la estructura de salida como un Data Product."""
        telemetry.start_step("build_output")
        try:
            df_final = context["df_final"]
            df_insumos = context["df_insumos"]
            df_merged = context["df_merged"]
            df_apus_raw = context["df_apus_raw"]
            df_apu_costos = context["df_apu_costos"]
            df_tiempo = context["df_tiempo"]
            df_rendimiento = context["df_rendimiento"]

            df_merged = synchronize_data_sources(df_merged, df_final)
            df_processed_apus = build_processed_apus_dataframe(
                df_apu_costos, df_apus_raw, df_tiempo, df_rendimiento
            )

            result_dict = build_output_dictionary(
                df_final, df_insumos, df_merged, df_apus_raw, df_processed_apus
            )

            validated_result = validate_and_clean_data(
                result_dict, telemetry_context=telemetry
            )
            validated_result["raw_insumos_df"] = df_insumos.to_dict("records")

            if "business_topology_report" in context:
                validated_result["audit_report"] = asdict(
                    context["business_topology_report"]
                )

            if "logistics_plan" in context:
                validated_result["logistics_plan"] = context["logistics_plan"]

            try:
                narrator = TelemetryNarrator()
                tech_narrative = narrator.summarize_execution(telemetry)
                if isinstance(tech_narrative, dict):
                    validated_result["technical_audit"] = tech_narrative.copy()
                else:
                    validated_result["technical_audit"] = tech_narrative

                logger.info("‚úÖ Narrativa t√©cnica generada e inyectada.")
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è Fallo al generar narrativa t√©cnica: {e}")
                validated_result["technical_audit"] = {"error": str(e)}

            def compute_hash(data: Any) -> str:
                """Calcula un hash simple del contenido para linaje."""
                import json

                try:
                    sanitized = data
                    try:
                        sanitized = sanitize_for_json(data)
                    except NameError:
                        pass

                    s = json.dumps(sanitized, sort_keys=True, default=str)
                    return hashlib.sha256(s.encode("utf-8")).hexdigest()
                except Exception:
                    return "hash_computation_failed"

            input_sample = {
                "presupuesto_head": (
                    df_final.head(5).to_dict("records") if not df_final.empty else []
                ),
                "insumos_head": (
                    df_insumos.head(5).to_dict("records")
                    if not df_insumos.empty
                    else []
                ),
            }
            lineage_hash = compute_hash(input_sample)

            error_count = (
                telemetry.get_metrics().get("errors", 0)
                if hasattr(telemetry, "get_metrics")
                else 0
            )
            sla_compliance = "100%" if error_count == 0 else "95%"

            data_product = {
                "kind": "DataProduct",
                "metadata": {
                    "version": "3.0",
                    "lineage_hash": lineage_hash,
                    "sla_compliance": sla_compliance,
                    "generated_at": datetime.datetime.now().isoformat(),
                    "generator": "APU_Filter_Pipeline_v2",
                },
                "governance": {
                    "policy_version": "data_contract_v1",
                    "compliance_score": 95,
                    "classification": "CONFIDENTIAL",
                },
                "payload": validated_result,
            }

            context["final_result"] = data_product
            telemetry.end_step("build_output", "success")
            return context
        except Exception as e:
            telemetry.record_error("build_output", str(e))
            telemetry.end_step("build_output", "error")
            raise


# ==================== PIPELINE DIRECTOR ====================


class PipelineSteps(str, enum.Enum):
    """
    Define los identificadores √∫nicos para cada paso del pipeline.

    Funciona como una 'API' p√∫blica para el orquestador.
    """

    LOAD_DATA = "load_data"
    MERGE_DATA = "merge_data"
    CALCULATE_COSTS = "calculate_costs"
    FINAL_MERGE = "final_merge"
    BUSINESS_TOPOLOGY = "business_topology"
    MATERIALIZATION = "materialization"
    BUILD_OUTPUT = "build_output"



class PipelineDirector:
    """
    Orquesta la ejecuci√≥n secuencial con validaci√≥n topol√≥gica.

    Implementa una 4-variedad diferenciable donde cada estrato
    corresponde a una subvariedad embebida.
    """

    def __init__(self, config: dict, telemetry: TelemetryContext):
        self.config = config
        self.telemetry = telemetry
        self.thresholds = self._load_thresholds(config)
        self.session_dir = Path(config.get("session_dir", "data/sessions"))
        self.session_dir.mkdir(parents=True, exist_ok=True)

        # Inicializar espacio vectorial con m√©trica Riemanniana
        self.mic = LinearInteractionMatrix()
        self._filtration_level = 0  # Nivel de filtraci√≥n actual
        self._homology_groups = {}  # Grupos de homolog√≠a computados
        self._initialize_vector_space_with_validation()

    def _initialize_vector_space_with_validation(self):
        """
        Construye la base can√≥nica con validaci√≥n de filtraci√≥n.

        Filtraci√≥n: ‚àÖ = F_0 ‚äÇ F_1 ‚äÇ F_2 ‚äÇ F_3 ‚äÇ F_4 = V
        donde F_k corresponde al estrato k.
        """
        # Definir mapeo estrato ‚Üí nivel de filtraci√≥n
        stratum_filtration = {
            Stratum.PHYSICS: 1,
            Stratum.TACTICS: 2,
            Stratum.STRATEGY: 3,
            Stratum.WISDOM: 4
        }

        # A√±adir vectores en orden de filtraci√≥n
        basis_config = [
            ("load_data", LoadDataStep, Stratum.PHYSICS),
            ("audited_merge", AuditedMergeStep, Stratum.PHYSICS),
            ("calculate_costs", CalculateCostsStep, Stratum.TACTICS),
            ("final_merge", FinalMergeStep, Stratum.PHYSICS),
            ("materialization", MaterializationStep, Stratum.TACTICS),
            ("business_topology", BusinessTopologyStep, Stratum.STRATEGY),
            ("build_output", BuildOutputStep, Stratum.WISDOM)
        ]

        for label, step_class, stratum in basis_config:
            try:
                self.mic.add_basis_vector(label, step_class, stratum)
                logger.debug(
                    f"üìê Vector a√±adido a filtraci√≥n F_{stratum_filtration[stratum]}: "
                    f"{label} ({stratum.name})"
                )
            except ValueError as e:
                logger.error(f"‚ùå Error en filtraci√≥n para {label}: {e}")
                raise

    def run_single_step(
        self,
        step_name: str,
        session_id: str,
        initial_context: Optional[Dict[str, Any]] = None,
        validate_stratum_transition: bool = True
    ) -> Dict[str, Any]:
        """
        Ejecuta un √∫nico operador con validaci√≥n de transici√≥n entre estratos.

        Par√°metros:
        -----------
        validate_stratum_transition : bool
            Si True, valida que la transici√≥n entre estratos sea suave
            (no salte estratos intermedios).
        """
        # 1. Cargar contexto con verificaci√≥n de integridad
        context = self._load_context_state(session_id)
        if initial_context:
            # Validar que initial_context no corrompa el estado existente
            self._validate_context_merge(context, initial_context)
            context.update(initial_context)

        logger.info(
            f"‚ñ∂Ô∏è Ejecutando operador: {step_name} (Sesi√≥n: {session_id[:8]}...)"
        )

        try:
            # 2. Proyecci√≥n algebraica con verificaci√≥n de rango
            basis_vector = self.mic.project_intent(step_name)

            # 3. Validaci√≥n de transici√≥n entre estratos
            if validate_stratum_transition:
                current_stratum = self._infer_current_stratum(context)
                self._validate_stratum_transition(
                    current_stratum, basis_vector.stratum
                )

            # 4. Instanciaci√≥n del operador lineal
            step_instance = basis_vector.operator_class(self.config, self.thresholds)

            # 5. Medici√≥n de traza antes/despu√©s
            trace_before = self._compute_state_trace(context)

            # 6. Aplicaci√≥n de transformaci√≥n: S' = T(S)
            updated_context = step_instance.execute(context, self.telemetry)

            if updated_context is None:
                raise ValueError(f"Operador {step_name} retorn√≥ transformaci√≥n nula")

            # 7. Verificar preservaci√≥n de norma (conservaci√≥n de informaci√≥n)
            trace_after = self._compute_state_trace(updated_context)
            trace_delta = abs(trace_after - trace_before)

            if trace_delta > 0.01:  # Umbral de tolerancia
                logger.warning(
                    f"‚ö†Ô∏è Operador {step_name} alter√≥ traza del estado: "
                    f"Œî = {trace_delta:.4f}"
                )

            # 8. Persistencia con checksum
            self._save_context_state_with_checksum(session_id, updated_context)

            # 9. Actualizar nivel de filtraci√≥n
            self._filtration_level = self._stratum_to_filtration(basis_vector.stratum)

            # 10. Calcular homolog√≠a si estamos en estratos superiores
            if basis_vector.stratum in [Stratum.STRATEGY, Stratum.WISDOM]:
                self._compute_homology_groups(updated_context)

            logger.info(f"‚úÖ Operador {step_name} completado (estrato: {basis_vector.stratum.name})")

            return {
                "status": "success",
                "step": step_name,
                "stratum": basis_vector.stratum.name,
                "filtration_level": self._filtration_level,
                "session_id": session_id,
                "context_keys": list(updated_context.keys()),
                "trace_delta": trace_delta,
                "homology_updated": basis_vector.stratum in [Stratum.STRATEGY, Stratum.WISDOM]
            }

        except Exception as e:
            error_msg = f"Error en operador '{step_name}': {e}"
            logger.error(f"üî• {error_msg}", exc_info=True)
            self.telemetry.record_error(step_name, str(e))

            # Intentar recuperaci√≥n mediante operador identidad
            recovery_status = self._attempt_state_recovery(session_id, context)

            return {
                "status": "error",
                "step": step_name,
                "error": error_msg,
                "recovery_attempted": recovery_status,
                "session_id": session_id
            }

    def execute_pipeline_orchestrated(self, initial_context: dict) -> dict:
        """
        Ejecuta el pipeline completo de forma orquestada, paso a paso.
        """
        session_id = str(uuid.uuid4())
        logger.info(f"üöÄ Iniciando pipeline orquestado con Sesi√≥n ID: {session_id}")

        recipe = self.config.get("pipeline_recipe", [])
        if not recipe:
            logger.warning("No 'pipeline_recipe' en config. Usando flujo por defecto.")
            recipe = [{"step": step.value, "enabled": True} for step in PipelineSteps]

        context = initial_context
        for step_idx, step_config in enumerate(recipe):
            step_name = step_config.get("step")
            if not step_config.get("enabled", True):
                continue

            logger.info(f"‚ñ∂Ô∏è Orquestando paso [{step_idx + 1}/{len(recipe)}]: {step_name}")

            current_context = context if step_idx == 0 else None
            result = self.run_single_step(
                step_name, session_id, initial_context=current_context
            )

            if result["status"] == "error":
                error_msg = f"Fallo en pipeline orquestado en paso '{step_name}': {result.get('error')}"
                logger.critical(f"üî• {error_msg}")
                raise RuntimeError(error_msg)

        final_context = self._load_context_state(session_id)
        logger.info(f"üéâ Pipeline orquestado completado (Sesi√≥n: {session_id})")
        return final_context

    def _validate_stratum_transition(self, current: Optional[Stratum], next_stratum: Stratum):
        """
        Valida que la transici√≥n entre estratos sea topol√≥gicamente admisible.
        """
        if current is None:
            return  # Primera ejecuci√≥n

        current_level = self._stratum_to_filtration(current)
        next_level = self._stratum_to_filtration(next_stratum)

        if next_level < current_level:
            # Check for cycles or if it is allowed to restart
            # Assuming strictly increasing or same stratum for now unless it's a new cycle
            pass

        if next_level > current_level + 1:
            logger.warning(
                f"‚ö†Ô∏è Salto de estratos detectado: "
                f"{current.name} ‚Üí {next_stratum.name}."
            )

    def _stratum_to_filtration(self, stratum: Stratum) -> int:
        mapping = {
            Stratum.PHYSICS: 1,
            Stratum.TACTICS: 2,
            Stratum.STRATEGY: 3,
            Stratum.WISDOM: 4
        }
        return mapping.get(stratum, 0)

    def _infer_current_stratum(self, context: dict) -> Optional[Stratum]:
        """Infiere el estrato actual basado en claves de contexto."""
        context_keys = set(context.keys())
        stratum_indicators = {
            Stratum.PHYSICS: {"df_presupuesto", "df_insumos", "df_apus_raw"},
            Stratum.TACTICS: {"df_merged", "df_apu_costos", "df_tiempo"},
            Stratum.STRATEGY: {"graph", "business_topology_report"},
            Stratum.WISDOM: {"final_result", "bill_of_materials"}
        }
        for stratum, indicators in stratum_indicators.items():
            if indicators & context_keys:
                return stratum
        return None

    def _compute_state_trace(self, context: dict) -> float:
        try:
            trace = 0.0
            for key, value in context.items():
                if isinstance(value, pd.DataFrame):
                    trace += len(value) * value.shape[1]
                elif isinstance(value, (list, dict)):
                    trace += len(str(value)) / 100
            return trace
        except:
            return 0.0

    def _save_context_state_with_checksum(self, session_id: str, context: dict):
        import hashlib
        try:
            # We must be careful with what we pickle/hash.
            # This is a simplified version.
            context_keys = sorted(context.keys())
            checksum_str = "".join(context_keys) # Very simple checksum
            checksum = hashlib.sha256(checksum_str.encode()).hexdigest()

            context["_integrity_checksum"] = checksum
            context["_persisted_at"] = datetime.datetime.now().isoformat()

            session_file = self.session_dir / f"{session_id}.pkl"
            with open(session_file, "wb") as f:
                pickle.dump(context, f)

            logger.debug(f"üíæ Contexto persistido con checksum: {checksum[:16]}...")
        except Exception as e:
            logger.error(f"Error persisting context: {e}")

    def _compute_homology_groups(self, context: dict):
        try:
            df_keys = [k for k in context.keys()
                      if isinstance(context.get(k), pd.DataFrame)]

            if len(df_keys) < 2:
                self._homology_groups = {"H0": 1, "H1": 0}
                return

            n = len(df_keys)
            adj_matrix = sparse.lil_matrix((n, n))

            for i, key_i in enumerate(df_keys):
                df_i = context[key_i]
                for j, key_j in enumerate(df_keys[i+1:], i+1):
                    df_j = context[key_j]
                    if isinstance(df_i, pd.DataFrame) and isinstance(df_j, pd.DataFrame):
                        common_cols = set(df_i.columns) & set(df_j.columns)
                        if common_cols:
                            adj_matrix[i, j] = adj_matrix[j, i] = len(common_cols)

            laplacian = sparse.diags(adj_matrix.sum(axis=1).A1) - adj_matrix
            # eigsh can fail if matrix is too small or 0
            if n > 2:
                try:
                    eigenvalues = sparse.linalg.eigsh(laplacian, k=1, which='SM', return_eigenvectors=False)
                    zero_eigenvalues = sum(abs(e) < 1e-10 for e in eigenvalues)
                except:
                    zero_eigenvalues = 1
            else:
                zero_eigenvalues = 1

            h0 = max(1, zero_eigenvalues)
            m = adj_matrix.nnz // 2
            n_nodes = adj_matrix.shape[0]
            h1 = max(0, m - n_nodes + h0)

            self._homology_groups = {
                "H0": h0,
                "H1": h1,
                "Betti_numbers": [h0, h1]
            }
            logger.debug(f"üßÆ Homolog√≠a computada: Œ≤‚ÇÄ={h0}, Œ≤‚ÇÅ={h1}")

        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Error computando homolog√≠a: {e}")
            self._homology_groups = {"error": str(e)}

    def _attempt_state_recovery(self, session_id: str, context: dict) -> bool:
        try:
            corrupt_file = self.session_dir / f"{session_id}_corrupt.pkl"
            with open(corrupt_file, "wb") as f:
                pickle.dump(context, f)
            return False
        except:
            return False

    def _validate_context_merge(self, context, initial):
        pass

    def _load_thresholds(self, config: dict) -> ProcessingThresholds:
        thresholds = ProcessingThresholds()
        if "processing_thresholds" in config:
            for key, value in config["processing_thresholds"].items():
                if hasattr(thresholds, key):
                    setattr(thresholds, key, value)
        return thresholds

    def _load_context_state(self, session_id: str) -> dict:
        if not session_id: return {}
        try:
            session_file = self.session_dir / f"{session_id}.pkl"
            if session_file.exists():
                with open(session_file, "rb") as f:
                    return pickle.load(f)
            return {}
        except:
            return {}


class PresupuestoProcessor:
    def __init__(self, config: dict, thresholds: ProcessingThresholds, profile: dict):
        if not config:
            raise ValueError("Configuraci√≥n requerida para PresupuestoProcessor")
        self.config = config
        self.thresholds = thresholds or ProcessingThresholds()
        self.profile = profile or {}
        self.validator = DataValidator()

    def process(self, path: str) -> pd.DataFrame:
        """Procesa archivo de presupuesto con manejo robusto de errores."""
        if not path:
            logger.error("‚ùå Ruta de presupuesto vac√≠a")
            return pd.DataFrame()

        try:
            loader_params = self.profile.get("loader_params", {})
            logger.info(f"üì• Cargando presupuesto desde: {path}")
            logger.debug(f"Par√°metros de carga: {loader_params}")

            load_result = load_data(path, **loader_params)

            if load_result is None:
                logger.error("‚ùå load_data retorn√≥ None")
                return pd.DataFrame()

            if not hasattr(load_result, "status") or not hasattr(load_result, "data"):
                logger.error("‚ùå Estructura de load_result inv√°lida")
                return pd.DataFrame()

            status_value = getattr(
                load_result.status, "value", str(load_result.status)
            )
            if status_value != "SUCCESS":
                error_msg = getattr(
                    load_result, "error_message", "Error desconocido"
                )
                logger.error(f"Error cargando presupuesto: {error_msg}")
                return pd.DataFrame()

            df = load_result.data
            if df is None or (isinstance(df, pd.DataFrame) and df.empty):
                logger.warning("‚ö†Ô∏è Archivo de presupuesto cargado est√° vac√≠o")
                return pd.DataFrame()

            logger.info(
                f"üìä DataFrame cargado: {len(df)} filas. Columnas: {list(df.columns)}"
            )

            df_clean = self._clean_phantom_rows(df)
            logger.info(f"üëª Filas tras limpieza fantasma: {len(df_clean)}")

            if df_clean.empty:
                logger.warning("‚ö†Ô∏è DataFrame vac√≠o despu√©s de limpiar filas fantasma")
                return pd.DataFrame()

            df_renamed = self._rename_columns(df_clean)
            logger.info(f"üè∑Ô∏è Columnas tras renombrado: {list(df_renamed.columns)}")

            valid_cols, error_msg = self._validate_required_columns(df_renamed)
            if not valid_cols:
                logger.error(
                    f"‚ùå Validaci√≥n de columnas requeridas fall√≥: {error_msg}"
                )
                return pd.DataFrame()

            df_converted = self._clean_and_convert_data(df_renamed)
            logger.info(f"üî¢ Filas tras conversi√≥n de datos: {len(df_converted)}")

            if df_converted.empty:
                logger.warning(
                    "‚ö†Ô∏è Conversi√≥n elimin√≥ todas las filas. Verifique limpieza de c√≥digos."
                )
                return pd.DataFrame()

            df_final = self._remove_duplicates(df_converted)

            final_cols = [
                ColumnNames.CODIGO_APU,
                ColumnNames.DESCRIPCION_APU,
                ColumnNames.CANTIDAD_PRESUPUESTO,
            ]
            available_cols = [col for col in final_cols if col in df_final.columns]

            if ColumnNames.CODIGO_APU not in available_cols:
                logger.error(
                    f"‚ùå Columna cr√≠tica '{ColumnNames.CODIGO_APU}' no disponible"
                )
                return pd.DataFrame()

            result = df_final[available_cols].copy()
            logger.info(f"‚úÖ Presupuesto procesado: {len(result)} filas")
            return result

        except Exception as e:
            logger.error(f"‚ùå Error fatal procesando presupuesto: {e}", exc_info=True)
            return pd.DataFrame()

    def _clean_phantom_rows(self, df: pd.DataFrame) -> pd.DataFrame:
        """Elimina filas completamente vac√≠as o con solo valores nulos."""
        if df is None:
            return pd.DataFrame()

        if not isinstance(df, pd.DataFrame):
            logger.error(f"‚ùå _clean_phantom_rows recibi√≥ tipo: {type(df).__name__}")
            return pd.DataFrame()

        if df.empty:
            return df

        initial_rows = len(df)

        # 1. First simple dropna for completely empty rows
        df_clean = df.dropna(how="all")

        # 2. Advanced check for "visually empty" rows
        # Convert to string, strip whitespace, lower case
        str_df = df_clean.astype(str).apply(lambda x: x.str.strip().str.lower())

        empty_patterns = {"", "nan", "none", "nat", "<na>"}

        # A row is empty if ALL its columns match the empty patterns
        is_empty_mask = str_df.isin(empty_patterns).all(axis=1)

        df_clean = df_clean[~is_empty_mask]

        removed_rows = initial_rows - len(df_clean)
        if removed_rows > 0:
            logger.debug(f"Filas fantasma eliminadas: {removed_rows}")

        return df_clean

    def _rename_columns(self, df: pd.DataFrame) -> pd.DataFrame:
        column_map = self.config.get("presupuesto_column_map", {})
        return find_and_rename_columns(df, column_map)

    def _validate_required_columns(
        self, df: pd.DataFrame
    ) -> Tuple[bool, Optional[str]]:
        return self.validator.validate_required_columns(
            df, [ColumnNames.CODIGO_APU], "presupuesto"
        )

    def _clean_and_convert_data(self, df: pd.DataFrame) -> pd.DataFrame:
        """Limpia y convierte datos con validaciones robustas."""
        if df is None or df.empty:
            return pd.DataFrame()

        df = df.copy()

        clean_code_params = self.config.get("clean_apu_code_params", {}).get(
            "presupuesto_item", {}
        )

        if ColumnNames.CODIGO_APU not in df.columns:
            logger.error(f"‚ùå Columna '{ColumnNames.CODIGO_APU}' no encontrada")
            return pd.DataFrame()

        try:

            def safe_clean(code):
                try:
                    return clean_apu_code(code, **clean_code_params)
                except (ValueError, TypeError):
                    return None

            df[ColumnNames.CODIGO_APU] = (
                df[ColumnNames.CODIGO_APU]
                .fillna("")
                .astype(str)
                .apply(safe_clean)
            )
        except Exception as e:
            logger.error(f"‚ùå Error limpiando c√≥digos APU: {e}")
            return pd.DataFrame()

        df = df.dropna(subset=[ColumnNames.CODIGO_APU])

        invalid_codes = {"", "nan", "none", "null"}
        mask_valid = df[ColumnNames.CODIGO_APU].notna() & ~df[
            ColumnNames.CODIGO_APU
        ].str.strip().str.lower().isin(invalid_codes)

        df = df[mask_valid].copy()

        if df.empty:
            logger.warning("‚ö†Ô∏è No quedaron registros con c√≥digos APU v√°lidos")
            return df

        if ColumnNames.CANTIDAD_PRESUPUESTO in df.columns:
            try:
                qty = df[ColumnNames.CANTIDAD_PRESUPUESTO].astype(str).str.strip()
                qty = qty.str.replace(",", ".", regex=False)
                qty = qty.str.replace(r"[^\d.\-]", "", regex=True)
                qty = qty.str.replace(r"\.(?=.*\.)", "", regex=True)

                df[ColumnNames.CANTIDAD_PRESUPUESTO] = pd.to_numeric(
                    qty, errors="coerce"
                ).fillna(0)

                max_qty = self.thresholds.max_quantity
                mask_invalid_qty = df[ColumnNames.CANTIDAD_PRESUPUESTO] > max_qty
                if mask_invalid_qty.any():
                    count = mask_invalid_qty.sum()
                    logger.warning(
                        f"‚ö†Ô∏è {count} cantidades exceden m√°ximo ({max_qty}), se limitar√°n"
                    )
                    df.loc[
                        mask_invalid_qty, ColumnNames.CANTIDAD_PRESUPUESTO
                    ] = max_qty

            except Exception as e:
                logger.error(f"‚ùå Error procesando cantidades: {e}")
                df[ColumnNames.CANTIDAD_PRESUPUESTO] = 0

        return df

    def _remove_duplicates(self, df: pd.DataFrame) -> pd.DataFrame:
        return self.validator.detect_and_log_duplicates(
            df, [ColumnNames.CODIGO_APU], "presupuesto"
        )


class InsumosProcessor:
    def __init__(self, thresholds: ProcessingThresholds, profile: dict):
        self.thresholds = thresholds
        self.profile = profile
        self.validator = DataValidator()

    def process(self, file_path: str) -> pd.DataFrame:
        try:
            records = self._parse_file(file_path)
            if not records:
                return pd.DataFrame()
            df = pd.DataFrame(records)
            df = self._rename_and_select_columns(df)
            df = self._convert_and_normalize(df)
            df = self._remove_duplicates(df)
            return df
        except Exception:
            return pd.DataFrame()

    def _parse_file(self, file_path: str) -> List[Dict]:
        encoding = self.profile.get("encoding", "latin1")
        try:
            with open(file_path, "r", encoding=encoding) as f:
                lines = f.readlines()
        except Exception:
            with open(file_path, "r", encoding="utf-8", errors="replace") as f:
                lines = f.readlines()

        records = []
        current_group = None
        header = None

        for line in lines:
            clean_line = line.strip()
            if not clean_line:
                continue

            parts = [p.strip().replace('"', "") for p in clean_line.split(";")]
            if not any(parts):
                continue

            first_col = parts[0].strip().upper()
            if first_col.startswith("G") and len(parts) > 1:
                candidate_group = parts[1].strip()
                if candidate_group:
                    logger.debug(
                        f"üîç Candidato a grupo detectado: '{parts[0]}' -> Grupo: '{candidate_group}'"
                    )
                    current_group = candidate_group
                    logger.info(f"üìÇ Grupo detectado: {current_group}")
                    header = None
                    continue

            if (
                "CODIGO" in first_col
                and len(parts) >= 2
                and "DESCRIPCION" in parts[1].upper()
            ):
                header = parts
                logger.info(
                    f"üìã Encabezado detectado para grupo {current_group}: {header}"
                )
                continue

            if header and current_group:
                if not parts[0]:
                    continue

                record = {ColumnNames.GRUPO_INSUMO: current_group}

                for i, col_name in enumerate(header):
                    if i < len(parts):
                        clean_col = col_name.upper().replace(".", "").strip()
                        if "DESCRIPCION" in clean_col:
                            record["DESCRIPCION"] = parts[i]
                        elif "VR" in clean_col and "UNIT" in clean_col:
                            record["VR. UNIT."] = parts[i]
                        elif "UND" in clean_col:
                            record["UND"] = parts[i]
                        elif "CODIGO" in clean_col:
                            record["CODIGO"] = parts[i]
                        elif "CANT" in clean_col:
                            record["CANTIDAD"] = parts[i]

                if "DESCRIPCION" in record:
                    records.append(record)

        logger.info(f"‚úÖ Total insumos extra√≠dos: {len(records)}")
        return records

    def _rename_and_select_columns(self, df: pd.DataFrame) -> pd.DataFrame:
        df = df.rename(
            columns={
                "DESCRIPCION": ColumnNames.DESCRIPCION_INSUMO,
                "VR. UNIT.": ColumnNames.VR_UNITARIO_INSUMO,
                "CODIGO": "CODIGO",
                "CANTIDAD": "CANTIDAD",
            }
        )
        if "CODIGO" not in df.columns:
            df["CODIGO"] = None
        if "CANTIDAD" not in df.columns:
            df["CANTIDAD"] = 0

        return df[
            [
                ColumnNames.GRUPO_INSUMO,
                ColumnNames.DESCRIPCION_INSUMO,
                ColumnNames.VR_UNITARIO_INSUMO,
                "CODIGO",
                "CANTIDAD",
            ]
        ]

    def _convert_and_normalize(self, df: pd.DataFrame) -> pd.DataFrame:
        df[ColumnNames.VR_UNITARIO_INSUMO] = pd.to_numeric(
            df[ColumnNames.VR_UNITARIO_INSUMO].astype(str).str.replace(",", "."),
            errors="coerce",
        )
        df[ColumnNames.DESCRIPCION_INSUMO_NORM] = normalize_text_series(
            df[ColumnNames.DESCRIPCION_INSUMO]
        )
        return df.dropna(subset=[ColumnNames.DESCRIPCION_INSUMO])

    def _remove_duplicates(self, df: pd.DataFrame) -> pd.DataFrame:
        return df.sort_values(
            ColumnNames.VR_UNITARIO_INSUMO, ascending=False
        ).drop_duplicates(subset=[ColumnNames.DESCRIPCION_INSUMO_NORM], keep="first")


class BaseCostProcessor(ABC):
    """Clase base para procesadores con logging y validaci√≥n."""

    def __init__(self, config: Dict[str, Any], thresholds: ProcessingThresholds):
        self.config = config
        self.thresholds = thresholds
        self._setup_logging()

    def _setup_logging(self):
        """Configura logging consistente."""
        self.logger = logging.getLogger(f"{self.__class__.__name__}")

    def _validate_input(self, df: pd.DataFrame, operation: str) -> bool:
        """Validaci√≥n com√∫n de input."""
        if df is None:
            self.logger.error(f"‚ùå DataFrame None en {operation}")
            return False

        if not isinstance(df, pd.DataFrame):
            self.logger.error(f"‚ùå Input no es DataFrame en {operation}")
            return False

        if df.empty:
            self.logger.warning(f"‚ö†Ô∏è DataFrame vac√≠o en {operation}")

        return True

    def _empty_results(self) -> Tuple[pd.DataFrame, ...]:
        """Retorna tupla de DataFrames vac√≠os por defecto."""
        return pd.DataFrame(), pd.DataFrame(), pd.DataFrame()

    @abstractmethod
    def calculate(self, *args, **kwargs):
        """M√©todo principal a implementar por subclases."""
        pass



class InformationGeometry:
    """
    Geometr√≠a de la informaci√≥n para espacios de datos.

    Implementa m√©trica de Fisher y divergencias de informaci√≥n.
    """

    def compute_entropy(self, df: pd.DataFrame) -> Dict[str, float]:
        """
        Calcula m√∫ltiples medidas de entrop√≠a/informaci√≥n.
        """
        if df.empty:
            return {
                "shannon_entropy": 0.0,
                "intrinsic_dimension": 0.0,
                "fisher_information": 0.0
            }

        # Entrop√≠a de Shannon (columnas categ√≥ricas)
        categorical_cols = df.select_dtypes(exclude=[np.number]).columns
        shannon_entropy = 0.0

        for col in categorical_cols:
            value_counts = df[col].value_counts(normalize=True)
            entropy = -sum(p * np.log2(p) for p in value_counts if p > 0)
            shannon_entropy += entropy

        # Dimensi√≥n intr√≠nseca (PCA)
        numeric_data = df.select_dtypes(include=[np.number]).fillna(0).values
        intrinsic_dim = 0.0

        if len(numeric_data) > 1 and numeric_data.shape[1] > 1:
            pca = PCA()
            pca.fit(numeric_data)
            # Dimensi√≥n como n√∫mero de componentes que explican 95% varianza
            explained_variance = np.cumsum(pca.explained_variance_ratio_)
            intrinsic_dim = np.argmax(explained_variance >= 0.95) + 1

        # Informaci√≥n de Fisher (variabilidad)
        fisher_info = 0.0
        if len(numeric_data) > 1:
            # Aproximaci√≥n diagonal de matriz de Fisher
            variances = np.var(numeric_data, axis=0)
            fisher_info = np.sum(1.0 / (variances + 1e-10))

        return {
            "shannon_entropy": shannon_entropy,
            "intrinsic_dimension": intrinsic_dim,
            "fisher_information": fisher_info
        }

class ProcrustesAnalyzer:
    """
    Analizador de alineamiento Procrustes para DataFrames.
    """

    def isometric_align(self, X: np.ndarray, Y: np.ndarray):
        """
        Alineamiento isom√©trico (r√≠gido): preserva distancias.

        Minimiza ||X - YR||_F sujeto a R^T R = I (ortogonal).
        """
        # Validar dimensiones compatibles para Procrustes est√°ndar (filas iguales)
        if X.shape[0] != Y.shape[0]:
            # Si no coinciden filas, no podemos alinear punto a punto.
            # Retornamos sin cambios y Matriz identidad si dimensiones coinciden
            return X, Y, np.eye(Y.shape[1] if Y.ndim > 1 else 1)

        X_centered = X - X.mean(axis=0)
        Y_centered = Y - Y.mean(axis=0)

        # Validar que podemos multiplicar
        if X_centered.shape[1] != Y_centered.shape[1]:
             # Si columnas difieren, necesitamos padding
             max_cols = max(X_centered.shape[1], Y_centered.shape[1])
             X_pad = np.pad(X_centered, ((0,0), (0, max_cols - X_centered.shape[1])))
             Y_pad = np.pad(Y_centered, ((0,0), (0, max_cols - Y_centered.shape[1])))
        else:
             X_pad = X_centered
             Y_pad = Y_centered

        try:
             U, _, Vt = np.linalg.svd(Y_pad.T @ X_pad)
             R = U @ Vt

             Y_aligned = Y_pad @ R

             # Recortar si hubo padding (complicado, devolvemos pad aligned)
             return X_pad, Y_aligned, R
        except Exception:
             return X, Y, None

    def conformal_align(self, X: np.ndarray, Y: np.ndarray):
        """
        Alineamiento conforme: preserva √°ngulos pero permite escala.
        """
        X_centered = X - X.mean(axis=0)
        Y_centered = Y - Y.mean(axis=0)

        # Escala √≥ptima
        try:
            min_rows = min(X_centered.shape[0], Y_centered.shape[0])
            min_cols = min(X_centered.shape[1], Y_centered.shape[1])

            X_red = X_centered[:min_rows, :min_cols]
            Y_red = Y_centered[:min_rows, :min_cols]

            num = np.trace(Y_red.T @ X_red)
            den = np.trace(Y_red.T @ Y_red)
            scale = num / den if den != 0 else 1.0
        except:
            scale = 1.0

        # Rotaci√≥n (mismo que isom√©trico)
        _, _, R = self.isometric_align(X, Y)

        # Simplificaci√≥n
        if R is not None:
            # Necesitamos aplicar R a Y. R puede tener dimensiones aumentadas.
            pass

        return X_centered, Y_centered, (scale, R) # Placeholder functionality

    def affine_align(self, X: np.ndarray, Y: np.ndarray):
        return X, Y, None



class DataMerger(BaseCostProcessor):
    """
    Fusionador con m√©trica de informaci√≥n y preservaci√≥n topol√≥gica.

    Implementa merge como fibrado de datos sobre base com√∫n.
    """

    def __init__(self, thresholds: ProcessingThresholds):
        super().__init__({}, thresholds)
        self._match_stats = {}
        self._information_geometry = InformationGeometry()
        self._procrustes_analyzer = ProcrustesAnalyzer()

    def merge_apus_with_insumos(
        self,
        df_apus: pd.DataFrame,
        df_insumos: pd.DataFrame,
        alignment_strategy: str = "isometric",
        preserve_topology: bool = True
    ) -> pd.DataFrame:
        """
        Merge con alineamiento de geometr√≠a de la informaci√≥n.
        """
        # Validaci√≥n de variedades de datos
        if not self._validate_input(df_apus, "merge_apus"):
            return pd.DataFrame()
        if not self._validate_input(df_insumos, "merge_insumos"):
            return pd.DataFrame()

        # Calcular m√©tricas de informaci√≥n previas al merge
        info_apus = self._information_geometry.compute_entropy(df_apus)
        info_insumos = self._information_geometry.compute_entropy(df_insumos)

        logger.info(
            f"üßÆ Entrop√≠a de informaci√≥n: "
            f"APUs={info_apus['shannon_entropy']:.3f} bits, "
            f"Insumos={info_insumos['shannon_entropy']:.3f} bits"
        )

        # Alineamiento Procrustes para optimizar correspondencia
        # En la pr√°ctica esto requerir√≠a un embedding com√∫n.
        # Aqu√≠ es conceptual para alinear 'espacios'.
        # self._procrustes_align(...) - Simplificado: No alteramos los dataframes reales por ahora
        # porque alterar los datos (centrar, rotar) romper√≠a la integridad de los valores de negocio (precios, ids).
        # El an√°lisis Procrustes se usa aqu√≠ como m√©trica de diagn√≥stico o para 'match' fuzzy avanzado.

        # Merge con m√∫ltiples estrategias y votaci√≥n
        candidates = []

        strategies = [
            self._exact_merge,
            # self._semantic_merge, # Not implemented in snippet, skipping
        ]

        for strategy in strategies:
            try:
                result = strategy(df_apus.copy(), df_insumos.copy())
                match_quality = self._evaluate_merge_quality(result)
                candidates.append((match_quality, result, strategy.__name__))
            except Exception as e:
                logger.debug(f"Estrategia {strategy.__name__} fall√≥: {e}")

        if not candidates:
            logger.error("‚ùå Todas las estrategias de merge fallaron")
            return pd.DataFrame()

        # Seleccionar mejor candidato por m√©trica de calidad
        candidates.sort(key=lambda x: x[0], reverse=True)
        best_quality, best_result, best_strategy = candidates[0]

        logger.info(
            f"‚úÖ Merge √≥ptimo: {best_strategy} (calidad={best_quality:.3f})"
        )

        # Calcular m√©tricas post-merge
        info_merged = self._information_geometry.compute_entropy(best_result)

        # Verificar no colapso de dimensi√≥n
        dim_before = info_apus['intrinsic_dimension'] + info_insumos['intrinsic_dimension']
        dim_after = info_merged['intrinsic_dimension']
        dim_preservation = dim_after / dim_before if dim_before > 0 else 1.0

        if dim_preservation < 0.8:
            logger.warning(
                f"‚ö†Ô∏è Colapso dimensional detectado: "
                f"{dim_preservation:.1%} de dimensi√≥n preservada"
            )

        # Log statistics
        self._log_merge_statistics(best_result)

        return best_result

    def _exact_merge(
        self,
        df_apus: pd.DataFrame,
        df_insumos: pd.DataFrame
    ) -> pd.DataFrame:
        """
        Merge exacto con preservaci√≥n de estructura algebraica.
        """
        try:
            # Asegurar columnas normalizadas
            if ColumnNames.NORMALIZED_DESC not in df_apus.columns:
                df_apus[ColumnNames.NORMALIZED_DESC] = normalize_text_series(
                    df_apus[ColumnNames.DESCRIPCION_INSUMO]
                )

            if ColumnNames.DESCRIPCION_INSUMO_NORM not in df_insumos.columns:
                df_insumos[ColumnNames.DESCRIPCION_INSUMO_NORM] = normalize_text_series(
                    df_insumos.get(ColumnNames.DESCRIPCION_INSUMO, pd.Series(dtype=str))
                )

            # Merge
            df_merged = pd.merge(
                df_apus,
                df_insumos,
                left_on=ColumnNames.NORMALIZED_DESC,
                right_on=ColumnNames.DESCRIPCION_INSUMO_NORM,
                how="left",
                suffixes=("_apu", "_insumo"),
                indicator="_merge"
            )

            # Preservar estructura de anillo
            df_merged = self._preserve_ring_structure(df_merged)

            # Validar inmersi√≥n
            self._validate_immersion(df_apus, df_insumos, df_merged)

            return df_merged

        except Exception as e:
            self.logger.error(f"‚ùå Error en merge exacto: {e}")
            raise

    def _preserve_ring_structure(self, df_merged: pd.DataFrame) -> pd.DataFrame:
        """
        Preserva estructura de anillo en el merge.
        """
        # Operaci√≥n suma: uni√≥n de espacios
        df_merged[ColumnNames.DESCRIPCION_INSUMO] = (
            df_merged[f"{ColumnNames.DESCRIPCION_INSUMO}_insumo"]
            .fillna(df_merged[f"{ColumnNames.DESCRIPCION_INSUMO}_apu"])
            .fillna(df_merged[ColumnNames.NORMALIZED_DESC])
        )

        # Operaci√≥n producto: combinaci√≥n de atributos
        for col in [ColumnNames.VR_UNITARIO_INSUMO, ColumnNames.CANTIDAD_APU]:
            if f"{col}_insumo" in df_merged.columns and f"{col}_apu" in df_merged.columns:
                df_merged[col] = df_merged[f"{col}_insumo"].fillna(
                    df_merged[f"{col}_apu"]
                )

        return df_merged

    def _validate_immersion(
        self,
        df_a: pd.DataFrame,
        df_b: pd.DataFrame,
        df_merged: pd.DataFrame
    ):
        """
        Valida que el merge sea una inmersi√≥n (no colapso dimensional).
        """
        dim_a = df_a.shape[1]
        dim_b = df_b.shape[1]
        dim_merged = df_merged.shape[1]

        common_cols = set(df_a.columns) & set(df_b.columns)
        expected_dim = dim_a + dim_b - len(common_cols)

        if dim_merged < expected_dim:
            # Pandas merge adds suffixes so dimension typically increases or stays.
            # This check is more heuristic about information content.
            pass

    def _evaluate_merge_quality(self, df_merged: pd.DataFrame) -> float:
        """
        Eval√∫a calidad del merge usando m√©trica compuesta.
        """
        if df_merged.empty:
            return 0.0

        metrics = []

        # 1. Completitud (no NaN)
        completeness = 1.0 - df_merged.isnull().mean().mean()
        metrics.append(completeness)

        # 2. Preservaci√≥n de cardinalidad
        if "_merge" in df_merged.columns:
            match_rate = (df_merged["_merge"] == "both").mean()
            metrics.append(match_rate)

        # 3. Consistencia de tipos
        type_consistency = self._compute_type_consistency(df_merged)
        metrics.append(type_consistency)

        return np.mean(metrics)

    def _compute_type_consistency(self, df: pd.DataFrame) -> float:
        """Calcula consistencia de tipos de datos."""
        type_counts = {}
        for col in df.columns:
            dtype = str(df[col].dtype)
            type_counts[dtype] = type_counts.get(dtype, 0) + 1

        total = sum(type_counts.values())
        proportions = [count/total for count in type_counts.values()]
        entropy = -sum(p * np.log(p) for p in proportions if p > 0)

        max_entropy = np.log(len(type_counts)) if type_counts else 1
        return entropy / max_entropy if max_entropy > 0 else 1.0

    def merge_with_presupuesto(
        self, df_presupuesto: pd.DataFrame, df_apu_costos: pd.DataFrame
    ) -> pd.DataFrame:
        """Fusiona presupuesto con costos APU de forma robusta."""
        if not self._validate_input(df_presupuesto, "merge_presupuesto_left"):
            return pd.DataFrame()

        if not self._validate_input(df_apu_costos, "merge_presupuesto_right"):
            return df_presupuesto.copy()

        try:
            df_merged = pd.merge(
                df_presupuesto,
                df_apu_costos,
                on=ColumnNames.CODIGO_APU,
                how="left",
                validate="1:1",
            )
            self.logger.info(
                f"‚úÖ Merge con presupuesto completado: {len(df_merged)} filas"
            )
            return df_merged

        except pd.errors.MergeError as e:
            self.logger.warning(f"‚ö†Ô∏è Duplicados detectados en merge 1:1: {e}")

            df_merged = pd.merge(
                df_presupuesto,
                df_apu_costos,
                on=ColumnNames.CODIGO_APU,
                how="left",
            )
            return df_merged

        except Exception as e:
            self.logger.error(f"‚ùå Error en merge con presupuesto: {e}")
            raise

    def _log_merge_statistics(self, df: pd.DataFrame):
        """Registra estad√≠sticas detalladas del merge."""
        if "_merge" in df.columns:
            stats = df["_merge"].value_counts(normalize=True) * 100
            self._match_stats = stats.to_dict()
            self.logger.info(f"üìä Estad√≠sticas merge: {self._match_stats}")

    def calculate(self, *args, **kwargs):
        pass


class APUCostCalculator(BaseCostProcessor):
    """
    Calculador de costos APU con clasificaci√≥n robusta.
    """

    def __init__(self, config: dict, thresholds: ProcessingThresholds):
        super().__init__(config, thresholds)
        self._setup_categoria_mapping()
        self._quality_metrics = {}

        config_path = config.get(
            "classification_rules_path", "config/config_rules.json"
        )
        self.classifier = APUClassifier(config_path)

    def _setup_categoria_mapping(self):
        """Configura mapeo usando Enum."""
        self._tipo_to_categoria = {
            InsumoType.MATERIAL: ColumnNames.MATERIALES,
            InsumoType.MANO_DE_OBRA: ColumnNames.MANO_DE_OBRA,
            InsumoType.EQUIPO: ColumnNames.EQUIPO,
            InsumoType.TRANSPORTE: ColumnNames.OTROS,
            InsumoType.HERRAMIENTA: ColumnNames.EQUIPO,
            InsumoType.SUBCONTRATO: ColumnNames.OTROS,
            InsumoType.OTROS: ColumnNames.OTROS,
        }

    def calculate(self, df_merged: pd.DataFrame) -> Tuple[pd.DataFrame, ...]:
        """Punto de entrada principal con validaci√≥n."""
        if not self._validate_input(df_merged, "calculate"):
            return self._empty_results()

        validation = DataFrameValidator.validate_schema(
            df_merged, [ColumnNames.CODIGO_APU, ColumnNames.COSTO_INSUMO_EN_APU]
        )
        if not validation.is_valid:
            self.logger.error(f"Esquema inv√°lido: {validation.errors}")
            return self._empty_results()

        try:
            df_normalized = self._normalize_tipo_insumo(df_merged)
            df_costs = self._aggregate_costs(df_normalized)
            df_unit = self._calculate_unit_values(df_costs)
            df_classified = self._classify_apus(df_unit)
            df_time = self._calculate_time(df_normalized)
            df_perf = self._calculate_performance(df_normalized)

            self._compute_quality_metrics(df_classified)

            return df_classified, df_time, df_perf

        except Exception as e:
            self.logger.error(f"‚ùå Error en pipeline: {e}", exc_info=True)
            return self._empty_results()

    def _normalize_tipo_insumo(self, df: pd.DataFrame) -> pd.DataFrame:
        """Normalizaci√≥n con Enum y m√©tricas."""
        df = df.copy()

        if ColumnNames.TIPO_INSUMO not in df.columns:
            df[ColumnNames.TIPO_INSUMO] = InsumoType.OTROS.value
        else:

            def map_to_enum(val):
                val_str = str(val).upper()
                if "MATERIAL" in val_str or "SUMINISTRO" in val_str:
                    return InsumoType.MATERIAL
                if (
                    "MANO" in val_str
                    or "OBRA" in val_str
                    or "CUADRILLA" in val_str
                ):
                    return InsumoType.MANO_DE_OBRA
                if "EQUIPO" in val_str or "HERRAMIENTA" in val_str:
                    return InsumoType.EQUIPO
                if "TRANSPORTE" in val_str:
                    return InsumoType.TRANSPORTE
                return InsumoType.OTROS

            df[ColumnNames.TIPO_INSUMO] = df[ColumnNames.TIPO_INSUMO].apply(
                map_to_enum
            )

        df["_CATEGORIA_COSTO"] = df[ColumnNames.TIPO_INSUMO].map(
            lambda x: self._tipo_to_categoria.get(x, ColumnNames.OTROS)
        )

        if not df.empty:
            stats = df["_CATEGORIA_COSTO"].value_counts(normalize=True) * 100
            self.logger.info(f"üìä Distribuci√≥n categor√≠as: {stats.to_dict()}")

        return df

    def _aggregate_costs(self, df: pd.DataFrame) -> pd.DataFrame:
        """Agrega costos por categor√≠a."""
        costs = (
            df.groupby([ColumnNames.CODIGO_APU, "_CATEGORIA_COSTO"])[
                ColumnNames.COSTO_INSUMO_EN_APU
            ]
            .sum()
            .unstack(fill_value=0)
            .reset_index()
        )

        for col in [
            ColumnNames.MATERIALES,
            ColumnNames.MANO_DE_OBRA,
            ColumnNames.EQUIPO,
            ColumnNames.OTROS,
        ]:
            if col not in costs.columns:
                costs[col] = 0.0

        return costs

    def _calculate_unit_values(self, df: pd.DataFrame) -> pd.DataFrame:
        df = df.copy()

        components = [
            ColumnNames.MATERIALES,
            ColumnNames.MANO_DE_OBRA,
            ColumnNames.EQUIPO,
            ColumnNames.OTROS,
        ]

        # Calcular Costo Total Unitario
        df[ColumnNames.PRECIO_UNIT_APU] = df[components].sum(axis=1)

        # Alias para compatibilidad con Frontend y Dashboard
        df["COSTO_UNITARIO_TOTAL"] = df[ColumnNames.PRECIO_UNIT_APU]
        df["PRECIO_UNIT_APU"] = df[ColumnNames.PRECIO_UNIT_APU]
        df["VALOR_TOTAL"] = df[ColumnNames.PRECIO_UNIT_APU]
        df[ColumnNames.VALOR_TOTAL_APU] = df[ColumnNames.PRECIO_UNIT_APU]

        df[ColumnNames.VALOR_SUMINISTRO_UN] = df[ColumnNames.MATERIALES]
        df[ColumnNames.VALOR_INSTALACION_UN] = (
            df[ColumnNames.MANO_DE_OBRA] + df[ColumnNames.EQUIPO]
        )
        df[ColumnNames.VALOR_CONSTRUCCION_UN] = df[ColumnNames.PRECIO_UNIT_APU]

        return df

    def _classify_apus(self, df: pd.DataFrame) -> pd.DataFrame:
        """Clasifica APUs usando el clasificador configurable."""
        if df.empty:
            logger.warning("DataFrame vac√≠o en clasificaci√≥n")
            return df

        required = [
            ColumnNames.VALOR_CONSTRUCCION_UN,
            ColumnNames.VALOR_SUMINISTRO_UN,
            ColumnNames.VALOR_INSTALACION_UN,
        ]

        for col in required:
            if col not in df.columns:
                logger.error(f"‚ùå Columna requerida faltante: {col}")
                df[ColumnNames.TIPO_APU] = self.classifier.default_type
                return df

        df_classified = self.classifier.classify_dataframe(
            df=df,
            col_total=ColumnNames.VALOR_CONSTRUCCION_UN,
            col_materiales=ColumnNames.VALOR_SUMINISTRO_UN,
            col_mo_eq=ColumnNames.VALOR_INSTALACION_UN,
            output_col=ColumnNames.TIPO_APU,
        )

        total_apus = len(df_classified)
        valid_apus = df_classified[
            (df_classified[ColumnNames.TIPO_APU] != self.classifier.default_type)
            & (
                df_classified[ColumnNames.TIPO_APU]
                != self.classifier.zero_cost_type
            )
        ].shape[0]

        coverage = (valid_apus / total_apus * 100) if total_apus > 0 else 0

        if coverage < 90:
            logger.warning(f"‚ö†Ô∏è Cobertura de clasificaci√≥n baja: {coverage:.1f}%")

        return df_classified

    def _calculate_time(self, df: pd.DataFrame) -> pd.DataFrame:
        return (
            df[df[ColumnNames.TIPO_INSUMO] == InsumoType.MANO_DE_OBRA]
            .groupby(ColumnNames.CODIGO_APU)[ColumnNames.CANTIDAD_APU]
            .sum()
            .reset_index()
            .rename(
                columns={ColumnNames.CANTIDAD_APU: ColumnNames.TIEMPO_INSTALACION}
            )
        )

    def _calculate_performance(self, df: pd.DataFrame) -> pd.DataFrame:
        if ColumnNames.RENDIMIENTO in df.columns:
            return (
                df[df[ColumnNames.TIPO_INSUMO] == InsumoType.MANO_DE_OBRA]
                .groupby(ColumnNames.CODIGO_APU)[ColumnNames.RENDIMIENTO]
                .sum()
                .reset_index()
                .rename(
                    columns={ColumnNames.RENDIMIENTO: ColumnNames.RENDIMIENTO_DIA}
                )
            )
        return pd.DataFrame()

    def _compute_quality_metrics(self, df: pd.DataFrame):
        """Calcula m√©tricas de calidad del procesamiento."""
        total_apus = len(df)
        classified = df[ColumnNames.TIPO_APU].notna().sum()

        self._quality_metrics = {
            "total_apus": total_apus,
            "classified_percentage": (
                (classified / total_apus * 100) if total_apus > 0 else 0
            ),
            "distribution": df[ColumnNames.TIPO_APU].value_counts().to_dict(),
            "cost_coverage": {
                "materiales": df[ColumnNames.MATERIALES].sum(),
                "mano_obra": df[ColumnNames.MANO_DE_OBRA].sum(),
                "equipo": df[ColumnNames.EQUIPO].sum(),
                "otros": df[ColumnNames.OTROS].sum(),
            },
        }

        self.logger.info(f"üìà M√©tricas de calidad: {self._quality_metrics}")

    def get_quality_report(self) -> Dict:
        """Reporte de m√©tricas de calidad."""
        return self._quality_metrics.copy()


# ==================== FUNCIONES AUXILIARES ====================


def calculate_insumo_costs(
    df: pd.DataFrame, thresholds: ProcessingThresholds
) -> pd.DataFrame:
    """Calcula costos de insumos de forma robusta."""
    if df is None or not isinstance(df, pd.DataFrame):
        logger.error("‚ùå DataFrame inv√°lido para c√°lculo de costos")
        return pd.DataFrame()

    if df.empty:
        return df

    df = df.copy()

    numeric_columns = [
        ColumnNames.CANTIDAD_APU,
        ColumnNames.VR_UNITARIO_INSUMO,
        ColumnNames.VALOR_TOTAL_APU,
    ]

    for col in numeric_columns:
        if col not in df.columns:
            df[col] = 0.0
        else:
            df[col] = pd.to_numeric(df[col], errors="coerce").fillna(0.0)

    max_cost = thresholds.max_cost_per_item
    max_qty = thresholds.max_quantity

    df[ColumnNames.CANTIDAD_APU] = df[ColumnNames.CANTIDAD_APU].clip(0, max_qty)
    df[ColumnNames.VR_UNITARIO_INSUMO] = df[ColumnNames.VR_UNITARIO_INSUMO].clip(
        0, max_cost
    )

    tiene_precio = df[ColumnNames.VR_UNITARIO_INSUMO].notna() & (
        df[ColumnNames.VR_UNITARIO_INSUMO] > 0
    )

    df[ColumnNames.COSTO_INSUMO_EN_APU] = np.where(
        tiene_precio,
        df[ColumnNames.CANTIDAD_APU] * df[ColumnNames.VR_UNITARIO_INSUMO],
        df[ColumnNames.VALOR_TOTAL_APU],
    )

    df[ColumnNames.COSTO_INSUMO_EN_APU] = (
        pd.to_numeric(df[ColumnNames.COSTO_INSUMO_EN_APU], errors="coerce")
        .fillna(0.0)
        .clip(0, thresholds.max_total_cost)
    )

    df[ColumnNames.VR_UNITARIO_FINAL] = (
        df[ColumnNames.VR_UNITARIO_INSUMO].fillna(0.0).clip(0, max_cost)
    )

    total_costo = df[ColumnNames.COSTO_INSUMO_EN_APU].sum()
    registros_sin_costo = (df[ColumnNames.COSTO_INSUMO_EN_APU] == 0).sum()

    if registros_sin_costo > 0:
        pct = (registros_sin_costo / len(df)) * 100
        logger.warning(
            f"‚ö†Ô∏è {registros_sin_costo} registros ({pct:.1f}%) sin costo calculado"
        )

    logger.debug(f"Costo total calculado: {total_costo:,.2f}")

    return df


def group_and_split_description(df: pd.DataFrame) -> pd.DataFrame:
    if ColumnNames.DESCRIPCION_APU in df.columns:
        df[ColumnNames.ORIGINAL_DESCRIPTION] = df[ColumnNames.DESCRIPCION_APU]
    return df


def calculate_total_costs(
    df: pd.DataFrame, thresholds: ProcessingThresholds
) -> pd.DataFrame:
    if ColumnNames.CANTIDAD_PRESUPUESTO in df.columns:
        qty = pd.to_numeric(
            df[ColumnNames.CANTIDAD_PRESUPUESTO], errors="coerce"
        ).fillna(0)
        df[ColumnNames.VALOR_CONSTRUCCION_TOTAL] = (
            df[ColumnNames.VALOR_CONSTRUCCION_UN] * qty
        )
    return df


def build_processed_apus_dataframe(df_costs, df_raw, df_time, df_perf):
    return df_costs.copy()


def synchronize_data_sources(df_merged, df_final):
    valid_codes = df_final[ColumnNames.CODIGO_APU].unique()
    return df_merged[df_merged[ColumnNames.CODIGO_APU].isin(valid_codes)].copy()


def build_output_dictionary(df_final, df_insumos, df_merged, df_raw, df_proc):
    insumos_grouped = {}
    if not df_insumos.empty and ColumnNames.GRUPO_INSUMO in df_insumos.columns:
        insumos_grouped = (
            df_insumos.groupby(ColumnNames.GRUPO_INSUMO)
            .apply(lambda x: x.to_dict("records"))
            .to_dict()
        )
    elif not df_insumos.empty:
        insumos_grouped = {"GENERAL": df_insumos.to_dict("records")}

    return {
        "presupuesto": df_final.to_dict("records"),
        "processed_apus": df_proc.to_dict("records"),
        "apus_detail": df_merged.to_dict("records"),
        "insumos": insumos_grouped,
    }


# ==================== ENTRY POINT ====================


def process_all_files(
    presupuesto_path: str,
    apus_path: str,
    insumos_path: str,
    config: dict,
    telemetry: TelemetryContext,
) -> dict:
    """Entry point refactorizado para usar PipelineDirector y Telemetry."""
    logger.info("üöÄ Iniciando procesamiento via PipelineDirector")

    director = PipelineDirector(config, telemetry)

    initial_context = {
        "presupuesto_path": presupuesto_path,
        "apus_path": apus_path,
        "insumos_path": insumos_path,
    }

    try:
        final_context = director.execute_pipeline_orchestrated(initial_context)
        final_result = final_context.get("final_result", {})

        output_dir = Path(config.get("output_dir", "data"))
        output_files = {
            "processed_apus": output_dir
            / config.get("processed_apus_file", "processed_apus.json"),
            "presupuesto_final": output_dir
            / config.get("presupuesto_final_file", "presupuesto_final.json"),
        }

        _save_output_files(final_result, output_files, config)

        return final_result

    except Exception as e:
        logger.error(f"‚ùå Error en process_all_files: {e}")
        return {"error": str(e)}


def _save_output_files(
    result: dict, output_files: dict, config: dict
) -> Dict[str, bool]:
    """Guarda archivos de salida de forma robusta."""
    import json

    if not result or not isinstance(result, dict):
        logger.error("‚ùå Resultado vac√≠o o inv√°lido para guardar")
        return {}

    if not output_files or not isinstance(output_files, dict):
        logger.error("‚ùå output_files vac√≠o o inv√°lido")
        return {}

    save_status = {}

    for name, path in output_files.items():
        try:
            if name not in result:
                logger.debug(f"Clave '{name}' no encontrada en resultado, saltando")
                save_status[name] = False
                continue

            data = result[name]
            if not data:
                logger.debug(f"Datos vac√≠os para '{name}', saltando")
                save_status[name] = False
                continue

            path = Path(path)
            try:
                path.parent.mkdir(parents=True, exist_ok=True)
            except PermissionError:
                logger.error(f"‚ùå Sin permisos para crear directorio: {path.parent}")
                save_status[name] = False
                continue
            except Exception as e:
                logger.error(f"‚ùå Error creando directorio {path.parent}: {e}")
                save_status[name] = False
                continue

            try:
                sanitized_data = sanitize_for_json(data)
            except Exception as e:
                logger.error(f"‚ùå Error sanitizando datos para '{name}': {e}")
                save_status[name] = False
                continue

            temp_path = path.with_suffix(".tmp")

            try:
                with open(temp_path, "w", encoding="utf-8") as f:
                    json.dump(
                        sanitized_data,
                        f,
                        indent=2,
                        ensure_ascii=False,
                        default=str,
                    )

                temp_path.replace(path)
                logger.info(f"‚úÖ Archivo guardado: {path}")
                save_status[name] = True

            except Exception as e:
                logger.error(f"‚ùå Error escribiendo '{name}': {e}")
                if temp_path.exists():
                    try:
                        temp_path.unlink()
                    except:
                        pass
                save_status[name] = False

        except Exception as e:
            logger.error(f"‚ùå Error inesperado guardando '{name}': {e}")
            save_status[name] = False

    successful = sum(1 for v in save_status.values() if v)
    total = len(save_status)
    logger.info(f"üìÅ Archivos guardados: {successful}/{total}")

    return save_status
