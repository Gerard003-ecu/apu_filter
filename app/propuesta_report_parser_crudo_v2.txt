def _compute_semantic_cache_key(self, line: str) -> str:
    """
    Computa clave de cache basada en invariantes topológicos.

    Preserva semántica mientras normaliza variaciones sintácticas superficiales.
    La función define una relación de equivalencia sobre el espacio de líneas,
    donde líneas topológicamente equivalentes colapsan al mismo punto en el
    espacio cociente.

    Args:
        line: La línea de texto.

    Retorna:
        str: El hash semántico que actúa como representante canónico de la clase.
    """
    # Normalización de espacios (homeomorfismo de espaciado: ℝⁿ → ℝⁿ/~)
    normalized = re.sub(r"\s+", " ", line.strip())

    # CORRECCIÓN: El regex anterior eliminaba ceros significativos en decimales
    # como "0.5" → ".5". Ahora solo normaliza ceros redundantes en enteros.
    # Ejemplo: "007" → "7", pero "0.5" permanece intacto
    normalized = re.sub(r"\b0+(\d+)\b(?!\.)", r"\1", normalized)

    # Normalización de separadores decimales para invariancia regional
    # (Preserva la estructura numérica bajo diferentes convenciones)
    normalized = re.sub(r"(\d),(\d{3})(?!\d)", r"\1\2", normalized)  # Miles: 1,000 → 1000

    # Para líneas muy largas: proyección a espacio de características
    if len(normalized) > self._CACHE_KEY_MAX_LENGTH:
        # Vector de características estructurales (invariantes topológicos)
        num_groups = len(re.findall(r"\d+[.,]?\d*", normalized))
        alpha_groups = len(re.findall(r"[A-Za-zÁÉÍÓÚáéíóúÑñ]+", normalized))
        sep_count = normalized.count(";")
        total_len = len(normalized)

        # Muestreo de fronteras (preserva información de borde)
        prefix = normalized[:50]
        suffix = normalized[-30:]

        # Checksum del contenido medio para reducir colisiones
        middle_start = len(normalized) // 3
        middle_sample = normalized[middle_start:middle_start + 20]

        feature_string = (
            f"{prefix}|{middle_sample}|{suffix}|"
            f"{num_groups}|{alpha_groups}|{sep_count}|{total_len}"
        )
        return hashlib.sha256(feature_string.encode()).hexdigest()[:32]

    return normalized


def _validate_with_lark(
    self, line: str, use_cache: bool = True
) -> Tuple[bool, Optional[Any], str]:
    """
    Valida una línea usando el parser Lark con optimización topológica.

    Implementa un functor de validación F: Líneas → (Bool × Árbol? × Mensaje)
    que preserva la estructura categórica del espacio de parsing.

    Args:
        line: La línea de texto a validar.
        use_cache: Si es True, intenta usar el cache de parsing.

    Retorna:
        Tuple[bool, Optional[Any], str]: (Es válido, Árbol Lark, Mensaje de error).
    """
    # === PRECONDICIONES TOPOLÓGICAS (Verificación de dominio) ===
    if self.lark_parser is None:
        return (True, None, "Lark no disponible - validación omitida")

    if not line or not isinstance(line, str):
        return (False, None, "Línea vacía o tipo inválido")

    line_clean = line.strip()
    line_len = len(line_clean)

    # Verificación de límites del espacio métrico acotado
    if line_len > self._MAX_LINE_LENGTH:
        return (
            False,
            None,
            f"Línea excede límite topológico: {line_len} > {self._MAX_LINE_LENGTH}",
        )
    if line_len < self._MIN_LINE_LENGTH:
        return (
            False,
            None,
            f"Línea insuficiente topológicamente: {line_len} < {self._MIN_LINE_LENGTH}",
        )

    # === CACHE SEMÁNTICO (Memoización sobre espacio cociente) ===
    cache_key = self._compute_semantic_cache_key(line_clean) if use_cache else None

    if use_cache and cache_key and cache_key in self._parse_cache:
        self.validation_stats.cached_parses += 1
        cached_result = self._parse_cache[cache_key]

        if isinstance(cached_result, tuple) and len(cached_result) == 2:
            is_valid, tree = cached_result
            # Verificar invariante: árbol válido implica estructura preservada
            if is_valid and tree is not None and not self._is_valid_tree(tree):
                del self._parse_cache[cache_key]
            else:
                reason = "" if is_valid else "Falló previamente (cache válido)"
                return (is_valid, tree, reason)

    # === VALIDACIÓN DE CONECTIVIDAD ESTRUCTURAL (Pre-filtro homotópico) ===
    if not self._has_minimal_structural_connectivity(line_clean):
        if use_cache and cache_key:
            self._cache_result(cache_key, False, None)
        return (False, None, "Falta conectividad estructural mínima")

    # === PARSING CON MANEJO JERÁRQUICO DE ERRORES (Estratificación del codominio) ===
    error_msg = ""
    try:
        tree = self.lark_parser.parse(line_clean)

        # Verificar homotopía del árbol resultante
        if not self._validate_tree_homotopy(tree):
            if use_cache and cache_key:
                self._cache_result(cache_key, False, None)
            return (False, None, "Árbol no cumple invariantes de homotopía")

        if use_cache and cache_key:
            self._cache_result(cache_key, True, tree)

        return (True, tree, "")

    except UnexpectedCharacters as uc:
        self.validation_stats.failed_lark_unexpected_chars += 1
        column = getattr(uc, "column", 0)
        context = self._get_topological_context(line_clean, column)
        error_msg = f"Carácter discontinuo en vecindad {context}"

    except UnexpectedToken as ut:
        self.validation_stats.failed_lark_parse += 1
        expected = list(ut.expected) if hasattr(ut, "expected") and ut.expected else []
        expected_space = self._map_tokens_to_topological_space(expected)
        token_repr = getattr(ut, "token", "desconocido")
        error_msg = f"Token '{token_repr}' fuera del espacio {expected_space}"

    except UnexpectedInput as ui:
        # CORRECCIÓN: Este caso nunca se capturaba, dejando el contador en 0
        self.validation_stats.failed_lark_unexpected_input += 1
        pos = getattr(ui, "pos_in_stream", 0)
        context = self._get_topological_context(line_clean, pos)
        error_msg = f"Entrada inesperada en posición {pos}: {context}"

    except UnexpectedEOF:
        self.validation_stats.failed_lark_parse += 1
        completeness = self._calculate_topological_completeness(line_clean)
        error_msg = f"Fin prematuro (compleción {completeness:.0%})"

    except LarkError as le:
        self.validation_stats.failed_lark_parse += 1
        error_msg = f"Error Lark: {str(le)[:100]}"

    except Exception as e:
        self.validation_stats.failed_lark_parse += 1
        logger.error(f"Error inesperado en validación Lark: {type(e).__name__}: {e}")
        error_msg = f"Error inesperado: {type(e).__name__}"

    # Punto de salida unificado para errores (morfismo terminal)
    if use_cache and cache_key:
        self._cache_result(cache_key, False, None)
    return (False, None, error_msg)


def _calculate_topological_completeness(self, line: str) -> float:
    """
    Calcula el grado de compleción topológica de una línea.

    Basado en la teoría de compleción de espacios métricos, mide qué tan
    cerca está la línea de ser un "punto límite" válido en el espacio
    de insumos APU.

    Args:
        line: La línea a evaluar.

    Returns:
        float: Grado de compleción en [0, 1].
    """
    if not line or not isinstance(line, str):
        return 0.0

    # CORRECCIÓN: Normalizar separadores decimales de forma consistente
    # Preservamos la línea original para algunos checks
    normalized_for_numbers = line.replace(",", ".")

    # Componentes esenciales para un insumo APU completo
    # Cada componente define un abierto en el espacio de características
    components = {
        "descripcion": bool(re.search(r"[A-Za-zÁÉÍÓÚáéíóúÑñ]{3,}", line)),
        # CORRECCIÓN: El regex anterior buscaba solo al final ($), ahora busca en cualquier parte
        "cantidad": bool(re.search(r"\d+\.?\d*", normalized_for_numbers)),
        "unidad": bool(
            re.search(r"\b(UND|UN|M|M2|M3|KG|L|LT|GLN|GAL|HR|DIA|ML|CM|TON)\b", line, re.I)
        ),
        "precio": bool(re.search(r"\d{1,3}(?:[.,]\d{3})*(?:[.,]\d{2})?", line)),
        "separadores": line.count(";") >= 3,
    }

    # Pesos que reflejan la importancia topológica de cada componente
    weights = {
        "descripcion": 0.30,
        "cantidad": 0.25,
        "unidad": 0.20,
        "precio": 0.15,
        "separadores": 0.10,
    }

    score = sum(weights[k] for k, v in components.items() if v)

    # Ajuste por densidad de información (factor de regularización)
    info_chunks = len(re.findall(r"\S+", line))
    separators = line.count(";")
    expected_chunks = max(separators + 1, 1)

    # Densidad: relación entre información presente y estructura esperada
    density_factor = min(info_chunks / expected_chunks, 1.5) / 1.5

    # Penalización suave si la densidad es muy baja (espacio ralo)
    if density_factor < 0.4:
        score *= 0.7 + (density_factor * 0.75)  # Escala de 0.7 a 1.0

    return min(max(score, 0.0), 1.0)


def _has_minimal_structural_connectivity(self, line: str) -> bool:
    """
    Verifica conectividad topológica mínima.

    Una línea tiene conectividad si forma un espacio conexo donde
    sus componentes (alfanuméricos y separadores) definen una
    partición no trivial del dominio.

    Args:
        line: La línea de texto.

    Retorna:
        bool: True si tiene conectividad mínima.
    """
    if not line:
        return False

    # Extracción de componentes estructurales
    alpha_sequences = re.findall(r"[A-Za-zÁÉÍÓÚáéíóúÑñ]{2,}", line)
    numeric_sequences = re.findall(r"\d+(?:[.,]\d+)?", line)
    separator_count = line.count(";")

    # Condiciones necesarias para conectividad
    has_alpha = len(alpha_sequences) >= 1
    has_numeric = len(numeric_sequences) >= 1
    min_separators = max(self._MIN_FIELDS_FOR_INSUMO - 1, 1)
    has_separators = separator_count >= min_separators

    if not (has_alpha and has_numeric and has_separators):
        return False

    line_len = len(line)
    if line_len < 10:
        return True  # Líneas cortas: conectividad trivial (espacio discreto)

    # MEJORA: Análisis de distribución topológica más robusto
    # Dividimos en tercios para mejor análisis de distribución
    third = line_len // 3
    segments = [
        line[:third],
        line[third:2*third],
        line[2*third:]
    ]

    # Contar segmentos con contenido semántico
    segments_with_content = sum(
        1 for seg in segments
        if re.search(r"[A-Za-z0-9]", seg)
    )

    # Contar segmentos con separadores (conexiones)
    segments_with_separators = sum(
        1 for seg in segments
        if ";" in seg
    )

    # Conectividad: contenido distribuido Y conexiones presentes
    # Debe haber contenido en al menos 2 segmentos
    # Y separadores en al menos 1 segmento (preferiblemente 2)
    well_distributed = segments_with_content >= 2 and segments_with_separators >= 1

    return well_distributed


def _is_apu_homeomorphic(self, tree: Any) -> bool:
    """
    Verifica que el árbol Lark sea homeomorfo a un registro de insumo APU válido.

    Un homeomorfismo preserva la estructura topológica: el árbol debe poder
    deformarse continuamente al esquema canónico de un APU sin romper
    conexiones esenciales.

    Args:
        tree: El árbol Lark a verificar.

    Returns:
        bool: True si el árbol es homeomorfo al esquema APU.
    """
    if not self._is_valid_tree(tree):
        return False

    # Componentes esenciales que definen el espacio APU
    essential_components = {
        "descripcion": False,
        "valor_numerico": False,
        "estructura_campos": False,
    }

    # MEJORA: Importar Token una sola vez (evitar import dentro de función)
    try:
        from lark import Token
    except ImportError:
        # Fallback: asumir homeomorfismo si no podemos verificar
        logger.warning("No se pudo importar Token de Lark para verificación homeomórfica")
        return True

    def analyze_node(node, depth: int = 0) -> None:
        """Recorre el árbol acumulando evidencia de componentes."""
        # Protección contra recursión excesiva
        if depth > 30:
            return

        if isinstance(node, Token):
            token_type = getattr(node, "type", "")
            val = str(getattr(node, "value", "")).strip()

            if token_type == "SEP" or val == ";":
                essential_components["estructura_campos"] = True
            elif token_type in ("FIELD_VALUE", "NUMBER", "DECIMAL"):
                if re.search(r"\d", val):
                    essential_components["valor_numerico"] = True
                if re.search(r"[a-zA-ZÁÉÍÓÚáéíóúÑñ]{3,}", val):
                    essential_components["descripcion"] = True
            elif token_type in ("WORD", "TEXT", "DESCRIPTION"):
                if len(val) >= 3:
                    essential_components["descripcion"] = True

        elif hasattr(node, "children") and node.children:
            # Verificar nombre del nodo para inferir estructura
            node_data = getattr(node, "data", "")
            if "field" in str(node_data).lower():
                essential_components["estructura_campos"] = True

            for child in node.children:
                analyze_node(child, depth + 1)

    try:
        analyze_node(tree)
    except RecursionError:
        logger.warning("Recursión excesiva en análisis homeomórfico")
        return False

    # Condición de homeomorfismo: descripción + valor numérico es suficiente
    # La estructura de campos es implícita en la gramática
    has_core_structure = (
        essential_components["descripcion"] and
        essential_components["valor_numerico"]
    )

    return has_core_structure


def _calculate_field_entropy(self, fields: List[str]) -> float:
    """
    Calcula la entropía de Shannon normalizada sobre la distribución de tipos de campos.

    La entropía mide el desorden/información en la tipificación, donde:
    - Entropía alta: campos heterogéneos (señal rica)
    - Entropía baja: campos homogéneos (posible ruido o datos incompletos)

    Args:
        fields: Lista de campos a analizar.

    Returns:
        float: Entropía normalizada en [0, 1].
    """
    if not fields:
        return 0.0

    n_fields = len(fields)
    if n_fields == 1:
        return 0.0  # Un solo campo: entropía cero (determinístico)

    # Clasificación de tipos por campo
    type_counts: Dict[str, int] = {"alpha": 0, "numeric": 0, "mixed": 0, "empty": 0}

    for field in fields:
        field_str = str(field).strip() if field else ""

        if not field_str:
            type_counts["empty"] += 1
        else:
            # Normalizar para detección numérica
            normalized = field_str.replace(",", "").replace(".", "").replace(" ", "")
            has_alpha = any(c.isalpha() for c in field_str)
            has_digit = any(c.isdigit() for c in field_str)

            if has_alpha and has_digit:
                type_counts["mixed"] += 1
            elif has_digit and normalized.isdigit():
                type_counts["numeric"] += 1
            elif has_alpha:
                type_counts["alpha"] += 1
            else:
                type_counts["empty"] += 1  # Símbolos puros → tratados como vacío

    # Cálculo de entropía de Shannon: H = -Σ p(x) log₂ p(x)
    entropy = 0.0
    for count in type_counts.values():
        if count > 0:
            p = count / n_fields
            # CORRECCIÓN: Usar math.log2 importado a nivel de módulo
            # Por seguridad, usamos la fórmula directa
            from math import log2
            entropy -= p * log2(p)

    # Normalización: dividir por entropía máxima posible
    # H_max = log₂(min(n_categorías, n_campos))
    n_categories = sum(1 for c in type_counts.values() if c > 0)
    if n_categories <= 1:
        return 0.0

    from math import log2
    max_entropy = log2(min(n_categories, n_fields))

    # CORRECCIÓN: Verificar división por cero explícitamente
    if max_entropy <= 0:
        return 0.0

    return min(entropy / max_entropy, 1.0)


def _calculate_numeric_cohesion(self, fields: List[str]) -> float:
    """
    Calcula la cohesión numérica: qué tan agrupados están los valores numéricos.

    Basado en la métrica de clustering: campos numéricos contiguos indican
    estructura coherente (ej: cantidad, precio_unitario, subtotal juntos).

    Args:
        fields: Lista de campos.

    Returns:
        float: Cohesión en [0, 1], donde 1 = máxima cohesión.
    """
    if not fields:
        return 0.0

    # Identificar posiciones de campos numéricos
    numeric_positions = []
    for i, f in enumerate(fields):
        field_str = str(f).strip() if f else ""
        # Campo es numérico si contiene dígitos significativos
        if field_str and re.search(r"\d+(?:[.,]\d+)?", field_str):
            # Verificar que no sea solo parte de un código alfanumérico
            digits_ratio = sum(c.isdigit() for c in field_str) / max(len(field_str), 1)
            if digits_ratio > 0.3:  # Al menos 30% dígitos
                numeric_positions.append(i)

    n_numeric = len(numeric_positions)

    if n_numeric == 0:
        return 0.0
    if n_numeric == 1:
        return 1.0  # Un solo número: cohesión perfecta trivial

    # MEJORA: Calcular cohesión como inversa de la dispersión normalizada
    # Dispersión = (max_pos - min_pos) / (n_fields - 1) si están dispersos
    # Cohesión ideal: todos contiguos → span = n_numeric - 1

    min_pos = min(numeric_positions)
    max_pos = max(numeric_positions)
    actual_span = max_pos - min_pos  # Rango cubierto
    ideal_span = n_numeric - 1  # Rango mínimo si fueran contiguos

    if actual_span == 0:
        return 1.0  # Todos en la misma posición (imposible, pero manejo edge case)

    # Cohesión: qué tan cerca está el span real del ideal
    # cohesion = ideal_span / actual_span (normalizado a [0,1])
    cohesion = ideal_span / actual_span

    # Factor de penalización por huecos: contar posiciones no-numéricas intercaladas
    gaps = 0
    for pos in range(min_pos, max_pos + 1):
        if pos not in numeric_positions:
            gaps += 1

    # Penalización suave por huecos
    gap_penalty = 1.0 - (gaps / max(actual_span, 1)) * 0.3

    return min(max(cohesion * gap_penalty, 0.0), 1.0)


def _detect_category(self, line_upper: str) -> Optional[str]:
    """
    Detección de categorías usando teoría de retículos sobre keywords.

    Cada categoría define un ideal principal en el retículo de strings,
    y buscamos el ínfimo (mejor match) en la semirretícula de categorías.

    Args:
        line_upper: Línea en mayúsculas.

    Returns:
        Optional[str]: Categoría detectada o None.
    """
    if not line_upper or not isinstance(line_upper, str):
        return None

    # Heurística de pre-filtro: líneas largas o muy numéricas no son categorías
    if len(line_upper) > 50:
        return None

    digit_count = sum(c.isdigit() for c in line_upper)
    if digit_count > 3:
        return None

    # Construir retículo de membresía: cada keyword genera un conjunto
    category_scores: Dict[str, float] = {}

    for canonical, variations in self.CATEGORY_KEYWORDS.items():
        total_score = 0.0
        for variation in variations:
            if self._is_category_match(variation, line_upper):
                total_score += self._calculate_match_strength(variation, line_upper)

        if total_score > 0:
            category_scores[canonical] = total_score

    if not category_scores:
        return None

    # Encontrar el supremo (mejor categoría) por fuerza de match
    best_category, best_score = max(category_scores.items(), key=lambda x: x[1])

    # Umbral topológico: debe superar un mínimo de confianza
    # MEJORA: Umbral adaptativo basado en longitud de línea
    min_threshold = 0.15 if len(line_upper) < 20 else 0.25

    if best_score >= min_threshold:
        return best_category

    return None


def _is_category_match(self, pattern: str, text: str) -> bool:
    """
    Verifica si pattern es un match válido para categoría en text.

    Maneja correctamente abreviaturas con punto y palabras completas.

    Args:
        pattern: Patrón de categoría a buscar.
        text: Texto donde buscar.

    Returns:
        bool: True si hay match.
    """
    if not pattern or not text:
        return False

    # CORRECCIÓN: Manejar abreviaturas con punto de forma especial
    if pattern.endswith("."):
        # Abreviatura: buscar con escape del punto, sin word boundary al final
        escaped = re.escape(pattern)
        regex = rf"\b{escaped}"
        return bool(re.search(regex, text, re.IGNORECASE))
    elif "." in pattern:
        # Punto interno (ej: "M.O."): escapar todo
        escaped = re.escape(pattern)
        return bool(re.search(escaped, text, re.IGNORECASE))
    else:
        # Palabra completa: usar word boundaries
        escaped = re.escape(pattern)
        # Permitir espacios flexibles
        escaped_flex = escaped.replace(r"\ ", r"\s+")
        regex = rf"\b{escaped_flex}\b"
        return bool(re.search(regex, text, re.IGNORECASE))


def _calculate_match_strength(self, pattern: str, text: str) -> float:
    """
    Calcula la fuerza del match en [0, 1] usando métrica topológica.

    Considera posición, completitud y contexto para ponderar el match.

    Args:
        pattern: Patrón encontrado.
        text: Texto completo.

    Returns:
        float: Fuerza del match.
    """
    text_len = len(text)
    if text_len == 0:
        return 0.0

    # Encontrar posición del match
    match_obj = re.search(re.escape(pattern), text, re.IGNORECASE)
    if not match_obj:
        return 0.0

    match_pos = match_obj.start()

    # 1. Peso por posición: matches al inicio son más significativos
    # Función de decaimiento exponencial suave
    position_weight = 1.0 - (match_pos / text_len) * 0.5

    # 2. Peso por completitud: palabra aislada vs parte de frase
    padded_text = f" {text} "
    padded_pattern = f" {pattern} "
    is_isolated = padded_pattern.upper() in padded_text.upper()
    completeness_weight = 1.0 if is_isolated else 0.7

    # 3. Peso contextual: línea corta sugiere encabezado de categoría
    if text_len < 20:
        context_weight = 2.0
    elif text_len < 35:
        context_weight = 1.5
    else:
        context_weight = 1.0

    # 4. MEJORA: Peso por cobertura del patrón sobre el texto
    coverage = len(pattern) / text_len
    coverage_weight = min(coverage * 2, 1.0)  # Normalizar a [0, 1]

    return position_weight * completeness_weight * context_weight * coverage_weight


def _build_insumo_record(
    self,
    context: APUContext,
    category: str,
    line: str,
    line_number: int,
    validation_result: LineValidationResult,
    fields: Optional[List[str]] = None,
) -> Dict[str, Any]:
    """
    Construye registro con métricas topológicas y firma estructural.

    El registro actúa como un punto en el espacio de insumos, con coordenadas
    dadas por las métricas topológicas que permiten clustering y clasificación.

    Args:
        context: Contexto del APU padre.
        category: Categoría del insumo.
        line: Línea original.
        line_number: Número de línea fuente.
        validation_result: Resultado de validación.
        fields: Campos pre-procesados (opcional).

    Returns:
        Dict[str, Any]: Registro estructurado del insumo.
    """
    if fields is None:
        fields = [f.strip() for f in line.split(";")]

    # Calcular métricas topológicas
    topological_metrics = {
        "field_entropy": self._calculate_field_entropy(fields),
        "structural_density": self._calculate_structural_density(line),
        "numeric_cohesion": self._calculate_numeric_cohesion(fields),
        "homogeneity_index": self._calculate_homogeneity_index(fields),
    }

    # Determinar clase de homeomorfismo
    homeomorphism_class = self._determine_homeomorphism_class(
        validation_result.validation_layer, topological_metrics
    )

    # MEJORA: No incluir el árbol Lark directamente (muy pesado para serialización)
    # En su lugar, guardamos si existía y su tipo de raíz
    lark_info = None
    if validation_result.lark_tree is not None:
        lark_info = {
            "has_tree": True,
            "root_type": getattr(validation_result.lark_tree, "data", "unknown"),
            "children_count": len(
                getattr(validation_result.lark_tree, "children", [])
            ),
        }

    record = {
        "apu_code": context.apu_code,
        "apu_desc": context.apu_desc,
        "apu_unit": context.apu_unit,
        "category": category,
        "insumo_line": line,
        "source_line": line_number,
        "fields_count": validation_result.fields_count,
        "validation_layer": validation_result.validation_layer,
        "homeomorphism_class": homeomorphism_class,
        "topological_metrics": topological_metrics,
        "_lark_info": lark_info,  # Metadata ligera en lugar del árbol completo
        "_structural_signature": self._compute_structural_signature(line),
    }

    return record


def get_parse_cache(self) -> Dict[str, Any]:
    """
    Retorna el cache de parsing para reutilización en APUProcessor.

    Filtra entradas inválidas y devuelve un diccionario de árboles válidos,
    realizando una proyección del cache al subespacio de árboles válidos.

    Returns:
        Dict[str, Any]: Diccionario {hash_semantico: arbol_lark}.
    """
    valid_cache: Dict[str, Any] = {}
    invalid_count = 0

    # CORRECCIÓN: Crear copia de items para evitar modificación durante iteración
    cache_items = list(self._parse_cache.items())

    for line, cached_value in cache_items:
        # Validar estructura del valor cacheado
        if not isinstance(cached_value, tuple) or len(cached_value) != 2:
            invalid_count += 1
            continue

        is_valid, tree = cached_value

        # Solo exportar árboles de parseos exitosos
        if not is_valid or tree is None:
            continue

        # Verificar integridad del árbol
        if not self._is_valid_tree(tree):
            invalid_count += 1
            continue

        # Computar clave normalizada para consistencia
        try:
            normalized_key = self._compute_semantic_cache_key(line)
            valid_cache[normalized_key] = tree
        except Exception as e:
            logger.debug(f"Error normalizando clave de cache: {e}")
            invalid_count += 1
            continue

    if invalid_count > 0:
        logger.debug(f"Cache: {invalid_count} entradas inválidas filtradas")

    logger.info(f"Cache de parsing exportado: {len(valid_cache)} árboles válidos")

    return valid_cache