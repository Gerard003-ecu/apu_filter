def _validate_with_lark(
    self, line: str, use_cache: bool = True
) -> Tuple[bool, Optional[Any], str]:
    """
    Valida una l√≠nea usando el parser Lark con optimizaci√≥n topol√≥gica.
    Refuerzo: Prefiltrado m√°s estricto, cache sem√°ntica y manejo jer√°rquico de errores.
    """
    # === PRECONDICIONES TOPOL√ìGICAS ===
    if self.lark_parser is None:
        return (True, None, "Lark no disponible - validaci√≥n omitida")
    
    if not line or not isinstance(line, str):
        return (False, None, "L√≠nea vac√≠a o tipo inv√°lido")
    
    line_clean = line.strip()
    
    # Filtro topol√≥gico: elimina l√≠neas que Lark nunca podr√≠a parsear
    if len(line_clean) > self._MAX_LINE_LENGTH:
        return (False, None, f"L√≠nea excede l√≠mite topol√≥gico: {len(line_clean)} > {self._MAX_LINE_LENGTH}")
    if len(line_clean) < self._MIN_LINE_LENGTH:
        return (False, None, f"L√≠nea insuficiente topol√≥gicamente: {len(line_clean)} < {self._MIN_LINE_LENGTH}")
    
    # === CACHE SEM√ÅNTICO ===
    # Normalizaci√≥n topol√≥gica: reduce variaciones sint√°cticas sin cambiar sem√°ntica
    cache_key = self._compute_semantic_cache_key(line_clean)
    
    if use_cache and cache_key in self._parse_cache:
        self.validation_stats.cached_parses += 1
        cached_result = self._parse_cache[cache_key]
        
        # Validaci√≥n invariante del cache
        if isinstance(cached_result, tuple) and len(cached_result) == 2:
            is_valid, tree = cached_result
            if is_valid and not self._is_valid_tree(tree):
                # Invariancia rota: reconstruir entrada
                del self._parse_cache[cache_key]
            else:
                return (is_valid, tree, "" if is_valid else "Fall√≥ en cach√© (v√°lido topol√≥gicamente)")
    
    # === PARSING CON MANEJO JER√ÅRQUICO DE ERRORES ===
    from lark.exceptions import (
        UnexpectedCharacters, UnexpectedToken, UnexpectedEOF,
        UnexpectedInput, LarkError
    )
    
    try:
        # An√°lisis de conectividad topol√≥gica: verificar que la l√≠nea tenga estructura m√≠nima
        if not self._has_minimal_structural_connectivity(line_clean):
            raise UnexpectedInput(f"Falta conectividad estructural: '{line_clean[:50]}...'")
        
        tree = self.lark_parser.parse(line_clean)
        
        # Validaci√≥n de homotop√≠a: el √°rbol debe ser deformable continuamente a un √°rbol v√°lido
        if not self._validate_tree_homotopy(tree):
            if use_cache:
                self._cache_result(cache_key, False, None)
            return (False, None, "√Årbol no cumple homotop√≠a estructural")
        
        # Cache de √©xito con invariante topol√≥gico
        if use_cache:
            self._cache_result(cache_key, True, tree)
        
        return (True, tree, "")
        
    except UnexpectedCharacters as uc:
        self.validation_stats.failed_lark_unexpected_chars += 1
        # An√°lisis de vecindad topol√≥gica del error
        context = self._get_topological_context(line_clean, uc.column)
        error_msg = f"Car√°cter discontinuo en vecindad {context}"
        if use_cache:
            self._cache_result(cache_key, False, None)
        return (False, None, f"Lark UnexpectedCharacters: {error_msg}")
        
    except UnexpectedToken as ut:
        self.validation_stats.failed_lark_parse += 1
        # Mapeo de tokens esperados a espacios topol√≥gicos
        expected_space = self._map_tokens_to_topological_space(ut.expected)
        error_msg = f"Token fuera del espacio esperado '{ut.token}' ‚àà {expected_space}"
        if use_cache:
            self._cache_result(cache_key, False, None)
        return (False, None, f"Lark UnexpectedToken: {error_msg}")
        
    except UnexpectedEOF as ueof:
        self.validation_stats.failed_lark_parse += 1
        # An√°lisis de compleci√≥n topol√≥gica
        completeness = self._calculate_topological_completeness(line_clean)
        error_msg = f"Fin prematuro (compleci√≥n {completeness:.0%})"
        if use_cache:
            self._cache_result(cache_key, False, None)
        return (False, None, f"Lark UnexpectedEOF: {error_msg}")
        
    except UnexpectedInput as ui:
        self.validation_stats.failed_lark_unexpected_input += 1
        if use_cache:
            self._cache_result(cache_key, False, None)
        return (False, None, f"Lark UnexpectedInput: {str(ui)[:100]}")
        
    except LarkError as le:
        self.validation_stats.failed_lark_parse += 1
        if use_cache:
            self._cache_result(cache_key, False, None)
        return (False, None, f"Lark Error: {str(le)[:100]}")
        
    except Exception as e:
        self.validation_stats.failed_lark_parse += 1
        logger.error(f"Error catastr√≥fico en validaci√≥n Lark: {type(e).__name__}: {e}")
        if use_cache:
            self._cache_result(cache_key, False, None)
        return (False, None, f"Error catastr√≥fico: {type(e).__name__}")

def _compute_semantic_cache_key(self, line: str) -> str:
    """
    Computa una clave de cache basada en invariantes topol√≥gicos.
    Conserva la sem√°ntica mientras normaliza variaciones sint√°cticas.
    """
    # Normalizaci√≥n topol√≥gica: colapsa espacios homeomorfos
    normalized = re.sub(r'\s+', ' ', line.strip())
    
    # Reducci√≥n a forma normal: quita ceros a la izquierda en n√∫meros
    normalized = re.sub(r'\b0+(\d+)', r'\1', normalized)
    
    # Normalizaci√≥n de separadores decimales (preserva topolog√≠a num√©rica)
    normalized = re.sub(r'(\d),(\d)', r'\1.\2', normalized)
    
    # Hash para l√≠neas muy largas (preserva la firma topol√≥gica)
    if len(normalized) > self._CACHE_KEY_MAX_LENGTH:
        import hashlib
        # Firma topol√≥gica: hash de caracter√≠sticas estructurales
        structural_features = [
            str(len(re.findall(r'\d+', normalized))),  # N√∫mero de grupos num√©ricos
            str(len(re.findall(r'[A-Za-z]+', normalized))),  # Grupos alfab√©ticos
            str(len(re.findall(r'[.;,]', normalized))),  # Separadores
        ]
        feature_hash = hashlib.md5('|'.join(structural_features).encode()).hexdigest()[:12]
        normalized = f"TOPOLOGICAL_HASH_{feature_hash}"
    
    return normalized

def _validate_tree_homotopy(self, tree: Any) -> bool:
    """
    Verifica que el √°rbol de parsing sea homot√≥picamente equivalente
    a un √°rbol v√°lido (puede deformarse continuamente a la forma correcta).
    """
    if tree is None:
        return False
    
    try:
        # Invariante fundamental: debe tener estructura de √°rbol
        if not hasattr(tree, 'data') or not hasattr(tree, 'children'):
            return False
        
        # Homotop√≠a tipo 1: la ra√≠z debe estar en el espacio de no-terminales v√°lidos
        valid_roots = {'insumo_line', 'descripcion', 'campo_numerico', 'unidad'}
        if tree.data not in valid_roots:
            return False
        
        # Homotop√≠a tipo 2: el n√∫mero de hijos debe formar un grupo conexo
        child_count = len(tree.children)
        if child_count == 0:
            # √Årbol degenerado (punto)
            return tree.data in {'unidad'}  # Algunos terminales pueden ser hojas
        elif child_count > 20:  # L√≠mite topol√≥gico de ramificaci√≥n
            return False
        
        # Homotop√≠a tipo 3: los hijos deben formar una secuencia continua
        # (sin huecos estructurales grandes entre ellos)
        positions = []
        if hasattr(tree, 'meta') and hasattr(tree.meta, 'positions'):
            positions = [p.start for p in tree.meta.positions if hasattr(p, 'start')]
        
        if positions and len(positions) > 1:
            # Verificar continuidad posicional (los nodos deben ser adyacentes)
            sorted_pos = sorted(positions)
            gaps = [sorted_pos[i+1] - sorted_pos[i] for i in range(len(sorted_pos)-1)]
            if max(gaps) > 50:  # Hueco topol√≥gicamente grande
                return False
        
        # Recursi√≥n para mantener la homotop√≠a en todo el √°rbol
        for child in tree.children:
            if hasattr(child, 'data'):  # Es un nodo no-terminal
                if not self._validate_tree_homotopy(child):
                    return False
        
        return True
        
    except Exception:
        return False

def _has_minimal_structural_connectivity(self, line: str) -> bool:
    """
    Verifica conectividad topol√≥gica m√≠nima: la l√≠nea debe contener
    componentes conectados (grupos alfanum√©ricos) con relaciones.
    """
    # Componentes conexos: secuencias de caracteres del mismo tipo
    alpha_sequences = re.findall(r'[A-Za-z√Å√â√ç√ì√ö√°√©√≠√≥√∫√ë√±]{2,}', line)
    numeric_sequences = re.findall(r'\d+\.?\d*', line)
    separator_count = line.count(';')
    
    # Teorema de conectividad APU: debe tener al menos un componente alfab√©tico,
    # uno num√©rico, y suficientes separadores para relacionarlos
    has_alpha = len(alpha_sequences) >= 1
    has_numeric = len(numeric_sequences) >= 1
    has_separators = separator_count >= self._MIN_FIELDS_FOR_INSUMO - 1
    
    # Verificar que los componentes est√©n adecuadamente distribuidos
    if has_alpha and has_numeric and has_separators:
        # An√°lisis de distribuci√≥n: los componentes no deben estar todos al inicio
        first_half = line[:len(line)//2]
        alpha_in_first = sum(1 for seq in alpha_sequences if seq in first_half)
        numeric_in_first = sum(1 for seq in numeric_sequences if seq in first_half)
        
        # Debe haber componentes en ambas mitades para una buena distribuci√≥n
        return (0 < alpha_in_first < len(alpha_sequences) or 
                0 < numeric_in_first < len(numeric_sequences))
    
    return False

def _get_topological_context(self, line: str, position: int, radius: int = 10) -> str:
    """
    Obtiene el contexto topol√≥gico alrededor de una posici√≥n.
    Muestra la vecindad Œµ del punto de error.
    """
    start = max(0, position - radius)
    end = min(len(line), position + radius)
    
    context = line[start:end]
    error_pos = position - start
    
    # Marcar el punto de error en el contexto
    if error_pos < len(context):
        marked = context[:error_pos] + '‚ü™' + context[error_pos] + '‚ü´' + context[error_pos+1:]
    else:
        marked = context + '‚ü™‚ê£‚ü´'
    
    return f"[...]{marked}[...]"

def _map_tokens_to_topological_space(self, expected_tokens: List[str]) -> str:
    """
    Mapea tokens esperados a espacios topol√≥gicos (categor√≠as).
    """
    token_spaces = {
        'NUMBER': 'Espacio Num√©rico ‚Ñù',
        'WORD': 'Espacio Lexical Œ£*',
        'UNIT': 'Espacio de Unidades ùí∞',
        'SEPARATOR': 'Espacio de Separaci√≥n ùíÆ',
        'DESCRIPTION': 'Espacio Descriptivo ùíü'
    }
    
    # Clasificar tokens esperados en espacios
    spaces = set()
    for token in expected_tokens:
        for key, space in token_spaces.items():
            if key in token.upper():
                spaces.add(space)
                break
        else:
            spaces.add('Espacio Desconocido ùí≥')
    
    return ' ‚à™ '.join(sorted(spaces)) if spaces else '‚àÖ'

def _calculate_topological_completeness(self, line: str) -> float:
    """
    Calcula el grado de compleci√≥n topol√≥gica de una l√≠nea.
    Basado en la teor√≠a de compleci√≥n de espacios m√©tricos.
    """
    # Componentes esenciales para un insumo APU completo
    components = {
        'descripcion': bool(re.search(r'[A-Za-z]{3,}', line)),  # 0.3
        'cantidad': bool(re.search(r'\d+\.?\d*\s*[A-Za-z]*$', line)),  # 0.25
        'unidad': bool(re.search(r'\b(UND|M|M2|M3|KG|L|GLN|HR|DIA)\b', line, re.I)),  # 0.2
        'precio': bool(re.search(r'\$\s*\d+\.?\d*|\d+\.?\d*\s*\$', line)),  # 0.15
        'separadores': line.count(';') >= 3,  # 0.1
    }
    
    weights = [0.3, 0.25, 0.2, 0.15, 0.1]
    score = sum(w for c, w in zip(components.values(), weights) if c)
    
    # Ajuste por densidad de informaci√≥n
    info_density = len(re.findall(r'\S+', line)) / max(len(line.split(';')), 1)
    score *= min(info_density, 1.5)  # Normalizar
    
    return min(score, 1.0)

def _validate_insumo_line(self, line: str, fields: List[str]) -> LineValidationResult:
    """
    Validaci√≥n topol√≥gica unificada con an√°lisis de invariantes.
    Refuerzo: Verificaci√≥n de homeomorfismo entre estructura y significado.
    """
    self.validation_stats.total_evaluated += 1
    
    # === PRIMERA CAPA: HOMEOMORFISMO DE TIPO ===
    if not line or not isinstance(line, str):
        return LineValidationResult(
            is_valid=False,
            reason="L√≠nea no es homeomorfa a string",
            fields_count=0,
            validation_layer="type_homeomorphism_failed",
        )
    
    if not fields or not isinstance(fields, list):
        return LineValidationResult(
            is_valid=False,
            reason="Campos no son homeomorfos a lista",
            fields_count=0,
            validation_layer="type_homeomorphism_failed",
        )
    
    # === SEGUNDA CAPA: INVARIANTES TOPOL√ìGICOS B√ÅSICOS ===
    basic_valid, basic_reason = self._validate_basic_structure(line, fields)
    
    if not basic_valid:
        # An√°lisis del grupo fundamental del error
        error_group = self._classify_basic_error_group(basic_reason)
        return LineValidationResult(
            is_valid=False,
            reason=f"Invariante roto [{error_group}]: {basic_reason}",
            fields_count=len(fields),
            has_numeric_fields=False,
            validation_layer="basic_invariant_failed",
        )
    
    # === TERCERA CAPA: HOMEOMORFISMO LARK ===
    lark_valid, lark_tree, lark_reason = self._validate_with_lark(line)
    
    if lark_valid:
        self.validation_stats.passed_lark += 1
        self.validation_stats.passed_both += 1
        
        # Verificar que el √°rbol Lark es homeomorfo a un registro APU
        if not self._is_apu_homeomorphic(lark_tree):
            return LineValidationResult(
                is_valid=False,
                reason="√Årbol no homeomorfo a esquema APU",
                fields_count=len(fields),
                has_numeric_fields=True,
                validation_layer="apu_homeomorphism_failed",
                lark_tree=lark_tree,
            )
        
        return LineValidationResult(
            is_valid=True,
            reason="Homeomorfismo completo preservado",
            fields_count=len(fields),
            has_numeric_fields=True,
            validation_layer="full_homeomorphism",
            lark_tree=lark_tree,
        )
    else:
        self._record_failed_sample(line, fields, lark_reason)
        
        # Clasificaci√≥n topol√≥gica del error Lark
        error_class = self._classify_lark_error_topology(lark_reason)
        return LineValidationResult(
            is_valid=False,
            reason=f"Homeomorfismo Lark roto [{error_class}]: {lark_reason}",
            fields_count=len(fields),
            has_numeric_fields=True,
            validation_layer="lark_homeomorphism_failed",
        )

def _classify_basic_error_group(self, reason: str) -> str:
    """Clasifica errores b√°sicos en grupos topol√≥gicos."""
    error_groups = {
        'campos': 'Grupo Cardinalidad G‚Çê',
        'num√©ricos': 'Grupo Medida G‚Çò',
        'subtotal': 'Grupo Agregaci√≥n G‚Çê',
        'decorativa': 'Grupo Trivial G‚ÇÄ',
    }
    
    for key, group in error_groups.items():
        if key in reason.lower():
            return group
    return 'Grupo Desconocido G‚Çì'

def _classify_lark_error_topology(self, reason: str) -> str:
    """Clasifica errores Lark en tipos topol√≥gicos."""
    if 'UnexpectedCharacters' in reason:
        return 'Espacio Discontinuo ùìì'
    elif 'UnexpectedToken' in reason:
        return 'Mapeo Incorrecto ùìú'
    elif 'UnexpectedEOF' in reason:
        return 'Borde Prematuro ùìë'
    elif 'UnexpectedInput' in reason:
        return 'Entrada Singular ùì¢'
    else:
        return 'Anomal√≠a ùìê'

def _is_apu_homeomorphic(self, tree: Any) -> bool:
    """
    Verifica que el √°rbol Lark sea homeomorfo (preserva estructura)
    a un registro de insumo APU v√°lido.
    """
    if not self._is_valid_tree(tree):
        return False
    
    # Un registro APU debe tener al menos estos componentes esenciales
    essential_components = {
        'descripcion': False,
        'valor_numerico': False,
        'separador': False,
    }
    
    # BFS para verificar componentes
    from collections import deque
    queue = deque([tree])
    
    while queue:
        node = queue.popleft()
        
        if hasattr(node, 'data'):
            node_type = str(node.data).lower()
            
            if 'desc' in node_type or 'word' in node_type:
                essential_components['descripcion'] = True
            elif 'number' in node_type or 'digit' in node_type:
                essential_components['valor_numerico'] = True
            elif 'separator' in node_type or 'semicolon' in node_type:
                essential_components['separador'] = True
            
            # A√±adir hijos al queue
            if hasattr(node, 'children'):
                queue.extend(node.children)
    
    # Debe tener todos los componentes esenciales
    return all(essential_components.values())

def _detect_category(self, line_upper: str) -> Optional[str]:
    """
    Detecci√≥n topol√≥gica de categor√≠as usando teor√≠a de ret√≠culos.
    Refuerzo: Identifica la categor√≠a como el √≠nfimo del conjunto de keywords.
    """
    if len(line_upper) > 50 or sum(c.isdigit() for c in line_upper) > 3:
        return None
    
    # Construir ret√≠culo de categor√≠as: cada keyword define un conjunto
    category_membership = {}
    
    for canonical, variations in self.CATEGORY_KEYWORDS.items():
        for variation in variations:
            # Usar l√≠mites superiores en el ret√≠culo (sup)
            if self._is_supremum_match(variation, line_upper):
                category_membership[canonical] = (
                    category_membership.get(canonical, 0) + 
                    self._calculate_match_strength(variation, line_upper)
                )
    
    if not category_membership:
        return None
    
    # Encontrar el √≠nfimo (mejor categor√≠a) por fuerza de match
    best_category = max(category_membership.items(), key=lambda x: x[1])
    
    # Umbral topol√≥gico: debe superar un m√≠nimo
    if best_category[1] > 0.5:
        return best_category[0]
    
    return None

def _is_supremum_match(self, pattern: str, text: str) -> bool:
    """
    Verifica si pattern es un supremo (l√≠mite superior) en el ret√≠culo de matches.
    Considera matches parciales, prefijos y sufijos.
    """
    # Normalizar para matching topol√≥gico
    pattern_norm = pattern.replace('.', '\\.').replace(' ', '\\s*')
    
    # Buscar como palabra completa o como prefijo/sufijo significativo
    if '.' in pattern:
        # Patr√≥n con abreviatura: match exacto
        return bool(re.search(rf'\b{pattern_norm}\b', text))
    else:
        # Palabra completa: puede ser parte de una frase
        return bool(re.search(rf'\b{pattern_norm}\b', text, re.IGNORECASE))

def _calculate_match_strength(self, pattern: str, text: str) -> float:
    """
    Calcula la fuerza del match en [0,1] usando m√©trica topol√≥gica.
    Considera posici√≥n, completitud y contexto.
    """
    # Peso por posici√≥n: matches al inicio son m√°s fuertes
    position_weight = 1.0
    match_pos = text.find(pattern)
    if match_pos >= 0:
        position_weight = 1.0 - (match_pos / len(text))
    
    # Peso por completitud: palabras completas vs parciales
    completeness_weight = 1.0 if f' {pattern} ' in f' {text} ' else 0.7
    
    # Peso contextual: l√≠nea corta sugiere categor√≠a, larga sugiere contenido
    context_weight = 2.0 if len(text) < 30 else 1.0
    
    return position_weight * completeness_weight * context_weight

def _build_insumo_record(
    self,
    context: APUContext,
    category: str,
    line: str,
    line_number: int,
    validation_result: LineValidationResult,
) -> Dict[str, Any]:
    """
    Construye registro con m√©tricas topol√≥gicas adicionales.
    Refuerzo: A√±ade invariantes y medidas de calidad estructural.
    """
    # Calcular m√©tricas topol√≥gicas
    fields = [f.strip() for f in line.split(';')]
    
    topological_metrics = {
        'field_entropy': self._calculate_field_entropy(fields),
        'structural_density': self._calculate_structural_density(line),
        'numeric_cohesion': self._calculate_numeric_cohesion(fields),
        'homogeneity_index': self._calculate_homogeneity_index(fields),
    }
    
    # Determinar clase de homeomorfismo
    homeomorphism_class = self._determine_homeomorphism_class(
        validation_result.validation_layer,
        topological_metrics
    )
    
    record = {
        "apu_code": context.apu_code,
        "apu_desc": context.apu_desc,
        "apu_unit": context.apu_unit,
        "category": category,
        "insumo_line": line,
        "source_line": line_number,
        "fields_count": validation_result.fields_count,
        "validation_layer": validation_result.validation_layer,
        "homeomorphism_class": homeomorphism_class,
        "topological_metrics": topological_metrics,
        "_lark_tree": validation_result.lark_tree,
        "_structural_signature": self._compute_structural_signature(line),
    }
    
    return record

def _calculate_field_entropy(self, fields: List[str]) -> float:
    """Calcula la entrop√≠a topol√≥gica de los campos."""
    if not fields:
        return 0.0
    
    # Distribuci√≥n de tipos por campo
    type_counts = {'alpha': 0, 'numeric': 0, 'mixed': 0, 'empty': 0}
    
    for field in fields:
        field = str(field).strip()
        if not field:
            type_counts['empty'] += 1
        elif field.replace('.', '').replace(',', '').isdigit():
            type_counts['numeric'] += 1
        elif any(c.isalpha() for c in field):
            if any(c.isdigit() for c in field):
                type_counts['mixed'] += 1
            else:
                type_counts['alpha'] += 1
    
    # Entrop√≠a de Shannon normalizada
    from math import log2
    
    total = len(fields)
    entropy = 0.0
    
    for count in type_counts.values():
        if count > 0:
            p = count / total
            entropy -= p * log2(p)
    
    # Normalizar a [0,1]
    max_entropy = log2(min(len(type_counts), total))
    return entropy / max_entropy if max_entropy > 0 else 0.0

def _calculate_structural_density(self, line: str) -> float:
    """Calcula la densidad estructural (informaci√≥n por car√°cter)."""
    # Informaci√≥n sem√°ntica aproximada
    words = re.findall(r'\b[A-Za-z]{3,}\b', line)
    numbers = re.findall(r'\d+\.?\d*', line)
    
    semantic_units = len(words) + len(numbers)
    total_chars = len(line)
    
    return semantic_units / total_chars if total_chars > 0 else 0.0

def _calculate_numeric_cohesion(self, fields: List[str]) -> float:
    """Calcula la cohesi√≥n num√©rica (qu√© tan juntos est√°n los n√∫meros)."""
    numeric_positions = [
        i for i, f in enumerate(fields) 
        if any(c.isdigit() for c in str(f))
    ]
    
    if len(numeric_positions) < 2:
        return 1.0 if numeric_positions else 0.0
    
    # Distancia promedio entre n√∫meros (normalizada)
    distances = [
        abs(numeric_positions[i] - numeric_positions[i-1]) 
        for i in range(1, len(numeric_positions))
    ]
    
    avg_distance = sum(distances) / len(distances)
    max_possible = len(fields) - 1
    
    # Cohesi√≥n inversa a la distancia
    return 1.0 - (avg_distance / max_possible) if max_possible > 0 else 1.0

def _calculate_homogeneity_index(self, fields: List[str]) -> float:
    """√çndice de homogeneidad (qu√© tan similares son los campos)."""
    if len(fields) < 2:
        return 1.0
    
    # Similaridad por tipo de datos
    field_types = []
    for field in fields:
        field_str = str(field).strip()
        if not field_str:
            field_types.append('empty')
        elif field_str.replace('.', '').replace(',', '').isdigit():
            field_types.append('numeric')
        elif any(c.isalpha() for c in field_str):
            if any(c.isdigit() for c in field_str):
                field_types.append('mixed')
            else:
                field_types.append('alpha')
        else:
            field_types.append('other')
    
    # Porcentaje del tipo m√°s com√∫n
    from collections import Counter
    type_counts = Counter(field_types)
    most_common_count = max(type_counts.values())
    
    return most_common_count / len(fields)

def _determine_homeomorphism_class(
    self, 
    validation_layer: str, 
    metrics: Dict[str, float]
) -> str:
    """Determina la clase de homeomorfismo del registro."""
    if validation_layer != "full_homeomorphism":
        return f"DEFECTIVO_{validation_layer.upper()}"
    
    # Clasificar seg√∫n m√©tricas topol√≥gicas
    if metrics['field_entropy'] > 0.7 and metrics['structural_density'] > 0.1:
        return "HOMEOMORFISMO_COMPLETO"
    elif metrics['numeric_cohesion'] > 0.8:
        return "HOMEOMORFISMO_NUMERICO"
    elif metrics['homogeneity_index'] > 0.6:
        return "HOMEOMORFISMO_HOMOGENEO"
    else:
        return "HOMEOMORFISMO_PARCIAL"

def _compute_structural_signature(self, line: str) -> str:
    """Computa una firma estructural √∫nica para la l√≠nea."""
    import hashlib
    
    # Extraer caracter√≠sticas estructurales invariantes
    features = [
        str(len(re.findall(r'[A-Z]', line))),
        str(len(re.findall(r'[a-z]', line))),
        str(len(re.findall(r'\d', line))),
        str(len(re.findall(r'[.;,]', line))),
        str(len(line.split())),
        str(len(line)),
    ]
    
    feature_string = '|'.join(features)
    return hashlib.sha256(feature_string.encode()).hexdigest()[:16]