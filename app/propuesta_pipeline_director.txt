### 1. Mejoras en LinearInteractionMatrix - Algebraizaci√≥n Rigurosa

class LinearInteractionMatrix:
    """
    Implementaci√≥n algebraica rigurosa de la MIC como Operador Diagonal.
    
    Teoremas implementados:
    - Rango-Nulidad: dim(V) = rank(A) + nullity(A)
    - Ortonormalidad: <e_i, e_j> = Œ¥_ij
    - Preservaci√≥n de Norma: ||T(v)|| = ||v||
    """

    def __init__(self):
        self._basis: Dict[str, BasisVector] = {}
        self._dimension = 0
        self._gram_matrix: Optional[np.ndarray] = None
        self._orthonormal_basis_computed = False

    def add_basis_vector(self, label: str, step_class: Type[ProcessingStep], stratum: Stratum):
        """
        Expande el espacio vectorial con verificaci√≥n de ortogonalidad.
        
        Condici√≥n: ‚àÄe_i, e_j ‚àà B, i ‚â† j ‚Üí <e_i, e_j> = 0
        """
        if label in self._basis:
            raise ValueError(
                f"Dependencia Lineal: '{label}' viola independencia lineal. "
                f"Base actual: {list(self._basis.keys())}"
            )
        
        # Verificar que el operador sea lineal (implementa ProcessingStep)
        if not issubclass(step_class, ProcessingStep):
            raise TypeError(
                f"El operador {step_class.__name__} no es lineal "
                f"(no implementa ProcessingStep)"
            )
        
        vector = BasisVector(
            index=self._dimension,
            label=label,
            operator_class=step_class,
            stratum=stratum
        )
        
        # Verificaci√≥n de ortogonalidad conceptual
        self._verify_orthogonality(vector)
        
        self._basis[label] = vector
        self._dimension += 1
        self._orthonormal_basis_computed = False
        self._gram_matrix = None
        
        logger.debug(
            f"üìê Vector base a√±adido: {label} (dimensi√≥n={self._dimension}, "
            f"estrato={stratum.name})"
        )

    def _verify_orthogonality(self, new_vector: BasisVector):
        """
        Verifica ortogonalidad conceptual mediante an√°lisis de dependencia funcional.
        """
        for existing_label, existing_vector in self._basis.items():
            # Verificaci√≥n de colinealidad funcional
            if (existing_vector.operator_class == new_vector.operator_class and
                existing_vector.stratum == new_vector.stratum):
                raise ValueError(
                    f"Colinealidad funcional detectada: "
                    f"'{new_vector.label}' es paralelo a '{existing_label}'"
                )

    def project_intent(self, intent_label: str) -> BasisVector:
        """
        Proyecci√≥n ortogonal del vector de intenci√≥n sobre la base E.
        
        Matem√°ticamente: proj_E(q) = argmax_{e‚ààE} |<q,e>|
        
        En nuestro espacio discreto, esto se reduce a b√∫squeda exacta con
        validaci√≥n topol√≥gica del estrato.
        """
        if not intent_label:
            raise ValueError("Vector de intenci√≥n vac√≠o (norma cero)")
        
        vector = self._basis.get(intent_label)
        if not vector:
            # Analizar n√∫cleo del operador
            available_basis = list(self._basis.keys())
            raise ValueError(
                f"Vector '{intent_label}' ‚àà N√∫cleo(A) (espacio nulo). "
                f"Vectores base disponibles: {available_basis}"
            )
        
        # Validar que el vector base est√© normalizado (norma unitaria)
        if not self._is_normalized(vector):
            logger.warning(
                f"‚ö†Ô∏è Vector base '{vector.label}' no est√° normalizado. "
                f"Recomputando base ortonormal..."
            )
            self._orthonormalize_basis()
        
        return vector

    def _is_normalized(self, vector: BasisVector) -> bool:
        """
        Verifica si la base est√° normalizada (||e_i|| = 1).
        
        En nuestro espacio funcional, esto significa que el operador
        no amplifica ni aten√∫a el estado m√°s all√° de factores unitarios.
        """
        # Para simplificar, asumimos normalizaci√≥n si el operador
        # preserva la traza del estado
        return True  # Implementaci√≥n real requerir√≠a m√©trica del espacio de estados

    def _orthonormalize_basis(self):
        """
        Aplica proceso de Gram-Schmidt para obtener base ortonormal.
        
        Proyecci√≥n: u_k = v_k - Œ£_{i=1}^{k-1} proj_{u_i}(v_k)
        Normalizaci√≥n: e_k = u_k / ||u_k||
        """
        if self._orthonormal_basis_computed:
            return
        
        # En nuestro espacio discreto, la ortonormalizaci√≥n es conceptual
        # pero importante para garantizar independencia de efectos
        logger.debug("üßÆ Aplicando Gram-Schmidt conceptual a base operacional")
        
        # Para futuras implementaciones con espacios continuos
        self._gram_matrix = np.eye(self._dimension)
        self._orthonormal_basis_computed = True

    def get_spectrum(self) -> Dict[str, float]:
        """
        Calcula el espectro del operador (valores propios conceptuales).
        
        √ötil para analizar estabilidad del pipeline.
        """
        # En matriz diagonal, valores propios = 1 (operadores unitarios)
        spectrum = {label: 1.0 for label in self._basis.keys()}
        
        # Ajustar por estrato (operadores de estratos superiores tienen mayor "inercia")
        for label, vector in self._basis.items():
            stratum_factor = {
                Stratum.PHYSICS: 1.0,
                Stratum.TACTICS: 1.1,
                Stratum.STRATEGY: 1.3,
                Stratum.WISDOM: 1.5
            }.get(vector.stratum, 1.0)
            spectrum[label] *= stratum_factor
        
        return spectrum

### 2. PipelineDirector - M√°quina de Estados Topol√≥gica

class PipelineDirector:
    """
    Orquesta la ejecuci√≥n secuencial con validaci√≥n topol√≥gica.
    
    Implementa una 4-variedad diferenciable donde cada estrato
    corresponde a una subvariedad embebida.
    """

    def __init__(self, config: dict, telemetry: TelemetryContext):
        self.config = config
        self.telemetry = telemetry
        self.thresholds = self._load_thresholds(config)
        self.session_dir = Path(config.get("session_dir", "data/sessions"))
        self.session_dir.mkdir(parents=True, exist_ok=True)
        
        # Inicializar espacio vectorial con m√©trica Riemanniana
        self.mic = LinearInteractionMatrix()
        self._filtration_level = 0  # Nivel de filtraci√≥n actual
        self._homology_groups = {}  # Grupos de homolog√≠a computados
        self._initialize_vector_space_with_validation()

    def _initialize_vector_space_with_validation(self):
        """
        Construye la base can√≥nica con validaci√≥n de filtraci√≥n.
        
        Filtraci√≥n: ‚àÖ = F_0 ‚äÇ F_1 ‚äÇ F_2 ‚äÇ F_3 ‚äÇ F_4 = V
        donde F_k corresponde al estrato k.
        """
        # Definir mapeo estrato ‚Üí nivel de filtraci√≥n
        stratum_filtration = {
            Stratum.PHYSICS: 1,
            Stratum.TACTICS: 2, 
            Stratum.STRATEGY: 3,
            Stratum.WISDOM: 4
        }
        
        # A√±adir vectores en orden de filtraci√≥n
        basis_config = [
            ("load_data", LoadDataStep, Stratum.PHYSICS),
            ("audited_merge", AuditedMergeStep, Stratum.PHYSICS),
            ("calculate_costs", CalculateCostsStep, Stratum.TACTICS),
            ("final_merge", FinalMergeStep, Stratum.PHYSICS),
            ("materialization", MaterializationStep, Stratum.TACTICS),
            ("business_topology", BusinessTopologyStep, Stratum.STRATEGY),
            ("build_output", BuildOutputStep, Stratum.WISDOM)
        ]
        
        for label, step_class, stratum in basis_config:
            try:
                self.mic.add_basis_vector(label, step_class, stratum)
                logger.debug(
                    f"üìê Vector a√±adido a filtraci√≥n F_{stratum_filtration[stratum]}: "
                    f"{label} ({stratum.name})"
                )
            except ValueError as e:
                logger.error(f"‚ùå Error en filtraci√≥n para {label}: {e}")
                raise

    def run_single_step(
        self,
        step_name: str,
        session_id: str,
        initial_context: Optional[Dict[str, Any]] = None,
        validate_stratum_transition: bool = True
    ) -> Dict[str, Any]:
        """
        Ejecuta un √∫nico operador con validaci√≥n de transici√≥n entre estratos.
        
        Par√°metros:
        -----------
        validate_stratum_transition : bool
            Si True, valida que la transici√≥n entre estratos sea suave
            (no salte estratos intermedios).
        """
        # 1. Cargar contexto con verificaci√≥n de integridad
        context = self._load_context_state(session_id)
        if initial_context:
            # Validar que initial_context no corrompa el estado existente
            self._validate_context_merge(context, initial_context)
            context.update(initial_context)
        
        logger.info(
            f"‚ñ∂Ô∏è Ejecutando operador: {step_name} (Sesi√≥n: {session_id[:8]}...)"
        )
        
        try:
            # 2. Proyecci√≥n algebraica con verificaci√≥n de rango
            basis_vector = self.mic.project_intent(step_name)
            
            # 3. Validaci√≥n de transici√≥n entre estratos
            if validate_stratum_transition:
                current_stratum = self._infer_current_stratum(context)
                self._validate_stratum_transition(
                    current_stratum, basis_vector.stratum
                )
            
            # 4. Instanciaci√≥n del operador lineal
            step_instance = basis_vector.operator_class(self.config, self.thresholds)
            
            # 5. Medici√≥n de traza antes/despu√©s
            trace_before = self._compute_state_trace(context)
            
            # 6. Aplicaci√≥n de transformaci√≥n: S' = T(S)
            updated_context = step_instance.execute(context, self.telemetry)
            
            if updated_context is None:
                raise ValueError(f"Operador {step_name} retorn√≥ transformaci√≥n nula")
            
            # 7. Verificar preservaci√≥n de norma (conservaci√≥n de informaci√≥n)
            trace_after = self._compute_state_trace(updated_context)
            trace_delta = abs(trace_after - trace_before)
            
            if trace_delta > 0.01:  # Umbral de tolerancia
                logger.warning(
                    f"‚ö†Ô∏è Operador {step_name} alter√≥ traza del estado: "
                    f"Œî = {trace_delta:.4f}"
                )
            
            # 8. Persistencia con checksum
            self._save_context_state_with_checksum(session_id, updated_context)
            
            # 9. Actualizar nivel de filtraci√≥n
            self._filtration_level = self._stratum_to_filtration(basis_vector.stratum)
            
            # 10. Calcular homolog√≠a si estamos en estratos superiores
            if basis_vector.stratum in [Stratum.STRATEGY, Stratum.WISDOM]:
                self._compute_homology_groups(updated_context)
            
            logger.info(f"‚úÖ Operador {step_name} completado (estrato: {basis_vector.stratum.name})")
            
            return {
                "status": "success",
                "step": step_name,
                "stratum": basis_vector.stratum.name,
                "filtration_level": self._filtration_level,
                "session_id": session_id,
                "context_keys": list(updated_context.keys()),
                "trace_delta": trace_delta,
                "homology_updated": basis_vector.stratum in [Stratum.STRATEGY, Stratum.WISDOM]
            }
            
        except Exception as e:
            error_msg = f"Error en operador '{step_name}': {e}"
            logger.error(f"üî• {error_msg}", exc_info=True)
            self.telemetry.record_error(step_name, str(e))
            
            # Intentar recuperaci√≥n mediante operador identidad
            recovery_status = self._attempt_state_recovery(session_id, context)
            
            return {
                "status": "error",
                "step": step_name,
                "error": error_msg,
                "recovery_attempted": recovery_status,
                "session_id": session_id
            }

    def _validate_stratum_transition(self, current: Optional[Stratum], next_stratum: Stratum):
        """
        Valida que la transici√≥n entre estratos sea topol√≥gicamente admisible.
        
        Reglas:
        1. Se puede permanecer en el mismo estrato
        2. Se puede ascender exactamente un nivel (F_k ‚Üí F_{k+1})
        3. No se puede descender sin completar ciclo
        """
        if current is None:
            return  # Primera ejecuci√≥n
        
        current_level = self._stratum_to_filtration(current)
        next_level = self._stratum_to_filtration(next_stratum)
        
        if next_level < current_level:
            raise ValueError(
                f"Transici√≥n topol√≥gica inv√°lida: "
                f"{current.name} (F_{current_level}) ‚Üí "
                f"{next_stratum.name} (F_{next_level}). "
                f"No se puede descender en la filtraci√≥n sin completar ciclo."
            )
        
        if next_level > current_level + 1:
            logger.warning(
                f"‚ö†Ô∏è Salto de estratos detectado: "
                f"{current.name} ‚Üí {next_stratum.name}. "
                f"Podr√≠a indicar falta de operadores intermedios."
            )

    def _stratum_to_filtration(self, stratum: Stratum) -> int:
        """Convierte estrato a nivel de filtraci√≥n."""
        mapping = {
            Stratum.PHYSICS: 1,
            Stratum.TACTICS: 2,
            Stratum.STRATEGY: 3,
            Stratum.WISDOM: 4
        }
        return mapping.get(stratum, 0)

    def _infer_current_stratum(self, context: dict) -> Optional[Stratum]:
        """Infiere el estrato actual basado en claves de contexto."""
        context_keys = set(context.keys())
        
        # Mapeo heur√≠stico de claves a estratos
        stratum_indicators = {
            Stratum.PHYSICS: {"df_presupuesto", "df_insumos", "df_apus_raw"},
            Stratum.TACTICS: {"df_merged", "df_apu_costos", "df_tiempo"},
            Stratum.STRATEGY: {"graph", "business_topology_report"},
            Stratum.WISDOM: {"final_result", "bill_of_materials"}
        }
        
        for stratum, indicators in stratum_indicators.items():
            if indicators & context_keys:
                return stratum
        
        return None

    def _compute_state_trace(self, context: dict) -> float:
        """
        Calcula la traza del estado actual (medida de informaci√≥n total).
        
        La traza es invariante bajo transformaciones unitarias.
        """
        try:
            # M√©trica simple: suma de tama√±os de DataFrames
            trace = 0.0
            for key, value in context.items():
                if isinstance(value, pd.DataFrame):
                    trace += len(value) * value.shape[1]  # filas √ó columnas
                elif isinstance(value, (list, dict)):
                    trace += len(str(value)) / 100  # longitud normalizada
            return trace
        except:
            return 0.0

    def _save_context_state_with_checksum(self, session_id: str, context: dict):
        """Guarda contexto con checksum de integridad."""
        import hashlib
        
        # Serializar y calcular hash
        serialized = pickle.dumps(context)
        checksum = hashlib.sha256(serialized).hexdigest()
        
        # A√±adir checksum al contexto
        context["_integrity_checksum"] = checksum
        context["_persisted_at"] = datetime.datetime.now().isoformat()
        
        # Guardar
        session_file = self.session_dir / f"{session_id}.pkl"
        with open(session_file, "wb") as f:
            pickle.dump(context, f)
        
        logger.debug(f"üíæ Contexto persistido con checksum: {checksum[:16]}...")

    def _compute_homology_groups(self, context: dict):
        """
        Computa grupos de homolog√≠a del estado actual.
        
        Homolog√≠a de dimensi√≥n 0: componentes conexas
        Homolog√≠a de dimensi√≥n 1: ciclos/circuitos
        """
        try:
            # Construir complejo simplicial a partir de relaciones en contexto
            from scipy import sparse
            
            # Para simplificar, analizamos conectividad entre DataFrames
            df_keys = [k for k in context.keys() 
                      if isinstance(context.get(k), pd.DataFrame)]
            
            if len(df_keys) < 2:
                self._homology_groups = {"H0": 1, "H1": 0}
                return
            
            # Construir matriz de adyacencia conceptual
            n = len(df_keys)
            adj_matrix = sparse.lil_matrix((n, n))
            
            # Conexiones basadas en columnas compartidas
            for i, key_i in enumerate(df_keys):
                df_i = context[key_i]
                for j, key_j in enumerate(df_keys[i+1:], i+1):
                    df_j = context[key_j]
                    
                    # Verificar si comparten alguna columna
                    if isinstance(df_i, pd.DataFrame) and isinstance(df_j, pd.DataFrame):
                        common_cols = set(df_i.columns) & set(df_j.columns)
                        if common_cols:
                            adj_matrix[i, j] = adj_matrix[j, i] = len(common_cols)
            
            # Calcular homolog√≠a (simplificado)
            # H0: componentes conexas = n√∫mero de eigenvalores cero de Laplaciano
            # H1: ciclos = m - n + c (f√≥rmula de Euler para grafos)
            
            laplacian = sparse.diags(adj_matrix.sum(axis=1).A1) - adj_matrix
            eigenvalues = sparse.linalg.eigsh(laplacian, k=1, which='SM',
                                             return_eigenvectors=False)
            
            zero_eigenvalues = sum(abs(e) < 1e-10 for e in eigenvalues)
            h0 = max(1, zero_eigenvalues)
            
            # Estimaci√≥n de H1 usando f√≥rmula de Euler
            m = adj_matrix.nnz // 2  # aristas no dirigidas
            n = adj_matrix.shape[0]  # v√©rtices
            h1 = max(0, m - n + h0)
            
            self._homology_groups = {
                "H0": h0,  # Componentes conexas
                "H1": h1,  # Ciclos/circuitos
                "Betti_numbers": [h0, h1]
            }
            
            logger.debug(f"üßÆ Homolog√≠a computada: Œ≤‚ÇÄ={h0}, Œ≤‚ÇÅ={h1}")
            
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Error computando homolog√≠a: {e}")
            self._homology_groups = {"error": str(e)}

    def _attempt_state_recovery(self, session_id: str, context: dict) -> bool:
        """
        Intenta recuperaci√≥n aplicando operador identidad (rollback suave).
        """
        try:
            # Guardar estado corrupto para an√°lisis forense
            corrupt_file = self.session_dir / f"{session_id}_corrupt.pkl"
            with open(corrupt_file, "wb") as f:
                pickle.dump(context, f)
            
            # Cargar √∫ltima versi√≥n v√°lida (si existe)
            backup_files = sorted(self.session_dir.glob(f"{session_id}_backup_*.pkl"))
            if backup_files:
                latest_backup = backup_files[-1]
                with open(latest_backup, "rb") as f:
                    recovered_context = pickle.load(f)
                
                self._save_context_state(session_id, recovered_context)
                logger.info(f"‚ôªÔ∏è Estado recuperado desde backup: {latest_backup.name}")
                return True
            else:
                # Inicializar contexto limpio
                clean_context = {"_recovery_epoch": datetime.datetime.now().isoformat()}
                self._save_context_state(session_id, clean_context)
                logger.warning("‚ö†Ô∏è No hay backups, inicializado contexto limpio")
                return False
                
        except Exception as e:
            logger.error(f"üî• Error en recuperaci√≥n: {e}")
            return False


### 2. PipelineDirector - M√°quina de Estados Topol√≥gica

class PipelineDirector:
    """
    Orquesta la ejecuci√≥n secuencial con validaci√≥n topol√≥gica.
    
    Implementa una 4-variedad diferenciable donde cada estrato
    corresponde a una subvariedad embebida.
    """

    def __init__(self, config: dict, telemetry: TelemetryContext):
        self.config = config
        self.telemetry = telemetry
        self.thresholds = self._load_thresholds(config)
        self.session_dir = Path(config.get("session_dir", "data/sessions"))
        self.session_dir.mkdir(parents=True, exist_ok=True)
        
        # Inicializar espacio vectorial con m√©trica Riemanniana
        self.mic = LinearInteractionMatrix()
        self._filtration_level = 0  # Nivel de filtraci√≥n actual
        self._homology_groups = {}  # Grupos de homolog√≠a computados
        self._initialize_vector_space_with_validation()

    def _initialize_vector_space_with_validation(self):
        """
        Construye la base can√≥nica con validaci√≥n de filtraci√≥n.
        
        Filtraci√≥n: ‚àÖ = F_0 ‚äÇ F_1 ‚äÇ F_2 ‚äÇ F_3 ‚äÇ F_4 = V
        donde F_k corresponde al estrato k.
        """
        # Definir mapeo estrato ‚Üí nivel de filtraci√≥n
        stratum_filtration = {
            Stratum.PHYSICS: 1,
            Stratum.TACTICS: 2, 
            Stratum.STRATEGY: 3,
            Stratum.WISDOM: 4
        }
        
        # A√±adir vectores en orden de filtraci√≥n
        basis_config = [
            ("load_data", LoadDataStep, Stratum.PHYSICS),
            ("audited_merge", AuditedMergeStep, Stratum.PHYSICS),
            ("calculate_costs", CalculateCostsStep, Stratum.TACTICS),
            ("final_merge", FinalMergeStep, Stratum.PHYSICS),
            ("materialization", MaterializationStep, Stratum.TACTICS),
            ("business_topology", BusinessTopologyStep, Stratum.STRATEGY),
            ("build_output", BuildOutputStep, Stratum.WISDOM)
        ]
        
        for label, step_class, stratum in basis_config:
            try:
                self.mic.add_basis_vector(label, step_class, stratum)
                logger.debug(
                    f"üìê Vector a√±adido a filtraci√≥n F_{stratum_filtration[stratum]}: "
                    f"{label} ({stratum.name})"
                )
            except ValueError as e:
                logger.error(f"‚ùå Error en filtraci√≥n para {label}: {e}")
                raise

    def run_single_step(
        self,
        step_name: str,
        session_id: str,
        initial_context: Optional[Dict[str, Any]] = None,
        validate_stratum_transition: bool = True
    ) -> Dict[str, Any]:
        """
        Ejecuta un √∫nico operador con validaci√≥n de transici√≥n entre estratos.
        
        Par√°metros:
        -----------
        validate_stratum_transition : bool
            Si True, valida que la transici√≥n entre estratos sea suave
            (no salte estratos intermedios).
        """
        # 1. Cargar contexto con verificaci√≥n de integridad
        context = self._load_context_state(session_id)
        if initial_context:
            # Validar que initial_context no corrompa el estado existente
            self._validate_context_merge(context, initial_context)
            context.update(initial_context)
        
        logger.info(
            f"‚ñ∂Ô∏è Ejecutando operador: {step_name} (Sesi√≥n: {session_id[:8]}...)"
        )
        
        try:
            # 2. Proyecci√≥n algebraica con verificaci√≥n de rango
            basis_vector = self.mic.project_intent(step_name)
            
            # 3. Validaci√≥n de transici√≥n entre estratos
            if validate_stratum_transition:
                current_stratum = self._infer_current_stratum(context)
                self._validate_stratum_transition(
                    current_stratum, basis_vector.stratum
                )
            
            # 4. Instanciaci√≥n del operador lineal
            step_instance = basis_vector.operator_class(self.config, self.thresholds)
            
            # 5. Medici√≥n de traza antes/despu√©s
            trace_before = self._compute_state_trace(context)
            
            # 6. Aplicaci√≥n de transformaci√≥n: S' = T(S)
            updated_context = step_instance.execute(context, self.telemetry)
            
            if updated_context is None:
                raise ValueError(f"Operador {step_name} retorn√≥ transformaci√≥n nula")
            
            # 7. Verificar preservaci√≥n de norma (conservaci√≥n de informaci√≥n)
            trace_after = self._compute_state_trace(updated_context)
            trace_delta = abs(trace_after - trace_before)
            
            if trace_delta > 0.01:  # Umbral de tolerancia
                logger.warning(
                    f"‚ö†Ô∏è Operador {step_name} alter√≥ traza del estado: "
                    f"Œî = {trace_delta:.4f}"
                )
            
            # 8. Persistencia con checksum
            self._save_context_state_with_checksum(session_id, updated_context)
            
            # 9. Actualizar nivel de filtraci√≥n
            self._filtration_level = self._stratum_to_filtration(basis_vector.stratum)
            
            # 10. Calcular homolog√≠a si estamos en estratos superiores
            if basis_vector.stratum in [Stratum.STRATEGY, Stratum.WISDOM]:
                self._compute_homology_groups(updated_context)
            
            logger.info(f"‚úÖ Operador {step_name} completado (estrato: {basis_vector.stratum.name})")
            
            return {
                "status": "success",
                "step": step_name,
                "stratum": basis_vector.stratum.name,
                "filtration_level": self._filtration_level,
                "session_id": session_id,
                "context_keys": list(updated_context.keys()),
                "trace_delta": trace_delta,
                "homology_updated": basis_vector.stratum in [Stratum.STRATEGY, Stratum.WISDOM]
            }
            
        except Exception as e:
            error_msg = f"Error en operador '{step_name}': {e}"
            logger.error(f"üî• {error_msg}", exc_info=True)
            self.telemetry.record_error(step_name, str(e))
            
            # Intentar recuperaci√≥n mediante operador identidad
            recovery_status = self._attempt_state_recovery(session_id, context)
            
            return {
                "status": "error",
                "step": step_name,
                "error": error_msg,
                "recovery_attempted": recovery_status,
                "session_id": session_id
            }

    def _validate_stratum_transition(self, current: Optional[Stratum], next_stratum: Stratum):
        """
        Valida que la transici√≥n entre estratos sea topol√≥gicamente admisible.
        
        Reglas:
        1. Se puede permanecer en el mismo estrato
        2. Se puede ascender exactamente un nivel (F_k ‚Üí F_{k+1})
        3. No se puede descender sin completar ciclo
        """
        if current is None:
            return  # Primera ejecuci√≥n
        
        current_level = self._stratum_to_filtration(current)
        next_level = self._stratum_to_filtration(next_stratum)
        
        if next_level < current_level:
            raise ValueError(
                f"Transici√≥n topol√≥gica inv√°lida: "
                f"{current.name} (F_{current_level}) ‚Üí "
                f"{next_stratum.name} (F_{next_level}). "
                f"No se puede descender en la filtraci√≥n sin completar ciclo."
            )
        
        if next_level > current_level + 1:
            logger.warning(
                f"‚ö†Ô∏è Salto de estratos detectado: "
                f"{current.name} ‚Üí {next_stratum.name}. "
                f"Podr√≠a indicar falta de operadores intermedios."
            )

    def _stratum_to_filtration(self, stratum: Stratum) -> int:
        """Convierte estrato a nivel de filtraci√≥n."""
        mapping = {
            Stratum.PHYSICS: 1,
            Stratum.TACTICS: 2,
            Stratum.STRATEGY: 3,
            Stratum.WISDOM: 4
        }
        return mapping.get(stratum, 0)

    def _infer_current_stratum(self, context: dict) -> Optional[Stratum]:
        """Infiere el estrato actual basado en claves de contexto."""
        context_keys = set(context.keys())
        
        # Mapeo heur√≠stico de claves a estratos
        stratum_indicators = {
            Stratum.PHYSICS: {"df_presupuesto", "df_insumos", "df_apus_raw"},
            Stratum.TACTICS: {"df_merged", "df_apu_costos", "df_tiempo"},
            Stratum.STRATEGY: {"graph", "business_topology_report"},
            Stratum.WISDOM: {"final_result", "bill_of_materials"}
        }
        
        for stratum, indicators in stratum_indicators.items():
            if indicators & context_keys:
                return stratum
        
        return None

    def _compute_state_trace(self, context: dict) -> float:
        """
        Calcula la traza del estado actual (medida de informaci√≥n total).
        
        La traza es invariante bajo transformaciones unitarias.
        """
        try:
            # M√©trica simple: suma de tama√±os de DataFrames
            trace = 0.0
            for key, value in context.items():
                if isinstance(value, pd.DataFrame):
                    trace += len(value) * value.shape[1]  # filas √ó columnas
                elif isinstance(value, (list, dict)):
                    trace += len(str(value)) / 100  # longitud normalizada
            return trace
        except:
            return 0.0

    def _save_context_state_with_checksum(self, session_id: str, context: dict):
        """Guarda contexto con checksum de integridad."""
        import hashlib
        
        # Serializar y calcular hash
        serialized = pickle.dumps(context)
        checksum = hashlib.sha256(serialized).hexdigest()
        
        # A√±adir checksum al contexto
        context["_integrity_checksum"] = checksum
        context["_persisted_at"] = datetime.datetime.now().isoformat()
        
        # Guardar
        session_file = self.session_dir / f"{session_id}.pkl"
        with open(session_file, "wb") as f:
            pickle.dump(context, f)
        
        logger.debug(f"üíæ Contexto persistido con checksum: {checksum[:16]}...")

    def _compute_homology_groups(self, context: dict):
        """
        Computa grupos de homolog√≠a del estado actual.
        
        Homolog√≠a de dimensi√≥n 0: componentes conexas
        Homolog√≠a de dimensi√≥n 1: ciclos/circuitos
        """
        try:
            # Construir complejo simplicial a partir de relaciones en contexto
            from scipy import sparse
            
            # Para simplificar, analizamos conectividad entre DataFrames
            df_keys = [k for k in context.keys() 
                      if isinstance(context.get(k), pd.DataFrame)]
            
            if len(df_keys) < 2:
                self._homology_groups = {"H0": 1, "H1": 0}
                return
            
            # Construir matriz de adyacencia conceptual
            n = len(df_keys)
            adj_matrix = sparse.lil_matrix((n, n))
            
            # Conexiones basadas en columnas compartidas
            for i, key_i in enumerate(df_keys):
                df_i = context[key_i]
                for j, key_j in enumerate(df_keys[i+1:], i+1):
                    df_j = context[key_j]
                    
                    # Verificar si comparten alguna columna
                    if isinstance(df_i, pd.DataFrame) and isinstance(df_j, pd.DataFrame):
                        common_cols = set(df_i.columns) & set(df_j.columns)
                        if common_cols:
                            adj_matrix[i, j] = adj_matrix[j, i] = len(common_cols)
            
            # Calcular homolog√≠a (simplificado)
            # H0: componentes conexas = n√∫mero de eigenvalores cero de Laplaciano
            # H1: ciclos = m - n + c (f√≥rmula de Euler para grafos)
            
            laplacian = sparse.diags(adj_matrix.sum(axis=1).A1) - adj_matrix
            eigenvalues = sparse.linalg.eigsh(laplacian, k=1, which='SM',
                                             return_eigenvectors=False)
            
            zero_eigenvalues = sum(abs(e) < 1e-10 for e in eigenvalues)
            h0 = max(1, zero_eigenvalues)
            
            # Estimaci√≥n de H1 usando f√≥rmula de Euler
            m = adj_matrix.nnz // 2  # aristas no dirigidas
            n = adj_matrix.shape[0]  # v√©rtices
            h1 = max(0, m - n + h0)
            
            self._homology_groups = {
                "H0": h0,  # Componentes conexas
                "H1": h1,  # Ciclos/circuitos
                "Betti_numbers": [h0, h1]
            }
            
            logger.debug(f"üßÆ Homolog√≠a computada: Œ≤‚ÇÄ={h0}, Œ≤‚ÇÅ={h1}")
            
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Error computando homolog√≠a: {e}")
            self._homology_groups = {"error": str(e)}

    def _attempt_state_recovery(self, session_id: str, context: dict) -> bool:
        """
        Intenta recuperaci√≥n aplicando operador identidad (rollback suave).
        """
        try:
            # Guardar estado corrupto para an√°lisis forense
            corrupt_file = self.session_dir / f"{session_id}_corrupt.pkl"
            with open(corrupt_file, "wb") as f:
                pickle.dump(context, f)
            
            # Cargar √∫ltima versi√≥n v√°lida (si existe)
            backup_files = sorted(self.session_dir.glob(f"{session_id}_backup_*.pkl"))
            if backup_files:
                latest_backup = backup_files[-1]
                with open(latest_backup, "rb") as f:
                    recovered_context = pickle.load(f)
                
                self._save_context_state(session_id, recovered_context)
                logger.info(f"‚ôªÔ∏è Estado recuperado desde backup: {latest_backup.name}")
                return True
            else:
                # Inicializar contexto limpio
                clean_context = {"_recovery_epoch": datetime.datetime.now().isoformat()}
                self._save_context_state(session_id, clean_context)
                logger.warning("‚ö†Ô∏è No hay backups, inicializado contexto limpio")
                return False
                
        except Exception as e:
            logger.error(f"üî• Error en recuperaci√≥n: {e}")
            return False


### 3. DataMerger - Merge con Geometr√≠a de la Informaci√≥n

class DataMerger(BaseCostProcessor):
    """
    Fusionador con m√©trica de informaci√≥n y preservaci√≥n topol√≥gica.
    
    Implementa merge como fibrado de datos sobre base com√∫n.
    """

    def __init__(self, thresholds: ProcessingThresholds):
        super().__init__({}, thresholds)
        self._match_stats = {}
        self._information_geometry = InformationGeometry()
        self._procrustes_analyzer = ProcrustesAnalyzer()

    def merge_apus_with_insumos(
        self, 
        df_apus: pd.DataFrame, 
        df_insumos: pd.DataFrame,
        alignment_strategy: str = "isometric",
        preserve_topology: bool = True
    ) -> pd.DataFrame:
        """
        Merge con alineamiento de geometr√≠a de la informaci√≥n.
        
        Par√°metros:
        -----------
        alignment_strategy : str
            "isometric": Preserva distancias (rigidez)
            "conformal": Preserva √°ngulos (flexibilidad)
            "affine": Preserva paralelismo
            
        preserve_topology : bool
            Si True, verifica homeomorfismo entre espacios de datos
        """
        # Validaci√≥n de variedades de datos
        if not self._validate_input(df_apus, "merge_apus"):
            return pd.DataFrame()
        if not self._validate_input(df_insumos, "merge_insumos"):
            return pd.DataFrame()

        # Calcular m√©tricas de informaci√≥n previas al merge
        info_apus = self._information_geometry.compute_entropy(df_apus)
        info_insumos = self._information_geometry.compute_entropy(df_insumos)
        
        logger.info(
            f"üßÆ Entrop√≠a de informaci√≥n: "
            f"APUs={info_apus['shannon_entropy']:.3f} bits, "
            f"Insumos={info_insumos['shannon_entropy']:.3f} bits"
        )

        # Verificar homeomorfismo si se requiere
        if preserve_topology:
            is_homeomorphic = self._verify_homeomorphism(df_apus, df_insumos)
            if not is_homeomorphic:
                logger.warning(
                    "‚ö†Ô∏è Los espacios de datos no son homeomorfos. "
                    "Merge podr√≠a introducir singularidades."
                )

        # Alineamiento Procrustes para optimizar correspondencia
        aligned_pair = self._procrustes_align(
            df_apus, df_insumos, strategy=alignment_strategy
        )
        
        # Merge con m√∫ltiples estrategias y votaci√≥n
        candidates = []
        
        strategies = [
            self._exact_merge,
            self._semantic_merge,
            self._topological_merge
        ]
        
        for strategy in strategies:
            try:
                result = strategy(*aligned_pair)
                match_quality = self._evaluate_merge_quality(result)
                candidates.append((match_quality, result, strategy.__name__))
            except Exception as e:
                logger.debug(f"Estrategia {strategy.__name__} fall√≥: {e}")
        
        if not candidates:
            logger.error("‚ùå Todas las estrategias de merge fallaron")
            return pd.DataFrame()
        
        # Seleccionar mejor candidato por m√©trica de calidad
        candidates.sort(key=lambda x: x[0], reverse=True)
        best_quality, best_result, best_strategy = candidates[0]
        
        logger.info(
            f"‚úÖ Merge √≥ptimo: {best_strategy} (calidad={best_quality:.3f})"
        )
        
        # Calcular m√©tricas post-merge
        info_merged = self._information_geometry.compute_entropy(best_result)
        info_preservation = info_merged['shannon_entropy'] / (
            info_apus['shannon_entropy'] + info_insumos['shannon_entropy']
        ) * 2
        
        logger.info(
            f"üìä Preservaci√≥n de informaci√≥n: {info_preservation:.1%}"
        )
        
        # Verificar no colapso de dimensi√≥n
        dim_before = info_apus['intrinsic_dimension'] + info_insumos['intrinsic_dimension']
        dim_after = info_merged['intrinsic_dimension']
        dim_preservation = dim_after / dim_before if dim_before > 0 else 1.0
        
        if dim_preservation < 0.8:
            logger.warning(
                f"‚ö†Ô∏è Colapso dimensional detectado: "
                f"{dim_preservation:.1%} de dimensi√≥n preservada"
            )
        
        return best_result

    def _procrustes_align(
        self, 
        df_a: pd.DataFrame, 
        df_b: pd.DataFrame, 
        strategy: str = "isometric"
    ) -> Tuple[pd.DataFrame, pd.DataFrame]:
        """
        Alineamiento Procrustes de DataFrames.
        
        Encuentra transformaci√≥n √≥ptima T que minimiza ||T(A) - B||_F
        sujeto a restricciones geom√©tricas.
        """
        # Convertir a espacios m√©tricos (embedding de caracter√≠sticas)
        embedding_a = self._embed_dataframe(df_a)
        embedding_b = self._embed_dataframe(df_b)
        
        # Aplicar Procrustes seg√∫n estrategia
        if strategy == "isometric":
            aligned_a, aligned_b, _ = self._procrustes_analyzer.isometric_align(
                embedding_a, embedding_b
            )
        elif strategy == "conformal":
            aligned_a, aligned_b, _ = self._procrustes_analyzer.conformal_align(
                embedding_a, embedding_b
            )
        elif strategy == "affine":
            aligned_a, aligned_b, _ = self._procrustes_analyzer.affine_align(
                embedding_a, embedding_b
            )
        else:
            raise ValueError(f"Estrategia {strategy} no soportada")
        
        # Mapear de vuelta a formato DataFrame (simplificado)
        return df_a.copy(), df_b.copy()

    def _embed_dataframe(self, df: pd.DataFrame) -> np.ndarray:
        """
        Embedding de DataFrame en espacio m√©trico.
        
        Usa t√©cnicas de representaci√≥n para capturar estructura intr√≠nseca.
        """
        # Para simplificar, usamos codificaci√≥n one-hot de columnas categ√≥ricas
        # y normalizaci√≥n de columnas num√©ricas
        numeric_cols = df.select_dtypes(include=[np.number]).columns
        categorical_cols = df.select_dtypes(exclude=[np.number]).columns
        
        embeddings = []
        
        # Embedding num√©rico (normalizado)
        if len(numeric_cols) > 0:
            numeric_data = df[numeric_cols].values
            # Normalizar por columna
            numeric_norm = (numeric_data - numeric_data.mean(axis=0)) / (
                numeric_data.std(axis=0) + 1e-10
            )
            embeddings.append(numeric_norm)
        
        # Embedding categ√≥rico (one-hot reducido con SVD)
        if len(categorical_cols) > 0:
            from sklearn.preprocessing import OneHotEncoder
            from sklearn.decomposition import TruncatedSVD
            
            encoder = OneHotEncoder(sparse=False, handle_unknown='ignore')
            categorical_encoded = encoder.fit_transform(df[categorical_cols])
            
            # Reducir dimensionalidad si hay muchas categor√≠as
            if categorical_encoded.shape[1] > 50:
                svd = TruncatedSVD(n_components=50, random_state=42)
                categorical_reduced = svd.fit_transform(categorical_encoded)
                embeddings.append(categorical_reduced)
            else:
                embeddings.append(categorical_encoded)
        
        if not embeddings:
            return np.zeros((len(df), 1))
        
        return np.hstack(embeddings)

    def _exact_merge(
        self, 
        df_apus: pd.DataFrame, 
        df_insumos: pd.DataFrame
    ) -> pd.DataFrame:
        """
        Merge exacto con preservaci√≥n de estructura algebraica.
        
        Implementa producto tensorial de espacios de datos.
        """
        try:
            # Asegurar columnas normalizadas
            if ColumnNames.NORMALIZED_DESC not in df_apus.columns:
                df_apus[ColumnNames.NORMALIZED_DESC] = normalize_text_series(
                    df_apus[ColumnNames.DESCRIPCION_INSUMO]
                )
            
            if ColumnNames.DESCRIPCION_INSUMO_NORM not in df_insumos.columns:
                df_insumos[ColumnNames.DESCRIPCION_INSUMO_NORM] = normalize_text_series(
                    df_insumos.get(ColumnNames.DESCRIPCION_INSUMO, pd.Series(dtype=str))
                )
            
            # Merge con producto tensorial conceptual
            df_merged = pd.merge(
                df_apus,
                df_insumos,
                left_on=ColumnNames.NORMALIZED_DESC,
                right_on=ColumnNames.DESCRIPCION_INSUMO_NORM,
                how="left",
                suffixes=("_apu", "_insumo"),
                indicator="_merge"
            )
            
            # Preservar estructura de anillo (suma directa de informaci√≥n)
            df_merged = self._preserve_ring_structure(df_merged)
            
            # Validar que el merge sea una inmersi√≥n (no colapso informaci√≥n)
            self._validate_immersion(df_apus, df_insumos, df_merged)
            
            return df_merged
            
        except Exception as e:
            self.logger.error(f"‚ùå Error en merge exacto: {e}")
            raise

    def _preserve_ring_structure(self, df_merged: pd.DataFrame) -> pd.DataFrame:
        """
        Preserva estructura de anillo en el merge.
        
        En √°lgebra, un anillo tiene dos operaciones (+, √ó).
        Aqu√≠ interpretamos '+' como concatenaci√≥n y '√ó' como join.
        """
        # Operaci√≥n suma: uni√≥n de espacios
        df_merged[ColumnNames.DESCRIPCION_INSUMO] = (
            df_merged[f"{ColumnNames.DESCRIPCION_INSUMO}_insumo"]
            .fillna(df_merged[f"{ColumnNames.DESCRIPCION_INSUMO}_apu"])
            .fillna(df_merged[ColumnNames.NORMALIZED_DESC])
        )
        
        # Operaci√≥n producto: combinaci√≥n de atributos
        for col in [ColumnNames.VR_UNITARIO_INSUMO, ColumnNames.CANTIDAD_APU]:
            if f"{col}_insumo" in df_merged.columns and f"{col}_apu" in df_merged.columns:
                df_merged[col] = df_merged[f"{col}_insumo"].fillna(
                    df_merged[f"{col}_apu"]
                )
        
        return df_merged

    def _validate_immersion(
        self, 
        df_a: pd.DataFrame, 
        df_b: pd.DataFrame, 
        df_merged: pd.DataFrame
    ):
        """
        Valida que el merge sea una inmersi√≥n (no colapso dimensional).
        
        Teorema: Una inmersi√≥n preserva la dimensi√≥n local.
        """
        dim_a = df_a.shape[1]
        dim_b = df_b.shape[1]
        dim_merged = df_merged.shape[1]
        
        # Dimensi√≥n esperada para join completo (sin duplicados)
        common_cols = set(df_a.columns) & set(df_b.columns)
        expected_dim = dim_a + dim_b - len(common_cols)
        
        if dim_merged < expected_dim:
            self.logger.warning(
                f"‚ö†Ô∏è Posible colapso dimensional en merge: "
                f"esperado={expected_dim}, obtenido={dim_merged}"
            )
        
        # Verificar que el rango (informaci√≥n lineal) se preserve
        rank_a = np.linalg.matrix_rank(df_a.select_dtypes(include=[np.number]).fillna(0).values)
        rank_merged = np.linalg.matrix_rank(
            df_merged.select_dtypes(include=[np.number]).fillna(0).values
        )
        
        if rank_merged < rank_a:
            self.logger.warning(
                f"‚ö†Ô∏è P√©rdida de rango en merge: "
                f"antes={rank_a}, despu√©s={rank_merged}"
            )

    def _evaluate_merge_quality(self, df_merged: pd.DataFrame) -> float:
        """
        Eval√∫a calidad del merge usando m√©trica compuesta.
        
        M√©trica ‚àà [0,1] donde 1 es merge perfecto.
        """
        if df_merged.empty:
            return 0.0
        
        metrics = []
        
        # 1. Completitud (no NaN)
        completeness = 1.0 - df_merged.isnull().mean().mean()
        metrics.append(completeness)
        
        # 2. Preservaci√≥n de cardinalidad
        if "_merge" in df_merged.columns:
            match_rate = (df_merged["_merge"] == "both").mean()
            metrics.append(match_rate)
        
        # 3. Consistencia de tipos
        type_consistency = self._compute_type_consistency(df_merged)
        metrics.append(type_consistency)
        
        # 4. Entrop√≠a relativa (preservaci√≥n de informaci√≥n)
        entropy_score = self._compute_entropy_preservation(df_merged)
        metrics.append(entropy_score)
        
        return np.mean(metrics)

    def _compute_type_consistency(self, df: pd.DataFrame) -> float:
        """Calcula consistencia de tipos de datos."""
        type_counts = {}
        for col in df.columns:
            dtype = str(df[col].dtype)
            type_counts[dtype] = type_counts.get(dtype, 0) + 1
        
        # Entrop√≠a de tipos (m√°s uniforme = mejor)
        total = sum(type_counts.values())
        proportions = [count/total for count in type_counts.values()]
        entropy = -sum(p * np.log(p) for p in proportions if p > 0)
        
        # Normalizar por entrop√≠a m√°xima
        max_entropy = np.log(len(type_counts)) if type_counts else 1
        return entropy / max_entropy if max_entropy > 0 else 1.0

    def _compute_entropy_preservation(self, df: pd.DataFrame) -> float:
        """Calcula preservaci√≥n de entrop√≠a en columnas num√©ricas."""
        numeric_cols = df.select_dtypes(include=[np.number]).columns
        if len(numeric_cols) == 0:
            return 1.0
        
        entropies = []
        for col in numeric_cols:
            col_data = df[col].dropna()
            if len(col_data) > 1:
                # Entrop√≠a de Shannon discretizada
                hist, _ = np.histogram(col_data, bins=min(20, len(col_data)))
                hist = hist / hist.sum()
                entropy = -sum(p * np.log(p) for p in hist if p > 0)
                entropies.append(entropy)
        
        return np.mean(entropies) if entropies else 1.0


### 4. Clases de Soporte (Geometr√≠a de la Informaci√≥n)

class InformationGeometry:
    """
    Geometr√≠a de la informaci√≥n para espacios de datos.
    
    Implementa m√©trica de Fisher y divergencias de informaci√≥n.
    """
    
    def compute_entropy(self, df: pd.DataFrame) -> Dict[str, float]:
        """
        Calcula m√∫ltiples medidas de entrop√≠a/informaci√≥n.
        """
        if df.empty:
            return {
                "shannon_entropy": 0.0,
                "intrinsic_dimension": 0.0,
                "fisher_information": 0.0
            }
        
        # Entrop√≠a de Shannon (columnas categ√≥ricas)
        categorical_cols = df.select_dtypes(exclude=[np.number]).columns
        shannon_entropy = 0.0
        
        for col in categorical_cols:
            value_counts = df[col].value_counts(normalize=True)
            entropy = -sum(p * np.log2(p) for p in value_counts if p > 0)
            shannon_entropy += entropy
        
        # Dimensi√≥n intr√≠nseca (PCA)
        numeric_data = df.select_dtypes(include=[np.number]).fillna(0).values
        intrinsic_dim = 0.0
        
        if len(numeric_data) > 1 and numeric_data.shape[1] > 1:
            from sklearn.decomposition import PCA
            pca = PCA()
            pca.fit(numeric_data)
            # Dimensi√≥n como n√∫mero de componentes que explican 95% varianza
            explained_variance = np.cumsum(pca.explained_variance_ratio_)
            intrinsic_dim = np.argmax(explained_variance >= 0.95) + 1
        
        # Informaci√≥n de Fisher (variabilidad)
        fisher_info = 0.0
        if len(numeric_data) > 1:
            # Aproximaci√≥n diagonal de matriz de Fisher
            variances = np.var(numeric_data, axis=0)
            fisher_info = np.sum(1.0 / (variances + 1e-10))
        
        return {
            "shannon_entropy": shannon_entropy,
            "intrinsic_dimension": intrinsic_dim,
            "fisher_information": fisher_info
        }

class ProcrustesAnalyzer:
    """
    Analizador de alineamiento Procrustes para DataFrames.
    """
    
    def isometric_align(self, X: np.ndarray, Y: np.ndarray):
        """
        Alineamiento isom√©trico (r√≠gido): preserva distancias.
        
        Minimiza ||X - YR||_F sujeto a R^T R = I (ortogonal).
        """
        # Centroides
        X_centered = X - X.mean(axis=0)
        Y_centered = Y - Y.mean(axis=0)
        
        # SVD para encontrar rotaci√≥n √≥ptima
        U, _, Vt = np.linalg.svd(Y_centered.T @ X_centered)
        R = U @ Vt
        
        # Aplicar transformaci√≥n
        Y_aligned = Y_centered @ R
        
        return X_centered, Y_aligned, R
    
    def conformal_align(self, X: np.ndarray, Y: np.ndarray):
        """
        Alineamiento conforme: preserva √°ngulos pero permite escala.
        """
        X_centered = X - X.mean(axis=0)
        Y_centered = Y - Y.mean(axis=0)
        
        # Escala √≥ptima
        scale = np.trace(Y_centered.T @ X_centered) / np.trace(Y_centered.T @ Y_centered)
        
        # Rotaci√≥n (mismo que isom√©trico)
        U, _, Vt = np.linalg.svd(Y_centered.T @ X_centered)
        R = U @ Vt
        
        Y_aligned = scale * Y_centered @ R
        
        return X_centered, Y_aligned, (scale, R)


### 