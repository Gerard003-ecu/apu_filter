1. DataValidator - M√©todos Robustecidos

class DataValidator:
    """Utilidades para validaci√≥n de DataFrames."""

    @staticmethod
    def validate_dataframe_not_empty(
        df: pd.DataFrame, name: str
    ) -> Tuple[bool, Optional[str]]:
        """Verifica que un DataFrame no est√© vac√≠o."""
        # ROBUSTECIDO: Verificaci√≥n de tipo antes de operaciones
        if df is None:
            error_msg = f"DataFrame '{name}' es None"
            logger.error(f"‚ùå {error_msg}")
            return False, error_msg
        
        if not isinstance(df, pd.DataFrame):
            error_msg = f"'{name}' no es un DataFrame v√°lido, es tipo: {type(df).__name__}"
            logger.error(f"‚ùå {error_msg}")
            return False, error_msg
        
        if df.empty:
            error_msg = f"DataFrame '{name}' est√° vac√≠o (0 filas)"
            logger.error(f"‚ùå {error_msg}")
            return False, error_msg
        
        # ROBUSTECIDO: Verificar que no sea solo columnas sin datos √∫tiles
        if df.dropna(how='all').empty:
            error_msg = f"DataFrame '{name}' contiene solo valores nulos"
            logger.error(f"‚ùå {error_msg}")
            return False, error_msg
            
        return True, None

    @staticmethod
    def validate_required_columns(
        df: pd.DataFrame, required_cols: List[str], df_name: str
    ) -> Tuple[bool, Optional[str]]:
        """Verifica que un DataFrame tenga las columnas requeridas."""
        # ROBUSTECIDO: Validaci√≥n de entrada
        if df is None:
            error_msg = f"DataFrame '{df_name}' es None, no se pueden validar columnas"
            logger.error(f"‚ùå {error_msg}")
            return False, error_msg
        
        if not isinstance(df, pd.DataFrame):
            error_msg = f"'{df_name}' no es un DataFrame v√°lido"
            logger.error(f"‚ùå {error_msg}")
            return False, error_msg
        
        if not required_cols:
            logger.warning(f"‚ö†Ô∏è Lista de columnas requeridas vac√≠a para '{df_name}'")
            return True, None
        
        # ROBUSTECIDO: Normalizar nombres para comparaci√≥n case-insensitive
        df_cols_upper = {col.upper().strip(): col for col in df.columns}
        missing_cols = []
        
        for req_col in required_cols:
            req_col_upper = req_col.upper().strip()
            if req_col not in df.columns and req_col_upper not in df_cols_upper:
                missing_cols.append(req_col)
        
        if missing_cols:
            available_cols = list(df.columns)[:10]  # Limitar para log
            error_msg = (
                f"Faltan columnas requeridas en '{df_name}': {missing_cols}. "
                f"Columnas disponibles (primeras 10): {available_cols}"
            )
            logger.error(f"‚ùå {error_msg}")
            return False, error_msg
            
        return True, None

    @staticmethod
    def detect_and_log_duplicates(
        df: pd.DataFrame, subset_cols: List[str], df_name: str, keep: str = "first"
    ) -> pd.DataFrame:
        """Detecta, loguea y elimina duplicados en un DataFrame."""
        # ROBUSTECIDO: Validaciones de entrada exhaustivas
        if df is None or not isinstance(df, pd.DataFrame):
            logger.error(f"‚ùå DataFrame '{df_name}' inv√°lido para detecci√≥n de duplicados")
            return pd.DataFrame()
        
        if df.empty:
            logger.debug(f"DataFrame '{df_name}' vac√≠o, no hay duplicados que detectar")
            return df
        
        if not subset_cols:
            logger.warning(f"‚ö†Ô∏è No se especificaron columnas para detectar duplicados en '{df_name}'")
            return df
        
        # ROBUSTECIDO: Verificar que las columnas existan
        missing_subset_cols = [col for col in subset_cols if col not in df.columns]
        if missing_subset_cols:
            logger.error(
                f"‚ùå Columnas para duplicados no existen en '{df_name}': {missing_subset_cols}"
            )
            # Usar solo las columnas que s√≠ existen
            subset_cols = [col for col in subset_cols if col in df.columns]
            if not subset_cols:
                return df
        
        # ROBUSTECIDO: Validar par√°metro keep
        valid_keep_values = {"first", "last", False}
        if keep not in valid_keep_values:
            logger.warning(f"‚ö†Ô∏è Valor 'keep={keep}' inv√°lido, usando 'first'")
            keep = "first"
        
        try:
            duplicates = df[df.duplicated(subset=subset_cols, keep=False)]
            if not duplicates.empty:
                unique_dupes = duplicates[subset_cols[0]].unique()
                num_dupes = len(duplicates)
                logger.warning(
                    f"‚ö†Ô∏è Se encontraron {num_dupes} filas duplicadas en '{df_name}' "
                    f"por {subset_cols}. Se conservar√°: '{keep}'"
                )
                # ROBUSTECIDO: Limitar cantidad de valores logueados
                dupes_sample = unique_dupes[:10].tolist()
                logger.debug(f"Muestra de valores duplicados en '{df_name}': {dupes_sample}")
                
                df_clean = df.drop_duplicates(subset=subset_cols, keep=keep)
                rows_removed = len(df) - len(df_clean)
                logger.info(f"‚úÖ Duplicados eliminados en '{df_name}': {rows_removed} filas removidas")
                return df_clean
            return df
        except Exception as e:
            logger.error(f"‚ùå Error detectando duplicados en '{df_name}': {e}")
            return df


2. FileValidator - M√©todo Robustecido

class FileValidator:
    """Utilidades para validaci√≥n de existencia de archivos."""

    # ROBUSTECIDO: Constantes para validaci√≥n
    MIN_FILE_SIZE_BYTES = 10  # Archivo debe tener al menos 10 bytes
    VALID_EXTENSIONS = {'.csv', '.xlsx', '.xls', '.json'}

    @staticmethod
    def validate_file_exists(
        file_path: str, 
        file_type: str,
        check_extension: bool = True,
        min_size: Optional[int] = None
    ) -> Tuple[bool, Optional[str]]:
        """Verifica que un archivo exista y sea accesible."""
        # ROBUSTECIDO: Validar entrada
        if not file_path:
            error_msg = f"Ruta de archivo de {file_type} est√° vac√≠a o es None"
            logger.error(f"‚ùå {error_msg}")
            return False, error_msg
        
        if not isinstance(file_path, (str, Path)):
            error_msg = f"Ruta de {file_type} no es v√°lida: tipo {type(file_path).__name__}"
            logger.error(f"‚ùå {error_msg}")
            return False, error_msg
        
        try:
            path = Path(file_path).resolve()  # ROBUSTECIDO: Resolver ruta absoluta
        except Exception as e:
            error_msg = f"Ruta de {file_type} no es v√°lida: {file_path}. Error: {e}"
            logger.error(f"‚ùå {error_msg}")
            return False, error_msg
        
        if not path.exists():
            error_msg = f"Archivo de {file_type} no encontrado: {path}"
            logger.error(f"‚ùå {error_msg}")
            return False, error_msg
        
        if not path.is_file():
            error_msg = f"La ruta de {file_type} no es un archivo: {path}"
            logger.error(f"‚ùå {error_msg}")
            return False, error_msg
        
        # ROBUSTECIDO: Verificar permisos de lectura
        try:
            if not os.access(path, os.R_OK):
                error_msg = f"Sin permisos de lectura para {file_type}: {path}"
                logger.error(f"‚ùå {error_msg}")
                return False, error_msg
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è No se pudo verificar permisos para {path}: {e}")
        
        # ROBUSTECIDO: Verificar tama√±o m√≠nimo
        min_size = min_size or FileValidator.MIN_FILE_SIZE_BYTES
        try:
            file_size = path.stat().st_size
            if file_size < min_size:
                error_msg = f"Archivo de {file_type} demasiado peque√±o ({file_size} bytes): {path}"
                logger.error(f"‚ùå {error_msg}")
                return False, error_msg
        except OSError as e:
            error_msg = f"No se pudo obtener informaci√≥n del archivo {file_type}: {e}"
            logger.error(f"‚ùå {error_msg}")
            return False, error_msg
        
        # ROBUSTECIDO: Verificar extensi√≥n v√°lida
        if check_extension:
            ext = path.suffix.lower()
            if ext not in FileValidator.VALID_EXTENSIONS:
                logger.warning(
                    f"‚ö†Ô∏è Extensi√≥n '{ext}' de {file_type} no es est√°ndar. "
                    f"Extensiones esperadas: {FileValidator.VALID_EXTENSIONS}"
                )
        
        logger.debug(f"‚úÖ Archivo de {file_type} validado: {path} ({file_size} bytes)")
        return True, None


3. LoadDataStep.execute - Robustecido

class LoadDataStep(ProcessingStep):
    """Paso de Carga de Datos."""

    def __init__(self, config: dict, thresholds: ProcessingThresholds):
        # ROBUSTECIDO: Validar config al inicializar
        if not config or not isinstance(config, dict):
            raise ValueError("Configuraci√≥n inv√°lida para LoadDataStep")
        self.config = config
        self.thresholds = thresholds or ProcessingThresholds()

    def execute(self, context: dict, telemetry: TelemetryContext) -> dict:
        """Ejecuta la carga y validaci√≥n inicial de archivos."""
        telemetry.start_step("load_data")
        
        try:
            # ROBUSTECIDO: Extraer rutas con validaci√≥n
            required_paths = ["presupuesto_path", "apus_path", "insumos_path"]
            paths = {}
            
            for path_key in required_paths:
                path_value = context.get(path_key)
                if not path_value:
                    error = f"Ruta requerida '{path_key}' no encontrada en contexto"
                    telemetry.record_error("load_data", error)
                    raise ValueError(error)
                paths[path_key] = path_value
            
            presupuesto_path = paths["presupuesto_path"]
            apus_path = paths["apus_path"]
            insumos_path = paths["insumos_path"]

            # Validar existencia de archivos
            file_validator = FileValidator()
            validations = [
                (presupuesto_path, "presupuesto"),
                (apus_path, "APUs"),
                (insumos_path, "insumos"),
            ]
            
            for file_path, file_type in validations:
                is_valid, error = file_validator.validate_file_exists(file_path, file_type)
                if not is_valid:
                    telemetry.record_error("load_data", error)
                    raise ValueError(error)

            # ROBUSTECIDO: Obtener perfiles con validaci√≥n
            file_profiles = self.config.get("file_profiles")
            if not file_profiles or not isinstance(file_profiles, dict):
                raise ValueError("'file_profiles' no encontrado o inv√°lido en config")

            # Procesar Presupuesto
            presupuesto_profile = file_profiles.get("presupuesto_default")
            if not presupuesto_profile:
                raise ValueError("No se encontr√≥ 'presupuesto_default' en file_profiles")

            p_processor = PresupuestoProcessor(
                self.config, self.thresholds, presupuesto_profile
            )
            df_presupuesto = p_processor.process(presupuesto_path)
            
            # ROBUSTECIDO: Validar resultado antes de continuar
            if df_presupuesto is None or df_presupuesto.empty:
                error = "Procesamiento de presupuesto retorn√≥ DataFrame vac√≠o"
                telemetry.record_error("load_data", error)
                raise ValueError(error)
            
            telemetry.record_metric("load_data", "presupuesto_rows", len(df_presupuesto))

            # Procesar Insumos
            insumos_profile = file_profiles.get("insumos_default")
            if not insumos_profile:
                raise ValueError("No se encontr√≥ 'insumos_default' en file_profiles")

            i_processor = InsumosProcessor(self.thresholds, insumos_profile)
            df_insumos = i_processor.process(insumos_path)
            
            if df_insumos is None or df_insumos.empty:
                error = "Procesamiento de insumos retorn√≥ DataFrame vac√≠o"
                telemetry.record_error("load_data", error)
                raise ValueError(error)
            
            telemetry.record_metric("load_data", "insumos_rows", len(df_insumos))

            # Procesar APUs
            apus_profile = file_profiles.get("apus_default")
            if not apus_profile:
                raise ValueError("No se encontr√≥ 'apus_default' en file_profiles")

            logger.info("‚ö°Ô∏è Iniciando DataFluxCondenser para APUs...")
            condenser_config_data = self.config.get("flux_condenser_config", {})
            
            # ROBUSTECIDO: Manejar errores de configuraci√≥n del condenser
            try:
                condenser_config = CondenserConfig(**condenser_config_data)
            except TypeError as e:
                logger.warning(f"‚ö†Ô∏è Error en config de condenser, usando defaults: {e}")
                condenser_config = CondenserConfig()

            condenser = DataFluxCondenser(
                config=self.config, profile=apus_profile, condenser_config=condenser_config
            )
            df_apus_raw = condenser.stabilize(apus_path)

            # ROBUSTECIDO: Verificar stats antes de registrar
            stats = condenser.get_processing_stats() or {}
            for metric_name, default_value in [
                ("avg_saturation", 0.0),
                ("max_flyback_voltage", 0.0),
                ("max_dissipated_power", 0.0),
                ("avg_kinetic_energy", 0.0),
                ("avg_batch_size", 0),
            ]:
                value = stats.get(metric_name, default_value)
                # ROBUSTECIDO: Sanitizar valores num√©ricos
                if not isinstance(value, (int, float)) or np.isnan(value) or np.isinf(value):
                    value = default_value
                telemetry.record_metric("flux_condenser", metric_name, value)

            if df_apus_raw is None or df_apus_raw.empty:
                error = "DataFluxCondenser retorn√≥ DataFrame vac√≠o"
                telemetry.record_error("load_data", error)
                raise ValueError(error)
            
            telemetry.record_metric("load_data", "apus_raw_rows", len(df_apus_raw))
            logger.info("‚úÖ DataFluxCondenser completado.")

            # Validaci√≥n final
            data_validator = DataValidator()
            dataframes = [
                (df_presupuesto, "presupuesto"),
                (df_insumos, "insumos"),
                (df_apus_raw, "APUs"),
            ]
            
            for df, name in dataframes:
                is_valid, error = data_validator.validate_dataframe_not_empty(df, name)
                if not is_valid:
                    telemetry.record_error("load_data", error)
                    raise ValueError(error)

            # ROBUSTECIDO: Actualizar contexto de forma inmutable
            context = {**context}
            context.update({
                "df_presupuesto": df_presupuesto,
                "df_insumos": df_insumos,
                "df_apus_raw": df_apus_raw,
            })
            
            telemetry.end_step("load_data", "success")
            return context

        except Exception as e:
            logger.error(f"‚ùå Error en LoadDataStep: {e}", exc_info=True)
            telemetry.record_error("load_data", str(e))
            telemetry.end_step("load_data", "error")
            raise


4. PresupuestoProcessor - M√©todos Robustecidos

class PresupuestoProcessor:
    def __init__(self, config: dict, thresholds: ProcessingThresholds, profile: dict):
        # ROBUSTECIDO: Validar par√°metros
        if not config:
            raise ValueError("Configuraci√≥n requerida para PresupuestoProcessor")
        self.config = config
        self.thresholds = thresholds or ProcessingThresholds()
        self.profile = profile or {}
        self.validator = DataValidator()

    def process(self, path: str) -> pd.DataFrame:
        """Procesa archivo de presupuesto con manejo robusto de errores."""
        # ROBUSTECIDO: Validar path al inicio
        if not path:
            logger.error("‚ùå Ruta de presupuesto vac√≠a")
            return pd.DataFrame()
        
        try:
            loader_params = self.profile.get("loader_params", {})
            logger.info(f"üì• Cargando presupuesto desde: {path}")
            logger.debug(f"Par√°metros de carga: {loader_params}")
            
            load_result = load_data(path, **loader_params)

            # ROBUSTECIDO: Verificar resultado de carga exhaustivamente
            if load_result is None:
                logger.error("‚ùå load_data retorn√≥ None")
                return pd.DataFrame()
            
            if not hasattr(load_result, 'status') or not hasattr(load_result, 'data'):
                logger.error("‚ùå Estructura de load_result inv√°lida")
                return pd.DataFrame()
            
            # ROBUSTECIDO: Comparaci√≥n segura de status
            status_value = getattr(load_result.status, 'value', str(load_result.status))
            if status_value != "SUCCESS":
                error_msg = getattr(load_result, 'error_message', 'Error desconocido')
                logger.error(f"Error cargando presupuesto: {error_msg}")
                return pd.DataFrame()

            df = load_result.data
            if df is None or (isinstance(df, pd.DataFrame) and df.empty):
                logger.warning("‚ö†Ô∏è Archivo de presupuesto cargado est√° vac√≠o")
                return pd.DataFrame()

            # Pipeline de procesamiento con validaciones intermedias
            df_clean = self._clean_phantom_rows(df)
            if df_clean.empty:
                logger.warning("‚ö†Ô∏è DataFrame vac√≠o despu√©s de limpiar filas fantasma")
                return pd.DataFrame()

            df_renamed = self._rename_columns(df_clean)
            if not self._validate_required_columns(df_renamed):
                logger.error("‚ùå Validaci√≥n de columnas requeridas fall√≥")
                return pd.DataFrame()

            df_converted = self._clean_and_convert_data(df_renamed)
            if df_converted.empty:
                logger.warning("‚ö†Ô∏è DataFrame vac√≠o despu√©s de conversi√≥n de datos")
                return pd.DataFrame()
            
            df_final = self._remove_duplicates(df_converted)

            # ROBUSTECIDO: Selecci√≥n segura de columnas
            final_cols = [
                ColumnNames.CODIGO_APU,
                ColumnNames.DESCRIPCION_APU,
                ColumnNames.CANTIDAD_PRESUPUESTO,
            ]
            available_cols = [col for col in final_cols if col in df_final.columns]
            
            if ColumnNames.CODIGO_APU not in available_cols:
                logger.error(f"‚ùå Columna cr√≠tica '{ColumnNames.CODIGO_APU}' no disponible")
                return pd.DataFrame()
            
            result = df_final[available_cols].copy()
            logger.info(f"‚úÖ Presupuesto procesado: {len(result)} filas")
            return result

        except Exception as e:
            logger.error(f"‚ùå Error fatal procesando presupuesto: {e}", exc_info=True)
            return pd.DataFrame()

    def _clean_phantom_rows(self, df: pd.DataFrame) -> pd.DataFrame:
        """Elimina filas completamente vac√≠as o con solo valores nulos."""
        if df is None:
            return pd.DataFrame()
        
        if not isinstance(df, pd.DataFrame):
            logger.error(f"‚ùå _clean_phantom_rows recibi√≥ tipo: {type(df).__name__}")
            return pd.DataFrame()
        
        if df.empty:
            return df
        
        # ROBUSTECIDO: Usar operaciones vectorizadas en lugar de apply
        initial_rows = len(df)
        
        # Eliminar filas completamente NA
        df_clean = df.dropna(how="all")
        
        # ROBUSTECIDO: Vectorizar la detecci√≥n de filas vac√≠as
        # Convertir a string y verificar si todos los valores son vac√≠os
        str_df = df_clean.astype(str)
        empty_patterns = {'', 'nan', 'none', 'nat', '<na>'}
        
        # Crear m√°scara vectorizada
        is_empty_mask = str_df.apply(
            lambda col: col.str.strip().str.lower().isin(empty_patterns),
            axis=0
        ).all(axis=1)
        
        df_clean = df_clean[~is_empty_mask]
        
        removed_rows = initial_rows - len(df_clean)
        if removed_rows > 0:
            logger.debug(f"Filas fantasma eliminadas: {removed_rows}")
        
        return df_clean

    def _clean_and_convert_data(self, df: pd.DataFrame) -> pd.DataFrame:
        """Limpia y convierte datos con validaciones robustas."""
        if df is None or df.empty:
            return pd.DataFrame()
        
        df = df.copy()  # ROBUSTECIDO: Evitar modificar DataFrame original
        
        # Limpiar c√≥digo APU
        clean_code_params = self.config.get("clean_apu_code_params", {}).get(
            "presupuesto_item", {}
        )
        
        # ROBUSTECIDO: Manejar columna CODIGO_APU de forma segura
        if ColumnNames.CODIGO_APU not in df.columns:
            logger.error(f"‚ùå Columna '{ColumnNames.CODIGO_APU}' no encontrada")
            return pd.DataFrame()
        
        try:
            df[ColumnNames.CODIGO_APU] = (
                df[ColumnNames.CODIGO_APU]
                .fillna('')
                .astype(str)
                .apply(lambda c: clean_apu_code(c, **clean_code_params))
            )
        except Exception as e:
            logger.error(f"‚ùå Error limpiando c√≥digos APU: {e}")
            return pd.DataFrame()
        
        # ROBUSTECIDO: Filtrar c√≥digos inv√°lidos de forma m√°s clara
        invalid_codes = {'', 'nan', 'none', 'null'}
        mask_valid = (
            df[ColumnNames.CODIGO_APU].notna() &
            ~df[ColumnNames.CODIGO_APU].str.strip().str.lower().isin(invalid_codes)
        )
        
        df = df[mask_valid].copy()
        
        if df.empty:
            logger.warning("‚ö†Ô∏è No quedaron registros con c√≥digos APU v√°lidos")
            return df

        # ROBUSTECIDO: Procesar cantidad con validaciones
        if ColumnNames.CANTIDAD_PRESUPUESTO in df.columns:
            try:
                qty = df[ColumnNames.CANTIDAD_PRESUPUESTO].astype(str).str.strip()
                # Normalizar separadores decimales
                qty = qty.str.replace(",", ".", regex=False)
                # Remover caracteres no num√©ricos excepto punto y signo negativo
                qty = qty.str.replace(r"[^\d.\-]", "", regex=True)
                # Manejar m√∫ltiples puntos decimales (mantener solo el √∫ltimo)
                qty = qty.str.replace(r"\.(?=.*\.)", "", regex=True)
                
                df[ColumnNames.CANTIDAD_PRESUPUESTO] = pd.to_numeric(
                    qty, errors="coerce"
                ).fillna(0)
                
                # ROBUSTECIDO: Validar rangos
                max_qty = self.thresholds.max_quantity
                mask_invalid_qty = df[ColumnNames.CANTIDAD_PRESUPUESTO] > max_qty
                if mask_invalid_qty.any():
                    count = mask_invalid_qty.sum()
                    logger.warning(f"‚ö†Ô∏è {count} cantidades exceden m√°ximo ({max_qty}), se limitar√°n")
                    df.loc[mask_invalid_qty, ColumnNames.CANTIDAD_PRESUPUESTO] = max_qty
                    
            except Exception as e:
                logger.error(f"‚ùå Error procesando cantidades: {e}")
                df[ColumnNames.CANTIDAD_PRESUPUESTO] = 0

        return df


5. APUCostCalculator._classify_apus - Robustecido (CR√çTICO: eliminaci√≥n de eval)

def _classify_apus(self, df: pd.DataFrame) -> pd.DataFrame:
    """
    Clasifica los APUs seg√∫n la proporci√≥n de costos.
    ROBUSTECIDO: Eliminado uso de eval() por seguridad.
    """
    if df is None or not isinstance(df, pd.DataFrame):
        logger.error("‚ùå DataFrame inv√°lido para clasificaci√≥n de APUs")
        return pd.DataFrame()
    
    if df.empty:
        return df
    
    df = df.copy()  # No modificar original

    # Asegurar columnas de costos
    cost_columns = [
        ColumnNames.VALOR_CONSTRUCCION_UN,
        ColumnNames.VALOR_SUMINISTRO_UN,
        ColumnNames.VALOR_INSTALACION_UN,
    ]
    
    for col in cost_columns:
        if col not in df.columns:
            df[col] = 0.0
        else:
            # ROBUSTECIDO: Asegurar tipo num√©rico
            df[col] = pd.to_numeric(df[col], errors='coerce').fillna(0.0)

    # Calcular porcentajes de forma segura
    total_cost = df[ColumnNames.VALOR_CONSTRUCCION_UN].copy()
    # Evitar divisi√≥n por cero
    total_cost = total_cost.replace(0, np.nan)
    
    df["_pct_materiales"] = (
        df[ColumnNames.VALOR_SUMINISTRO_UN] / total_cost * 100
    ).fillna(0)
    
    df["_pct_mo_eq"] = (
        df[ColumnNames.VALOR_INSTALACION_UN] / total_cost * 100
    ).fillna(0)

    # Default
    df[ColumnNames.TIPO_APU] = APUTypes.INDEFINIDO

    # ROBUSTECIDO: Aplicar reglas de forma segura sin eval()
    rules = self.config.get("apu_classification_rules", [])
    
    if not rules:
        logger.warning("‚ö†Ô∏è No hay reglas de clasificaci√≥n, usando l√≥gica por defecto")
        # L√≥gica por defecto basada en thresholds
        th = self.thresholds
        
        # Instalaci√≥n: alto % mano de obra
        mask_instalacion = df["_pct_mo_eq"] >= th.instalacion_mo_threshold
        df.loc[mask_instalacion, ColumnNames.TIPO_APU] = APUTypes.INSTALACION
        
        # Suministro: alto % materiales, bajo % MO
        mask_suministro = (
            (df["_pct_materiales"] >= th.suministro_mat_threshold) &
            (df["_pct_mo_eq"] <= th.suministro_mo_max)
        )
        df.loc[mask_suministro, ColumnNames.TIPO_APU] = APUTypes.SUMINISTRO
        
        # Suministro Prefabricado
        mask_prefab = (
            (df["_pct_materiales"] >= th.prefabricado_mat_threshold) &
            (df["_pct_mo_eq"] >= th.prefabricado_mo_min) &
            (df["_pct_mo_eq"] < th.instalacion_mo_threshold)
        )
        df.loc[mask_prefab, ColumnNames.TIPO_APU] = APUTypes.SUMINISTRO_PREFABRICADO
        
        # Obra Completa: resto con costos v√°lidos
        mask_obra_completa = (
            (df[ColumnNames.TIPO_APU] == APUTypes.INDEFINIDO) &
            (total_cost.notna()) &
            (total_cost > 0)
        )
        df.loc[mask_obra_completa, ColumnNames.TIPO_APU] = APUTypes.OBRA_COMPLETA
    else:
        # ROBUSTECIDO: Procesar reglas de configuraci√≥n de forma segura
        for rule in rules:
            try:
                rule_type = rule.get("type")
                if not rule_type:
                    continue
                
                # Construir m√°scara seg√∫n operadores definidos expl√≠citamente
                conditions = rule.get("conditions", [])
                if not conditions:
                    continue
                
                mask = pd.Series([True] * len(df), index=df.index)
                
                for cond in conditions:
                    field = cond.get("field")
                    operator = cond.get("operator")
                    value = cond.get("value")
                    
                    if not all([field, operator, value is not None]):
                        continue
                    
                    # Mapear campos a columnas internas
                    field_map = {
                        "porcentaje_materiales": "_pct_materiales",
                        "porcentaje_mo_eq": "_pct_mo_eq",
                        "pct_materiales": "_pct_materiales",
                        "pct_mo_eq": "_pct_mo_eq",
                    }
                    col_name = field_map.get(field, field)
                    
                    if col_name not in df.columns:
                        logger.warning(f"‚ö†Ô∏è Campo '{field}' no existe en DataFrame")
                        continue
                    
                    # ROBUSTECIDO: Operadores expl√≠citos sin eval
                    if operator == ">=":
                        mask &= df[col_name] >= value
                    elif operator == ">":
                        mask &= df[col_name] > value
                    elif operator == "<=":
                        mask &= df[col_name] <= value
                    elif operator == "<":
                        mask &= df[col_name] < value
                    elif operator == "==":
                        mask &= df[col_name] == value
                    elif operator == "!=":
                        mask &= df[col_name] != value
                    else:
                        logger.warning(f"‚ö†Ô∏è Operador desconocido: {operator}")
                
                df.loc[mask, ColumnNames.TIPO_APU] = rule_type
                
            except Exception as e:
                logger.error(f"‚ùå Error aplicando regla {rule}: {e}")

    # Limpieza de columnas temporales
    df.drop(columns=["_pct_materiales", "_pct_mo_eq"], inplace=True, errors="ignore")

    return df


6. DataMerger - M√©todos Robustecidos

class DataMerger:
    def __init__(self, thresholds: ProcessingThresholds):
        self.thresholds = thresholds or ProcessingThresholds()

    def merge_apus_with_insumos(
        self, df_apus: pd.DataFrame, df_insumos: pd.DataFrame
    ) -> pd.DataFrame:
        """Fusiona APUs con insumos de forma robusta."""
        # ROBUSTECIDO: Validaciones exhaustivas
        if df_apus is None or not isinstance(df_apus, pd.DataFrame):
            logger.error("‚ùå df_apus inv√°lido para merge")
            return pd.DataFrame()
        
        if df_insumos is None or not isinstance(df_insumos, pd.DataFrame):
            logger.error("‚ùå df_insumos inv√°lido para merge")
            return df_apus.copy()  # Retornar APUs sin enriquecer
        
        if df_apus.empty:
            logger.warning("‚ö†Ô∏è df_apus vac√≠o")
            return pd.DataFrame()
        
        if df_insumos.empty:
            logger.warning("‚ö†Ô∏è df_insumos vac√≠o, retornando APUs sin enriquecer")
            return df_apus.copy()
        
        df_apus = df_apus.copy()
        df_insumos = df_insumos.copy()
        
        # ROBUSTECIDO: Verificar columnas necesarias antes de merge
        if ColumnNames.DESCRIPCION_INSUMO not in df_apus.columns:
            logger.error(f"‚ùå Columna '{ColumnNames.DESCRIPCION_INSUMO}' no existe en df_apus")
            return df_apus
        
        if ColumnNames.DESCRIPCION_INSUMO_NORM not in df_insumos.columns:
            logger.warning("‚ö†Ô∏è Creando columna normalizada en df_insumos")
            df_insumos[ColumnNames.DESCRIPCION_INSUMO_NORM] = normalize_text_series(
                df_insumos.get(ColumnNames.DESCRIPCION_INSUMO, pd.Series(dtype=str))
            )
        
        # Crear columna normalizada en APUs si no existe
        if ColumnNames.NORMALIZED_DESC not in df_apus.columns:
            df_apus[ColumnNames.NORMALIZED_DESC] = normalize_text_series(
                df_apus[ColumnNames.DESCRIPCION_INSUMO]
            )

        try:
            # ROBUSTECIDO: Merge con manejo de errores
            df_merged = pd.merge(
                df_apus,
                df_insumos,
                left_on=ColumnNames.NORMALIZED_DESC,
                right_on=ColumnNames.DESCRIPCION_INSUMO_NORM,
                how="left",
                suffixes=("_apu", "_insumo"),
                validate="m:1",
            )
        except pd.errors.MergeError as e:
            logger.warning(f"‚ö†Ô∏è Error de validaci√≥n en merge (m:1): {e}. Reintentando sin validaci√≥n.")
            df_merged = pd.merge(
                df_apus,
                df_insumos,
                left_on=ColumnNames.NORMALIZED_DESC,
                right_on=ColumnNames.DESCRIPCION_INSUMO_NORM,
                how="left",
                suffixes=("_apu", "_insumo"),
            )
        except Exception as e:
            logger.error(f"‚ùå Error en merge APUs-Insumos: {e}")
            return df_apus

        # ROBUSTECIDO: Consolidar columnas de forma segura
        if ColumnNames.CATEGORIA in df_merged.columns:
            df_merged[ColumnNames.TIPO_INSUMO] = df_merged[ColumnNames.CATEGORIA]
        elif ColumnNames.TIPO_INSUMO not in df_merged.columns:
            df_merged[ColumnNames.TIPO_INSUMO] = InsumoTypes.OTRO
        
        # Consolidar descripci√≥n
        desc_col_apu = f"{ColumnNames.DESCRIPCION_INSUMO}_apu"
        desc_col_insumo = f"{ColumnNames.DESCRIPCION_INSUMO}_insumo"
        
        if ColumnNames.DESCRIPCION_INSUMO in df_merged.columns:
            pass  # Ya existe
        elif desc_col_insumo in df_merged.columns:
            df_merged[ColumnNames.DESCRIPCION_INSUMO] = df_merged[desc_col_insumo]
        elif desc_col_apu in df_merged.columns:
            df_merged[ColumnNames.DESCRIPCION_INSUMO] = df_merged[desc_col_apu]
        
        # Consolidar unidad
        unidad_col_apu = f"{ColumnNames.UNIDAD_INSUMO}_apu"
        if unidad_col_apu in df_merged.columns:
            if ColumnNames.UNIDAD_INSUMO not in df_merged.columns:
                df_merged[ColumnNames.UNIDAD_INSUMO] = df_merged[unidad_col_apu]
            df_merged.drop(columns=[unidad_col_apu], errors='ignore', inplace=True)
        
        logger.info(f"‚úÖ Merge APUs-Insumos completado: {len(df_merged)} filas")
        return df_merged

    def merge_with_presupuesto(
        self, df_presupuesto: pd.DataFrame, df_apu_costos: pd.DataFrame
    ) -> pd.DataFrame:
        """Fusiona presupuesto con costos APU de forma robusta."""
        # ROBUSTECIDO: Validaciones
        if df_presupuesto is None or df_presupuesto.empty:
            logger.error("‚ùå df_presupuesto vac√≠o o None")
            return pd.DataFrame()
        
        if df_apu_costos is None or df_apu_costos.empty:
            logger.warning("‚ö†Ô∏è df_apu_costos vac√≠o, retornando presupuesto sin costos")
            return df_presupuesto.copy()
        
        # ROBUSTECIDO: Verificar columna de join
        if ColumnNames.CODIGO_APU not in df_presupuesto.columns:
            logger.error(f"‚ùå '{ColumnNames.CODIGO_APU}' no existe en presupuesto")
            return df_presupuesto.copy()
        
        if ColumnNames.CODIGO_APU not in df_apu_costos.columns:
            logger.error(f"‚ùå '{ColumnNames.CODIGO_APU}' no existe en apu_costos")
            return df_presupuesto.copy()
        
        try:
            df_merged = pd.merge(
                df_presupuesto,
                df_apu_costos,
                on=ColumnNames.CODIGO_APU,
                how="left",
                validate="1:1",
            )
            logger.info(f"‚úÖ Merge con presupuesto completado: {len(df_merged)} filas")
            return df_merged
            
        except pd.errors.MergeError as e:
            logger.warning(f"‚ö†Ô∏è Duplicados detectados en merge 1:1: {e}")
            
            # ROBUSTECIDO: Diagn√≥stico y correcci√≥n
            dupes_pres = df_presupuesto[
                df_presupuesto[ColumnNames.CODIGO_APU].duplicated(keep=False)
            ][ColumnNames.CODIGO_APU].unique()
            
            dupes_costos = df_apu_costos[
                df_apu_costos[ColumnNames.CODIGO_APU].duplicated(keep=False)
            ][ColumnNames.CODIGO_APU].unique()
            
            if len(dupes_pres) > 0:
                logger.warning(f"Duplicados en presupuesto: {dupes_pres[:5].tolist()}")
            if len(dupes_costos) > 0:
                logger.warning(f"Duplicados en costos: {dupes_costos[:5].tolist()}")
            
            # Intentar merge sin validaci√≥n
            df_merged = pd.merge(
                df_presupuesto,
                df_apu_costos,
                on=ColumnNames.CODIGO_APU,
                how="left",
            )
            logger.info(f"‚úÖ Merge sin validaci√≥n completado: {len(df_merged)} filas")
            return df_merged
            
        except Exception as e:
            logger.error(f"‚ùå Error en merge con presupuesto: {e}")
            raise


7. PipelineDirector.execute - Robustecido

def execute(self, initial_context: dict) -> dict:
    """
    Ejecuta el pipeline completo con manejo robusto de errores.
    """
    # ROBUSTECIDO: Validar contexto inicial
    if initial_context is None:
        raise ValueError("Contexto inicial es None")
    
    if not isinstance(initial_context, dict):
        raise ValueError(f"Contexto inicial debe ser dict, recibido: {type(initial_context)}")
    
    recipe = self.config.get("pipeline_recipe", [])
    
    if not recipe:
        logger.warning("No 'pipeline_recipe' en config. Usando flujo por defecto.")
        recipe = [
            {"step": "load_data", "enabled": True},
            {"step": "merge_data", "enabled": True},
            {"step": "calculate_costs", "enabled": True},
            {"step": "final_merge", "enabled": True},
            {"step": "build_output", "enabled": True},
        ]
    
    # ROBUSTECIDO: Validar receta
    if not isinstance(recipe, list):
        raise ValueError("pipeline_recipe debe ser una lista")

    # Crear copia inmutable del contexto
    context = dict(initial_context)
    executed_steps = []  # Para tracking y posible rollback

    for step_idx, step_config in enumerate(recipe):
        # ROBUSTECIDO: Validar configuraci√≥n del paso
        if not isinstance(step_config, dict):
            logger.warning(f"‚ö†Ô∏è Configuraci√≥n de paso #{step_idx} inv√°lida, saltando")
            continue
        
        step_name = step_config.get("step")
        if not step_name:
            logger.warning(f"‚ö†Ô∏è Paso #{step_idx} sin nombre, saltando")
            continue
        
        if not step_config.get("enabled", True):
            logger.info(f"‚è≠Ô∏è Saltando paso deshabilitado: {step_name}")
            continue

        step_class = self.STEP_REGISTRY.get(step_name)
        if not step_class:
            error_msg = f"Paso desconocido en receta: {step_name}"
            logger.error(f"‚ùå {error_msg}")
            self.telemetry.record_error("pipeline_director", error_msg)
            
            # ROBUSTECIDO: Decidir si continuar o fallar
            if step_config.get("required", True):
                raise ValueError(error_msg)
            continue

        logger.info(f"‚ñ∂Ô∏è Ejecutando paso [{step_idx + 1}/{len(recipe)}]: {step_name}")
        
        try:
            # Instanciar paso
            step_instance = step_class(self.config, self.thresholds)
            
            # Ejecutar con timeout impl√≠cito si es necesario
            context = step_instance.execute(context, self.telemetry)
            
            # ROBUSTECIDO: Validar que context sigue siendo v√°lido
            if context is None:
                raise ValueError(f"Paso '{step_name}' retorn√≥ contexto None")
            
            if not isinstance(context, dict):
                raise ValueError(
                    f"Paso '{step_name}' retorn√≥ tipo inv√°lido: {type(context)}"
                )
            
            executed_steps.append(step_name)
            logger.info(f"‚úÖ Paso completado: {step_name}")
            
        except Exception as e:
            error_msg = f"Error cr√≠tico en paso '{step_name}': {e}"
            logger.critical(f"üî• {error_msg}")
            self.telemetry.record_error(step_name, str(e))
            
            # ROBUSTECIDO: Registrar pasos ejecutados para diagn√≥stico
            self.telemetry.record_metric(
                "pipeline_director", 
                "executed_steps_before_failure",
                len(executed_steps)
            )
            
            # ROBUSTECIDO: Informaci√≥n de diagn√≥stico
            context["_pipeline_error"] = {
                "step": step_name,
                "step_index": step_idx,
                "error": str(e),
                "executed_steps": executed_steps,
            }
            
            raise RuntimeError(error_msg) from e

    logger.info(f"üéâ Pipeline completado exitosamente. Pasos ejecutados: {len(executed_steps)}")
    return context


8. calculate_insumo_costs - Robustecido

def calculate_insumo_costs(
    df: pd.DataFrame, thresholds: ProcessingThresholds
) -> pd.DataFrame:
    """Calcula costos de insumos de forma robusta."""
    # ROBUSTECIDO: Validaciones de entrada
    if df is None or not isinstance(df, pd.DataFrame):
        logger.error("‚ùå DataFrame inv√°lido para c√°lculo de costos")
        return pd.DataFrame()
    
    if df.empty:
        return df
    
    df = df.copy()  # No modificar original
    
    # ROBUSTECIDO: Asegurar existencia de columnas con tipos correctos
    numeric_columns = [
        ColumnNames.CANTIDAD_APU,
        ColumnNames.VR_UNITARIO_INSUMO,
        ColumnNames.VALOR_TOTAL_APU,
    ]
    
    for col in numeric_columns:
        if col not in df.columns:
            df[col] = 0.0
        else:
            # Convertir a num√©rico de forma segura
            df[col] = pd.to_numeric(df[col], errors='coerce').fillna(0.0)
    
    # ROBUSTECIDO: Validar rangos antes de calcular
    max_cost = thresholds.max_cost_per_item
    max_qty = thresholds.max_quantity
    
    # Limitar valores extremos
    df[ColumnNames.CANTIDAD_APU] = df[ColumnNames.CANTIDAD_APU].clip(0, max_qty)
    df[ColumnNames.VR_UNITARIO_INSUMO] = df[ColumnNames.VR_UNITARIO_INSUMO].clip(0, max_cost)
    
    # ROBUSTECIDO: Calcular costo con manejo expl√≠cito de casos
    tiene_precio = (
        df[ColumnNames.VR_UNITARIO_INSUMO].notna() & 
        (df[ColumnNames.VR_UNITARIO_INSUMO] > 0)
    )
    
    # Calcular costo: cantidad * precio unitario si hay precio, sino usar valor total
    df[ColumnNames.COSTO_INSUMO_EN_APU] = np.where(
        tiene_precio,
        df[ColumnNames.CANTIDAD_APU] * df[ColumnNames.VR_UNITARIO_INSUMO],
        df[ColumnNames.VALOR_TOTAL_APU]
    )
    
    # ROBUSTECIDO: Validar resultado y limitar
    df[ColumnNames.COSTO_INSUMO_EN_APU] = (
        pd.to_numeric(df[ColumnNames.COSTO_INSUMO_EN_APU], errors='coerce')
        .fillna(0.0)
        .clip(0, thresholds.max_total_cost)
    )
    
    # VR_UNITARIO_FINAL
    df[ColumnNames.VR_UNITARIO_FINAL] = (
        df[ColumnNames.VR_UNITARIO_INSUMO]
        .fillna(0.0)
        .clip(0, max_cost)
    )
    
    # ROBUSTECIDO: Log de estad√≠sticas para monitoreo
    total_costo = df[ColumnNames.COSTO_INSUMO_EN_APU].sum()
    registros_sin_costo = (df[ColumnNames.COSTO_INSUMO_EN_APU] == 0).sum()
    
    if registros_sin_costo > 0:
        pct = (registros_sin_costo / len(df)) * 100
        logger.warning(f"‚ö†Ô∏è {registros_sin_costo} registros ({pct:.1f}%) sin costo calculado")
    
    logger.debug(f"Costo total calculado: {total_costo:,.2f}")
    
    return df


9. _save_output_files - Robustecido

def _save_output_files(
    result: dict, 
    output_files: dict, 
    config: dict
) -> Dict[str, bool]:
    """
    Guarda archivos de salida de forma robusta.
    Retorna diccionario con estado de cada archivo.
    """
    import json
    
    # ROBUSTECIDO: Validar entradas
    if not result or not isinstance(result, dict):
        logger.error("‚ùå Resultado vac√≠o o inv√°lido para guardar")
        return {}
    
    if not output_files or not isinstance(output_files, dict):
        logger.error("‚ùå output_files vac√≠o o inv√°lido")
        return {}
    
    save_status = {}
    
    for name, path in output_files.items():
        try:
            # ROBUSTECIDO: Verificar que hay datos para guardar
            if name not in result:
                logger.debug(f"Clave '{name}' no encontrada en resultado, saltando")
                save_status[name] = False
                continue
            
            data = result[name]
            if not data:
                logger.debug(f"Datos vac√≠os para '{name}', saltando")
                save_status[name] = False
                continue
            
            # ROBUSTECIDO: Crear directorio si no existe
            path = Path(path)
            try:
                path.parent.mkdir(parents=True, exist_ok=True)
            except PermissionError as e:
                logger.error(f"‚ùå Sin permisos para crear directorio: {path.parent}")
                save_status[name] = False
                continue
            except Exception as e:
                logger.error(f"‚ùå Error creando directorio {path.parent}: {e}")
                save_status[name] = False
                continue
            
            # ROBUSTECIDO: Sanitizar datos antes de serializar
            try:
                sanitized_data = sanitize_for_json(data)
            except Exception as e:
                logger.error(f"‚ùå Error sanitizando datos para '{name}': {e}")
                save_status[name] = False
                continue
            
            # ROBUSTECIDO: Escritura at√≥mica con archivo temporal
            temp_path = path.with_suffix('.tmp')
            
            try:
                with open(temp_path, "w", encoding="utf-8") as f:
                    json.dump(
                        sanitized_data, 
                        f, 
                        indent=2, 
                        ensure_ascii=False,
                        default=str  # Fallback para tipos no serializables
                    )
                
                # Mover archivo temporal a destino final
                temp_path.replace(path)
                logger.info(f"‚úÖ Archivo guardado: {path}")
                save_status[name] = True
                
            except Exception as e:
                logger.error(f"‚ùå Error escribiendo '{name}' a {path}: {e}")
                # Limpiar archivo temporal si existe
                if temp_path.exists():
                    try:
                        temp_path.unlink()
                    except:
                        pass
                save_status[name] = False
                
        except Exception as e:
            logger.error(f"‚ùå Error inesperado guardando '{name}': {e}")
            save_status[name] = False
    
    # ROBUSTECIDO: Resumen de guardado
    successful = sum(1 for v in save_status.values() if v)
    total = len(save_status)
    logger.info(f"üìÅ Archivos guardados: {successful}/{total}")
    
    return save_status
