def _validate_with_lark(
    self, line: str, use_cache: bool = True
) -> Tuple[bool, Optional[Any], str]:
    """
    Valida una línea usando el parser Lark con optimización topológica.
    Refuerzo: Prefiltrado estricto, cache semántica y manejo jerárquico de errores.
    """
    # === PRECONDICIONES TOPOLÓGICAS ===
    if self.lark_parser is None:
        return (True, None, "Lark no disponible - validación omitida")

    if not line or not isinstance(line, str):
        return (False, None, "Línea vacía o tipo inválido")

    line_clean = line.strip()
    line_len = len(line_clean)

    if line_len > self._MAX_LINE_LENGTH:
        return (False, None, f"Línea excede límite topológico: {line_len} > {self._MAX_LINE_LENGTH}")
    if line_len < self._MIN_LINE_LENGTH:
        return (False, None, f"Línea insuficiente topológicamente: {line_len} < {self._MIN_LINE_LENGTH}")

    # === CACHE SEMÁNTICO ===
    cache_key = self._compute_semantic_cache_key(line_clean) if use_cache else None

    if use_cache and cache_key in self._parse_cache:
        self.validation_stats.cached_parses += 1
        cached_result = self._parse_cache[cache_key]

        if isinstance(cached_result, tuple) and len(cached_result) == 2:
            is_valid, tree = cached_result
            # Solo revalidar si fue exitoso pero el árbol es inválido (invariante roto)
            if is_valid and tree is not None and not self._is_valid_tree(tree):
                del self._parse_cache[cache_key]
            else:
                reason = "" if is_valid else "Falló previamente (cache válido)"
                return (is_valid, tree, reason)

    # === VALIDACIÓN DE CONECTIVIDAD ESTRUCTURAL ===
    if not self._has_minimal_structural_connectivity(line_clean):
        if use_cache and cache_key:
            self._cache_result(cache_key, False, None)
        return (False, None, "Falta conectividad estructural mínima")

    # === PARSING CON MANEJO JERÁRQUICO DE ERRORES ===
    try:
        tree = self.lark_parser.parse(line_clean)

        if not self._validate_tree_homotopy(tree):
            if use_cache and cache_key:
                self._cache_result(cache_key, False, None)
            return (False, None, "Árbol no cumple invariantes de homotopía")

        if use_cache and cache_key:
            self._cache_result(cache_key, True, tree)

        return (True, tree, "")

    except UnexpectedCharacters as uc:
        self.validation_stats.failed_lark_unexpected_chars += 1
        context = self._get_topological_context(line_clean, getattr(uc, 'column', 0))
        error_msg = f"Carácter discontinuo en vecindad {context}"
        
    except UnexpectedToken as ut:
        self.validation_stats.failed_lark_parse += 1
        expected = list(ut.expected) if hasattr(ut, 'expected') and ut.expected else []
        expected_space = self._map_tokens_to_topological_space(expected)
        token_repr = getattr(ut, 'token', 'desconocido')
        error_msg = f"Token '{token_repr}' fuera del espacio {expected_space}"
        
    except UnexpectedEOF:
        self.validation_stats.failed_lark_parse += 1
        completeness = self._calculate_topological_completeness(line_clean)
        error_msg = f"Fin prematuro (compleción {completeness:.0%})"
        
    except LarkError as le:
        self.validation_stats.failed_lark_parse += 1
        error_msg = f"Error Lark: {str(le)[:100]}"
        
    except Exception as e:
        self.validation_stats.failed_lark_parse += 1
        logger.error(f"Error inesperado en validación Lark: {type(e).__name__}: {e}")
        error_msg = f"Error inesperado: {type(e).__name__}"

    # Punto de salida unificado para errores
    if use_cache and cache_key:
        self._cache_result(cache_key, False, None)
    return (False, None, error_msg)


def _compute_semantic_cache_key(self, line: str) -> str:
    """
    Computa clave de cache basada en invariantes topológicos.
    Preserva semántica mientras normaliza variaciones sintácticas superficiales.
    """
    # Normalización de espacios (homeomorfismo de espaciado)
    normalized = re.sub(r'\s+', ' ', line.strip())
    
    # Normalización de ceros no significativos en posiciones decimales
    # Preservamos formato de miles (1,000) vs decimales contextuales
    normalized = re.sub(r'\b0+(\d+\.\d+)', r'\1', normalized)
    
    # Para líneas muy largas: firma topológica compacta
    if len(normalized) > self._CACHE_KEY_MAX_LENGTH:
        # Características estructurales que preservan la topología
        num_groups = len(re.findall(r'\d+[.,]?\d*', normalized))
        alpha_groups = len(re.findall(r'[A-Za-zÁÉÍÓÚáéíóúÑñ]+', normalized))
        sep_count = normalized.count(';')
        total_len = len(normalized)
        
        # Incluir prefijo para colisiones reducidas
        prefix = normalized[:50]
        suffix = normalized[-30:]
        
        feature_string = f"{prefix}|{num_groups}|{alpha_groups}|{sep_count}|{total_len}|{suffix}"
        return hashlib.sha256(feature_string.encode()).hexdigest()[:32]

    return normalized


def _validate_tree_homotopy(self, tree: Any) -> bool:
    """
    Verifica que el árbol de parsing sea homotópicamente válido.
    Un árbol es homotópicamente válido si puede deformarse continuamente
    a la estructura canónica esperada.
    """
    if tree is None:
        return False

    try:
        if not hasattr(tree, 'data') or not hasattr(tree, 'children'):
            return False

        # Invariante 1: La raíz debe pertenecer al espacio de no-terminales válidos
        if not isinstance(tree.data, str):
            return False

        # Invariante 2: Límite de ramificación (evita estructuras degeneradas)
        child_count = len(tree.children) if tree.children else 0
        if child_count > 50:  # Árbol anormalmente ancho
            return False

        # Invariante 3: Profundidad acotada (evita recursión infinita)
        max_depth = 20
        
        def check_depth_and_validity(node, current_depth: int) -> bool:
            if current_depth > max_depth:
                return False
            
            if hasattr(node, 'children') and node.children:
                for child in node.children:
                    if hasattr(child, 'data'):  # Es un nodo no-terminal
                        if not check_depth_and_validity(child, current_depth + 1):
                            return False
            return True

        if not check_depth_and_validity(tree, 0):
            return False

        # Invariante 4: Debe existir al menos un token terminal
        def has_terminal(node) -> bool:
            if not hasattr(node, 'children') or not node.children:
                return True  # Nodo hoja
            for child in node.children:
                if not hasattr(child, 'data'):  # Es un Token
                    return True
                if has_terminal(child):
                    return True
            return False

        return has_terminal(tree)

    except Exception:
        return False


def _has_minimal_structural_connectivity(self, line: str) -> bool:
    """
    Verifica conectividad topológica mínima.
    Una línea tiene conectividad si sus componentes están distribuidos
    y relacionados mediante separadores.
    """
    alpha_sequences = re.findall(r'[A-Za-zÁÉÍÓÚáéíóúÑñ]{2,}', line)
    numeric_sequences = re.findall(r'\d+\.?\d*', line)
    separator_count = line.count(';')

    has_alpha = len(alpha_sequences) >= 1
    has_numeric = len(numeric_sequences) >= 1
    min_separators = self._MIN_FIELDS_FOR_INSUMO - 1
    has_separators = separator_count >= min_separators

    if not (has_alpha and has_numeric and has_separators):
        return False

    # Análisis de distribución topológica
    line_len = len(line)
    if line_len < 10:
        return True  # Líneas cortas: conectividad trivial
    
    midpoint = line_len // 2
    first_half = line[:midpoint]
    second_half = line[midpoint:]
    
    # Verificar que información semántica existe en ambas mitades
    has_content_first = bool(re.search(r'[A-Za-z0-9]', first_half))
    has_content_second = bool(re.search(r'[A-Za-z0-9]', second_half))
    
    # Verificar distribución de separadores (conexiones entre componentes)
    seps_first = first_half.count(';')
    seps_second = second_half.count(';')
    
    # Debe haber separadores en ambas mitades para buena conectividad
    # o al menos contenido en ambas
    well_distributed = (
        (has_content_first and has_content_second) and
        (seps_first >= 1 or seps_second >= 1)
    )
    
    return well_distributed


def _is_apu_homeomorphic(self, tree: Any) -> bool:
    """
    Verifica que el árbol Lark sea homeomorfo a un registro APU válido.
    Un registro APU debe contener: descripción, valores numéricos, y estructura.
    """
    if not self._is_valid_tree(tree):
        return False

    from lark import Token
    
    essential_components = {
        'descripcion': False,
        'valor_numerico': False,
    }

    # Iteración con pila explícita (evita recursión profunda)
    stack = [tree]
    max_iterations = 1000  # Límite de seguridad
    iterations = 0

    while stack and iterations < max_iterations:
        iterations += 1
        node = stack.pop()

        if isinstance(node, Token):
            val = str(node.value).strip()
            if len(val) >= 2:
                if re.search(r'\d', val):
                    essential_components['valor_numerico'] = True
                if re.search(r'[a-zA-ZáéíóúÁÉÍÓÚñÑ]{3,}', val):
                    essential_components['descripcion'] = True
                    
            # Optimización: salir temprano si ambos encontrados
            if all(essential_components.values()):
                return True
                
        elif hasattr(node, 'children') and node.children:
            stack.extend(node.children)

    return essential_components['descripcion'] and essential_components['valor_numerico']


def _validate_insumo_line(self, line: str, fields: List[str]) -> LineValidationResult:
    """
    Validación topológica unificada con análisis de invariantes homeomórficos.
    """
    self.validation_stats.total_evaluated += 1

    # === CAPA 0: HOMEOMORFISMO DE TIPO ===
    if not isinstance(line, str) or not line:
        return LineValidationResult(
            is_valid=False,
            reason="Entrada no homeomorfa a string válido",
            fields_count=0,
            validation_layer="type_check_failed",
        )

    if not isinstance(fields, list):
        return LineValidationResult(
            is_valid=False,
            reason="Campos no homeomorfos a lista",
            fields_count=0,
            validation_layer="type_check_failed",
        )

    fields_count = len(fields)

    # === CAPA 1: INVARIANTES ESTRUCTURALES BÁSICOS ===
    basic_valid, basic_reason = self._validate_basic_structure(line, fields)

    if not basic_valid:
        error_group = self._classify_basic_error_group(basic_reason)
        return LineValidationResult(
            is_valid=False,
            reason=f"[{error_group}] {basic_reason}",
            fields_count=fields_count,
            has_numeric_fields=False,
            validation_layer="basic_invariant_failed",
        )

    # === CAPA 2: HOMEOMORFISMO LARK ===
    lark_valid, lark_tree, lark_reason = self._validate_with_lark(line)

    has_numeric = any(
        self._NUMERIC_PATTERN.search(f.strip()) 
        for f in fields[1:] if f
    )

    if not lark_valid:
        self._record_failed_sample(line, fields, lark_reason)
        error_class = self._classify_lark_error_topology(lark_reason)
        return LineValidationResult(
            is_valid=False,
            reason=f"[{error_class}] {lark_reason}",
            fields_count=fields_count,
            has_numeric_fields=has_numeric,
            validation_layer="lark_validation_failed",
        )

    self.validation_stats.passed_lark += 1
    self.validation_stats.passed_both += 1

    # === CAPA 3: HOMEOMORFISMO APU ===
    if lark_tree and not self._is_apu_homeomorphic(lark_tree):
        return LineValidationResult(
            is_valid=False,
            reason="Árbol sintáctico no mapeable a esquema APU",
            fields_count=fields_count,
            has_numeric_fields=has_numeric,
            validation_layer="apu_schema_mismatch",
            lark_tree=lark_tree,
        )

    return LineValidationResult(
        is_valid=True,
        reason="Validación completa exitosa",
        fields_count=fields_count,
        has_numeric_fields=has_numeric,
        validation_layer="full_homeomorphism",
        lark_tree=lark_tree,
    )


def _detect_category(self, line_upper: str) -> Optional[str]:
    """
    Detección de categorías usando análisis de pertenencia a conjuntos.
    Retorna la categoría con mayor score de coincidencia.
    """
    # Filtro rápido: líneas largas o con muchos números no son categorías
    if len(line_upper) > 50:
        return None
    
    digit_count = sum(c.isdigit() for c in line_upper)
    if digit_count > 3:
        return None

    best_match: Optional[str] = None
    best_score: float = 0.0
    
    line_stripped = line_upper.strip()

    for canonical, variations in self.CATEGORY_KEYWORDS.items():
        for variation in variations:
            # Coincidencia exacta de palabra completa
            pattern = rf'\b{re.escape(variation)}\b'
            match = re.search(pattern, line_stripped, re.IGNORECASE)
            
            if match:
                # Score basado en: posición + longitud de match + contexto
                position_score = 1.0 - (match.start() / max(len(line_stripped), 1))
                length_score = len(variation) / max(len(line_stripped), 1)
                
                # Bonus si la línea es principalmente esta palabra
                context_score = 1.5 if len(line_stripped) < 25 else 1.0
                
                total_score = (position_score * 0.4 + length_score * 0.3) * context_score
                
                if total_score > best_score:
                    best_score = total_score
                    best_match = canonical

    # Umbral mínimo de confianza
    return best_match if best_score > 0.15 else None


def _calculate_field_entropy(self, fields: List[str]) -> float:
    """
    Calcula entropía de Shannon normalizada sobre tipos de campos.
    Mide la diversidad estructural de la línea.
    """
    if not fields:
        return 0.0

    n = len(fields)
    if n == 1:
        return 0.0

    type_counts = {'alpha': 0, 'numeric': 0, 'mixed': 0, 'empty': 0}

    for field in fields:
        field_str = str(field).strip()
        if not field_str:
            type_counts['empty'] += 1
        elif re.match(r'^[\d.,\-+]+$', field_str):
            type_counts['numeric'] += 1
        elif not any(c.isdigit() for c in field_str):
            type_counts['alpha'] += 1
        else:
            type_counts['mixed'] += 1

    # Entropía de Shannon
    entropy = 0.0
    for count in type_counts.values():
        if count > 0:
            p = count / n
            entropy -= p * math.log2(p)

    # Normalizar por máxima entropía posible
    num_categories = sum(1 for c in type_counts.values() if c > 0)
    max_entropy = math.log2(num_categories) if num_categories > 1 else 1.0
    
    return entropy / max_entropy if max_entropy > 0 else 0.0


def _build_insumo_record(
    self,
    context: APUContext,
    category: str,
    line: str,
    line_number: int,
    validation_result: LineValidationResult,
    fields: Optional[List[str]] = None,
) -> Dict[str, Any]:
    """
    Construye registro con métricas topológicas.
    Acepta campos pre-procesados para evitar re-splitting.
    """
    if fields is None:
        fields = [f.strip() for f in line.split(';')]

    # Métricas topológicas
    field_entropy = self._calculate_field_entropy(fields)
    structural_density = self._calculate_structural_density(line)
    numeric_cohesion = self._calculate_numeric_cohesion(fields)
    homogeneity_index = self._calculate_homogeneity_index(fields)

    topological_metrics = {
        'field_entropy': round(field_entropy, 4),
        'structural_density': round(structural_density, 4),
        'numeric_cohesion': round(numeric_cohesion, 4),
        'homogeneity_index': round(homogeneity_index, 4),
    }

    homeomorphism_class = self._determine_homeomorphism_class(
        validation_result.validation_layer,
        topological_metrics
    )

    return {
        "apu_code": context.apu_code,
        "apu_desc": context.apu_desc,
        "apu_unit": context.apu_unit,
        "category": category,
        "insumo_line": line,
        "source_line": line_number,
        "fields_count": validation_result.fields_count,
        "validation_layer": validation_result.validation_layer,
        "homeomorphism_class": homeomorphism_class,
        "topological_metrics": topological_metrics,
        "_lark_tree": validation_result.lark_tree,
        "_structural_signature": self._compute_structural_signature(line),
    }


def _calculate_structural_density(self, line: str) -> float:
    """
    Calcula densidad estructural: ratio de unidades semánticas por carácter.
    """
    if not line:
        return 0.0
    
    # Unidades semánticas: palabras significativas + números
    words = re.findall(r'\b[A-Za-zÁÉÍÓÚáéíóúÑñ]{3,}\b', line)
    numbers = re.findall(r'\d+(?:[.,]\d+)?', line)

    semantic_units = len(words) + len(numbers)
    
    # Normalizar por longitud efectiva (sin espacios redundantes)
    effective_length = len(re.sub(r'\s+', ' ', line.strip()))
    
    return semantic_units / effective_length if effective_length > 0 else 0.0


def _calculate_numeric_cohesion(self, fields: List[str]) -> float:
    """
    Mide qué tan agrupados están los campos numéricos.
    Cohesión alta = números contiguos; baja = dispersos.
    """
    if not fields:
        return 0.0

    numeric_positions = [
        i for i, f in enumerate(fields)
        if f and re.search(r'\d', str(f))
    ]

    n_numeric = len(numeric_positions)
    
    if n_numeric <= 1:
        return 1.0 if n_numeric == 1 else 0.0

    # Calcular dispersión: diferencia entre span real y span ideal
    span = numeric_positions[-1] - numeric_positions[0] + 1
    ideal_span = n_numeric  # Si estuvieran contiguos
    
    # Cohesión = qué tan cerca del ideal
    cohesion = ideal_span / span if span > 0 else 1.0
    
    return min(cohesion, 1.0)


def _determine_homeomorphism_class(
    self,
    validation_layer: str,
    metrics: Dict[str, float]
) -> str:
    """
    Clasifica el registro según su clase de equivalencia topológica.
    """
    if validation_layer != "full_homeomorphism":
        return f"PARCIAL_{validation_layer.upper()}"

    entropy = metrics.get('field_entropy', 0)
    density = metrics.get('structural_density', 0)
    cohesion = metrics.get('numeric_cohesion', 0)
    homogeneity = metrics.get('homogeneity_index', 0)

    # Clasificación jerárquica
    if entropy > 0.6 and density > 0.08 and cohesion > 0.7:
        return "CLASE_A_COMPLETO"
    elif cohesion > 0.85 and homogeneity > 0.5:
        return "CLASE_B_NUMERICO"
    elif homogeneity > 0.7:
        return "CLASE_C_HOMOGENEO"
    elif entropy > 0.4 or density > 0.05:
        return "CLASE_D_MIXTO"
    else:
        return "CLASE_E_IRREGULAR"