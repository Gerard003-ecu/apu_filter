"""
Este componente implementa un motor de inferencia determinista diseÃ±ado para
categorizar la naturaleza ontolÃ³gica de un APU (AnÃ¡lisis de Precio Unitario).
Utiliza un sistema de reglas jerÃ¡rquicas vectorizadas para particionar el espacio
vectorial definido por las proporciones de costos.

MetodologÃ­a de ClasificaciÃ³n (V2 - Vectorial):
----------------------------------------------
1. CompilaciÃ³n de Reglas (`_compile_rules`):
   Convierte las reglas de texto ("porcentaje > 60") en funciones vectorizadas de NumPy
   altamente optimizadas, permitiendo clasificar millones de registros en milisegundos.

2. ValidaciÃ³n de Cobertura Espacial:
   Utiliza un muestreo denso sobre el espacio [0,1]Â² para verificar matemÃ¡ticamente
   que las reglas cubren el espacio de posibilidades, detectando "agujeros" lÃ³gicos
   sin la complejidad computacional de Voronoi en altas dimensiones.

3. AnÃ¡lisis Espacial:
   Calcula centroides geomÃ©tricos de las categorÃ­as clasificadas para entender la
   distribuciÃ³n topolÃ³gica de los costos en el espacio de fase.

4. StructuralClassifier:
   Extiende la lÃ³gica para considerar la topologÃ­a de la red de insumos (Nivel 3),
   detectando estructuras como 'Islas' (Suministro Puro sin instalaciÃ³n).
"""

import json
import logging
import re
from dataclasses import dataclass
from pathlib import Path
from typing import ClassVar, Dict, List, Optional, Tuple, Set

import numpy as np
import pandas as pd
# from scipy.spatial import Voronoi, Delaunay # Removed: Using Grid Sampling optimization

logger = logging.getLogger(__name__)


@dataclass
class ClassificationRule:
    """Estructura para reglas de clasificaciÃ³n con validaciÃ³n integrada."""

    rule_type: str
    priority: int
    condition: str
    description: str

    _ALLOWED_VARS: ClassVar[frozenset] = None
    _CONDITION_PATTERN: ClassVar[re.Pattern] = None

    def __post_init__(self):
        """Normaliza y valida la condiciÃ³n al instanciar."""
        if ClassificationRule._ALLOWED_VARS is None:
            ClassificationRule._ALLOWED_VARS = frozenset(
                {"porcentaje_materiales", "porcentaje_mo_eq"}
            )
            ClassificationRule._CONDITION_PATTERN = re.compile(
                r"^[\d\s\.\(\)><=!]+$|porcentaje_materiales|porcentaje_mo_eq|and|or|not"
            )

        self.condition = self._normalize_condition(self.condition)
        self._validate_syntax()

    def _normalize_condition(self, condition: str) -> str:
        condition = re.sub(r"\bAND\b", "and", condition, flags=re.IGNORECASE)
        condition = re.sub(r"\bOR\b", "or", condition, flags=re.IGNORECASE)
        condition = re.sub(r"\bNOT\b", "not", condition, flags=re.IGNORECASE)
        return condition.strip()

    def _validate_syntax(self) -> None:
        test_expr = self.condition
        allowed_vars = ClassificationRule._ALLOWED_VARS or frozenset({"porcentaje_materiales", "porcentaje_mo_eq"})

        for var in allowed_vars:
            test_expr = test_expr.replace(var, " ")
        test_expr = re.sub(r"\b\d+\.?\d*\b", " ", test_expr)
        allowed_tokens = [">=", "<=", "==", "!=", ">", "<", "and", "or", "not", "(", ")"]
        for token in allowed_tokens:
            test_expr = test_expr.replace(token, " ")

        remaining = test_expr.strip()
        if remaining:
            raise ValueError(f"CondiciÃ³n contiene elementos no permitidos: '{remaining}'")

        try:
            compile(self.condition, "<condition>", "eval")
        except SyntaxError as e:
            raise ValueError(f"Sintaxis invÃ¡lida en condiciÃ³n: {e}")

    def evaluate(self, pct_materiales: float, pct_mo_eq: float) -> bool:
        try:
            safe_context = {
                "__builtins__": {},
                "porcentaje_materiales": pct_materiales * 100.0,
                "porcentaje_mo_eq": pct_mo_eq * 100.0,
            }
            return bool(eval(self.condition, safe_context))
        except Exception as e:
            logger.error(f"Error evaluando regla '{self.rule_type}': {e}")
            return False

    def get_coverage_bounds(self) -> Tuple[Tuple[float, float], Tuple[float, float]]:
        mat_min, mat_max = 0.0, 1.0
        mo_min, mo_max = 0.0, 1.0

        bound_patterns = [
            (r"porcentaje_materiales\s*>=\s*(\d+\.?\d*)", "mat", "min"),
            (r"porcentaje_materiales\s*>\s*(\d+\.?\d*)", "mat", "min"),
            (r"porcentaje_materiales\s*<=\s*(\d+\.?\d*)", "mat", "max"),
            (r"porcentaje_materiales\s*<\s*(\d+\.?\d*)", "mat", "max"),
            (r"porcentaje_mo_eq\s*>=\s*(\d+\.?\d*)", "mo", "min"),
            (r"porcentaje_mo_eq\s*>\s*(\d+\.?\d*)", "mo", "min"),
            (r"porcentaje_mo_eq\s*<=\s*(\d+\.?\d*)", "mo", "max"),
            (r"porcentaje_mo_eq\s*<\s*(\d+\.?\d*)", "mo", "max"),
        ]

        for pattern, var_type, bound_type in bound_patterns:
            match = re.search(pattern, self.condition)
            if match:
                value = float(match.group(1)) / 100.0
                if var_type == "mat":
                    if bound_type == "min": mat_min = max(mat_min, value)
                    else: mat_max = min(mat_max, value)
                else:
                    if bound_type == "min": mo_min = max(mo_min, value)
                    else: mo_max = min(mo_max, value)

        return ((mat_min, mat_max), (mo_min, mo_max))


class APUClassifier:
    """Clasificador robusto de APUs basado en reglas vectorizadas y validaciÃ³n espacial."""

    def __init__(self, config_path: Optional[str] = None):
        self.rules: List[ClassificationRule] = []
        self.default_type = "INDEFINIDO"
        self.zero_cost_type = "SIN_COSTO"
        self._rule_cache: Dict[str, callable] = {}

        self._load_config(config_path)
        self._validate_rules()
        self._compile_rules()

    def _load_config(self, config_path: Optional[str]) -> None:
        if not config_path or not Path(config_path).exists():
            logger.warning("âš ï¸ ConfiguraciÃ³n no encontrada, usando reglas por defecto")
            self._load_default_rules()
            return

        try:
            with open(config_path, "r", encoding="utf-8") as f:
                config = json.load(f)

            rules_config = config.get("apu_classification_rules", {})
            for rule_dict in rules_config.get("rules", []):
                try:
                    rule = ClassificationRule(
                        rule_type=rule_dict["type"],
                        priority=rule_dict.get("priority", 99),
                        condition=rule_dict["condition"],
                        description=rule_dict.get("description", ""),
                    )
                    self.rules.append(rule)
                except Exception as e:
                    logger.warning(f"âš ï¸ Regla invÃ¡lida: {e}")

            if not self.rules:
                self._load_default_rules()
                return

            self.rules.sort(key=lambda r: r.priority)
            self.default_type = rules_config.get("default_type", "INDEFINIDO")
            self.zero_cost_type = rules_config.get("zero_cost_type", "SIN_COSTO")

        except Exception as e:
            logger.error(f"âŒ Error cargando configuraciÃ³n: {e}")
            self._load_default_rules()

    def _load_default_rules(self) -> None:
        self.rules = [
            ClassificationRule("INSTALACION", 1, "porcentaje_mo_eq >= 60.0", "Predomina MO/equipo (â‰¥60%)"),
            ClassificationRule("SUMINISTRO", 2, "porcentaje_materiales >= 60.0", "Predomina materiales (â‰¥60%)"),
            ClassificationRule(
                "CONSTRUCCION_MIXTO", 3,
                "(porcentaje_materiales >= 40.0 and porcentaje_materiales < 60.0) or (porcentaje_mo_eq >= 40.0 and porcentaje_mo_eq < 60.0)",
                "ComposiciÃ³n mixta (40-60%)"
            ),
            ClassificationRule("OBRA_COMPLETA", 4, "porcentaje_materiales >= 0 and porcentaje_mo_eq >= 0", "Cobertura residual"),
        ]

    def _compile_rules(self) -> None:
        for rule in self.rules:
            try:
                # Usamos la condiciÃ³n original, sin reemplazos manuales,
                # ya que pd.eval maneja las variables por nombre del contexto local.
                self._rule_cache[rule.rule_type] = self._create_vectorized_function(rule.condition)
            except Exception as e:
                logger.warning(f"No se pudo compilar regla {rule.rule_type}: {e}")

    def _create_vectorized_function(self, condition: str) -> callable:
        """Crea funciÃ³n vectorizada robusta usando pd.eval para manejar precedencia."""

        def rule_func(x: np.ndarray, y: np.ndarray) -> np.ndarray:
            try:
                # Pandas eval soporta sintaxis Python natural (and/or) y vectorizaciÃ³n.
                # Pasamos x e y mapeados a los nombres que usan las reglas.
                context = {
                    "porcentaje_materiales": x * 100.0,
                    "porcentaje_mo_eq": y * 100.0
                }
                # engine='python' es necesario si numexpr no estÃ¡ instalado o para features complejas,
                # pero 'numexpr' es mÃ¡s rÃ¡pido. Dejamos que pandas decida o forzamos si falla.
                # Sin embargo, para consistencia con operadores 'and'/'or' textuales, engine='numexpr'
                # a veces requiere '&'/'|'. Pandas eval suele intentar traducir.
                # Para mÃ¡xima seguridad con strings "and"/"or", el parser de pandas funciona bien.
                return pd.eval(condition, local_dict=context)
            except Exception as e:
                # logger.error(f"Error evaluando regla vectorizada: {e}")
                return np.zeros_like(x, dtype=bool)

        return rule_func

    def _validate_rules(self) -> None:
        if not self.rules:
            raise ValueError("No hay reglas de clasificaciÃ³n definidas")

        # Restaurar detecciÃ³n de duplicados para pasar tests existentes
        types = [r.rule_type for r in self.rules]
        duplicates = {t for t in types if types.count(t) > 1}
        if duplicates:
            logger.warning(f"âš ï¸ Tipos duplicados: {duplicates}")

        coverage_gaps = self._sample_uncovered_regions()
        if coverage_gaps:
            gap_area = len(coverage_gaps) / 2500.0
            msg = f"âš ï¸ Reglas no cubren {gap_area:.1%} del espacio. Puntos sin cobertura detectados."
            if gap_area > 0.1:
                # logger.warning(msg) # Ya loggeado abajo o por caller
                pass
            logger.warning(msg)
        else:
             logger.info("Cobertura topolÃ³gica validada.")

    def _sample_uncovered_regions(self, grid_size: int = 50) -> List[Tuple[float, float]]:
        """
        Analiza cobertura muestreando el espacio [0,1]x[0,1].
        Alias mantenido para compatibilidad con tests.
        """
        x = np.linspace(0, 1, grid_size)
        y = np.linspace(0, 1, grid_size)

        uncovered = []
        for i in x:
            for j in y:
                covered = False
                for rule in self.rules:
                    if rule.evaluate(float(i), float(j)):
                        covered = True
                        break
                if not covered:
                    uncovered.append((float(i), float(j)))
        return uncovered

    # Alias para compatibilidad interna si se llama distinto
    _analyze_coverage_voronoi = _sample_uncovered_regions

    def classify_single(self, pct_materiales: float, pct_mo_eq: float, total_cost: float = 1.0) -> str:
        pct_materiales = max(0.0, min(1.0, float(pct_materiales)))
        pct_mo_eq = max(0.0, min(1.0, float(pct_mo_eq)))

        if total_cost <= 0 or (isinstance(total_cost, float) and np.isnan(total_cost)):
            return self.zero_cost_type

        # Intentar usar cachÃ© vectorizada (aunque sea overhead para 1 item, asegura consistencia)
        for rule in self.rules:
            if rule.rule_type in self._rule_cache:
                try:
                    func = self._rule_cache[rule.rule_type]
                    # Pasar como arrays de 1 elemento
                    res = func(np.array([pct_materiales]), np.array([pct_mo_eq]))
                    # pd.eval puede devolver scalar bool o array bool
                    if np.ndim(res) == 0:
                        if res: return rule.rule_type
                    elif res[0]:
                        return rule.rule_type
                except Exception:
                    # Fallback
                    if rule.evaluate(pct_materiales, pct_mo_eq): return rule.rule_type
            else:
                 if rule.evaluate(pct_materiales, pct_mo_eq): return rule.rule_type

        return self.default_type

    def classify_dataframe(self, df: pd.DataFrame, col_total: str = "VALOR_CONSTRUCCION_UN",
                          col_materiales: str = "VALOR_SUMINISTRO_UN", col_mo_eq: str = "VALOR_INSTALACION_UN",
                          output_col: str = "TIPO_APU") -> pd.DataFrame:
        df = df.copy()
        required = [col_total, col_materiales, col_mo_eq]
        if any(c not in df.columns for c in required):
            logger.error(f"âŒ Columnas faltantes")
            df[output_col] = self.default_type
            return df

        totales = pd.to_numeric(df[col_total], errors='coerce').fillna(0).values
        materiales = pd.to_numeric(df[col_materiales], errors='coerce').fillna(0).values
        mo_eq = pd.to_numeric(df[col_mo_eq], errors='coerce').fillna(0).values

        with np.errstate(divide='ignore', invalid='ignore'):
            totales_safe = np.where(totales == 0, 1.0, totales)
            pct_mat = np.clip(materiales / totales_safe, 0.0, 1.0)
            pct_mo = np.clip(mo_eq / totales_safe, 0.0, 1.0)

        tipos = self._classify_vectorized_optimized(totales, pct_mat, pct_mo)
        df[output_col] = tipos

        self._analyze_classification_quality(df, output_col)
        return df

    def _classify_vectorized_optimized(self, totales: np.ndarray, pct_mat: np.ndarray, pct_mo: np.ndarray) -> np.ndarray:
        n = len(totales)
        tipos = np.full(n, self.default_type, dtype=object)

        mask_sin_costo = (totales <= 0) | np.isnan(totales)
        tipos[mask_sin_costo] = self.zero_cost_type

        mask_validos = ~mask_sin_costo
        if not mask_validos.any(): return tipos

        valid_indices = np.where(mask_validos)[0]
        mat_valid = pct_mat[mask_validos]
        mo_valid = pct_mo[mask_validos]

        rule_masks = {}
        for rule in self.rules:
            if rule.rule_type in self._rule_cache:
                try:
                    func = self._rule_cache[rule.rule_type]
                    # AquÃ­ pd.eval devolverÃ¡ un array bool del tamaÃ±o de mat_valid
                    rule_masks[rule.rule_type] = np.asarray(func(mat_valid, mo_valid), dtype=bool)
                except Exception as e:
                    logger.error(f"Error regla {rule.rule_type}: {e}")

        assigned = np.zeros(len(valid_indices), dtype=bool)
        for rule in sorted(self.rules, key=lambda r: r.priority):
            if rule.rule_type in rule_masks:
                mask = rule_masks[rule.rule_type]
                to_assign = mask & (~assigned)
                if to_assign.any():
                    tipos[valid_indices[to_assign]] = rule.rule_type
                    assigned[to_assign] = True
        return tipos

    def _analyze_classification_quality(self, df: pd.DataFrame, tipo_col: str) -> None:
        if tipo_col not in df.columns: return
        stats = df[tipo_col].value_counts()
        logger.info("="*60 + "\nðŸ“Š CALIDAD CLASIFICACIÃ“N\n" + "="*60)
        for t, c in stats.items():
            pct = c / len(df) * 100
            bar = "â–ˆ" * int(pct/2) + "â–‘" * (50 - int(pct/2))
            logger.info(f"{t:20} {c:6} ({pct:5.1f}%) {bar}")
        self._analyze_spatial_distribution(df, tipo_col)

    def _analyze_spatial_distribution(self, df: pd.DataFrame, tipo_col: str) -> None:
        cols_mat = [c for c in df.columns if "PORCENTAJE_MATERIALES" in c.upper()]
        cols_mo = [c for c in df.columns if "PORCENTAJE_MO" in c.upper()]
        if cols_mat and cols_mo:
            c_mat, c_mo = cols_mat[0], cols_mo[0]
            logger.info("\nðŸ“ CENTROIDES TOPOLÃ“GICOS:")
            for tipo in df[tipo_col].unique():
                mask = df[tipo_col] == tipo
                if mask.any():
                    cx, cy = df.loc[mask, c_mat].mean(), df.loc[mask, c_mo].mean()
                    logger.info(f"  {tipo:20}: ({cx:.2f}, {cy:.2f})")

    def get_coverage_report(self) -> pd.DataFrame:
        data = []
        for rule in self.rules:
            (mat_min, mat_max), (mo_min, mo_max) = rule.get_coverage_bounds()
            area = (mat_max - mat_min) * (mo_max - mo_min)
            data.append({
                "tipo": rule.rule_type,
                "prioridad": rule.priority,
                "mat_range": f"[{mat_min:.0%}, {mat_max:.0%}]",
                "mo_range": f"[{mo_min:.0%}, {mo_max:.0%}]",
                "area_estimada": f"{area:.1%}",
                "condicion": rule.condition
            })
        return pd.DataFrame(data)

class StructuralClassifier(APUClassifier):
    def classify_by_structure(self, insumos_del_apu: List[Dict], min_support_threshold: float = 0.1) -> Tuple[str, Dict[str, float]]:
        if not insumos_del_apu: return "ESTRUCTURA_VACIA", {}
        valores = {}
        total = 0.0
        for i in insumos_del_apu:
            t = i.get('TIPO_INSUMO', 'OTRO')
            v = float(i.get('VALOR_TOTAL', 0.0))
            valores[t] = valores.get(t, 0.0) + v
            total += v

        if total <= 0: return "SIN_VALOR_ESTRUCTURAL", valores

        pcts = {k: v/total for k, v in valores.items()}
        mo_pct = pcts.get('MANO_DE_OBRA', 0.0)
        mat_pct = pcts.get('SUMINISTRO', 0.0)

        if mo_pct > 0.9: return "SERVICIO_PURO", pcts
        elif mat_pct > 0.9: return "SUMINISTRO_PURO", pcts
        elif mat_pct > 0 and mo_pct == 0: return "SUMINISTRO_AISLADO", pcts
        return "ESTRUCTURA_MIXTA", pcts
