"""
M√≥dulo de Capacitancia L√≥gica para el procesamiento de flujos de datos.

Este m√≥dulo introduce el `DataFluxCondenser`, un componente de alto nivel que
act√∫a como una fachada estabilizadora para el pipeline de procesamiento de
An√°lisis de Precios Unitarios (APU). Su funci√≥n principal es garantizar la
integridad, coherencia y estabilidad del flujo de datos antes de que ingrese
al n√∫cleo del sistema.

Principios de Dise√±o:
- **Capacitancia L√≥gica:** Inspirado en los principios de un circuito RLC,
  el condensador "absorbe" datos crudos y los "descarga" de manera controlada,
  filtrando el ruido y la turbulencia.
- **Orquestaci√≥n, no Implementaci√≥n:** No contiene l√≥gica de negocio de bajo
  nivel. En su lugar, orquesta componentes especializados como `ReportParserCrudo`
  (el "Guardia") y `APUProcessor` (el "Cirujano").
- **Telemetr√≠a F√≠sica:** Incorpora un `FluxPhysicsEngine` para calcular
  m√©tricas de saturaci√≥n, complejidad e inductancia (flyback), proporcionando
  una visi√≥n cuantitativa de la "salud" del flujo de datos entrante.
- **Control Adaptativo (PID):** Implementa un lazo de control Proporcional-Integral
  para ajustar din√°micamente el flujo de procesamiento (tama√±o de lote) en funci√≥n
  de la saturaci√≥n y complejidad detectada, asegurando "Flujo Laminar".
- **Robustez y Tolerancia a Fallos:** Implementa validaciones estrictas en cada
  etapa y un manejo de errores detallado para prevenir la propagaci√≥n de datos
  corruptos.
"""
import logging
import math
import time
from dataclasses import dataclass
from pathlib import Path
from typing import Any, Dict, List, NamedTuple, Optional, Set

import pandas as pd

from .apu_processor import APUProcessor
from .report_parser_crudo import ReportParserCrudo

logger = logging.getLogger(__name__)


# ============================================================================
# CONSTANTES DEL SISTEMA
# ============================================================================
class SystemConstants:
    """Constantes del sistema para evitar n√∫meros m√°gicos."""

    # L√≠mites de tiempo
    MIN_DELTA_TIME: float = 0.001  # Segundos m√≠nimos entre c√°lculos PID

    # L√≠mites f√≠sicos
    MIN_ENERGY_THRESHOLD: float = 1e-10  # Julios m√≠nimos para c√°lculos
    MAX_EXPONENTIAL_ARG: float = 100.0   # L√≠mite para evitar overflow en exp()

    # Diagn√≥stico
    LOW_INERTIA_THRESHOLD: float = 0.1
    HIGH_PRESSURE_RATIO: float = 1000.0
    HIGH_FLYBACK_THRESHOLD: float = 0.5
    OVERHEAT_POWER_THRESHOLD: float = 50.0  # Watts

    # Control de flujo
    EMERGENCY_BRAKE_FACTOR: float = 0.5
    MAX_ITERATIONS_MULTIPLIER: int = 10  # max_iterations = total_records * multiplier

    # Validaci√≥n de archivos
    VALID_FILE_EXTENSIONS: Set[str] = {'.csv', '.txt', '.tsv'}

    # Resistencia din√°mica
    COMPLEXITY_RESISTANCE_FACTOR: float = 5.0


# ============================================================================
# CLASES DE EXCEPCIONES
# ============================================================================
class DataFluxCondenserError(Exception):
    """Clase base para todas las excepciones personalizadas del condensador."""
    pass


class InvalidInputError(DataFluxCondenserError):
    """Indica un problema con los datos de entrada, como un archivo inv√°lido."""
    pass


class ProcessingError(DataFluxCondenserError):
    """Se√±ala un error durante una de las etapas de procesamiento de datos."""
    pass


class ConfigurationError(DataFluxCondenserError):
    """Indica un problema con la configuraci√≥n del sistema."""
    pass


# ============================================================================
# ESTRUCTURAS DE DATOS
# ============================================================================
class ParsedData(NamedTuple):
    """
    Estructura de datos inmutable para los resultados del parseo inicial.

    Agrupa la salida del `ReportParserCrudo` para asegurar que los datos
    crudos y la cach√© de parseo se mantengan juntos a trav√©s del pipeline.

    Atributos:
        raw_records (List[Dict[str, Any]]): Lista de registros de insumos
            extra√≠dos del archivo de APU, sin procesamiento profundo.
        parse_cache (Dict[str, Any]): Metadatos generados durante el parseo,
            √∫tiles para optimizar el procesamiento posterior (e.g., l√≠neas
            ya validadas por Lark).
    """
    raw_records: List[Dict[str, Any]]
    parse_cache: Dict[str, Any]


@dataclass(frozen=True)
class CondenserConfig:
    """
    Configuraci√≥n inmutable y validada para el `DataFluxCondenser`.

    Define los umbrales operativos y comportamientos del condensador,
    incluyendo sus par√°metros para el motor de simulaci√≥n f√≠sica y el PID.

    Atributos:
        min_records_threshold (int): N√∫mero m√≠nimo de registros necesarios para
            considerar un archivo como v√°lido para el procesamiento.
        enable_strict_validation (bool): Si es `True`, activa validaciones
            adicionales en el DataFrame de salida.
        log_level (str): Nivel de logging para la instancia del condensador.
        system_capacitance (float): Par√°metro f√≠sico RLC (Faradios).
        base_resistance (float): Par√°metro f√≠sico RLC (Ohmios).
        system_inductance (float): Par√°metro f√≠sico RLC (Henrios).
        pid_setpoint (float): Objetivo de saturaci√≥n (0.0-1.0).
        pid_kp (float): Ganancia Proporcional del PID.
        pid_ki (float): Ganancia Integral del PID.
        min_batch_size (int): Tama√±o m√≠nimo del lote de procesamiento.
        max_batch_size (int): Tama√±o m√°ximo del lote de procesamiento.
        enable_partial_recovery (bool): Permite continuar procesamiento si falla un batch.
        max_failed_batches (int): M√°ximo de batches que pueden fallar antes de abortar.
    """
    min_records_threshold: int = 1
    enable_strict_validation: bool = True
    log_level: str = "INFO"

    # Configuraci√≥n F√≠sica RLC
    system_capacitance: float = 5000.0
    base_resistance: float = 10.0
    system_inductance: float = 2.0

    # Configuraci√≥n PID
    pid_setpoint: float = 0.30
    pid_kp: float = 2000.0
    pid_ki: float = 100.0
    min_batch_size: int = 50
    max_batch_size: int = 5000

    # Configuraci√≥n de recuperaci√≥n
    enable_partial_recovery: bool = False
    max_failed_batches: int = 3

    # Anti-windup
    integral_limit_factor: float = 2.0  # M√∫ltiplo del rango de salida

    def __post_init__(self):
        """Valida la configuraci√≥n despu√©s de la inicializaci√≥n."""
        self._validate_configuration()

    def _validate_configuration(self) -> None:
        """Valida que todos los par√°metros est√©n en rangos v√°lidos."""
        errors = []

        # Validar threshold
        if self.min_records_threshold < 0:
            errors.append(f"min_records_threshold debe ser >= 0, recibido: {self.min_records_threshold}")

        # Validar par√°metros f√≠sicos
        if self.system_capacitance <= 0:
            errors.append(f"system_capacitance debe ser > 0, recibido: {self.system_capacitance}")

        if self.base_resistance <= 0:
            errors.append(f"base_resistance debe ser > 0, recibido: {self.base_resistance}")

        if self.system_inductance <= 0:
            errors.append(f"system_inductance debe ser > 0, recibido: {self.system_inductance}")

        # Validar PID
        if not 0.0 <= self.pid_setpoint <= 1.0:
            errors.append(f"pid_setpoint debe estar en [0.0, 1.0], recibido: {self.pid_setpoint}")

        if self.pid_kp < 0:
            errors.append(f"pid_kp debe ser >= 0, recibido: {self.pid_kp}")

        if self.pid_ki < 0:
            errors.append(f"pid_ki debe ser >= 0, recibido: {self.pid_ki}")

        # Validar batch sizes
        if self.min_batch_size <= 0:
            errors.append(f"min_batch_size debe ser > 0, recibido: {self.min_batch_size}")

        if self.max_batch_size <= 0:
            errors.append(f"max_batch_size debe ser > 0, recibido: {self.max_batch_size}")

        if self.min_batch_size > self.max_batch_size:
            errors.append(
                f"min_batch_size ({self.min_batch_size}) no puede ser mayor que "
                f"max_batch_size ({self.max_batch_size})"
            )

        # Validar recuperaci√≥n
        if self.max_failed_batches < 0:
            errors.append(f"max_failed_batches debe ser >= 0, recibido: {self.max_failed_batches}")

        if self.integral_limit_factor <= 0:
            errors.append(f"integral_limit_factor debe ser > 0, recibido: {self.integral_limit_factor}")

        # Validar log level
        valid_log_levels = {'DEBUG', 'INFO', 'WARNING', 'ERROR', 'CRITICAL'}
        if self.log_level.upper() not in valid_log_levels:
            errors.append(f"log_level debe ser uno de {valid_log_levels}, recibido: {self.log_level}")

        if errors:
            raise ConfigurationError(
                "Errores de configuraci√≥n detectados:\n" + "\n".join(f"  - {e}" for e in errors)
            )


@dataclass
class ProcessingStats:
    """Estad√≠sticas del procesamiento para observabilidad."""
    total_records: int = 0
    processed_records: int = 0
    failed_records: int = 0
    total_batches: int = 0
    failed_batches: int = 0
    processing_time: float = 0.0
    avg_batch_size: float = 0.0
    avg_saturation: float = 0.0
    max_dissipated_power: float = 0.0
    max_flyback_voltage: float = 0.0
    avg_kinetic_energy: float = 0.0
    emergency_brakes_triggered: int = 0

    def add_batch_stats(self, batch_size: int, saturation: float, power: float, flyback: float, kinetic: float, success: bool) -> None:
        """Actualiza estad√≠sticas con datos de un batch procesado."""
        self.total_batches += 1
        if success:
            self.processed_records += batch_size
        else:
            self.failed_records += batch_size
            self.failed_batches += 1

        # Promedios m√≥viles
        n = self.total_batches
        self.avg_batch_size = ((n - 1) * self.avg_batch_size + batch_size) / n
        self.avg_saturation = ((n - 1) * self.avg_saturation + saturation) / n
        self.avg_kinetic_energy = ((n - 1) * self.avg_kinetic_energy + kinetic) / n
        self.max_dissipated_power = max(self.max_dissipated_power, power)
        self.max_flyback_voltage = max(self.max_flyback_voltage, flyback)


# ============================================================================
# CONTROLADOR PI DISCRETO
# ============================================================================
class PIController:
    """
    Implementaci√≥n robusta de un Controlador PI Discreto seg√∫n la teor√≠a de control.

    Objetivo: Mantener la saturaci√≥n del sistema en un Setpoint (SP) estable,
    ajustando din√°micamente la variable de control (Tama√±o del Batch).
    
    Mejoras implementadas:
    - Validaci√≥n exhaustiva de par√°metros
    - Anti-windup expl√≠cito con l√≠mites configurables
    - Protecci√≥n contra delta_time inv√°lido
    - Reset capability para reutilizaci√≥n
    - Logging detallado de estado
    """

    def __init__(
        self,
        kp: float,
        ki: float,
        setpoint: float,
        min_output: int,
        max_output: int,
        integral_limit_factor: float = 2.0
    ):
        """
        Inicializa el controlador PI con validaci√≥n de par√°metros.
        
        Args:
            kp: Ganancia proporcional (debe ser >= 0)
            ki: Ganancia integral (debe ser >= 0)
            setpoint: Valor objetivo (debe estar en [0, 1])
            min_output: Salida m√≠nima del actuador (debe ser > 0)
            max_output: Salida m√°xima del actuador (debe ser > min_output)
            integral_limit_factor: Factor para l√≠mites de anti-windup
        
        Raises:
            ConfigurationError: Si alg√∫n par√°metro es inv√°lido
        """
        self._validate_parameters(kp, ki, setpoint, min_output, max_output, integral_limit_factor)

        self.Kp = kp
        self.Ki = ki
        self.setpoint = setpoint

        self.min_output = min_output
        self.max_output = max_output

        # Calcular salida base (punto medio del rango)
        self._base_output = (self.max_output + self.min_output) / 2.0
        self._output_range = self.max_output - self.min_output

        # Anti-windup: L√≠mites para el t√©rmino integral
        self._integral_limit = self._output_range * integral_limit_factor

        # Estado interno
        self._integral_error = 0.0
        self._last_time: Optional[float] = None
        self._iteration_count = 0

        self.logger = logging.getLogger(f"{self.__class__.__name__}")

    def _validate_parameters(
        self,
        kp: float,
        ki: float,
        setpoint: float,
        min_output: int,
        max_output: int,
        integral_limit_factor: float
    ) -> None:
        """Valida todos los par√°metros del controlador."""
        errors = []

        if kp < 0:
            errors.append(f"Kp debe ser >= 0, recibido: {kp}")

        if ki < 0:
            errors.append(f"Ki debe ser >= 0, recibido: {ki}")

        if not 0.0 <= setpoint <= 1.0:
            errors.append(f"setpoint debe estar en [0.0, 1.0], recibido: {setpoint}")

        if min_output <= 0:
            errors.append(f"min_output debe ser > 0, recibido: {min_output}")

        if max_output <= min_output:
            errors.append(
                f"max_output ({max_output}) debe ser > min_output ({min_output})"
            )

        if integral_limit_factor <= 0:
            errors.append(f"integral_limit_factor debe ser > 0, recibido: {integral_limit_factor}")

        if errors:
            raise ConfigurationError(
                "Par√°metros inv√°lidos del PIController:\n" + "\n".join(f"  - {e}" for e in errors)
            )

    def compute(self, process_variable: float) -> int:
        """
        Calcula la nueva salida de control (u(t)) basada en el error actual.

        Ecuaci√≥n Posicional Discreta con Anti-Windup:
        u(k) = base_output + Kp * e(k) + Ki * sum(e) * dt
        
        donde sum(e) est√° limitado para prevenir saturaci√≥n del integrador.
        
        Args:
            process_variable: Valor actual del proceso (saturaci√≥n medida)
        
        Returns:
            Se√±al de control (batch size) clampeada al rango v√°lido
        
        Raises:
            ValueError: Si process_variable est√° fuera del rango v√°lido
        """
        # Validar entrada
        if not isinstance(process_variable, (int, float)):
            raise ValueError(
                f"process_variable debe ser num√©rico, recibido: {type(process_variable)}"
            )

        if math.isnan(process_variable) or math.isinf(process_variable):
            self.logger.warning(
                f"process_variable inv√°lido ({process_variable}), usando setpoint como fallback"
            )
            process_variable = self.setpoint

        # Normalizar a rango v√°lido [0, 1]
        process_variable = max(0.0, min(1.0, process_variable))

        # Calcular delta de tiempo
        current_time = time.time()

        if self._last_time is None:
            # Primera iteraci√≥n: no hay historia
            dt = SystemConstants.MIN_DELTA_TIME
        else:
            dt = current_time - self._last_time
            # Protecci√≥n contra tiempo inv√°lido
            if dt <= 0 or dt > 3600:  # Si dt > 1 hora, algo est√° mal
                self.logger.warning(f"Delta de tiempo an√≥malo: {dt}s, usando m√≠nimo")
                dt = SystemConstants.MIN_DELTA_TIME

        # 1. Calcular Error (e(t))
        # Invertimos el signo: saturaci√≥n alta -> error negativo -> reducir batch
        error = self.setpoint - process_variable

        # 2. T√©rmino Proporcional
        P = self.Kp * error

        # 3. T√©rmino Integral con Anti-Windup
        # Acumular error
        self._integral_error += error * dt

        # Aplicar l√≠mites de anti-windup (clamping del integrador)
        self._integral_error = max(
            -self._integral_limit,
            min(self._integral_limit, self._integral_error)
        )

        I = self.Ki * self._integral_error

        # 4. Se√±al de Control (u)
        control_signal = self._base_output + P + I

        # 5. Saturaci√≥n del Actuador (Clamping de salida)
        output = int(round(control_signal))
        output = max(self.min_output, min(self.max_output, output))

        # Actualizar estado
        self._last_time = current_time
        self._iteration_count += 1

        # Logging cada 10 iteraciones para evitar spam
        if self._iteration_count % 10 == 0:
            self.logger.debug(
                f"[PID] Iter={self._iteration_count} | PV={process_variable:.3f} | "
                f"Error={error:.3f} | P={P:.1f} | I={I:.1f} | Out={output}"
            )

        return output

    def reset(self) -> None:
        """Resetea el estado interno del controlador para reutilizaci√≥n."""
        self._integral_error = 0.0
        self._last_time = None
        self._iteration_count = 0
        self.logger.debug("[PID] Controlador reseteado")

    def get_state(self) -> Dict[str, Any]:
        """Retorna el estado actual del controlador para observabilidad."""
        return {
            "integral_error": self._integral_error,
            "iteration_count": self._iteration_count,
            "last_time": self._last_time,
            "integral_limit": self._integral_limit
        }


# ============================================================================
# MOTOR DE F√çSICA AVANZADO
# ============================================================================
class FluxPhysicsEngine:
    """
    Simula el comportamiento f√≠sico RLC bas√°ndose en la ENERG√çA.

    Unifica Capacitancia e Inductancia bajo funciones escalares de Energ√≠a (Julios).
    - Energ√≠a Potencial (Ec): Presi√≥n acumulada por el volumen de datos.
    - Energ√≠a Cin√©tica (El): Inercia de la calidad del flujo.
    - Energ√≠a Disipada (Er): Calor generado por la fricci√≥n de datos sucios.
    
    Mejoras implementadas:
    - Validaci√≥n de par√°metros f√≠sicos
    - Protecci√≥n contra overflow matem√°tico
    - Normalizaci√≥n de m√©tricas
    - Diagn√≥stico basado en constantes nombradas
    - Manejo robusto de casos l√≠mite
    """

    def __init__(self, capacitance: float, resistance: float, inductance: float):
        """
        Inicializa el motor de f√≠sica con validaci√≥n de par√°metros.
        
        Args:
            capacitance: Capacitancia del sistema (Faradios, debe ser > 0)
            resistance: Resistencia base (Ohmios, debe ser > 0)
            inductance: Inductancia del sistema (Henrios, debe ser > 0)
        
        Raises:
            ConfigurationError: Si alg√∫n par√°metro es inv√°lido
        """
        self._validate_parameters(capacitance, resistance, inductance)

        self.C = capacitance
        self.R = resistance
        self.L = inductance

        self.logger = logging.getLogger(f"{self.__class__.__name__}")

        self.logger.info(
            f"Motor RLC inicializado: C={self.C}F, R={self.R}Œ©, L={self.L}H"
        )

    def _validate_parameters(
        self,
        capacitance: float,
        resistance: float,
        inductance: float
    ) -> None:
        """Valida que los par√°metros f√≠sicos sean v√°lidos."""
        errors = []

        if capacitance <= 0:
            errors.append(f"capacitance debe ser > 0, recibido: {capacitance}")

        if resistance <= 0:
            errors.append(f"resistance debe ser > 0, recibido: {resistance}")

        if inductance <= 0:
            errors.append(f"inductance debe ser > 0, recibido: {inductance}")

        for param in [capacitance, resistance, inductance]:
            if math.isnan(param) or math.isinf(param):
                errors.append(f"Par√°metro inv√°lido (NaN o Inf): {param}")

        if errors:
            raise ConfigurationError(
                "Par√°metros f√≠sicos inv√°lidos:\n" + "\n".join(f"  - {e}" for e in errors)
            )

    def calculate_metrics(self, total_records: int, cache_hits: int) -> Dict[str, float]:
        """
        Calcula m√©tricas vectoriales y escalares (energ√≠a) del flujo.
        
        Args:
            total_records: N√∫mero total de registros en el batch
            cache_hits: N√∫mero de registros con hit en cach√©
        
        Returns:
            Diccionario con m√©tricas normalizadas del sistema
        """
        # Validar entradas
        if total_records < 0 or cache_hits < 0:
            self.logger.error(
                f"Par√°metros negativos detectados: total_records={total_records}, "
                f"cache_hits={cache_hits}"
            )
            return self._get_zero_metrics()

        if cache_hits > total_records:
            self.logger.warning(
                f"cache_hits ({cache_hits}) > total_records ({total_records}), "
                f"normalizando a total_records"
            )
            cache_hits = total_records

        # Caso especial: sin datos
        if total_records == 0:
            return self._get_zero_metrics()

        try:
            # --- VARIABLES DE ESTADO ---
            # Corriente (I): Calidad del flujo (0.0 a 1.0)
            current_I = cache_hits / total_records
            current_I = max(0.0, min(1.0, current_I))  # Normalizar

            # Complejidad: Inversa a la corriente (fracci√≥n de datos sin cach√©)
            complexity = 1.0 - current_I

            # Resistencia Din√°mica (R_dyn)
            dynamic_R = self.R * (1.0 + complexity * SystemConstants.COMPLEXITY_RESISTANCE_FACTOR)

            # Saturaci√≥n (V): Ecuaci√≥n de carga del condensador
            # V(t) = V_max * (1 - e^(-t/œÑ)) donde œÑ = R*C
            tau_c = dynamic_R * self.C

            # Prevenir overflow en exponencial
            exponent = -float(total_records) / tau_c if tau_c > 0 else -SystemConstants.MAX_EXPONENTIAL_ARG
            exponent = max(-SystemConstants.MAX_EXPONENTIAL_ARG, exponent)

            saturation_V = 1.0 - math.exp(exponent)
            saturation_V = max(0.0, min(1.0, saturation_V))  # Normalizar

            # --- C√ÅLCULOS DE ENERG√çA (ESCALARES) ---

            # 1. Energ√≠a Potencial (Ec = 1/2 * C * V^2)
            potential_energy = 0.5 * self.C * (saturation_V ** 2)

            # 2. Energ√≠a Cin√©tica/Magn√©tica (El = 1/2 * L * I^2)
            kinetic_energy = 0.5 * self.L * (current_I ** 2)

            # 3. Potencia Disipada (P = I_ruido^2 * R)
            noise_current = 1.0 - current_I
            dissipated_power = (noise_current ** 2) * dynamic_R

            # --- C√ÅLCULO DE FLYBACK (Tensi√≥n Inductiva) ---
            # V_L = L * (di/dt) -> Cambio en la calidad
            delta_i = 1.0 - current_I
            dt = math.log1p(total_records)  # log(1 + x) es m√°s estable que log(x)

            flyback_voltage = (self.L * delta_i / dt) if dt > 0 else 0.0
            flyback_voltage = max(0.0, min(10.0, flyback_voltage))  # Limitar a rango razonable

            # Validar que no haya NaN o Inf en resultados
            metrics = {
                "saturation": saturation_V,
                "complexity": complexity,
                "flyback_voltage": flyback_voltage,
                "potential_energy": potential_energy,
                "kinetic_energy": kinetic_energy,
                "dissipated_power": dissipated_power
            }

            # Sanitizar m√©tricas
            for key, value in metrics.items():
                if math.isnan(value) or math.isinf(value):
                    self.logger.warning(f"M√©trica {key} inv√°lida: {value}, reemplazando con 0.0")
                    metrics[key] = 0.0

            return metrics

        except Exception as e:
            self.logger.error(f"Error calculando m√©tricas f√≠sicas: {e}", exc_info=True)
            return self._get_zero_metrics()

    def _get_zero_metrics(self) -> Dict[str, float]:
        """Retorna un diccionario de m√©tricas con valores en cero."""
        return {
            "saturation": 0.0,
            "complexity": 0.0,
            "flyback_voltage": 0.0,
            "potential_energy": 0.0,
            "kinetic_energy": 0.0,
            "dissipated_power": 0.0
        }

    def get_system_diagnosis(self, metrics: Dict[str, float]) -> str:
        """
        Genera diagn√≥stico del sistema basado en balance energ√©tico.
        
        Args:
            metrics: Diccionario de m√©tricas del sistema
        
        Returns:
            Cadena con el diagn√≥stico del estado del sistema
        """
        try:
            ec = metrics.get("potential_energy", 0.0)
            el = metrics.get("kinetic_energy", 0.0)
            flyback = metrics.get("flyback_voltage", 0.0)

            # Prevenir divisi√≥n por cero
            if el < SystemConstants.MIN_ENERGY_THRESHOLD:
                return "üî¥ SISTEMA ESTANCADO (Inercia cr√≠tica baja)"

            energy_ratio = ec / el

            # Diagn√≥stico jer√°rquico
            if energy_ratio > SystemConstants.HIGH_PRESSURE_RATIO:
                return "üü† SOBRECARGA DE PRESI√ìN (Riesgo de ruptura)"
            elif flyback > SystemConstants.HIGH_FLYBACK_THRESHOLD:
                return "‚ö° PICO INDUCTIVO DETECTADO (Inestabilidad)"
            elif el < SystemConstants.LOW_INERTIA_THRESHOLD:
                return "üü° BAJA INERCIA (Flujo d√©bil)"
            else:
                return "üü¢ EQUILIBRIO ENERG√âTICO (Estable)"

        except Exception as e:
            self.logger.error(f"Error en diagn√≥stico del sistema: {e}")
            return "‚ùì DIAGN√ìSTICO INDETERMINADO"


# ============================================================================
# DATA FLUX CONDENSER
# ============================================================================
class DataFluxCondenser:
    """
    Orquesta el pipeline de validaci√≥n y procesamiento de archivos de APU.

    Implementa una arquitectura de "Caja de Cristal" con control adaptativo PID.
    El sistema monitorea la "f√≠sica" del procesamiento en tiempo real y ajusta
    la velocidad de ingesti√≥n (batch size) para mantener la estabilidad.
    
    Mejoras implementadas:
    - Validaci√≥n exhaustiva de configuraci√≥n
    - Protecci√≥n contra loops infinitos
    - Recuperaci√≥n parcial opcional ante fallos
    - Telemetr√≠a detallada con estad√≠sticas
    - Manejo robusto de casos l√≠mite
    - Logging estructurado
    """

    REQUIRED_CONFIG_KEYS: Set[str] = {'parser_settings', 'processor_settings'}
    REQUIRED_PROFILE_KEYS: Set[str] = {'columns_mapping', 'validation_rules'}

    def __init__(
        self,
        config: Dict[str, Any],
        profile: Dict[str, Any],
        condenser_config: Optional[CondenserConfig] = None
    ):
        """
        Inicializa el Condensador con Motor RLC y Controlador PID.
        
        Args:
            config: Configuraci√≥n del sistema (debe contener parser_settings, processor_settings)
            profile: Perfil de procesamiento (debe contener columns_mapping, validation_rules)
            condenser_config: Configuraci√≥n espec√≠fica del condensador (opcional)
        
        Raises:
            InvalidInputError: Si config o profile son inv√°lidos
            ConfigurationError: Si condenser_config es inv√°lido
        """
        self._validate_initialization_params(config, profile)

        self.config = config
        self.profile = profile

        # Inicializar configuraci√≥n (esto validar√° los par√°metros internamente)
        try:
            self.condenser_config = condenser_config or CondenserConfig()
        except ConfigurationError as e:
            raise ConfigurationError(f"Error en configuraci√≥n del condensador: {e}") from e

        self.logger = logging.getLogger(self.__class__.__name__)
        self.logger.setLevel(self.condenser_config.log_level)

        # Inicializar Motor de F√≠sica RLC
        try:
            self.physics = FluxPhysicsEngine(
                capacitance=self.condenser_config.system_capacitance,
                resistance=self.condenser_config.base_resistance,
                inductance=self.condenser_config.system_inductance
            )
        except ConfigurationError as e:
            raise ConfigurationError(f"Error inicializando motor f√≠sico: {e}") from e

        # Inicializar Controlador PI
        try:
            self.controller = PIController(
                kp=self.condenser_config.pid_kp,
                ki=self.condenser_config.pid_ki,
                setpoint=self.condenser_config.pid_setpoint,
                min_output=self.condenser_config.min_batch_size,
                max_output=self.condenser_config.max_batch_size,
                integral_limit_factor=self.condenser_config.integral_limit_factor
            )
        except ConfigurationError as e:
            raise ConfigurationError(f"Error inicializando controlador PID: {e}") from e

        # Estad√≠sticas de procesamiento
        self._stats = ProcessingStats()

        self.logger.info(
            f"DataFluxCondenser inicializado | "
            f"PID: Kp={self.condenser_config.pid_kp}, Ki={self.condenser_config.pid_ki} | "
            f"Batch: [{self.condenser_config.min_batch_size}, {self.condenser_config.max_batch_size}]"
        )

    def _validate_initialization_params(
        self,
        config: Dict[str, Any],
        profile: Dict[str, Any]
    ) -> None:
        """
        Valida que config y profile sean diccionarios con las claves requeridas.
        
        Raises:
            InvalidInputError: Si la validaci√≥n falla
        """
        if not isinstance(config, dict):
            raise InvalidInputError(
                f"config debe ser un diccionario, recibido: {type(config).__name__}"
            )

        if not isinstance(profile, dict):
            raise InvalidInputError(
                f"profile debe ser un diccionario, recibido: {type(profile).__name__}"
            )

        # Validar claves requeridas (modo warning, no error)
        missing_config_keys = self.REQUIRED_CONFIG_KEYS - set(config.keys())
        if missing_config_keys:
            self.logger.warning(
                f"Claves faltantes en config (modo tolerante): {missing_config_keys}"
            )

        missing_profile_keys = self.REQUIRED_PROFILE_KEYS - set(profile.keys())
        if missing_profile_keys:
            self.logger.warning(
                f"Claves faltantes en profile (modo tolerante): {missing_profile_keys}"
            )

    def stabilize(self, file_path: str) -> pd.DataFrame:
        """
        Proceso de Carga y Descarga CONTROLADO por PID.
        Procesa el archivo en flujo continuo (Streaming por Lotes Adaptativo).

        El sistema lee el archivo y lo divide en lotes cuyo tama√±o es ajustado
        din√°micamente por el controlador PID bas√°ndose en la 'saturaci√≥n' detectada
        en el lote anterior.
        
        Args:
            file_path: Ruta al archivo de APU a procesar
        
        Returns:
            DataFrame con los datos procesados
        
        Raises:
            InvalidInputError: Si el archivo es inv√°lido
            ProcessingError: Si ocurre un error durante el procesamiento
        """
        start_time = time.time()
        path_obj = Path(file_path)

        # Resetear estad√≠sticas y controlador
        self._stats = ProcessingStats()
        self.controller.reset()

        self.logger.info(
            f"‚ö° [CONTROL ADAPTATIVO] Iniciando lazo de control para: {path_obj.name}"
        )

        try:
            validated_path = self._validate_input_file(file_path)

            # Fase 1: Inicializar el Parser (Guardia)
            parser = self._initialize_parser(validated_path)

            # Fase 2: Extract - Leer contenido crudo
            full_raw_records, full_cache = self._extract_raw_data(parser)

            if not full_raw_records:
                self.logger.warning("El archivo no contiene registros crudos v√°lidos.")
                return pd.DataFrame()

            total_records = len(full_raw_records)
            self._stats.total_records = total_records

            # Fase 3: Validar umbral m√≠nimo
            if total_records < self.condenser_config.min_records_threshold:
                self.logger.warning(
                    f"[VALIDACI√ìN] Registros insuficientes: {total_records} < "
                    f"{self.condenser_config.min_records_threshold}"
                )
                return pd.DataFrame()

            # Fase 4: Procesamiento por lotes con control PID
            processed_batches = self._process_batches_with_pid(
                full_raw_records,
                full_cache,
                total_records
            )

            # Fase 5: Consolidar resultados
            df_final = self._consolidate_results(processed_batches)

            # Fase 6: Validar salida
            self._validate_output(df_final)

            # Registrar estad√≠sticas finales
            self._stats.processing_time = time.time() - start_time
            self._log_final_stats()

            self.logger.info(
                f"‚úÖ [ESTABILIZADO] Proceso completado en {self._stats.processing_time:.2f}s. "
                f"Registros procesados: {self._stats.processed_records}/{total_records}"
            )

            return df_final

        except (InvalidInputError, ProcessingError):
            # Re-lanzar errores conocidos
            raise
        except Exception as e:
            self.logger.exception(f"[ERROR CR√çTICO] Error inesperado: {e}")
            raise ProcessingError(f"Error inesperado durante estabilizaci√≥n: {e}") from e

    def _initialize_parser(self, validated_path: Path) -> ReportParserCrudo:
        """
        Inicializa el parser con manejo robusto de errores.
        
        Args:
            validated_path: Ruta validada al archivo
        
        Returns:
            Instancia de ReportParserCrudo configurada
        
        Raises:
            ProcessingError: Si falla la inicializaci√≥n
        """
        try:
            parser = ReportParserCrudo(
                str(validated_path),
                profile=self.profile,
                config=self.config
            )
            self.logger.debug(f"Parser inicializado para: {validated_path.name}")
            return parser
        except Exception as e:
            raise ProcessingError(
                f"Error inicializando ReportParserCrudo: {e}"
            ) from e

    def _extract_raw_data(
        self,
        parser: ReportParserCrudo
    ) -> tuple[List[Dict[str, Any]], Dict[str, Any]]:
        """
        Extrae datos crudos del parser con validaci√≥n.
        
        Args:
            parser: Instancia del parser configurado
        
        Returns:
            Tupla (registros_crudos, cache)
        
        Raises:
            ProcessingError: Si falla la extracci√≥n
        """
        try:
            full_raw_records = parser.parse_to_raw()

            if not isinstance(full_raw_records, list):
                raise ProcessingError(
                    f"parse_to_raw() debe retornar lista, recibido: {type(full_raw_records).__name__}"
                )

            full_cache = parser.get_parse_cache() or {}

            if not isinstance(full_cache, dict):
                self.logger.warning(
                    f"get_parse_cache() retorn√≥ tipo inesperado: {type(full_cache).__name__}, "
                    f"usando dict vac√≠o"
                )
                full_cache = {}

            self.logger.info(
                f"[EXTRACT] Extra√≠dos {len(full_raw_records)} registros | "
                f"Cache: {len(full_cache)} entradas"
            )

            return full_raw_records, full_cache

        except Exception as e:
            raise ProcessingError(f"Error extrayendo datos crudos: {e}") from e

    def _process_batches_with_pid(
        self,
        full_raw_records: List[Dict[str, Any]],
        full_cache: Dict[str, Any],
        total_records: int
    ) -> List[pd.DataFrame]:
        """
        Procesa registros en lotes con control PID adaptativo.
        
        Args:
            full_raw_records: Lista completa de registros crudos
            full_cache: Cache de parseo completo
            total_records: N√∫mero total de registros
        
        Returns:
            Lista de DataFrames procesados (uno por batch)
        
        Raises:
            ProcessingError: Si se excede el l√≠mite de batches fallidos
        """
        processed_batches = []
        current_index = 0
        current_batch_size = self.condenser_config.min_batch_size
        failed_batches_count = 0

        # Protecci√≥n contra loops infinitos
        max_iterations = total_records * SystemConstants.MAX_ITERATIONS_MULTIPLIER
        iteration_count = 0

        self.logger.info(
            f"[PID LOOP] Iniciando procesamiento | Total: {total_records} registros | "
            f"Batch inicial: {current_batch_size}"
        )

        while current_index < total_records:
            iteration_count += 1

            # Protecci√≥n contra loop infinito
            if iteration_count > max_iterations:
                raise ProcessingError(
                    f"Excedido l√≠mite de iteraciones ({max_iterations}). "
                    f"Posible loop infinito detectado. "
                    f"√çndice actual: {current_index}/{total_records}"
                )

            # Validar que batch_size sea v√°lido
            if current_batch_size <= 0:
                self.logger.error(
                    f"Batch size inv√°lido detectado: {current_batch_size}. "
                    f"Reseteando a m√≠nimo: {self.condenser_config.min_batch_size}"
                )
                current_batch_size = self.condenser_config.min_batch_size

            # 1. Extraer lote actual
            end_index = min(current_index + current_batch_size, total_records)
            batch_records = full_raw_records[current_index:end_index]

            if not batch_records:
                self.logger.warning(
                    f"Batch vac√≠o detectado en √≠ndice {current_index}, avanzando..."
                )
                current_index = end_index
                continue

            # 2. Calcular cache hits para el lote
            batch_cache_hits = self._calculate_cache_hits(batch_records, full_cache)

            # 3. Medir estado del sistema (Sensores)
            metrics = self.physics.calculate_metrics(len(batch_records), batch_cache_hits)

            # 4. Acci√≥n de control PID
            new_batch_size = self.controller.compute(metrics["saturation"])

            # 5. Protecci√≥n basada en Energ√≠a Disipada (Diodo de Rueda Libre)
            if metrics["dissipated_power"] > SystemConstants.OVERHEAT_POWER_THRESHOLD:
                self.logger.warning(
                    f"üî• [SOBRECALENTAMIENTO] Disipaci√≥n alta "
                    f"({metrics['dissipated_power']:.1f}W). Aplicando freno de emergencia."
                )
                new_batch_size = int(new_batch_size * SystemConstants.EMERGENCY_BRAKE_FACTOR)
                new_batch_size = max(self.condenser_config.min_batch_size, new_batch_size)
                self._stats.emergency_brakes_triggered += 1

            # 6. Telemetr√≠a
            diagnosis = self.physics.get_system_diagnosis(metrics)
            self.logger.info(
                f"üîÑ [PID] Batch #{self._stats.total_batches + 1} | "
                f"Size: {len(batch_records)} | "
                f"Sat: {metrics['saturation']:.2%} | "
                f"Ec: {metrics['potential_energy']:.0f}J | "
                f"El: {metrics['kinetic_energy']:.2f}J | "
                f"Disip: {metrics['dissipated_power']:.1f}W | "
                f"‚Üí Next: {new_batch_size} | {diagnosis}"
            )

            # Advertencia de flyback
            if metrics["flyback_voltage"] > SystemConstants.HIGH_FLYBACK_THRESHOLD:
                self.logger.warning(
                    f"üõ°Ô∏è [DIODO FLYBACK] Pico de inestabilidad detectado "
                    f"(V_L={metrics['flyback_voltage']:.2f}V) en batch {current_index}-{end_index}"
                )

            # 7. Procesar el lote
            batch_data = ParsedData(batch_records, full_cache)
            batch_success = False

            try:
                df_batch = self._rectify_signal(batch_data)
                processed_batches.append(df_batch)
                batch_success = True

            except ProcessingError as e:
                failed_batches_count += 1
                self.logger.error(
                    f"[ERROR] Batch {current_index}-{end_index} fall√≥: {e} | "
                    f"Fallos acumulados: {failed_batches_count}/{self.condenser_config.max_failed_batches}"
                )

                # Decidir si abortar o continuar
                if self.condenser_config.enable_partial_recovery:
                    if failed_batches_count > self.condenser_config.max_failed_batches:
                        raise ProcessingError(
                            f"Excedido l√≠mite de batches fallidos "
                            f"({self.condenser_config.max_failed_batches}). Abortando."
                        ) from e
                    else:
                        self.logger.warning(
                            "[RECOVERY] Continuando con siguiente batch (modo recuperaci√≥n parcial)"
                        )
                else:
                    # Modo estricto: un fallo aborta todo
                    raise

            # 8. Actualizar estad√≠sticas
            self._stats.add_batch_stats(
                batch_size=len(batch_records),
                saturation=metrics["saturation"],
                power=metrics["dissipated_power"],
                flyback=metrics["flyback_voltage"],
                kinetic=metrics["kinetic_energy"],
                success=batch_success
            )

            # 9. Avanzar al siguiente batch
            current_index = end_index
            current_batch_size = new_batch_size

        self.logger.info(
            f"[PID LOOP] Completado | Batches procesados: {self._stats.total_batches} | "
            f"Batches fallidos: {self._stats.failed_batches}"
        )

        return processed_batches

    def _calculate_cache_hits(
        self,
        batch_records: List[Dict[str, Any]],
        full_cache: Dict[str, Any]
    ) -> int:
        """
        Calcula el n√∫mero de cache hits para un batch de registros.
        
        Args:
            batch_records: Lista de registros del batch
            full_cache: Diccionario de cache completo
        
        Returns:
            N√∫mero de registros con hit en cach√©
        """
        if not full_cache:
            return 0

        cache_hits = 0
        for record in batch_records:
            # Intentar varias claves posibles para linkear con cache
            for key in ['insumo_line', 'line', 'raw_line', '_line']:
                line_content = record.get(key)
                if line_content and line_content in full_cache:
                    cache_hits += 1
                    break

        return cache_hits

    def _consolidate_results(self, processed_batches: List[pd.DataFrame]) -> pd.DataFrame:
        """
        Consolida m√∫ltiples DataFrames de batches en uno solo.
        
        Args:
            processed_batches: Lista de DataFrames procesados
        
        Returns:
            DataFrame consolidado
        """
        if not processed_batches:
            self.logger.warning("[CONSOLIDATE] No hay batches para consolidar")
            return pd.DataFrame()

        try:
            # Filtrar batches vac√≠os
            non_empty_batches = [df for df in processed_batches if not df.empty]

            if not non_empty_batches:
                self.logger.warning("[CONSOLIDATE] Todos los batches est√°n vac√≠os")
                return pd.DataFrame()

            df_final = pd.concat(non_empty_batches, ignore_index=True)

            self.logger.info(
                f"[CONSOLIDATE] Consolidados {len(non_empty_batches)} batches ‚Üí "
                f"{len(df_final)} registros finales"
            )

            return df_final

        except Exception as e:
            raise ProcessingError(f"Error consolidando resultados: {e}") from e

    def _validate_input_file(self, file_path: str) -> Path:
        """
        Valida que el archivo de entrada exista y sea accesible.
        
        Args:
            file_path: Ruta al archivo
        
        Returns:
            Objeto Path validado
        
        Raises:
            InvalidInputError: Si el archivo es inv√°lido
        """
        if not file_path or not isinstance(file_path, str):
            raise InvalidInputError(
                f"file_path debe ser una cadena no vac√≠a, recibido: {type(file_path).__name__}"
            )

        path = Path(file_path)

        if not path.exists():
            raise InvalidInputError(f"El archivo no existe: {file_path}")

        if not path.is_file():
            raise InvalidInputError(f"La ruta no es un archivo v√°lido: {file_path}")

        if path.suffix.lower() not in SystemConstants.VALID_FILE_EXTENSIONS:
            self.logger.warning(
                f"Extensi√≥n inusual detectada: {path.suffix}. "
                f"Se esperaba una de: {SystemConstants.VALID_FILE_EXTENSIONS}"
            )

        self.logger.debug(f"[VALIDACI√ìN] Archivo validado: {path}")
        return path

    def _rectify_signal(self, parsed_data: ParsedData) -> pd.DataFrame:
        """
        Usa APUProcessor para convertir la se√±al filtrada en datos utilizables.
        
        Args:
            parsed_data: Datos parseados con cache
        
        Returns:
            DataFrame procesado
        
        Raises:
            ProcessingError: Si falla el procesamiento
        """
        try:
            # 1. Instanciar APUProcessor
            processor = APUProcessor(
                config=self.config,
                profile=self.profile,
                parse_cache=parsed_data.parse_cache
            )

            # 2. Pasar raw_records directamente
            processor.raw_records = parsed_data.raw_records

            # 3. Procesar
            df_result = processor.process_all()

            if not isinstance(df_result, pd.DataFrame):
                raise ProcessingError(
                    f"APUProcessor.process_all() debe retornar DataFrame, "
                    f"recibido: {type(df_result).__name__}"
                )

            return df_result

        except Exception as e:
            raise ProcessingError(
                f"Error durante la rectificaci√≥n con APUProcessor: {e}"
            ) from e

    def _validate_output(self, df: pd.DataFrame) -> None:
        """
        Valida el DataFrame de salida antes de retornarlo.
        
        Args:
            df: DataFrame a validar
        
        Raises:
            ProcessingError: Si la validaci√≥n falla cr√≠ticamente
        """
        if not isinstance(df, pd.DataFrame):
            raise ProcessingError(
                f"La salida debe ser DataFrame, recibido: {type(df).__name__}"
            )

        if self.condenser_config.enable_strict_validation:
            if df.empty:
                self.logger.warning(
                    "[VALIDACI√ìN] DataFrame vac√≠o generado "
                    "(puede ser v√°lido dependiendo del archivo)"
                )

            # Detectar columnas completamente nulas
            null_columns = df.columns[df.isnull().all()].tolist()
            if null_columns:
                self.logger.warning(
                    f"[VALIDACI√ìN] Columnas completamente nulas detectadas: {null_columns}"
                )

            # Detectar columnas con alta proporci√≥n de nulos
            if not df.empty:
                high_null_cols = []
                for col in df.columns:
                    null_ratio = df[col].isnull().sum() / len(df)
                    if null_ratio > 0.9:  # >90% nulos
                        high_null_cols.append((col, f"{null_ratio:.1%}"))

                if high_null_cols:
                    self.logger.warning(
                        f"[VALIDACI√ìN] Columnas con alta proporci√≥n de nulos: {high_null_cols}"
                    )

    def _log_final_stats(self) -> None:
        """Registra estad√≠sticas finales del procesamiento."""
        self.logger.info(
            f"\n{'='*70}\n"
            f"üìä ESTAD√çSTICAS FINALES\n"
            f"{'='*70}\n"
            f"  Registros totales:       {self._stats.total_records}\n"
            f"  Registros procesados:    {self._stats.processed_records}\n"
            f"  Registros fallidos:      {self._stats.failed_records}\n"
            f"  Batches totales:         {self._stats.total_batches}\n"
            f"  Batches fallidos:        {self._stats.failed_batches}\n"
            f"  Tiempo de proceso:       {self._stats.processing_time:.2f}s\n"
            f"  Tama√±o promedio batch:   {self._stats.avg_batch_size:.0f}\n"
            f"  Saturaci√≥n promedio:     {self._stats.avg_saturation:.2%}\n"
            f"  Potencia m√°x. disipada:  {self._stats.max_dissipated_power:.1f}W\n"
            f"  Frenos de emergencia:    {self._stats.emergency_brakes_triggered}\n"
            f"{'='*70}"
        )

    def get_processing_stats(self) -> Dict[str, Any]:
        """
        Retorna estad√≠sticas del √∫ltimo procesamiento.
        
        Returns:
            Diccionario con estad√≠sticas completas
        """
        return {
            "condenser_config": {
                "min_records_threshold": self.condenser_config.min_records_threshold,
                "strict_validation": self.condenser_config.enable_strict_validation,
                "log_level": self.condenser_config.log_level,
                "pid_mode": True,
                "partial_recovery": self.condenser_config.enable_partial_recovery
            },
            "config_keys": list(self.config.keys()),
            "profile_keys": list(self.profile.keys()),
            "statistics": {
                "total_records": self._stats.total_records,
                "processed_records": self._stats.processed_records,
                "failed_records": self._stats.failed_records,
                "total_batches": self._stats.total_batches,
                "failed_batches": self._stats.failed_batches,
                "processing_time": self._stats.processing_time,
                "avg_batch_size": self._stats.avg_batch_size,
                "avg_saturation": self._stats.avg_saturation,
                "max_dissipated_power": self._stats.max_dissipated_power,
                "max_flyback_voltage": self._stats.max_flyback_voltage,
                "avg_kinetic_energy": self._stats.avg_kinetic_energy,
                "emergency_brakes_triggered": self._stats.emergency_brakes_triggered
            },
            "controller_state": self.controller.get_state(),
            "success_rate": (
                self._stats.processed_records / self._stats.total_records
                if self._stats.total_records > 0 else 0.0
            )
        }

    def reset(self) -> None:
        """Resetea el estado interno del condensador para reutilizaci√≥n."""
        self.controller.reset()
        self._stats = ProcessingStats()
        self.logger.info("[RESET] Condensador reseteado y listo para nuevo procesamiento")
