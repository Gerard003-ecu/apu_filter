## IngenierÃ­a Bajo el CapÃ³: La GarantÃ­a de Estabilidad

Aunque nuestra prioridad es su negocio, la solidez tÃ©cnica es nuestra garantÃ­a. APU Filter estÃ¡ construido sobre una arquitectura modular que separa claramente las responsabilidades, garantizando robustez y escalabilidad. Sus tres pilares fundamentales son:

### 1. Condensador de Flujo de Datos (Data Flux Condenser)
- **Componente Clave:** `app/flux_condenser.py`
- **FunciÃ³n:** ActÃºa como un estabilizador dinÃ¡mico de seÃ±al a la entrada del sistema. Implementa una arquitectura de **"Caja de Cristal"**, transformando la ingesta de datos en un proceso observable, medible y auto-regulado mediante principios de fÃ­sica y teorÃ­a de control.

#### âš™ï¸ Nivel 1: Motor de FÃ­sica RLC (El Sensor)
El sistema evoluciona mÃ¡s allÃ¡ de mÃ©tricas simples hacia un **Modelo EnergÃ©tico Escalar**. En lugar de monitorear solo voltaje o corriente, unificamos las unidades bajo un lenguaje comÃºn: La EnergÃ­a (Julios).

1.  **EnergÃ­a Potencial ($E_c = \frac{1}{2}CV^2$) - PresiÃ³n de Datos:**
    *   Representa la "carga de trabajo" acumulada por el volumen de registros.
    *   Calcula la presiÃ³n que ejerce el lote de datos sobre el sistema.
2.  **EnergÃ­a CinÃ©tica ($E_l = \frac{1}{2}LI^2$) - Inercia de Calidad:**
    *   Representa el momento o "inercia" de la calidad del flujo.
    *   Un flujo de alta calidad ($I \approx 1.0$) tiene una inercia fuerte que resiste perturbaciones, dificultando que errores menores desestabilicen el proceso.
3.  **Potencia Disipada ($P = I_{ruido}^2 R$) - Calor/FricciÃ³n:**
    *   **TermodinÃ¡mica del Software:** Calcula el "calor" generado por la resistencia dinÃ¡mica de los datos sucios.
    *   Si el sistema gasta demasiada energÃ­a procesando basura (ruido), se genera sobrecalentamiento lÃ³gico.

#### ğŸ§  Nivel 2: Controlador PI Discreto (El Cerebro)
Sobre la capa fÃ­sica, opera un **Lazo de Control Cerrado (Feedback Loop)** que ajusta el comportamiento del sistema en tiempo real, ahora con protecciÃ³n tÃ©rmica:

*   **Algoritmo:** Controlador **Proporcional-Integral (PI)** discreto.
*   **Setpoint:** Mantiene una saturaciÃ³n estable (Flujo Laminar).
*   **Variable de Control:** El tamaÃ±o del lote de procesamiento (*Batch Size*).
*   **Disyuntor TÃ©rmico (Nuevo):**
    *   AdemÃ¡s del PID, el sistema implementa un "Diodo de Rueda Libre" tÃ©rmico.
    *   Si la **Potencia Disipada** supera un umbral crÃ­tico (> 50W), el sistema activa un freno de emergencia, reduciendo drÃ¡sticamente el tamaÃ±o del lote independientemente de la saturaciÃ³n, para "enfriar" el proceso y evitar colapsos por calidad de datos.

**Resultado:** Un sistema bi-mimÃ©tico que no solo adapta su velocidad, sino que tambiÃ©n gestiona su "temperatura" operativa para garantizar una estabilidad del 100% bajo cualquier condiciÃ³n.

#### ğŸ›¡ï¸ Resiliencia y RecuperaciÃ³n
El sistema implementa mecanismos de defensa avanzados:
*   **Anti-Windup PID:** Previene la saturaciÃ³n del controlador ante cargas sostenidas.
*   **RecuperaciÃ³n Parcial:** Capacidad de aislar lotes corruptos y continuar el procesamiento del resto del archivo.
*   **ProtecciÃ³n TÃ©rmica:** Freno de emergencia automÃ¡tico si la disipaciÃ³n de energÃ­a (fricciÃ³n de datos) supera los umbrales de seguridad.

### 2. Pipeline de Procesamiento de Datos
- **Componente Clave:** `app/procesador_csv.py`
- **FunciÃ³n:** Es el orquestador central que toma los datos crudos del parser y los transforma en un modelo de costos consolidado.
- **Mecanismo:** Utiliza un patrÃ³n `Pipeline` con pasos secuenciales y bien definidos:
    1.  **Carga de Datos:** Ingiere los tres archivos principales (Presupuesto, APUs, Insumos).
    2.  **FusiÃ³n de Datos:** Enriquece los insumos de los APUs con los precios del catÃ¡logo maestro de insumos.
    3.  **CÃ¡lculo de Costos:** Agrega los costos de los insumos para calcular el valor total de cada APU, desglosado por categorÃ­a (Materiales, Mano de Obra, Equipo).
    4.  **Merge Final:** Une los costos calculados de los APUs con las cantidades del archivo de presupuesto para generar el informe final.

### 3. Estimador Inteligente
- **Componente Clave:** `app/estimator.py`
- **FunciÃ³n:** Proporciona una capacidad de bÃºsqueda avanzada para generar cotizaciones rÃ¡pidas para nuevos proyectos, basÃ¡ndose en el conocimiento extraÃ­do de APUs histÃ³ricos.
- **Mecanismo Dual:**
    - **BÃºsqueda por Palabras Clave:** Un mÃ©todo tradicional y rÃ¡pido que busca coincidencias directas de texto.
    - **BÃºsqueda SemÃ¡ntica (Vectorial):** Su capacidad mÃ¡s potente. Utiliza modelos de `sentence-transformers` para convertir las descripciones de los APUs en vectores numÃ©ricos (embeddings). Luego, usa **FAISS** para encontrar los APUs mÃ¡s *conceptualmente similares* a una nueva descripciÃ³n, incluso si no comparten las mismas palabras.

## TecnologÃ­as Utilizadas

La plataforma estÃ¡ construida sobre una pila de tecnologÃ­as modernas de alto rendimiento:

- **Backend:** **Flask** para la API web.
- **AnÃ¡lisis de Datos y ML:**
    - **Pandas:** Utilizado como la base para la manipulaciÃ³n de datos.
    - **Sentence-Transformers:** Para la generaciÃ³n de embeddings de texto que potencian la bÃºsqueda semÃ¡ntica.
    - **FAISS (Facebook AI Similarity Search):** Para la bÃºsqueda vectorial de alta velocidad de los APUs mÃ¡s similares.
- **Parsing y Estructura de Datos:**
    - **Lark:** Para el parsing robusto de la gramÃ¡tica de los insumos en los archivos de APU.
    - **Dataclasses:** Para la creaciÃ³n de esquemas de datos (`schemas.py`) que garantizan la consistencia y validaciÃ³n.
- **Entorno y Dependencias:**
    - **Conda:** Para gestionar el entorno y las dependencias complejas con componentes binarios (ej. `faiss-cpu`).
- **Redis:** Para la gestiÃ³n de sesiones de usuario, garantizando la persistencia de datos entre solicitudes.
    - **uv & pip:** Para la gestiÃ³n rÃ¡pida y eficiente del resto de las dependencias de Python.
- **Calidad de CÃ³digo y Pruebas:**
    - **Pytest:** Para una suite de pruebas exhaustiva que cubre desde unidades hasta la integraciÃ³n completa.
    - **Ruff:** Para el formateo y linting del cÃ³digo, asegurando un estilo consistente y de alta calidad.

## InstalaciÃ³n y Uso

Esta secciÃ³n describe cÃ³mo configurar el entorno tÃ©cnico para su equipo de TI, garantizando una implementaciÃ³n robusta y segura.

### La Arquitectura de la InstalaciÃ³n: Una AnalogÃ­a de Engranajes

Para entender por quÃ© seguimos un orden de instalaciÃ³n especÃ­fico, podemos visualizar nuestro entorno como una caja de cambios de precisiÃ³n compuesta por tres engranajes diferentes, cada uno con una funciÃ³n especializada.

1.  **Conda: El Engranaje Principal y de Potencia (El Engranaje Grande)**
    *   **Rol:** Mueve las piezas mÃ¡s pesadas y complejas que no son de Python puro y dependen del sistema operativo (ej. librerÃ­as C++).
    *   **CaracterÃ­stica:** Es potente y fiable, diseÃ±ado para buscar e instalar paquetes pre-compilados que encajan perfectamente con la arquitectura de la mÃ¡quina.
    *   **En APU Filter:** Su Ãºnica tarea es instalar `faiss-cpu`, una librerÃ­a con dependencias complejas a nivel de sistema.

2.  **Pip (con `--index-url`): La Herramienta Especializada**
    *   **Rol:** Se utiliza para una pieza crÃ­tica que necesita una instalaciÃ³n muy especÃ­fica desde un repositorio exclusivo.
    *   **CaracterÃ­stica:** Comunica una intenciÃ³n precisa: "Ve Ãºnicamente a este almacÃ©n especÃ­fico (el de PyTorch para CPU) y trae la pieza exacta que encuentres allÃ­".
    *   **En APU Filter:** Su Ãºnica tarea es instalar la versiÃ³n `torch` optimizada exclusivamente para CPU, evitando la descarga de las pesadas librerÃ­as de CUDA.

3.  **uv/pip: El Engranaje de Alta Velocidad y PrecisiÃ³n (El Engranaje PequeÃ±o)**
    *   **Rol:** Ensambla todos los componentes de la aplicaciÃ³n que son de Python puro, comunicÃ¡ndose directamente con el ecosistema de Python (PyPI).
    *   **CaracterÃ­stica:** Es ultrarrÃ¡pido y Ã¡gil, ideal para manejar dependencias estÃ¡ndar de Python, pero no tiene la fuerza para gestionar las piezas pesadas que maneja Conda.
    *   **En APU Filter:** Su tarea es instalar todo lo demÃ¡s desde `requirements.txt` de forma eficiente.

### Pasos Detallados de InstalaciÃ³n

**Requisito Previo:** AsegÃºrese de tener instalado Miniconda o Anaconda. Puede descargarlo desde [aquÃ­](https://www.anaconda.com/products/distribution).

**Paso 1: Crear el Entorno Base (Conda)**
Cree un nuevo entorno Conda llamado `apu_filter_env` con Python 3.10, la versiÃ³n sobre la cual se construirÃ¡n los demÃ¡s componentes.
```bash
conda create --name apu_filter_env python=3.10
```

**Paso 2: Activar el Entorno**
Active el entorno reciÃ©n creado. **Debe hacer esto cada vez que trabaje en el proyecto.**
```bash
conda activate apu_filter_env
```

**Paso 3: Instalar Componentes Pesados (Conda y Pip Especializado)**
Instale los "engranajes" principales que requieren compilaciones y dependencias complejas.

*   **Instalar `faiss-cpu` (El Engranaje de Potencia):**
    ```bash
    conda install -c pytorch faiss-cpu
    ```

*   **Instalar `torch` (La Herramienta Especializada):**
    ```bash
    pip install torch --index-url https://download.pytorch.org/whl/cpu
    ```

**Paso 4: Instalar Dependencias de la AplicaciÃ³n (uv)**
Instale todas las demÃ¡s dependencias de Python puro con el "engranaje de alta velocidad".
```bash
uv pip install -r requirements.txt
uv pip install -r requirements-dev.txt
```

**Paso 5: Instalar y Configurar el Servidor de Sesiones (Redis)**
Para garantizar la persistencia de los datos del usuario entre solicitudes, la aplicaciÃ³n utiliza Redis.

*   **Instalar `redis` (El Engranaje de Estabilidad):**
    Es crucial instalar Redis a travÃ©s del canal `conda-forge` para asegurar la compatibilidad entre diferentes sistemas operativos, incluyendo macOS y Linux.
    ```bash
    conda install -c conda-forge redis
    ```

**Nota Importante:** El archivo `requirements.txt` no debe contener `faiss-cpu` ni `torch`. Si alguna vez necesita regenerar este archivo (ej. usando `uv pip compile requirements.in`), asegÃºrese de excluir estas dos librerÃ­as para evitar conflictos de instalaciÃ³n.

## Flujo de Trabajo del Proyecto

El ciclo de vida del desarrollo y uso de la aplicaciÃ³n sigue estos pasos:

1.  **ConfiguraciÃ³n:** La lÃ³gica de negocio (mapeo de columnas, umbrales, reglas del estimador) se gestiona en `app/config.json`.
2.  **Pre-procesamiento:** Si los datos de los APUs cambian, debe regenerar los embeddings ejecutando:
    ```bash
    python scripts/generate_embeddings.py --input path/to/processed_apus.json
    ```
3.  **EjecuciÃ³n de la AplicaciÃ³n:** Con el entorno activado, inicie el servidor Flask:
    ```bash
    python -m flask run --port=5002
    ```
4.  **ValidaciÃ³n y Pruebas:** Para verificar la integridad del cÃ³digo, ejecute la suite de pruebas completa:
    ```bash
    pytest -vv
    ```

## Estructura del Directorio

El proyecto estÃ¡ organizado con una clara separaciÃ³n de responsabilidades para facilitar la mantenibilidad y la escalabilidad.

```
apu_filter/
â”‚
â”œâ”€â”€ app/                        # LÃ³gica principal de la aplicaciÃ³n Flask
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ app.py                  # Factory de la app, endpoints API y carga de modelos
â”‚   â”œâ”€â”€ procesador_csv.py       # Orquestador del pipeline de procesamiento de datos
â”‚   â”œâ”€â”€ report_parser_crudo.py  # Parser especializado para archivos de APU semi-estructurados
â”‚   â”œâ”€â”€ apu_processor.py        # Motor de transformaciÃ³n que aplica lÃ³gica de negocio a los datos parseados
â”‚   â”œâ”€â”€ estimator.py            # LÃ³gica de estimaciÃ³n con bÃºsqueda semÃ¡ntica y por keywords
â”‚   â”œâ”€â”€ flux_condenser.py       # LÃ³gica del condensador de flujos de datos
â”‚   â”œâ”€â”€ data_loader.py          # Capa de abstracciÃ³n para leer datos (.csv, .xlsx, .pdf)
â”‚   â”œâ”€â”€ schemas.py              # DefiniciÃ³n de los esquemas de datos (dataclasses)
â”‚   â”œâ”€â”€ utils.py                # Funciones de utilidad generales (normalizaciÃ³n, parsing, etc.)
â”‚   â”œâ”€â”€ config.json             # Archivo de configuraciÃ³n de la lÃ³gica de negocio
â”‚   â””â”€â”€ embeddings/             # Directorio para los artefactos de ML (Ã­ndice FAISS, mapeo)
â”‚
â”œâ”€â”€ data/                       # Datos de entrada y resultados intermedmedios
â”‚   â”œâ”€â”€ presupuesto_clean.csv   # VersiÃ³n sanitizada del presupuesto, lista para el pipeline
â”‚   â”œâ”€â”€ insumos_clean.csv       # VersiÃ³n sanitizada de insumos, lista para el pipeline
â”‚   â””â”€â”€ apus_clean.csv          # VersiÃ³n sanitizada de apus, lista para el pipeline  
â”‚
â”œâ”€â”€ data_dirty/                 # Datos crudos y sin procesar
â”‚   â”œâ”€â”€ presupuesto.csv         # Archivo de presupuesto original con posibles errores
â”‚   â”œâ”€â”€ insumos.csv             # Archivo de insumos original con posibles errores
â”‚   â””â”€â”€ apus.csv                # Archivo de apus original con posibles errores  
â”‚
â”œâ”€â”€ models/                     # MÃ³dulos de lÃ³gica de negocio y anÃ¡lisis avanzado
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ probability_models.py   # Motor de simulaciÃ³n Monte Carlo para anÃ¡lisis de riesgos
â”‚
â”œâ”€â”€ scripts/                    # Herramientas de lÃ­nea de comandos para desarrolladores
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ generate_embeddings.py       # Script para generar el Ã­ndice de bÃºsqueda semÃ¡ntica
â”‚   â”œâ”€â”€ diagnose_apus_file.py        # Herramienta para analizar formatos de archivo de APU
â”‚   â”œâ”€â”€ diagnose_insumos_file.py     # Herramienta para analizar formatos de archivo de insumos
â”‚   â”œâ”€â”€ diagnose_presupuesto_file.py # Herramienta para analizar formatos de archivo de presupuesto
â”‚   â””â”€â”€ clean_csv.py                 # Herramienta para limpiar caracteres sucios y crear un archivo csv limpio 
â”‚
â”œâ”€â”€ tests/                      # Suite de pruebas completa del proyecto
â”‚   â”œâ”€â”€ test_app.py             # Pruebas de integraciÃ³n para los endpoints de la API
â”‚   â”œâ”€â”€ test_procesador_csv.py  # Pruebas para el orquestador del pipeline
â”‚   â”œâ”€â”€ test_apu_processor.py   # Pruebas para el motor de transformaciÃ³n
â”‚   â”œâ”€â”€ test_estimator.py       # Pruebas para la lÃ³gica de estimaciÃ³n
â”‚   â”œâ”€â”€ test_data_loader.py     # Pruebas para la capa de carga de datos
â”‚   â””â”€â”€ test_data.py            # Datos de prueba centralizados
â”‚
â”œâ”€â”€ templates/                  # Plantillas HTML para la interfaz (si aplica)
â”œâ”€â”€ uploads/                    # Directorio temporal para archivos subidos
â”‚
â”œâ”€â”€ requirements.in             # Archivo fuente para definir dependencias
â”œâ”€â”€ requirements.txt            # Archivo de dependencias "congelado" generado por uv
â””â”€â”€ pyproject.toml              # Archivo de configuraciÃ³n del proyecto Python
```
