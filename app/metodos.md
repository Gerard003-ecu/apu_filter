## IngenierÃ­a Bajo el CapÃ³: La GarantÃ­a de Estabilidad

Aunque nuestra prioridad es su negocio, la solidez tÃ©cnica es nuestra garantÃ­a. APU Filter estÃ¡ construido sobre una arquitectura modular que separa claramente las responsabilidades, garantizando robustez y escalabilidad. Sus tres pilares fundamentales son:

### 1. Condensador de Flujo de Datos (Data Flux Condenser)
- **Componente Clave:** `app/flux_condenser.py` (**El Estabilizador**)
- **FunciÃ³n:** ActÃºa como un amortiguador industrial a la entrada del sistema.

#### La AnalogÃ­a del Amortiguador Industrial
Imagine que los datos de entrada son un vehÃ­culo transitando por un terreno agreste (archivos con formatos rotos, caracteres extraÃ±os, errores humanos). Sin suspensiÃ³n, el motor (el procesador) se romperÃ­a con el primer bache.

Nuestro **Condensador de Flujo** funciona como una suspensiÃ³n activa avanzada. Usa fÃ­sica real para absorber los impactos del "camino" (datos sucios), entregando un viaje suave y constante al "pasajero" (su lÃ³gica de negocio). Si el camino es muy malo, el sistema reduce la velocidad automÃ¡ticamente para no volcar, pero **nunca se detiene**.

#### IngenierÃ­a de Confiabilidad (SRE) aplicada a Datos
Esta no es una metÃ¡fora decorativa. Utilizamos ecuaciones de sistemas dinÃ¡micos para gestionar la "fricciÃ³n" de los datos corruptos.

*   **Detectar FricciÃ³n:** Identificar cuÃ¡ndo la "suciedad" de los datos estÃ¡ generando resistencia.
*   **Disipar Calor:** Liberar la "presiÃ³n" reduciendo la velocidad de ingesta antes de un fallo crÃ­tico.
*   **Mantener el Flujo:** Garantizar que el sistema procese lo recuperable sin detenerse.

#### âš™ï¸ Nivel 1: Motor de FÃ­sica RLC (El Sensor)
El sistema evoluciona mÃ¡s allÃ¡ de mÃ©tricas simples hacia un **Modelo EnergÃ©tico Escalar**. En lugar de monitorear solo voltaje o corriente, unificamos las unidades bajo un lenguaje comÃºn: La EnergÃ­a (Julios).

1.  **EnergÃ­a Potencial ($E_c = \frac{1}{2}CV^2$) - PresiÃ³n de Datos:**
    *   Representa la "carga de trabajo" acumulada por el volumen de registros.
    *   Calcula la presiÃ³n que ejerce el lote de datos sobre el sistema.
2.  **EnergÃ­a CinÃ©tica ($E_l = \frac{1}{2}LI^2$) - Inercia de Calidad:**
    *   Representa el momento o "inercia" de la calidad del flujo.
    *   Un flujo de alta calidad ($I \approx 1.0$) tiene una inercia fuerte que resiste perturbaciones, dificultando que errores menores desestabilicen el proceso.
3.  **Potencia Disipada ($P = I_{ruido}^2 R$) - Calor/FricciÃ³n:**
    *   **TermodinÃ¡mica del Software:** Calcula el "calor" generado por la resistencia dinÃ¡mica de los datos sucios.
    *   Si el sistema gasta demasiada energÃ­a procesando basura (ruido), se genera sobrecalentamiento lÃ³gico.

#### ğŸ§  Nivel 2: Controlador PI Discreto (El Cerebro)
Sobre la capa fÃ­sica, opera un **Lazo de Control Cerrado (Feedback Loop)** que ajusta el comportamiento del sistema en tiempo real, ahora con protecciÃ³n tÃ©rmica:

*   **Algoritmo:** Controlador **Proporcional-Integral (PI)** discreto.
*   **Setpoint:** Mantiene una saturaciÃ³n estable (Flujo Laminar).
*   **Variable de Control:** El tamaÃ±o del lote de procesamiento (*Batch Size*).
*   **Disyuntor TÃ©rmico (Nuevo):**
    *   AdemÃ¡s del PID, el sistema implementa un "Diodo de Rueda Libre" tÃ©rmico.
    *   Si la **Potencia Disipada** supera un umbral crÃ­tico (> 50W), el sistema activa un freno de emergencia, reduciendo drÃ¡sticamente el tamaÃ±o del lote independientemente de la saturaciÃ³n, para "enfriar" el proceso y evitar colapsos por calidad de datos.

**Resultado:** Un sistema bi-mimÃ©tico que no solo adapta su velocidad, sino que tambiÃ©n gestiona su "temperatura" operativa para garantizar una estabilidad del 100% bajo cualquier condiciÃ³n.

#### ğŸ›¡ï¸ Resiliencia y RecuperaciÃ³n
El sistema implementa mecanismos de defensa avanzados:
*   **Anti-Windup PID:** Previene la saturaciÃ³n del controlador ante cargas sostenidas.
*   **RecuperaciÃ³n Parcial:** Capacidad de aislar lotes corruptos y continuar el procesamiento del resto del archivo.
*   **ProtecciÃ³n TÃ©rmica:** Freno de emergencia automÃ¡tico si la disipaciÃ³n de energÃ­a (fricciÃ³n de datos) supera los umbrales de seguridad.

### 2. Pipeline Narrativo de Datos
- **Componente Clave:** `app/procesador_csv.py` (**El Orquestador**)

El flujo de datos no es una simple tuberÃ­a, es una historia de transformaciÃ³n contada en cuatro actos:

1.  **El Ingreso (Load):** Los datos crudos llegan a la recepciÃ³n. AquÃ­, **El Guardia** (`ReportParserCrudo`) detiene a los sospechosos (lÃ­neas corruptas) en la puerta.
2.  **El DiagnÃ³stico (Merge):** **El Cirujano** (`APUProcessor`) examina los pacientes admitidos. Cruza la informaciÃ³n del APU con el catÃ¡logo maestro de insumos para completar los vacÃ­os (precios faltantes).
3.  **La OperaciÃ³n (Calculate):** Se realiza la suma de alta precisiÃ³n. Se agregan costos de materiales, mano de obra y equipos para obtener el costo real por unidad.
4.  **El Alta (Final Merge):** El APU curado y valorado se une al presupuesto general, listo para ser presentado en la oferta final.

### 3. Estimador Inteligente: FilosofÃ­a de "Caja Blanca"
- **Componente Clave:** `app/estimator.py` (**El Estratega**)

En ingenierÃ­a de costos, una "Caja Negra" (un sistema que da respuestas sin explicaciones) es inaceptable. Un gerente necesita saber **por quÃ©** se sugiere un precio.

El Estratega opera con **Transparencia Radical**:

#### Evidencia, no Magia
Cuando el sistema sugiere un APU histÃ³rico para un nuevo concepto, no solo entrega el precio, entrega la **Evidencia MatemÃ¡tica** de su decisiÃ³n.

*   **Coincidencia SemÃ¡ntica (El "Parecido Conceptual"):**
    *   El sistema entiende que *"Muro de ladrillo tolete"* es conceptualmente idÃ©ntico a *"MamposterÃ­a en arcilla cocida"*, aunque no compartan palabras.
    *   **Log:** `âœ… Coincidencia semÃ¡ntica encontrada: 0.92` (El sistema tiene un 92% de certeza de que son lo mismo).

*   **Coincidencia por Palabras Clave (El "Parecido Exacto"):**
    *   Si no hay similitud conceptual, busca tÃ©rminos especÃ­ficos.
    *   **Log:** `âœ… Match FLEXIBLE encontrado (80%)` (Coincidieron 4 de 5 palabras clave).

Esto permite al ingeniero humano auditar al "robot", validando si un 92% de similitud es suficiente para aceptar el precio o si requiere revisiÃ³n.

## TecnologÃ­as Utilizadas

La plataforma estÃ¡ construida sobre una pila de tecnologÃ­as modernas de alto rendimiento:

- **Backend:** **Flask** y **Redis** para una API robusta y con estado.
- **Inteligencia Artificial:**
    - **Sentence-Transformers & FAISS:** El cerebro detrÃ¡s de la bÃºsqueda semÃ¡ntica y la memoria institucional.
- **FÃ­sica de Datos:**
    - **Modelado RLC:** Algoritmos propios de control de flujo.
- **Calidad:**
    - **Pytest & Ruff:** EstÃ¡ndares de cÃ³digo de nivel industrial.

### La Arquitectura de la InstalaciÃ³n: Una AnalogÃ­a de Engranajes

Para entender por quÃ© seguimos un orden de instalaciÃ³n especÃ­fico, podemos visualizar nuestro entorno como una caja de cambios de precisiÃ³n compuesta por tres engranajes diferentes, cada uno con una funciÃ³n especializada.

1.  **Conda: El Engranaje Principal y de Potencia (El Engranaje Grande)**
    *   **Rol:** Mueve las piezas mÃ¡s pesadas y complejas que no son de Python puro y dependen del sistema operativo (ej. librerÃ­as C++).
    *   **CaracterÃ­stica:** Es potente y fiable, diseÃ±ado para buscar e instalar paquetes pre-compilados que encajan perfectamente con la arquitectura de la mÃ¡quina.
    *   **En APU Filter:** Su Ãºnica tarea es instalar `faiss-cpu`, una librerÃ­a con dependencias complejas a nivel de sistema.

2.  **Pip (con `--index-url`): La Herramienta Especializada**
    *   **Rol:** Se utiliza para una pieza crÃ­tica que necesita una instalaciÃ³n muy especÃ­fica desde un repositorio exclusivo.
    *   **CaracterÃ­stica:** Comunica una intenciÃ³n precisa: "Ve Ãºnicamente a este almacÃ©n especÃ­fico (el de PyTorch para CPU) y trae la pieza exacta que encuentres allÃ­".
    *   **En APU Filter:** Su Ãºnica tarea es instalar la versiÃ³n `torch` optimizada exclusivamente para CPU, evitando la descarga de las pesadas librerÃ­as de CUDA.

3.  **uv/pip: El Engranaje de Alta Velocidad y PrecisiÃ³n (El Engranaje PequeÃ±o)**
    *   **Rol:** Ensambla todos los componentes de la aplicaciÃ³n que son de Python puro, comunicÃ¡ndose directamente con el ecosistema de Python (PyPI).
    *   **CaracterÃ­stica:** Es ultrarrÃ¡pido y Ã¡gil, ideal para manejar dependencias estÃ¡ndar de Python, pero no tiene la fuerza para gestionar las piezas pesadas que maneja Conda.
    *   **En APU Filter:** Su tarea es instalar todo lo demÃ¡s desde `requirements.txt` de forma eficiente.

### Pasos Detallados de InstalaciÃ³n

**Requisito Previo:** AsegÃºrese de tener instalado Miniconda o Anaconda. Puede descargarlo desde [aquÃ­](https://www.anaconda.com/products/distribution).

**Paso 1: Crear el Entorno Base (Conda)**
Cree un nuevo entorno Conda llamado `apu_filter_env` con Python 3.10, la versiÃ³n sobre la cual se construirÃ¡n los demÃ¡s componentes.
```bash
conda create --name apu_filter_env python=3.10
```

**Paso 2: Activar el Entorno**
Active el entorno reciÃ©n creado. **Debe hacer esto cada vez que trabaje en el proyecto.**
```bash
conda activate apu_filter_env
```

**Paso 3: Instalar Componentes Pesados (Conda y Pip Especializado)**
Instale los "engranajes" principales que requieren compilaciones y dependencias complejas.

*   **Instalar `faiss-cpu` (El Engranaje de Potencia):**
    ```bash
    conda install -c pytorch faiss-cpu
    ```

*   **Instalar `torch` (La Herramienta Especializada):**
    ```bash
    pip install torch --index-url https://download.pytorch.org/whl/cpu
    ```

**Paso 4: Instalar Dependencias de la AplicaciÃ³n (uv)**
Instale todas las demÃ¡s dependencias de Python puro con el "engranaje de alta velocidad".
```bash
uv pip install -r requirements.txt
uv pip install -r requirements-dev.txt
```

**Paso 5: Instalar y Configurar el Servidor de Sesiones (Redis)**
Para garantizar la persistencia de los datos del usuario entre solicitudes, la aplicaciÃ³n utiliza Redis.

*   **Instalar `redis` (El Engranaje de Estabilidad):**
    Es crucial instalar Redis a travÃ©s del canal `conda-forge` para asegurar la compatibilidad entre diferentes sistemas operativos, incluyendo macOS y Linux.
    ```bash
    conda install -c conda-forge redis
    ```

**Nota Importante:** El archivo `requirements.txt` no debe contener `faiss-cpu` ni `torch`. Si alguna vez necesita regenerar este archivo (ej. usando `uv pip compile requirements.in`), asegÃºrese de excluir estas dos librerÃ­as para evitar conflictos de instalaciÃ³n.

## Flujo de Trabajo del Proyecto

El ciclo de vida del desarrollo y uso de la aplicaciÃ³n sigue estos pasos:

1.  **ConfiguraciÃ³n:** La lÃ³gica de negocio (mapeo de columnas, umbrales, reglas del estimador) se gestiona en `app/config.json`.
2.  **Pre-procesamiento:** Si los datos de los APUs cambian, debe regenerar los embeddings ejecutando:
    ```bash
    python scripts/generate_embeddings.py --input path/to/processed_apus.json
    ```
3.  **EjecuciÃ³n de la AplicaciÃ³n:** Con el entorno activado, inicie el servidor Flask:
    ```bash
    python -m flask run --port=5002
    ```
4.  **ValidaciÃ³n y Pruebas:** Para verificar la integridad del cÃ³digo, ejecute la suite de pruebas completa:
    ```bash
    pytest -vv
    ```

## Estructura del Directorio

El proyecto estÃ¡ organizado con una clara separaciÃ³n de responsabilidades para facilitar la mantenibilidad y la escalabilidad.

```
apu_filter/
â”‚
â”œâ”€â”€ app/                        # LÃ³gica principal de la aplicaciÃ³n Flask
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ app.py                  # Factory de la app, endpoints API y carga de modelos
â”‚   â”œâ”€â”€ procesador_csv.py       # Orquestador del pipeline de procesamiento de datos
â”‚   â”œâ”€â”€ report_parser_crudo.py  # Parser especializado para archivos de APU semi-estructurados
â”‚   â”œâ”€â”€ apu_processor.py        # Motor de transformaciÃ³n que aplica lÃ³gica de negocio a los datos parseados
â”‚   â”œâ”€â”€ estimator.py            # LÃ³gica de estimaciÃ³n con bÃºsqueda semÃ¡ntica y por keywords
â”‚   â”œâ”€â”€ flux_condenser.py       # LÃ³gica del condensador de flujos de datos
â”‚   â”œâ”€â”€ data_loader.py          # Capa de abstracciÃ³n para leer datos (.csv, .xlsx, .pdf)
â”‚   â”œâ”€â”€ schemas.py              # DefiniciÃ³n de los esquemas de datos (dataclasses)
â”‚   â”œâ”€â”€ utils.py                # Funciones de utilidad generales (normalizaciÃ³n, parsing, etc.)
â”‚   â”œâ”€â”€ config.json             # Archivo de configuraciÃ³n de la lÃ³gica de negocio
â”‚   â””â”€â”€ embeddings/             # Directorio para los artefactos de ML (Ã­ndice FAISS, mapeo)
â”‚
â”œâ”€â”€ data/                       # Datos de entrada y resultados intermedmedios
â”‚   â”œâ”€â”€ presupuesto_clean.csv   # VersiÃ³n sanitizada del presupuesto, lista para el pipeline
â”‚   â”œâ”€â”€ insumos_clean.csv       # VersiÃ³n sanitizada de insumos, lista para el pipeline
â”‚   â””â”€â”€ apus_clean.csv          # VersiÃ³n sanitizada de apus, lista para el pipeline  
â”‚
â”œâ”€â”€ data_dirty/                 # Datos crudos y sin procesar
â”‚   â”œâ”€â”€ presupuesto.csv         # Archivo de presupuesto original con posibles errores
â”‚   â”œâ”€â”€ insumos.csv             # Archivo de insumos original con posibles errores
â”‚   â””â”€â”€ apus.csv                # Archivo de apus original con posibles errores  
â”‚
â”œâ”€â”€ models/                     # MÃ³dulos de lÃ³gica de negocio y anÃ¡lisis avanzado
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ probability_models.py   # Motor de simulaciÃ³n Monte Carlo para anÃ¡lisis de riesgos
â”‚
â”œâ”€â”€ scripts/                    # Herramientas de lÃ­nea de comandos para desarrolladores
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ generate_embeddings.py       # Script para generar el Ã­ndice de bÃºsqueda semÃ¡ntica
â”‚   â”œâ”€â”€ diagnose_apus_file.py        # Herramienta para analizar formatos de archivo de APU
â”‚   â”œâ”€â”€ diagnose_insumos_file.py     # Herramienta para analizar formatos de archivo de insumos
â”‚   â”œâ”€â”€ diagnose_presupuesto_file.py # Herramienta para analizar formatos de archivo de presupuesto
â”‚   â””â”€â”€ clean_csv.py                 # Herramienta para limpiar caracteres sucios y crear un archivo csv limpio 
â”‚
â”œâ”€â”€ tests/                      # Suite de pruebas completa del proyecto
â”‚   â”œâ”€â”€ test_app.py             # Pruebas de integraciÃ³n para los endpoints de la API
â”‚   â”œâ”€â”€ test_procesador_csv.py  # Pruebas para el orquestador del pipeline
â”‚   â”œâ”€â”€ test_apu_processor.py   # Pruebas para el motor de transformaciÃ³n
â”‚   â”œâ”€â”€ test_estimator.py       # Pruebas para la lÃ³gica de estimaciÃ³n
â”‚   â”œâ”€â”€ test_data_loader.py     # Pruebas para la capa de carga de datos
â”‚   â””â”€â”€ test_data.py            # Datos de prueba centralizados
â”‚
â”œâ”€â”€ templates/                  # Plantillas HTML para la interfaz (si aplica)
â”œâ”€â”€ uploads/                    # Directorio temporal para archivos subidos
â”‚
â”œâ”€â”€ requirements.in             # Archivo fuente para definir dependencias
â”œâ”€â”€ requirements.txt            # Archivo de dependencias "congelado" generado por uv
â””â”€â”€ pyproject.toml              # Archivo de configuraciÃ³n del proyecto Python
```