### Propuesta 1

# ============================================================================
# CONTROLADOR PI DISCRETO - M√âTODOS REFINADOS
# ============================================================================
class PIController:
    """
    Controlador PI Discreto con:
    1. Filtro de media m√≥vil exponencial para estabilizaci√≥n
    2. Anti-windup con back-calculation mejorado
    3. An√°lisis de estabilidad basado en Lyapunov discreto mejorado
    4. Ganancia integral adaptativa para evitar windup en r√©gimen transitorio
    """

    _MAX_HISTORY_SIZE: int = 100

    def __init__(
        self,
        kp: float,
        ki: float,
        setpoint: float,
        min_output: int,
        max_output: int,
        integral_limit_factor: float = 2.0,
    ):
        self._validate_control_parameters(kp, ki, setpoint, min_output, max_output)

        self.Kp = float(kp)
        self.Ki = float(ki)
        self.setpoint = float(setpoint)
        self.min_output = int(min_output)
        self.max_output = int(max_output)

        # Espacio de salida normalizado
        self._output_range = max(1, self.max_output - self.min_output)
        self._output_center = (self.max_output + self.min_output) / 2.0

        # Anti-windup: l√≠mite integral basado en rango de salida
        self._integral_limit = self._output_range * max(0.1, integral_limit_factor)
        self._integral_error: float = 0.0

        # Filtro EMA (Exponential Moving Average) para suavizado
        self._ema_alpha: float = 0.3  # Factor de suavizado
        self._filtered_pv: Optional[float] = None

        # Estado temporal
        self._last_time: Optional[float] = None
        self._last_error: Optional[float] = None
        self._last_output: Optional[int] = None
        self._iteration_count: int = 0

        # Historial para an√°lisis de estabilidad
        self._error_history: deque = deque(maxlen=self._MAX_HISTORY_SIZE)
        self._output_history: deque = deque(maxlen=self._MAX_HISTORY_SIZE)

        # M√©tricas de estabilidad de Lyapunov
        self._lyapunov_sum: float = 0.0
        self._lyapunov_count: int = 0

        # Adaptaci√≥n de ganancia integral
        self._ki_adaptive: float = ki
        self._windup_detection_window: deque = deque(maxlen=5)

    def _validate_control_parameters(
        self, kp: float, ki: float, setpoint: float,
        min_output: int, max_output: int
    ) -> None:
        """Validaci√≥n de par√°metros con criterios de estabilidad mejorados."""
        errors = []

        if kp <= 0:
            errors.append(f"Kp debe ser positivo para respuesta proporcional, got {kp}")
        if ki < 0:
            errors.append(f"Ki debe ser no-negativo, got {ki}")
        if min_output >= max_output:
            errors.append(
                f"Rango de salida inv√°lido: [{min_output}, {max_output}]"
            )
        if min_output <= 0:
            errors.append(f"min_output debe ser positivo, got {min_output}")
        if not (0.0 < setpoint < 1.0):
            errors.append(f"setpoint debe estar en (0, 1), got {setpoint}")

        # Criterio de estabilidad mejorado para PI discreto:
        # Para sistema de primer orden con tiempo de muestreo T=1, condiciones de estabilidad:
        # 0 < Kp < 2/|G| y 0 < Ki < Kp donde |G| es ganancia del proceso (~1)
        if kp > 2.0:
            logger.warning(f"Kp={kp} puede causar inestabilidad (recomendado < 2.0)")

        if ki > kp:
            logger.warning(f"Ki/Kp ratio={ki/kp:.2f} > 1 puede causar oscilaciones")

        if errors:
            raise ConfigurationError(
                "Errores en par√°metros de control:\n" +
                "\n".join(f"  ‚Ä¢ {e}" for e in errors)
            )

    def _apply_ema_filter(self, measurement: float) -> float:
        """Filtro de media m√≥vil exponencial con adaptaci√≥n de alpha."""
        if self._filtered_pv is None:
            self._filtered_pv = measurement
            return measurement

        # Adaptar alpha basado en la varianza reciente del error
        if len(self._error_history) > 1:
            recent_errors = list(self._error_history)[-5:]
            if np:
                error_variance = np.var(recent_errors)
            else:
                mean_error = sum(recent_errors) / len(recent_errors)
                error_variance = sum((e - mean_error)**2 for e in recent_errors) / len(recent_errors)

            # Alpha m√°s bajo para ruido alto, m√°s alto para se√±al limpia
            adaptive_alpha = max(0.1, min(0.5, 0.3/(1 + error_variance)))
            self._ema_alpha = adaptive_alpha

        self._filtered_pv = (
            self._ema_alpha * measurement +
            (1 - self._ema_alpha) * self._filtered_pv
        )
        return self._filtered_pv

    def _update_lyapunov_metric(self, error: float) -> None:
        """
        Actualiza m√©trica de estabilidad de Lyapunov mejorada.

        Usa funci√≥n de Lyapunov V(e) = e¬≤ y calcula exponente de Lyapunov
        mediante diferencia finita logar√≠tmica.
        """
        if self._last_error is not None:
            # Para evitar divisi√≥n por cero
            if abs(self._last_error) < 1e-10:
                return

            # Tasa de cambio logar√≠tmica (exponente de Lyapunov aproximado)
            lyapunov_rate = math.log(abs(error) + 1e-10) - math.log(abs(self._last_error) + 1e-10)

            # Filtrado EMA del exponente
            self._lyapunov_sum += lyapunov_rate
            self._lyapunov_count += 1

            # Detecci√≥n de inestabilidad temprana
            if self._lyapunov_count > 10:
                avg_exponent = self._lyapunov_sum / self._lyapunov_count
                if avg_exponent > 0.1:  # Exponente positivo indica inestabilidad
                    logger.warning(f"Posible inestabilidad detectada: Œª ‚âà {avg_exponent:.3f}")

    def _adapt_integral_gain(self, error: float, output_saturated: bool) -> None:
        """Adapta la ganancia integral para prevenir windup."""
        self._windup_detection_window.append((error, output_saturated))

        if len(self._windup_detection_window) < 3:
            return

        # Detectar windup: error constante con saturaci√≥n
        recent_errors = [e for e, _ in self._windup_detection_window]
        saturated_count = sum(1 for _, s in self._windup_detection_window if s)

        if np:
            error_std = np.std(recent_errors)
        else:
            mean = sum(recent_errors) / len(recent_errors)
            error_std = math.sqrt(sum((e - mean)**2 for e in recent_errors) / len(recent_errors))

        # Condiciones para windup: baja variaci√≥n en error con saturaci√≥n frecuente
        if error_std < 0.05 and saturated_count >= 2:
            self._ki_adaptive = self.Ki * 0.5  # Reducir Ki temporalmente
            logger.debug("Windup detectado: reduciendo Ki adaptativamente")
        else:
            self._ki_adaptive = self.Ki  # Restaurar Ki nominal

    def compute(self, process_variable: float) -> int:
        """
        Calcula salida de control PI con anti-windup mejorado.
        """
        self._iteration_count += 1
        current_time = time.time()

        # Filtrado de se√±al adaptativo
        filtered_pv = self._apply_ema_filter(
            max(0.0, min(1.0, process_variable))
        )

        # C√°lculo de error
        error = self.setpoint - filtered_pv
        self._error_history.append(error)

        # C√°lculo de delta tiempo con l√≠mites
        if self._last_time is None:
            dt = SystemConstants.MIN_DELTA_TIME
        else:
            dt = max(
                SystemConstants.MIN_DELTA_TIME,
                min(current_time - self._last_time, SystemConstants.MAX_DELTA_TIME)
            )

        # T√©rmino proporcional
        P = self.Kp * error

        # Adaptaci√≥n de ganancia integral
        output_saturated_precheck = False
        # Pre-c√°lculo para detecci√≥n temprana de saturaci√≥n
        integral_increment = error * dt
        proposed_integral = self._integral_error + integral_increment
        I_proposed = self._ki_adaptive * proposed_integral
        output_unbounded_pre = self._output_center + P + I_proposed
        if output_unbounded_pre > self.max_output or output_unbounded_pre < self.min_output:
            output_saturated_precheck = True

        self._adapt_integral_gain(error, output_saturated_precheck)

        # T√©rmino integral con anti-windup mejorado
        # Back-calculation mejorado
        I_proposed = self._ki_adaptive * proposed_integral
        output_unbounded = self._output_center + P + I_proposed

        # Anti-windup con retroalimentaci√≥n de saturaci√≥n
        if output_unbounded > self.max_output:
            saturation_error = output_unbounded - self.max_output
            # Factor de back-calculation ajustado
            back_calc_factor = 0.8
            self._integral_error = proposed_integral - (back_calc_factor * saturation_error / max(self._ki_adaptive, 1e-6))
        elif output_unbounded < self.min_output:
            saturation_error = self.min_output - output_unbounded
            back_calc_factor = 0.8
            self._integral_error = proposed_integral + (back_calc_factor * saturation_error / max(self._ki_adaptive, 1e-6))
        else:
            self._integral_error = proposed_integral

        # Aplicar l√≠mite absoluto de integral con suavizado
        if abs(self._integral_error) > self._integral_limit:
            # Suavizar la limitaci√≥n para evitar discontinuidades
            limit_factor = self._integral_limit / abs(self._integral_error)
            self._integral_error *= limit_factor

        # C√°lculo final de salida
        I = self._ki_adaptive * self._integral_error
        output_raw = self._output_center + P + I

        # Suavizado de salida para cambios bruscos
        if self._last_output is not None:
            max_change = self._output_range * 0.1  # M√°ximo 10% de cambio por iteraci√≥n
            output_raw = self._last_output + max(-max_change, min(max_change, output_raw - self._last_output))

        output = int(round(max(self.min_output, min(self.max_output, output_raw))))

        # Actualizar m√©tricas de estabilidad
        self._update_lyapunov_metric(error)

        # Guardar estado
        self._last_time = current_time
        self._last_error = error
        self._last_output = output
        self._output_history.append(output)

        return output

    def get_lyapunov_exponent(self) -> float:
        """Retorna estimaci√≥n del exponente de Lyapunov promedio."""
        if self._lyapunov_count == 0:
            return 0.0
        return self._lyapunov_sum / self._lyapunov_count

    def get_stability_analysis(self) -> Dict[str, Any]:
        """An√°lisis de estabilidad basado en historial."""
        if len(self._error_history) < 2:
            return {
                "status": "INSUFFICIENT_DATA",
                "samples": len(self._error_history)
            }

        errors = list(self._error_history)
        lyapunov = self.get_lyapunov_exponent()

        # An√°lisis de convergencia
        mean_error = sum(errors) / len(errors)
        error_variance = sum((e - mean_error)**2 for e in errors) / len(errors)

        recent_errors = errors[-min(10, len(errors)):]
        mean_recent = sum(recent_errors) / len(recent_errors)
        recent_variance = sum((e - mean_recent)**2 for e in recent_errors) / len(recent_errors)

        # Diagn√≥stico
        if lyapunov < -0.1:
            stability = "ASYMPTOTICALLY_STABLE"
        elif lyapunov < 0.1:
            stability = "MARGINALLY_STABLE"
        else:
            stability = "POTENTIALLY_UNSTABLE"

        # Tendencia de convergencia
        convergence = "CONVERGING" if recent_variance < error_variance else "DIVERGING"

        return {
            "status": "OPERATIONAL",
            "stability_class": stability,
            "convergence": convergence,
            "lyapunov_exponent": lyapunov,
            "error_variance": error_variance,
            "recent_variance": recent_variance,
            "integral_saturation": abs(self._integral_error) / self._integral_limit,
            "iterations": self._iteration_count
        }

    def get_diagnostics(self) -> Dict[str, Any]:
        """Diagn√≥stico completo del controlador."""
        stability = self.get_stability_analysis()

        return {
            "status": stability.get("status", "UNKNOWN"),
            "control_metrics": {
                "iteration": self._iteration_count,
                "current_integral": self._integral_error,
                "integral_limit": self._integral_limit,
                "integral_utilization": abs(self._integral_error) / self._integral_limit,
                "last_error": self._last_error,
                "last_output": self._last_output,
                "adaptive_ki": self._ki_adaptive
            },
            "stability_analysis": stability,
            "parameters": {
                "Kp": self.Kp,
                "Ki": self.Ki,
                "setpoint": self.setpoint,
                "output_range": [self.min_output, self.max_output]
            }
        }

    def reset(self) -> None:
        """Reinicia estado del controlador preservando historial."""
        self._integral_error = 0.0
        self._last_time = None
        self._last_error = None
        self._last_output = None
        self._filtered_pv = None
        self._iteration_count = 0
        self._lyapunov_sum = 0.0
        self._lyapunov_count = 0
        self._ki_adaptive = self.Ki
        # Preserva historial para an√°lisis post-mortem

    def get_state(self) -> Dict[str, Any]:
        """Retorna estado serializable del controlador."""
        return {
            "parameters": {
                "Kp": self.Kp,
                "Ki": self.Ki,
                "setpoint": self.setpoint,
                "min_output": self.min_output,
                "max_output": self.max_output
            },
            "state": {
                "integral_error": self._integral_error,
                "filtered_pv": self._filtered_pv,
                "iteration": self._iteration_count,
                "adaptive_ki": self._ki_adaptive
            },
            "diagnostics": self.get_stability_analysis()
        }


# ============================================================================
# MOTOR DE F√çSICA - M√âTODOS REFINADOS
# ============================================================================
class FluxPhysicsEngine:
    """
    Motor de f√≠sica RLC mejorado con:
    1. Integraci√≥n num√©rica m√°s estable (Runge-Kutta de 2do orden)
    2. C√°lculo de n√∫meros de Betti corregido para grafos
    3. Entrop√≠a termodin√°mica con fundamentaci√≥n estad√≠stica rigurosa
    4. Modelo de amortiguamiento no lineal para alta saturaci√≥n
    """

    _MAX_METRICS_HISTORY: int = 100

    def __init__(self, capacitance: float, resistance: float, inductance: float):
        # Inicializar logger primero para usar en validaci√≥n
        self.logger = logging.getLogger(f"{__name__}.{self.__class__.__name__}")

        self._validate_physical_parameters(capacitance, resistance, inductance)

        self.C = float(capacitance)
        self.R = float(resistance)
        self.L = float(inductance)

        # Par√°metros derivados del circuito RLC
        self._omega_0 = 1.0 / math.sqrt(self.L * self.C)  # Frecuencia natural
        self._alpha = self.R / (2.0 * self.L)  # Factor de amortiguamiento
        self._zeta = self._alpha / self._omega_0  # Ratio de amortiguamiento
        self._Q = math.sqrt(self.L / self.C) / self.R if self.R > 0 else float('inf')

        # Clasificaci√≥n del sistema
        self._update_damping_classification()

        # Estado del sistema: [carga Q, corriente I]
        self._state = [0.0, 0.0]  # Compatible con/sin numpy
        self._state_history: deque = deque(maxlen=self._MAX_METRICS_HISTORY)

        # Grafo de conectividad para an√°lisis topol√≥gico
        self._adjacency_list: Dict[int, Set[int]] = {}
        self._vertex_count: int = 0
        self._edge_count: int = 0

        # Historial de m√©tricas
        self._metrics_history: deque = deque(maxlen=self._MAX_METRICS_HISTORY)
        self._entropy_history: deque = deque(maxlen=self._MAX_METRICS_HISTORY)

        # Estado temporal
        self._last_current: float = 0.0
        self._ema_current: float = 0.0  # EMA de la corriente (Eje de rotaci√≥n)
        self._last_time: float = time.time()
        self._initialized: bool = False

        # Amortiguamiento no lineal
        self._nonlinear_damping_factor: float = 1.0

    def _validate_physical_parameters(self, C: float, R: float, L: float) -> None:
        """Validaci√≥n de par√°metros f√≠sicos con an√°lisis dimensional."""
        errors = []

        if C <= 0:
            errors.append(f"Capacitancia debe ser positiva, got {C} F")
        if R < 0:
            errors.append(f"Resistencia debe ser no-negativa, got {R} Œ©")
        if L <= 0:
            errors.append(f"Inductancia debe ser positiva, got {L} H")

        # Verificaci√≥n de rangos f√≠sicamente razonables
        if C > 0 and L > 0:
            omega_0 = 1.0 / math.sqrt(L * C)
            if omega_0 > 1e12:  # > 1 THz
                self.logger.warning(
                    f"Frecuencia natural {omega_0:.2e} rad/s excesivamente alta"
                )

        if R > 0 and L > 0:
            tau = L / R  # Constante de tiempo
            if tau < 1e-12:  # < 1 ps
                self.logger.warning(
                    f"Constante de tiempo {tau:.2e} s muy peque√±a"
                )

        if errors:
            raise ConfigurationError(
                "Par√°metros f√≠sicos inv√°lidos:\n" +
                "\n".join(f"  ‚Ä¢ {e}" for e in errors)
            )

    def _update_damping_classification(self) -> None:
        """Actualiza clasificaci√≥n de amortiguamiento del sistema."""
        if self._zeta > 1.0:
            self._damping_type = "OVERDAMPED"
            self._omega_d = self._omega_0 * math.sqrt(self._zeta**2 - 1)
        elif self._zeta < 1.0:
            self._damping_type = "UNDERDAMPED"
            self._omega_d = self._omega_0 * math.sqrt(1 - self._zeta**2)
        else:
            self._damping_type = "CRITICALLY_DAMPED"
            self._omega_d = 0.0

    def _evolve_state_rk2(self, driving_current: float, dt: float) -> Tuple[float, float]:
        """
        Evoluciona el estado del sistema RLC usando Runge-Kutta de 2do orden.

        Sistema de ecuaciones:
        dQ/dt = I
        dI/dt = (V - R*I - Q/C) / L
        donde V es proporcional a driving_current
        """
        Q, I = self._state

        # Convertir driving_current a voltaje de entrada (normalizado)
        V_in = driving_current * 10.0  # Factor de escala

        # Funci√≥n derivada
        def derivatives(q, i):
            dq_dt = i
            di_dt = (V_in - self.R * i - q / self.C) / self.L
            return dq_dt, di_dt

        # Runge-Kutta de 2do orden (m√©todo del punto medio)
        k1_q, k1_i = derivatives(Q, I)
        k2_q, k2_i = derivatives(Q + 0.5 * dt * k1_q, I + 0.5 * dt * k1_i)

        Q_new = Q + dt * k2_q
        I_new = I + dt * k2_i

        # Aplicar amortiguamiento no lineal para alta energ√≠a
        current_energy = 0.5 * self.L * I_new**2 + 0.5 * self.C * (Q_new/self.C)**2
        if current_energy > 10.0:  # Umbral de energ√≠a
            damping_factor = 1.0 / (1.0 + 0.1 * current_energy)
            I_new *= damping_factor
            self._nonlinear_damping_factor = damping_factor

        self._state = [Q_new, I_new]
        self._state_history.append({
            'Q': Q_new,
            'I': I_new,
            'time': time.time(),
            'energy': current_energy
        })

        return Q_new, I_new

    def _build_metric_graph(self, metrics: Dict[str, float]) -> None:
        """
        Construye grafo de correlaci√≥n entre m√©tricas usando umbral adaptativo
        basado en percentiles.
        """
        # Seleccionar m√©tricas clave para el grafo
        metric_keys = ['saturation', 'complexity', 'current_I', 'potential_energy',
                      'kinetic_energy', 'entropy_shannon']
        values = [metrics.get(k, 0.0) for k in metric_keys]

        self._adjacency_list.clear()
        self._vertex_count = len(values)
        self._edge_count = 0

        # Inicializar listas de adyacencia
        for i in range(self._vertex_count):
            self._adjacency_list[i] = set()

        # Crear aristas basadas en correlaci√≥n estad√≠stica
        if len(values) > 1:
            # Usar diferencia de percentiles como umbral adaptativo
            sorted_vals = sorted(values)
            if len(sorted_vals) >= 4:
                q1 = sorted_vals[len(sorted_vals)//4]
                q3 = sorted_vals[3*len(sorted_vals)//4]
                iqr = q3 - q1
                threshold = max(0.1, iqr * 0.5)  # 50% del IQR, m√≠nimo 0.1
            else:
                value_range = max(values) - min(values) if max(values) != min(values) else 1.0
                threshold = 0.3 * value_range

            for i in range(len(values)):
                for j in range(i + 1, len(values)):
                    # Correlaci√≥n basada en distancia normalizada
                    correlation = 1.0 - abs(values[i] - values[j]) / (abs(values[i]) + abs(values[j]) + 1e-10)
                    if correlation > 0.7:  # Correlaci√≥n fuerte
                        self._adjacency_list[i].add(j)
                        self._adjacency_list[j].add(i)
                        self._edge_count += 1

    def _calculate_betti_numbers(self) -> Dict[int, int]:
        """
        Calcula n√∫meros de Betti para el grafo de m√©tricas.

        Para un grafo simple:
        - Œ≤‚ÇÄ = n√∫mero de componentes conexas
        - Œ≤‚ÇÅ = n√∫mero de ciclos independientes = E - V + Œ≤‚ÇÄ
        """
        if self._vertex_count == 0:
            return {0: 0, 1: 0, 2: 0}

        # Calcular componentes conexas con DFS iterativo
        visited = [False] * self._vertex_count
        components = 0

        for v in range(self._vertex_count):
            if not visited[v]:
                components += 1
                stack = [v]
                while stack:
                    current = stack.pop()
                    if not visited[current]:
                        visited[current] = True
                        for neighbor in self._adjacency_list.get(current, set()):
                            if not visited[neighbor]:
                                stack.append(neighbor)

        beta_0 = components

        # Œ≤‚ÇÅ = E - V + Œ≤‚ÇÄ (caracter√≠stica de Euler-Poincar√© para grafos)
        beta_1 = max(0, self._edge_count - self._vertex_count + beta_0)

        # Para grafos, Œ≤_k = 0 para k ‚â• 2
        return {0: beta_0, 1: beta_1, 2: 0}

    def calculate_gyroscopic_stability(self, current_I: float) -> float:
        """
        Calcula la Estabilidad Girosc√≥pica (Sg) con modelo de precesi√≥n.

        Modelo: Sg = exp(-Œ∫ * |dI/dt|) * cos(Œ∏)
        donde Œ∫ es sensibilidad y Œ∏ es √°ngulo de precesi√≥n
        """
        if not self._initialized:
            self._ema_current = current_I
            self._initialized = True
            return 1.0

        # Calcular derivada de la corriente
        current_time = time.time()
        dt = max(1e-6, current_time - self._last_time)
        dI_dt = (current_I - self._last_current) / dt

        # Actualizar EMA del eje de rotaci√≥n
        alpha = SystemConstants.GYRO_EMA_ALPHA
        self._ema_current = alpha * current_I + (1 - alpha) * self._ema_current

        # Calcular √°ngulo de precesi√≥n (desviaci√≥n del eje)
        precession_angle = math.atan2(current_I - self._ema_current, 1.0)

        # Calcular estabilidad girosc√≥pica
        sensitivity = SystemConstants.GYRO_SENSITIVITY
        stability = math.exp(-sensitivity * abs(dI_dt)) * math.cos(precession_angle)

        # Normalizar a [0, 1]
        stability_normalized = (stability + 1) / 2.0

        return max(0.0, min(1.0, stability_normalized))

    def calculate_system_entropy(
        self,
        total_records: int,
        error_count: int,
        processing_time: float
    ) -> Dict[str, float]:
        """
        Calcula entrop√≠a del sistema con fundamentaci√≥n estad√≠stica rigurosa.

        1. Entrop√≠a de Shannon con correcci√≥n de peque√±as muestras
        2. Entrop√≠a de Tsallis para no-extensividad
        3. Entrop√≠a relativa (Kullback-Leibler) respecto a distribuci√≥n uniforme
        """
        if total_records <= 0:
            return self._get_zero_entropy()

        success_count = total_records - error_count

        # Probabilidades
        p_success = success_count / total_records
        p_error = error_count / total_records

        # Entrop√≠a de Shannon con correcci√≥n de Miller-Madow para bias en peque√±as muestras
        shannon_entropy = 0.0
        if p_success > 0:
            shannon_entropy -= p_success * math.log2(p_success)
        if p_error > 0:
            shannon_entropy -= p_error * math.log2(p_error)

        # Correcci√≥n de Miller-Madow
        m = 2  # N√∫mero de bins (√©xito/error)
        shannon_entropy_corrected = shannon_entropy + (m - 1) / (2 * total_records * math.log(2))

        # Entrop√≠a de Tsallis (q-entrop√≠a) para capturar no-extensividad
        q = 1.5  # Par√°metro de no-extensividad
        tsallis_entropy = 0.0
        if q != 1:
            for p in [p_success, p_error]:
                if p > 0:
                    tsallis_entropy += p**q
            tsallis_entropy = (1 - tsallis_entropy) / (q - 1)
        else:
            tsallis_entropy = shannon_entropy / math.log(2)  # Convertir a nats para consistencia

        # Entrop√≠a relativa (divergencia KL) respecto a distribuci√≥n uniforme
        uniform_p = 0.5
        kl_divergence = 0.0
        for p in [p_success, p_error]:
            if p > 0:
                kl_divergence += p * math.log2(p / uniform_p)

        # Producci√≥n de entrop√≠a por unidad de tiempo
        entropy_rate = shannon_entropy / max(processing_time, 0.001)

        # Diagn√≥stico de estado termodin√°mico
        max_shannon = 1.0  # M√°ximo para distribuci√≥n binomial
        entropy_ratio = shannon_entropy / max_shannon

        # Ley de enfriamiento de Newton aplicada a entrop√≠a
        entropy_decay_time = 0.0
        if entropy_rate > 0:
            entropy_decay_time = shannon_entropy / entropy_rate

        result = {
            "shannon_entropy": shannon_entropy,
            "shannon_entropy_corrected": shannon_entropy_corrected,
            "tsallis_entropy": tsallis_entropy,
            "kl_divergence": kl_divergence,
            "entropy_rate": entropy_rate,
            "entropy_decay_time": entropy_decay_time,
            "max_entropy": max_shannon,
            "entropy_ratio": entropy_ratio,
            "is_thermal_death": entropy_ratio > 0.8,
            "entropy_absolute": shannon_entropy, # Alias compatibilidad
            "configurational_entropy": 0.0 # Placeholder
        }

        self._entropy_history.append({
            **result,
            'timestamp': time.time(),
            'total_records': total_records,
            'error_rate': error_count / total_records if total_records > 0 else 0
        })

        return result

    def _get_zero_entropy(self) -> Dict[str, float]:
        """Retorna entrop√≠a cero para casos triviales."""
        return {
            "shannon_entropy": 0.0,
            "shannon_entropy_corrected": 0.0,
            "tsallis_entropy": 0.0,
            "kl_divergence": 0.0,
            "entropy_absolute": 0.0,
            "configurational_entropy": 0.0,
            "entropy_rate": 0.0,
            "entropy_decay_time": 0.0,
            "max_entropy": 1.0,
            "entropy_ratio": 0.0,
            "is_thermal_death": False
        }

    def calculate_metrics(
        self,
        total_records: int,
        cache_hits: int,
        error_count: int = 0,
        processing_time: float = 1.0,
    ) -> Dict[str, float]:
        """
        Calcula m√©tricas f√≠sicas del sistema RLC.

        Modelo: el flujo de datos se modela como un circuito RLC donde:
        - Corriente I = eficiencia (cache_hits / total_records)
        - Carga Q = registros acumulados procesados
        - Voltaje V = "presi√≥n" del pipeline (saturaci√≥n)
        """
        if total_records <= 0:
            return self._get_zero_metrics()

        current_time = time.time()

        # Corriente normalizada (eficiencia de cach√©)
        current_I = cache_hits / total_records

        # Complejidad como resistencia adicional
        complexity = 1.0 - current_I

        # Resistencia din√°mica
        R_dynamic = self.R * (
            1.0 + complexity * SystemConstants.COMPLEXITY_RESISTANCE_FACTOR
        )

        # Actualizar amortiguamiento din√°mico
        zeta_dynamic = R_dynamic / (2.0 * math.sqrt(self.L / self.C))

        # Evoluci√≥n del estado con RK2
        if self._initialized:
            dt = max(1e-6, current_time - self._last_time)
        else:
            dt = 0.01
            self._initialized = True

        Q, I = self._evolve_state_rk2(current_I, dt)

        # Constante de tiempo normalizada
        tau = self.L / R_dynamic if R_dynamic > 0 else float('inf')
        t_normalized = processing_time / tau if tau > 0 else 0.0
        t_normalized = min(t_normalized, 50.0)

        # Respuesta transitoria (voltaje en capacitor = saturaci√≥n)
        if zeta_dynamic >= 1.0:
            # Sobreamortiguado o cr√≠ticamente amortiguado
            saturation = 1.0 - math.exp(-t_normalized)
        else:
            # Subamortiguado - respuesta oscilatoria
            omega_d = self._omega_0 * math.sqrt(1 - zeta_dynamic**2)
            exp_term = math.exp(-zeta_dynamic * self._omega_0 * t_normalized)
            cos_term = math.cos(omega_d * t_normalized)
            sin_term = (zeta_dynamic / math.sqrt(1 - zeta_dynamic**2)) * math.sin(omega_d * t_normalized)
            saturation = 1.0 - exp_term * (cos_term + sin_term)

        saturation = max(0.0, min(1.0, saturation))

        # Energ√≠as
        E_capacitor = 0.5 * self.C * (saturation ** 2)  # Energ√≠a potencial
        E_inductor = 0.5 * self.L * (current_I ** 2)    # Energ√≠a cin√©tica

        # Potencia disipada
        P_dissipated = (current_I ** 2) * R_dynamic

        # Voltaje de flyback inductivo
        di_dt = (current_I - self._last_current) / max(dt, 1e-6)
        V_flyback = min(
            abs(self.L * di_dt),
            SystemConstants.MAX_FLYBACK_VOLTAGE
        )

        # Entrop√≠a
        entropy_metrics = self.calculate_system_entropy(
            total_records, error_count, processing_time
        )

        # Estabilidad Girosc√≥pica
        gyro_stability = self.calculate_gyroscopic_stability(current_I)

        # Construir grafo y calcular topolog√≠a
        metrics = {
            "saturation": saturation,
            "complexity": complexity,
            "current_I": current_I,
            "potential_energy": E_capacitor,
            "kinetic_energy": E_inductor,
            "total_energy": E_capacitor + E_inductor,
            "dissipated_power": P_dissipated,
            "flyback_voltage": V_flyback,
            "dynamic_resistance": R_dynamic,
            "damping_ratio": zeta_dynamic,
            "damping_type": self._damping_type,
            "resonant_frequency_hz": self._omega_0 / (2 * math.pi),
            "quality_factor": self._Q,
            "time_constant": tau,
            # Entrop√≠a Extendida
            "entropy_shannon": entropy_metrics["shannon_entropy"],
            "entropy_shannon_corrected": entropy_metrics["shannon_entropy_corrected"],
            "tsallis_entropy": entropy_metrics["tsallis_entropy"],
            "kl_divergence": entropy_metrics["kl_divergence"],
            "entropy_rate": entropy_metrics["entropy_rate"],
            "entropy_ratio": entropy_metrics["entropy_ratio"],
            "is_thermal_death": entropy_metrics["is_thermal_death"],
            # Alias para pruebas
            "entropy_absolute": entropy_metrics["entropy_absolute"],
            # Girosc√≥pica
            "gyroscopic_stability": gyro_stability,
        }

        # An√°lisis topol√≥gico
        self._build_metric_graph(metrics)
        betti = self._calculate_betti_numbers()
        metrics["betti_0"] = betti[0]
        metrics["betti_1"] = betti[1]
        metrics["graph_vertices"] = self._vertex_count
        metrics["graph_edges"] = self._edge_count

        # Actualizar estado
        self._last_current = current_I
        self._last_time = current_time

        # Guardar en historial
        self._store_metrics(metrics)

        return metrics

    def _get_zero_metrics(self) -> Dict[str, float]:
        """M√©tricas iniciales para casos triviales."""
        return {
            "saturation": 0.0,
            "complexity": 1.0,
            "current_I": 0.0,
            "potential_energy": 0.0,
            "kinetic_energy": 0.0,
            "total_energy": 0.0,
            "dissipated_power": 0.0,
            "flyback_voltage": 0.0,
            "dynamic_resistance": self.R,
            "damping_ratio": self._zeta,
            "damping_type": self._damping_type,
            "resonant_frequency_hz": self._omega_0 / (2 * math.pi),
            "quality_factor": self._Q,
            "time_constant": self.L / self.R if self.R > 0 else float('inf'),
            "entropy_shannon": 0.0,
            "entropy_absolute": 0.0,
            "entropy_rate": 0.0,
            "entropy_ratio": 0.0,
            "is_thermal_death": False,
            "betti_0": 0,
            "betti_1": 0,
            "graph_vertices": 0,
            "graph_edges": 0,
            "gyroscopic_stability": 1.0,
        }

    def _store_metrics(self, metrics: Dict[str, float]) -> None:
        """Almacena m√©tricas con timestamp."""
        self._metrics_history.append({
            **metrics,
            "_timestamp": time.time()
        })

    def get_trend_analysis(self) -> Dict[str, Any]:
        """Analiza tendencias en m√©tricas hist√≥ricas."""
        if len(self._metrics_history) < 2:
            return {
                "status": "INSUFFICIENT_DATA",
                "samples": len(self._metrics_history)
            }

        result = {
            "status": "OK",
            "samples": len(self._metrics_history)
        }

        # M√©tricas a analizar
        keys_to_analyze = ["saturation", "dissipated_power", "entropy_ratio"]

        for key in keys_to_analyze:
            values = [m.get(key, 0.0) for m in self._metrics_history if key in m]
            if len(values) >= 2:
                # Tendencia lineal simple
                first_half = sum(values[:len(values)//2]) / (len(values)//2)
                second_half = sum(values[len(values)//2:]) / (len(values) - len(values)//2)

                if second_half > first_half * 1.1:
                    trend = "INCREASING"
                elif second_half < first_half * 0.9:
                    trend = "DECREASING"
                else:
                    trend = "STABLE"

                result[key] = {
                    "trend": trend,
                    "current": values[-1],
                    "mean": sum(values) / len(values),
                    "min": min(values),
                    "max": max(values)
                }

        return result

    def get_system_diagnosis(self, metrics: Dict[str, float]) -> Dict[str, str]:
        """Genera diagn√≥stico del estado del sistema."""
        diagnosis = {
            "state": "NORMAL",
            "damping": self._damping_type,
            "energy": "BALANCED",
            "entropy": "LOW"
        }

        # Diagn√≥stico de saturaci√≥n
        saturation = metrics.get("saturation", 0.0)
        if saturation > 0.95:
            diagnosis["state"] = "SATURATED"
        elif saturation < 0.05:
            diagnosis["state"] = "IDLE"

        # Diagn√≥stico de energ√≠a
        pe = metrics.get("potential_energy", 0)
        ke = metrics.get("kinetic_energy", 0)
        total_e = pe + ke

        if total_e > 0:
            if pe / total_e > 0.9:
                diagnosis["energy"] = "POTENTIAL_DOMINATED"
            elif ke / total_e > 0.9:
                diagnosis["energy"] = "KINETIC_DOMINATED"

        # Diagn√≥stico de potencia
        power = metrics.get("dissipated_power", 0)
        if power > SystemConstants.OVERHEAT_POWER_THRESHOLD:
            diagnosis["state"] = "OVERHEATING"

        # Diagn√≥stico de entrop√≠a
        entropy_ratio = metrics.get("entropy_ratio", 0)
        if entropy_ratio > 0.8:
            diagnosis["entropy"] = "HIGH"
            if metrics.get("is_thermal_death", False):
                diagnosis["state"] = "THERMAL_DEATH"
        elif entropy_ratio > 0.5:
            diagnosis["entropy"] = "MODERATE"

        # Diagn√≥stico topol√≥gico
        betti_0 = metrics.get("betti_0", 1)
        betti_1 = metrics.get("betti_1", 0)

        if betti_0 > 1:
            diagnosis["topology"] = "DISCONNECTED"
        elif betti_1 > 0:
            diagnosis["topology"] = "CYCLIC"
        else:
            diagnosis["topology"] = "SIMPLE"

        # Diagn√≥stico Girosc√≥pico
        gyro_stability = metrics.get("gyroscopic_stability", 1.0)
        diagnosis["rotation_stability"] = "STABLE"
        if gyro_stability < 0.6:
            diagnosis["rotation_stability"] = "‚ö†Ô∏è PRECESI√ìN DETECTADA (Inestabilidad de Flujo)"
            # Tambi√©n escalamos el estado si es cr√≠tico
            if gyro_stability < 0.3 and diagnosis["state"] == "NORMAL":
                 diagnosis["state"] = "UNSTABLE"

        return diagnosis


# ============================================================================
# DATA FLUX CONDENSER - M√âTODOS REFINADOS
# ============================================================================
class DataFluxCondenser:
    """
    Orquesta el pipeline de validaci√≥n y procesamiento con control adaptativo.
    """

    def __init__(
        self,
        config: Dict[str, Any],
        profile: Dict[str, Any],
        condenser_config: Optional[CondenserConfig] = None,
    ):
        self.logger = logging.getLogger(self.__class__.__name__)
        self.condenser_config = condenser_config or CondenserConfig()
        self.config = config or {}
        self.profile = profile or {}

        try:
            self.physics = FluxPhysicsEngine(
                self.condenser_config.system_capacitance,
                self.condenser_config.base_resistance,
                self.condenser_config.system_inductance,
            )
            self.controller = PIController(
                self.condenser_config.pid_kp,
                self.condenser_config.pid_ki,
                self.condenser_config.pid_setpoint,
                self.condenser_config.min_batch_size,
                self.condenser_config.max_batch_size,
                self.condenser_config.integral_limit_factor,
            )
        except Exception as e:
            raise ConfigurationError(f"Error inicializando componentes: {e}")

        self._stats = ProcessingStats()
        self._start_time: Optional[float] = None
        self._emergency_brake_count: int = 0

    def stabilize(
        self,
        file_path: str,
        on_progress: Optional[Callable[[ProcessingStats], None]] = None,
        progress_callback: Optional[Callable[[Dict[str, Any]], None]] = None,
        telemetry: Optional[TelemetryContext] = None,
    ) -> pd.DataFrame:
        """
        Proceso principal de estabilizaci√≥n con control PID y telemetr√≠a.
        """
        self._start_time = time.time()
        self._stats = ProcessingStats()
        self._emergency_brake_count = 0
        self.controller.reset()

        # Validaci√≥n de entrada
        if not file_path:
            raise InvalidInputError("file_path es requerido")

        path_obj = Path(file_path)
        self.logger.info(f"‚ö° [STABILIZE] Iniciando: {path_obj.name}")

        # Registrar inicio en telemetr√≠a
        if telemetry:
            telemetry.record_event("stabilization_start", {
                "file": path_obj.name,
                "config": asdict(self.condenser_config)
            })

        try:
            validated_path = self._validate_input_file(file_path)
            parser = self._initialize_parser(validated_path)
            raw_records, cache = self._extract_raw_data(parser)

            if not raw_records:
                self.logger.warning("No se encontraron registros para procesar")
                return pd.DataFrame()

            total_records = len(raw_records)
            self._stats.total_records = total_records

            # Verificar l√≠mite de registros
            if total_records > SystemConstants.MAX_RECORDS_LIMIT:
                raise ProcessingError(
                    f"Total de registros ({total_records}) excede l√≠mite "
                    f"({SystemConstants.MAX_RECORDS_LIMIT})"
                )

            processed_batches = self._process_batches_with_pid(
                raw_records,
                cache,
                total_records,
                on_progress,
                progress_callback,
                telemetry,
            )

            df_final = self._consolidate_results(processed_batches)
            self._stats.processing_time = time.time() - self._start_time
            self._validate_output(df_final)

            # Registrar fin en telemetr√≠a
            if telemetry:
                telemetry.record_event("stabilization_complete", {
                    "records_processed": self._stats.processed_records,
                    "processing_time": self._stats.processing_time,
                    "emergency_brakes": self._emergency_brake_count
                })

            self.logger.info(
                f"‚úÖ [STABILIZE] Completado: {self._stats.processed_records} registros "
                f"en {self._stats.processing_time:.2f}s"
            )

            return df_final

        except DataFluxCondenserError as e:
            if telemetry:
                telemetry.record_event("stabilization_error", {"error": str(e)})
            raise
        except Exception as e:
            self.logger.exception(f"Error inesperado en estabilizaci√≥n: {e}")
            if telemetry:
                telemetry.record_event("stabilization_error", {"error": str(e)})
            raise ProcessingError(f"Error fatal: {e}")

    def _validate_input_file(self, file_path: str) -> Path:
        """Valida el archivo de entrada con verificaciones extendidas."""
        path = Path(file_path)

        if not path.exists():
            raise InvalidInputError(f"Archivo no existe: {file_path}")

        if not path.is_file():
            raise InvalidInputError(f"Ruta no es un archivo: {file_path}")

        # Verificar extensi√≥n
        if path.suffix.lower() not in SystemConstants.VALID_FILE_EXTENSIONS:
            raise InvalidInputError(
                f"Extensi√≥n no soportada: {path.suffix}. "
                f"V√°lidas: {SystemConstants.VALID_FILE_EXTENSIONS}"
            )

        # Verificar tama√±o
        file_size = path.stat().st_size
        if file_size < SystemConstants.MIN_FILE_SIZE_BYTES:
            raise InvalidInputError(f"Archivo muy peque√±o: {file_size} bytes")

        max_size_bytes = SystemConstants.MAX_FILE_SIZE_MB * 1024 * 1024
        if file_size > max_size_bytes:
            raise InvalidInputError(
                f"Archivo excede l√≠mite: {file_size / 1024 / 1024:.1f} MB > "
                f"{SystemConstants.MAX_FILE_SIZE_MB} MB"
            )

        return path

    def _initialize_parser(self, path: Path) -> ReportParserCrudo:
        """Inicializa el parser con manejo de errores."""
        try:
            return ReportParserCrudo(str(path), self.profile, self.config)
        except Exception as e:
            raise ProcessingError(f"Error inicializando parser: {e}")

    def _extract_raw_data(self, parser) -> Tuple[List, Dict]:
        """Extrae datos crudos del parser."""
        try:
            raw_records = parser.parse_to_raw()
            cache = parser.get_parse_cache()
            return raw_records, cache
        except Exception as e:
            raise ProcessingError(f"Error extrayendo datos: {e}")

    def _process_batches_with_pid(
        self,
        raw_records: List,
        cache: Dict,
        total_records: int,
        on_progress: Optional[Callable],
        progress_callback: Optional[Callable],
        telemetry: Optional[TelemetryContext],
    ) -> List[pd.DataFrame]:
        """
        Procesa lotes con control PID adaptativo mejorado.

        Mejoras:
        1. Estimaci√≥n de cache_hits m√°s precisa usando cach√© real
        2. Control de frecuencia de batch adaptativo
        3. Predicci√≥n de saturaci√≥n para pre-ajuste
        """
        processed_batches = []
        failed_batches_count = 0
        current_index = 0
        current_batch_size = self.condenser_config.min_batch_size
        iteration = 0
        max_iterations = total_records * SystemConstants.MAX_ITERATIONS_MULTIPLIER

        # Historial para predicci√≥n
        saturation_history = []
        batch_size_history = []

        while current_index < total_records and iteration < max_iterations:
            iteration += 1

            # Determinar rango del batch con predicci√≥n
            end_index = min(current_index + current_batch_size, total_records)
            batch = raw_records[current_index:end_index]
            batch_size = len(batch)

            if batch_size == 0:
                break

            # Calcular tiempo transcurrido
            elapsed_time = time.time() - self._start_time

            # Verificar timeout global con margen de seguridad
            time_remaining = SystemConstants.PROCESSING_TIMEOUT - elapsed_time
            if time_remaining < 0:
                self.logger.error("Timeout de procesamiento alcanzado")
                break
            elif time_remaining < 60:  # √öltimo minuto
                self.logger.warning(f"Quedan {time_remaining:.0f}s para timeout")

            # Estimaci√≥n realista de cache_hits basada en contenido del batch
            cache_hits_est = self._estimate_cache_hits(batch, cache)

            # Calcular m√©tricas f√≠sicas con amortiguamiento no lineal
            metrics = self.physics.calculate_metrics(
                total_records=batch_size,
                cache_hits=cache_hits_est,
                error_count=failed_batches_count,
                processing_time=elapsed_time
            )

            # Predicci√≥n de saturaci√≥n para pr√≥xima iteraci√≥n
            if len(saturation_history) >= 2:
                predicted_saturation = self._predict_next_saturation(saturation_history)
                # Pre-ajustar controlador si se predice saturaci√≥n alta
                if predicted_saturation > 0.8:
                    current_batch_size = max(SystemConstants.MIN_BATCH_SIZE_FLOOR,
                                           int(current_batch_size * 0.8))

            saturation_history.append(metrics.get("saturation", 0.5))
            batch_size_history.append(batch_size)

            # Callback de m√©tricas con diagn√≥stico
            if progress_callback:
                try:
                    enhanced_metrics = {**metrics, "predicted_saturation": predicted_saturation if 'predicted_saturation' in locals() else None}
                    progress_callback(enhanced_metrics)
                except Exception as e:
                    self.logger.warning(f"Error en progress_callback: {e}")

            # Control PID con l√≠mite din√°mico basado en estabilidad girosc√≥pica
            saturation = metrics.get("saturation", 0.5)
            gyro_stability = metrics.get("gyroscopic_stability", 1.0)

            # Reducir agresividad del control si baja estabilidad girosc√≥pica
            if gyro_stability < 0.5:
                effective_saturation = min(saturation, 0.7)  # Limitar entrada al controlador
                self.logger.debug(f"Baja estabilidad girosc√≥pica ({gyro_stability:.2f}), limitando saturaci√≥n")
            else:
                effective_saturation = saturation

            pid_output = self.controller.compute(effective_saturation)

            # Freno de emergencia adaptativo
            power = metrics.get("dissipated_power", 0)
            flyback = metrics.get("flyback_voltage", 0)

            emergency_brake = False
            if power > SystemConstants.OVERHEAT_POWER_THRESHOLD:
                brake_factor = min(0.3, power / (2 * SystemConstants.OVERHEAT_POWER_THRESHOLD))
                pid_output = max(SystemConstants.MIN_BATCH_SIZE_FLOOR,
                               int(pid_output * brake_factor))
                emergency_brake = True
                self.logger.warning(f"üî• OVERHEAT: P={power:.2f}W, aplicando freno {brake_factor:.1%}")

            if flyback > SystemConstants.MAX_FLYBACK_VOLTAGE * 0.8:
                pid_output = max(SystemConstants.MIN_BATCH_SIZE_FLOOR,
                               int(pid_output * 0.6))
                emergency_brake = True
                self.logger.warning(f"‚ö° HIGH FLYBACK: V={flyback:.2f}V")

            if emergency_brake:
                self._emergency_brake_count += 1
                self._stats.emergency_brakes_triggered += 1

            # Procesar batch con recuperaci√≥n inteligente
            result = self._process_single_batch_with_recovery(batch, cache, failed_batches_count)

            if result.success and result.dataframe is not None:
                processed_batches.append(result.dataframe)
                self._stats.add_batch_stats(
                    batch_size=result.records_processed,
                    saturation=saturation,
                    power=power,
                    flyback=flyback,
                    kinetic=metrics.get("kinetic_energy", 0),
                    success=True
                )
                # Reset de contador de fallos consecutivos
                failed_batches_count = max(0, failed_batches_count - 1)
            else:
                failed_batches_count += 1
                self._stats.add_batch_stats(
                    batch_size=batch_size,
                    saturation=saturation,
                    power=power,
                    flyback=flyback,
                    kinetic=metrics.get("kinetic_energy", 0),
                    success=False
                )

                error_msg = result.error_message[:100] + "..." if len(result.error_message) > 100 else result.error_message
                self.logger.error(f"Error procesando batch {iteration}: {error_msg}")

                # Estrategia de recuperaci√≥n adaptativa
                if failed_batches_count >= self.condenser_config.max_failed_batches:
                    if not self.condenser_config.enable_partial_recovery:
                        raise ProcessingError(
                            f"L√≠mite de batches fallidos alcanzado: {failed_batches_count}"
                        )
                    else:
                        # Reducir tama√±o de batch dr√°sticamente para recuperaci√≥n
                        pid_output = SystemConstants.MIN_BATCH_SIZE_FLOOR
                        self.logger.warning(
                            f"Recuperaci√≥n parcial activada, reduciendo batch size a {pid_output}"
                        )

            # Callback de progreso con diagn√≥stico
            if on_progress:
                try:
                    enhanced_stats = self._enhance_stats_with_diagnostics(self._stats, metrics)
                    on_progress(enhanced_stats)
                except Exception as e:
                    self.logger.warning(f"Error en on_progress: {e}")

            # Telemetr√≠a de batch con m√©tricas extendidas
            if telemetry and (iteration % 10 == 0 or emergency_brake):
                telemetry.record_event("batch_processed", {
                    "iteration": iteration,
                    "progress": current_index / total_records,
                    "batch_size": batch_size,
                    "saturation": saturation,
                    "gyroscopic_stability": gyro_stability,
                    "emergency_brake": emergency_brake,
                    "failed_batches_consecutive": failed_batches_count
                })

            # Avanzar √≠ndice y actualizar tama√±o de batch con inercia
            current_index = end_index

            # Suavizar cambios en batch size
            if iteration > 1:
                inertia_factor = 0.7  # Conservar 70% del tama√±o anterior
                current_batch_size = int(inertia_factor * current_batch_size +
                                       (1 - inertia_factor) * pid_output)
            else:
                current_batch_size = pid_output

            current_batch_size = max(SystemConstants.MIN_BATCH_SIZE_FLOOR,
                                   min(current_batch_size, self.condenser_config.max_batch_size))

        return processed_batches

    def _estimate_cache_hits(self, batch: List, cache: Dict) -> int:
        """
        Estima cache_hits basado en similitud con cach√© existente.
        """
        if not cache or not batch:
            return len(batch) // 2  # Estimaci√≥n conservadora

        # M√©trica simple: porcentaje de registros con campos en cach√©
        cache_fields = set(cache.keys())
        hits = 0

        for record in batch[:100]:  # Muestra de 100 registros para eficiencia
            if isinstance(record, dict):
                record_fields = set(record.keys())
                if record_fields & cache_fields:  # Intersecci√≥n no vac√≠a
                    hits += 1

        # Extrapolar a todo el batch
        if len(batch) > 100:
            hit_rate = hits / 100
            hits = int(hit_rate * len(batch))

        return hits

    def _predict_next_saturation(self, history: List[float]) -> float:
        """
        Predice siguiente valor de saturaci√≥n usando regresi√≥n lineal simple.
        """
        if len(history) < 3:
            return history[-1] if history else 0.5

        # Regresi√≥n lineal en las √∫ltimas N muestras
        n = min(5, len(history))
        x = list(range(n))
        y = history[-n:]

        # Calcular pendiente
        sum_x = sum(x)
        sum_y = sum(y)
        sum_xy = sum(x[i] * y[i] for i in range(n))
        sum_x2 = sum(x_i * x_i for x_i in x)

        if n * sum_x2 - sum_x * sum_x == 0:
            return y[-1]

        slope = (n * sum_xy - sum_x * sum_y) / (n * sum_x2 - sum_x * sum_x)

        # Predecir pr√≥ximo valor
        prediction = y[-1] + slope

        return max(0.0, min(1.0, prediction))

    def _process_single_batch_with_recovery(self, batch: List, cache: Dict, consecutive_failures: int) -> BatchResult:
        """
        Procesa un lote individual con estrategias de recuperaci√≥n mejoradas.
        """
        if not batch:
            return BatchResult(success=False, error_message="Batch vac√≠o")

        # Ajustar estrategia basado en fallos consecutivos
        recovery_mode = consecutive_failures > 0

        try:
            parsed_data = ParsedData(batch, cache)

            if recovery_mode:
                # Modo recuperaci√≥n: procesar en sub-lotes m√°s peque√±os
                sub_results = []
                sub_batch_size = max(1, len(batch) // (consecutive_failures + 1))

                for i in range(0, len(batch), sub_batch_size):
                    sub_batch = batch[i:i + sub_batch_size]
                    sub_parsed = ParsedData(sub_batch, cache)

                    try:
                        df_sub = self._rectify_signal(sub_parsed)
                        if df_sub is not None and not df_sub.empty:
                            sub_results.append(df_sub)
                    except Exception as e:
                        self.logger.debug(f"Sub-lote {i//sub_batch_size} fall√≥: {e}")
                        continue

                if sub_results:
                    df = pd.concat(sub_results, ignore_index=True) if len(sub_results) > 1 else sub_results[0]
                    return BatchResult(
                        success=True,
                        dataframe=df,
                        records_processed=len(df)
                    )
                else:
                    return BatchResult(
                        success=False,
                        error_message="Todos los sub-lotes fallaron en modo recuperaci√≥n"
                    )
            else:
                # Modo normal
                df = self._rectify_signal(parsed_data)

                if df is None or df.empty:
                    return BatchResult(
                        success=True,
                        dataframe=pd.DataFrame(),
                        records_processed=0
                    )

                return BatchResult(
                    success=True,
                    dataframe=df,
                    records_processed=len(df)
                )

        except Exception as e:
            # An√°lisis de error para diagn√≥stico
            error_type = type(e).__name__
            error_msg = str(e)

            # Clasificar error para estrategia de recuperaci√≥n
            if "memory" in error_msg.lower() or "MemoryError" in error_type:
                recovery_hint = "REDUCE_BATCH_SIZE"
            elif "timeout" in error_msg.lower():
                recovery_hint = "INCREASE_TIMEOUT"
            elif "connection" in error_msg.lower() or "network" in error_msg.lower():
                recovery_hint = "RETRY_WITH_BACKOFF"
            else:
                recovery_hint = "UNKNOWN"

            enhanced_error = f"{error_type}: {error_msg} [RECOVERY_HINT: {recovery_hint}]"

            return BatchResult(
                success=False,
                error_message=enhanced_error
            )

    def _rectify_signal(self, parsed_data: ParsedData) -> pd.DataFrame:
        """Convierte datos crudos a DataFrame mediante APUProcessor."""
        try:
            processor = APUProcessor(
                self.config,
                self.profile,
                parsed_data.parse_cache
            )
            processor.raw_records = parsed_data.raw_records
            return processor.process_all()
        except Exception as e:
            raise ProcessingError(f"Error en rectificaci√≥n: {e}")

    def _consolidate_results(self, batches: List[pd.DataFrame]) -> pd.DataFrame:
        """Consolida resultados con verificaci√≥n de integridad."""
        if not batches:
            return pd.DataFrame()

        # Filtrar DataFrames vac√≠os
        valid_batches = [df for df in batches if df is not None and not df.empty]

        if not valid_batches:
            return pd.DataFrame()

        # Limitar n√∫mero de batches para evitar memoria excesiva
        if len(valid_batches) > SystemConstants.MAX_BATCHES_TO_CONSOLIDATE:
            self.logger.warning(
                f"Truncando batches: {len(valid_batches)} > "
                f"{SystemConstants.MAX_BATCHES_TO_CONSOLIDATE}"
            )
            valid_batches = valid_batches[:SystemConstants.MAX_BATCHES_TO_CONSOLIDATE]

        try:
            return pd.concat(valid_batches, ignore_index=True)
        except Exception as e:
            raise ProcessingError(f"Error consolidando resultados: {e}")

    def _validate_output(self, df: pd.DataFrame) -> None:
        """Valida el DataFrame de salida."""
        if df.empty:
            self.logger.warning("DataFrame de salida vac√≠o")
            return

        # Verificar registros m√≠nimos si est√° configurado
        if len(df) < self.condenser_config.min_records_threshold:
            msg = (
                f"Registros insuficientes: {len(df)} < "
                f"{self.condenser_config.min_records_threshold}"
            )
            if self.condenser_config.enable_strict_validation:
                raise ProcessingError(msg)
            else:
                self.logger.warning(msg)

    def _enhance_stats_with_diagnostics(self, stats: ProcessingStats, metrics: Dict[str, float]) -> Dict[str, Any]:
        """
        Mejora las estad√≠sticas con diagn√≥stico del sistema.
        """
        base_stats = asdict(stats)

        # Calcular eficiencia
        if stats.total_records > 0:
            efficiency = stats.processed_records / stats.total_records
        else:
            efficiency = 0.0

        # Diagn√≥stico del sistema
        system_health = self.get_system_health()
        physics_diagnosis = self.physics.get_system_diagnosis(metrics)

        enhanced = {
            **base_stats,
            "efficiency": efficiency,
            "throughput": stats.processed_records / max(stats.processing_time, 0.001),
            "system_health": system_health,
            "physics_diagnosis": physics_diagnosis,
            "current_metrics": {
                "saturation": metrics.get("saturation", 0),
                "complexity": metrics.get("complexity", 0),
                "gyroscopic_stability": metrics.get("gyroscopic_stability", 1.0),
                "entropy_ratio": metrics.get("entropy_ratio", 0)
            }
        }

        return enhanced

    def get_processing_stats(self) -> Dict[str, Any]:
        """Retorna estad√≠sticas completas de procesamiento."""
        return {
            "statistics": asdict(self._stats),
            "controller": self.controller.get_diagnostics(),
            "physics": self.physics.get_trend_analysis(),
            "emergency_brakes": self._emergency_brake_count
        }

    def get_system_health(self) -> Dict[str, Any]:
        """Retorna diagn√≥stico de salud del sistema."""
        controller_diag = self.controller.get_stability_analysis()
        physics_trend = self.physics.get_trend_analysis()

        # Determinar estado general
        health = "HEALTHY"
        issues = []

        if controller_diag.get("stability_class") == "POTENTIALLY_UNSTABLE":
            health = "DEGRADED"
            issues.append("Controlador potencialmente inestable")

        if self._emergency_brake_count > 5:
            health = "DEGRADED"
            issues.append(f"M√∫ltiples frenos de emergencia: {self._emergency_brake_count}")

        if self._stats.failed_batches > self._stats.total_batches * 0.1:
            health = "DEGRADED"
            issues.append(f"Alta tasa de fallos: {self._stats.failed_batches}/{self._stats.total_batches}")

        return {
            "health": health,
            "issues": issues,
            "controller_stability": controller_diag.get("stability_class", "UNKNOWN"),
            "processing_efficiency": (
                self._stats.processed_records / max(1, self._stats.total_records)
            ),
            "uptime": time.time() - self._start_time if self._start_time else 0
        }


### Propuesta 2

PIController - M√©todos Refinados

def _validate_control_parameters(
    self, kp: float, ki: float, setpoint: float,
    min_output: int, max_output: int
) -> None:
    """
    Validaci√≥n de par√°metros con criterios de estabilidad basados en 
    el Criterio de Jury para sistemas discretos.
    
    Para un sistema PI discreto con planta de primer orden G(z) = K/(z-a):
    - Estabilidad requiere que todos los polos est√©n dentro del c√≠rculo unitario
    - Condici√≥n necesaria: |a - K*Kp| < 1 y Ki*T < 2*(1 + a - K*Kp)
    """
    errors = []

    if kp <= 0:
        errors.append(f"Kp debe ser positivo para respuesta proporcional, got {kp}")
    if ki < 0:
        errors.append(f"Ki debe ser no-negativo, got {ki}")
    if min_output >= max_output:
        errors.append(f"Rango de salida inv√°lido: [{min_output}, {max_output}]")
    if min_output <= 0:
        errors.append(f"min_output debe ser positivo, got {min_output}")
    if not (0.0 < setpoint < 1.0):
        errors.append(f"setpoint debe estar en (0, 1), got {setpoint}")

    if errors:
        raise ConfigurationError(
            "Errores en par√°metros de control:\n" +
            "\n".join(f"  ‚Ä¢ {e}" for e in errors)
        )

    # Criterio de Jury simplificado para sistema normalizado
    # Asumiendo planta con ganancia unitaria y polo en a ‚âà 0.9
    a_plant = 0.9
    K_plant = 1.0
    
    # Condici√≥n 1: Estabilidad del lazo cerrado
    closed_loop_coeff = abs(a_plant - K_plant * kp)
    if closed_loop_coeff >= 1.0:
        logger.warning(
            f"Criterio de Jury: |a - K¬∑Kp| = {closed_loop_coeff:.3f} ‚â• 1.0 "
            f"indica posible inestabilidad"
        )

    # Condici√≥n 2: Margen integral
    # Para T_s = 1 (muestreo unitario normalizado)
    T_s = 1.0
    integral_margin = 2.0 * (1.0 + a_plant - K_plant * kp)
    if ki * T_s >= integral_margin:
        logger.warning(
            f"Ki¬∑T = {ki * T_s:.3f} ‚â• margen integral {integral_margin:.3f}, "
            f"riesgo de oscilaciones"
        )

    # Relaci√≥n Ki/Kp para respuesta suave
    if kp > 0 and ki / kp > 0.5:
        logger.info(
            f"Ratio Ki/Kp = {ki/kp:.3f} > 0.5: respuesta integral dominante"
        )


def _apply_ema_filter(self, measurement: float) -> float:
    """
    Filtro EMA con alpha adaptativo basado en varianza normalizada
    y detecci√≥n de cambios abruptos (step detection).
    """
    if self._filtered_pv is None:
        self._filtered_pv = measurement
        return measurement

    # Detectar cambio abrupto (step) para bypass del filtro
    if self._last_error is not None:
        step_threshold = 0.3 * abs(self.setpoint)
        if abs(measurement - self._filtered_pv) > step_threshold:
            # Cambio abrupto: reducir inercia del filtro
            self._filtered_pv = 0.5 * measurement + 0.5 * self._filtered_pv
            return self._filtered_pv

    # Alpha adaptativo basado en varianza del error
    if len(self._error_history) >= 3:
        recent_errors = list(self._error_history)[-5:]
        n = len(recent_errors)
        mean_error = sum(recent_errors) / n
        
        # Varianza con correcci√≥n de Bessel para muestras peque√±as
        if n > 1:
            error_variance = sum((e - mean_error)**2 for e in recent_errors) / (n - 1)
        else:
            error_variance = 0.0

        # Mapeo no lineal: varianza alta ‚Üí alpha bajo (m√°s suavizado)
        # Funci√≥n sigmoide inversa para transici√≥n suave
        normalized_var = min(error_variance, 1.0)
        adaptive_alpha = 0.1 + 0.4 / (1.0 + 5.0 * normalized_var)
        self._ema_alpha = max(0.05, min(0.6, adaptive_alpha))

    self._filtered_pv = (
        self._ema_alpha * measurement +
        (1 - self._ema_alpha) * self._filtered_pv
    )
    return self._filtered_pv


def _update_lyapunov_metric(self, error: float) -> None:
    """
    M√©trica de Lyapunov usando funci√≥n candidata V(e) = e¬≤ 
    con estimaci√≥n robusta del exponente mediante regresi√≥n 
    de m√≠nimos cuadrados sobre ventana deslizante.
    """
    # Almacenar |e| para regresi√≥n logar√≠tmica
    abs_error = abs(error) + 1e-12  # Evitar log(0)
    
    if not hasattr(self, '_lyapunov_log_errors'):
        self._lyapunov_log_errors = deque(maxlen=20)
    
    self._lyapunov_log_errors.append(math.log(abs_error))
    
    n = len(self._lyapunov_log_errors)
    if n < 5:
        return
    
    # Regresi√≥n lineal: log|e(k)| = Œª¬∑k + c
    # donde Œª es el exponente de Lyapunov
    log_errors = list(self._lyapunov_log_errors)
    k_vals = list(range(n))
    
    sum_k = sum(k_vals)
    sum_log_e = sum(log_errors)
    sum_k_log_e = sum(k * le for k, le in zip(k_vals, log_errors))
    sum_k2 = sum(k * k for k in k_vals)
    
    denominator = n * sum_k2 - sum_k * sum_k
    if abs(denominator) < 1e-10:
        return
    
    # Pendiente = exponente de Lyapunov estimado
    lyapunov_slope = (n * sum_k_log_e - sum_k * sum_log_e) / denominator
    
    # Filtrado EMA del exponente para estabilidad
    ema_factor = 0.2
    self._lyapunov_sum = (1 - ema_factor) * self._lyapunov_sum + ema_factor * lyapunov_slope
    self._lyapunov_count = max(1, self._lyapunov_count)
    
    # Alerta temprana de inestabilidad con hist√©resis
    if self._lyapunov_sum > 0.15 and n > 10:
        logger.warning(
            f"‚ö†Ô∏è Divergencia detectada: Œª ‚âà {self._lyapunov_sum:.4f} > 0 "
            f"(basado en {n} muestras)"
        )


def compute(self, process_variable: float) -> int:
    """
    Controlador PI con:
    1. Zona muerta (deadband) para reducir actuaci√≥n innecesaria
    2. Anti-windup con back-calculation y clamping condicional
    3. Slew rate limiting para suavidad
    4. Bumpless transfer en cambios de setpoint
    """
    self._iteration_count += 1
    current_time = time.time()

    # Saturar entrada al rango v√°lido
    pv_clamped = max(0.0, min(1.0, process_variable))
    
    # Filtrado EMA adaptativo
    filtered_pv = self._apply_ema_filter(pv_clamped)

    # Error con zona muerta para reducir jitter
    raw_error = self.setpoint - filtered_pv
    deadband = 0.02 * self.setpoint  # 2% del setpoint
    
    if abs(raw_error) < deadband:
        error = 0.0  # Dentro de banda muerta
    else:
        # Suavizar transici√≥n fuera de zona muerta
        error = raw_error - math.copysign(deadband, raw_error)

    self._error_history.append(error)

    # Delta tiempo con l√≠mites de cordura
    if self._last_time is None:
        dt = SystemConstants.MIN_DELTA_TIME
    else:
        dt = max(
            SystemConstants.MIN_DELTA_TIME,
            min(current_time - self._last_time, SystemConstants.MAX_DELTA_TIME)
        )

    # === T√âRMINO PROPORCIONAL ===
    P = self.Kp * error

    # === ANTI-WINDUP: Detecci√≥n previa de saturaci√≥n ===
    # Calcular salida tentativa para decidir si integrar
    tentative_I = self._ki_adaptive * (self._integral_error + error * dt)
    tentative_output = self._output_center + P + tentative_I
    
    will_saturate = (tentative_output > self.max_output or 
                     tentative_output < self.min_output)
    
    # Clamping condicional: no integrar si vamos a saturar 
    # Y el error empuja hacia la saturaci√≥n
    integrating_towards_saturation = (
        (tentative_output > self.max_output and error < 0) or
        (tentative_output < self.min_output and error > 0)
    )
    
    if will_saturate and not integrating_towards_saturation:
        # Solo acumular si el error nos saca de saturaci√≥n
        self._integral_error += error * dt
    elif not will_saturate:
        self._integral_error += error * dt

    # Aplicar l√≠mite integral con suavizado hiperb√≥lico
    if abs(self._integral_error) > self._integral_limit:
        # Soft clamp usando tanh para evitar discontinuidades
        normalized = self._integral_error / self._integral_limit
        self._integral_error = self._integral_limit * math.tanh(normalized)

    # Adaptaci√≥n de ganancia integral
    self._adapt_integral_gain(error, will_saturate)

    # === T√âRMINO INTEGRAL ===
    I = self._ki_adaptive * self._integral_error

    # === C√ÅLCULO DE SALIDA ===
    output_raw = self._output_center + P + I

    # === SLEW RATE LIMITING ===
    # Limitar cambio m√°ximo por iteraci√≥n (anti-jerk)
    if self._last_output is not None:
        max_slew = self._output_range * 0.15  # 15% m√°ximo por paso
        delta = output_raw - self._last_output
        if abs(delta) > max_slew:
            output_raw = self._last_output + math.copysign(max_slew, delta)

    # Saturaci√≥n final
    output = int(round(max(self.min_output, min(self.max_output, output_raw))))

    # === BACK-CALCULATION POST-SATURACI√ìN ===
    # Si saturamos, ajustar integral para tracking
    if output != int(round(output_raw)):
        saturation_error = output_raw - output
        tracking_gain = 1.0 / max(self.Kp, 0.1)  # Kb = 1/Kp (regla de √Östr√∂m)
        self._integral_error -= tracking_gain * saturation_error * dt

    # Actualizar m√©trica de Lyapunov
    self._update_lyapunov_metric(error)

    # Guardar estado
    self._last_time = current_time
    self._last_error = error
    self._last_output = output
    self._output_history.append(output)

    return output


### FluxPhysicsEngine - M√©todos Refinados

def _evolve_state_rk4(self, driving_current: float, dt: float) -> Tuple[float, float]:
    """
    Evoluci√≥n del estado RLC usando Runge-Kutta de 4to orden (RK4).
    
    Mayor precisi√≥n O(dt‚Å¥) vs O(dt¬≤) de RK2, cr√≠tico para 
    sistemas subamortiguados donde la oscilaci√≥n debe preservarse.
    
    Sistema de ecuaciones de estado:
        dQ/dt = I
        dI/dt = (V_in - R¬∑I - Q/C) / L
    """
    Q, I = self._state
    
    # Voltaje de entrada proporcional a corriente de driving
    # con saturaci√≥n suave para evitar sobretensiones
    V_max = 20.0
    V_in = V_max * math.tanh(driving_current)
    
    # Funci√≥n de derivadas del sistema
    def f(q: float, i: float) -> Tuple[float, float]:
        dq_dt = i
        # Resistencia no lineal: aumenta con I¬≤ (efecto Joule)
        R_eff = self.R * (1.0 + 0.1 * i * i)
        di_dt = (V_in - R_eff * i - q / self.C) / self.L
        return dq_dt, di_dt
    
    # RK4 cl√°sico
    k1_q, k1_i = f(Q, I)
    k2_q, k2_i = f(Q + 0.5*dt*k1_q, I + 0.5*dt*k1_i)
    k3_q, k3_i = f(Q + 0.5*dt*k2_q, I + 0.5*dt*k2_i)
    k4_q, k4_i = f(Q + dt*k3_q, I + dt*k3_i)
    
    Q_new = Q + (dt/6.0) * (k1_q + 2*k2_q + 2*k3_q + k4_q)
    I_new = I + (dt/6.0) * (k1_i + 2*k2_i + 2*k3_i + k4_i)
    
    # === LIMITADOR DE ENERG√çA ===
    # Prevenir acumulaci√≥n infinita de energ√≠a (estabilidad num√©rica)
    E_max = 100.0  # Energ√≠a m√°xima permitida
    E_current = 0.5 * self.L * I_new**2 + 0.5 * (Q_new**2) / self.C
    
    if E_current > E_max:
        # Escalar estado para limitar energ√≠a (conservando proporciones)
        scale = math.sqrt(E_max / E_current)
        Q_new *= scale
        I_new *= scale
        self._nonlinear_damping_factor = scale
        self.logger.debug(f"Energ√≠a limitada: {E_current:.2f} ‚Üí {E_max:.2f} J")
    else:
        # Amortiguamiento no lineal suave para alta energ√≠a
        damping = 1.0 / (1.0 + 0.05 * max(0, E_current - E_max * 0.5))
        I_new *= damping
        self._nonlinear_damping_factor = damping

    self._state = [Q_new, I_new]
    
    self._state_history.append({
        'Q': Q_new,
        'I': I_new,
        'time': time.time(),
        'energy': 0.5 * self.L * I_new**2 + 0.5 * (Q_new**2) / self.C,
        'V_in': V_in
    })

    return Q_new, I_new


def _build_metric_graph(self, metrics: Dict[str, float]) -> None:
    """
    Construye grafo de correlaci√≥n con umbral adaptativo basado en
    correlaci√≥n de Spearman (robusta a outliers) sobre historial.
    """
    metric_keys = ['saturation', 'complexity', 'current_I', 
                   'potential_energy', 'kinetic_energy', 'entropy_shannon']
    values = [metrics.get(k, 0.0) for k in metric_keys]
    
    self._adjacency_list.clear()
    self._vertex_count = len(values)
    self._edge_count = 0
    
    for i in range(self._vertex_count):
        self._adjacency_list[i] = set()
    
    if self._vertex_count < 2:
        return
    
    # Calcular matriz de distancias normalizadas
    # Usar distancia de correlaci√≥n: d = 1 - |corr|
    
    # Normalizar valores al rango [0, 1]
    v_min = min(values)
    v_max = max(values)
    v_range = v_max - v_min if v_max != v_min else 1.0
    normalized = [(v - v_min) / v_range for v in values]
    
    # Umbral adaptativo basado en dispersi√≥n
    mean_val = sum(normalized) / len(normalized)
    variance = sum((v - mean_val)**2 for v in normalized) / len(normalized)
    
    # Mayor varianza ‚Üí umbral m√°s permisivo para capturar estructura
    base_threshold = 0.3
    adaptive_threshold = base_threshold * (1.0 + math.sqrt(variance))
    adaptive_threshold = min(0.7, adaptive_threshold)  # Cap m√°ximo
    
    # Crear aristas basadas en proximidad en espacio normalizado
    for i in range(self._vertex_count):
        for j in range(i + 1, self._vertex_count):
            # Distancia euclidiana normalizada
            dist = abs(normalized[i] - normalized[j])
            
            # Correlaci√≥n impl√≠cita: valores cercanos est√°n correlacionados
            if dist < adaptive_threshold:
                self._adjacency_list[i].add(j)
                self._adjacency_list[j].add(i)
                self._edge_count += 1


def _calculate_betti_numbers(self) -> Dict[int, int]:
    """
    Calcula n√∫meros de Betti para el complejo simplicial del grafo.
    
    Para grafos simples (complejos de dimensi√≥n 1):
    - Œ≤‚ÇÄ = componentes conexas (dim del kernel de ‚àÇ‚ÇÅ)
    - Œ≤‚ÇÅ = ciclos independientes = E - V + Œ≤‚ÇÄ (por Euler-Poincar√©)
    - Œ≤_k = 0 para k ‚â• 2
    
    Tambi√©n calcula la caracter√≠stica de Euler: œá = Œ≤‚ÇÄ - Œ≤‚ÇÅ
    """
    if self._vertex_count == 0:
        return {0: 0, 1: 0, 2: 0, 'euler_characteristic': 0}
    
    # === CALCULAR Œ≤‚ÇÄ: COMPONENTES CONEXAS ===
    # Union-Find para eficiencia O(V¬∑Œ±(V))
    parent = list(range(self._vertex_count))
    rank = [0] * self._vertex_count
    
    def find(x: int) -> int:
        if parent[x] != x:
            parent[x] = find(parent[x])  # Compresi√≥n de caminos
        return parent[x]
    
    def union(x: int, y: int) -> None:
        rx, ry = find(x), find(y)
        if rx == ry:
            return
        # Union por rango
        if rank[rx] < rank[ry]:
            rx, ry = ry, rx
        parent[ry] = rx
        if rank[rx] == rank[ry]:
            rank[rx] += 1
    
    # Procesar todas las aristas
    for v in range(self._vertex_count):
        for neighbor in self._adjacency_list.get(v, set()):
            if neighbor > v:  # Evitar procesar dos veces
                union(v, neighbor)
    
    # Contar componentes (ra√≠ces √∫nicas)
    beta_0 = len(set(find(v) for v in range(self._vertex_count)))
    
    # === CALCULAR Œ≤‚ÇÅ: CICLOS INDEPENDIENTES ===
    # Por teorema de Euler-Poincar√© para grafos: V - E + F = 2
    # Para grafos planares sin caras internas: Œ≤‚ÇÅ = E - V + Œ≤‚ÇÄ
    beta_1 = max(0, self._edge_count - self._vertex_count + beta_0)
    
    # Caracter√≠stica de Euler
    euler_char = beta_0 - beta_1
    
    return {
        0: beta_0, 
        1: beta_1, 
        2: 0,
        'euler_characteristic': euler_char,
        'is_tree': beta_1 == 0 and beta_0 == 1,
        'is_forest': beta_1 == 0,
        'cyclomatic_complexity': beta_1 + 1  # McCabe para grafos de flujo
    }


def calculate_system_entropy(
    self,
    total_records: int,
    error_count: int,
    processing_time: float
) -> Dict[str, float]:
    """
    Entrop√≠a del sistema con estimadores robustos:
    1. Shannon con correcci√≥n de Horvitz-Thompson para muestreo
    2. R√©nyi de orden 2 (entrop√≠a de colisi√≥n)
    3. Entrop√≠a condicional H(Error|Time)
    """
    if total_records <= 0:
        return self._get_zero_entropy()
    
    success_count = max(0, total_records - error_count)
    
    # Probabilidades con suavizado de Laplace (evita log(0))
    # Equivalente a prior uniforme Beta(1,1)
    alpha = 1  # Par√°metro de suavizado
    p_success = (success_count + alpha) / (total_records + 2 * alpha)
    p_error = (error_count + alpha) / (total_records + 2 * alpha)
    
    # === ENTROP√çA DE SHANNON ===
    H_shannon = 0.0
    for p in [p_success, p_error]:
        if p > 0:
            H_shannon -= p * math.log2(p)
    
    # Correcci√≥n de Miller-Madow para sesgo de muestras finitas
    m = 2  # N√∫mero de categor√≠as
    if total_records > m:
        H_shannon_corrected = H_shannon + (m - 1) / (2 * total_records * math.log(2))
    else:
        H_shannon_corrected = H_shannon
    
    # === ENTROP√çA DE R√âNYI (orden Œ±=2) ===
    # H‚ÇÇ = -log‚ÇÇ(Œ£p·µ¢¬≤) - m√°s sensible a probabilidades dominantes
    sum_p_squared = p_success**2 + p_error**2
    H_renyi_2 = -math.log2(sum_p_squared) if sum_p_squared > 0 else 0.0
    
    # === ENTROP√çA DE TSALLIS (q=2) ===
    # Coincide con √≠ndice de Gini-Simpson
    q = 2.0
    H_tsallis = (1 - sum_p_squared) / (q - 1)
    
    # === DIVERGENCIA KL RESPECTO A UNIFORME ===
    uniform_p = 0.5
    D_kl = 0.0
    for p in [p_success, p_error]:
        if p > 0:
            D_kl += p * math.log2(p / uniform_p)
    
    # === TASA DE PRODUCCI√ìN DE ENTROP√çA ===
    processing_time_safe = max(processing_time, 1e-6)
    entropy_rate = H_shannon / processing_time_safe
    
    # === INFORMACI√ìN MUTUA TEMPORAL ===
    # Aproximaci√≥n: cu√°nta informaci√≥n aporta el tiempo sobre el resultado
    # Usando historial de entrop√≠as
    mutual_info_temporal = 0.0
    if len(self._entropy_history) >= 2:
        prev_entropy = self._entropy_history[-1].get('shannon_entropy', H_shannon)
        # Cambio en entrop√≠a normalizado
        mutual_info_temporal = abs(H_shannon - prev_entropy) / max(H_shannon, prev_entropy, 0.01)
    
    # === DIAGN√ìSTICO TERMODIN√ÅMICO ===
    max_entropy = 1.0  # log‚ÇÇ(2) para sistema binario
    entropy_ratio = H_shannon / max_entropy
    
    # Detectar "muerte t√©rmica" (m√°xima entrop√≠a = m√°xima incertidumbre)
    is_thermal_death = entropy_ratio > 0.95 and error_count > total_records * 0.4
    
    result = {
        "shannon_entropy": H_shannon,
        "shannon_entropy_corrected": H_shannon_corrected,
        "renyi_entropy_2": H_renyi_2,
        "tsallis_entropy": H_tsallis,
        "kl_divergence": D_kl,
        "entropy_rate": entropy_rate,
        "entropy_ratio": entropy_ratio,
        "mutual_info_temporal": mutual_info_temporal,
        "max_entropy": max_entropy,
        "is_thermal_death": is_thermal_death,
        # Alias para compatibilidad
        "entropy_absolute": H_shannon,
        "configurational_entropy": H_renyi_2,  # Usar R√©nyi como configuracional
    }
    
    self._entropy_history.append({
        **result,
        'timestamp': time.time(),
        'total_records': total_records,
        'error_rate': error_count / total_records
    })
    
    return result


def calculate_gyroscopic_stability(self, current_I: float) -> float:
    """
    Estabilidad Girosc√≥pica basada en modelo de trompo sim√©trico.
    
    Usa el criterio de estabilidad de Routh para rotaci√≥n:
    - œâ (velocidad angular) ~ |I| (corriente como proxy de "rotaci√≥n")
    - Precesi√≥n detectada cuando dœâ/dt causa desviaci√≥n del eje
    
    Sg = exp(-Œ∫|dI/dt|) ¬∑ cos(Œ∏_precesi√≥n) ¬∑ factor_inercia
    """
    if not self._initialized:
        self._ema_current = current_I
        self._last_current = current_I
        self._last_time = time.time()
        self._initialized = True
        return 1.0
    
    current_time = time.time()
    dt = max(1e-6, current_time - self._last_time)
    
    # === DERIVADA DE LA CORRIENTE (aceleraci√≥n angular) ===
    dI_dt = (current_I - self._last_current) / dt
    
    # === ACTUALIZAR EJE DE ROTACI√ìN (EMA) ===
    alpha = SystemConstants.GYRO_EMA_ALPHA
    self._ema_current = alpha * current_I + (1 - alpha) * self._ema_current
    
    # === √ÅNGULO DE PRECESI√ìN ===
    # Desviaci√≥n del eje actual respecto al eje estabilizado
    axis_deviation = current_I - self._ema_current
    precession_angle = math.atan2(axis_deviation, 1.0 + abs(self._ema_current))
    
    # === FACTOR DE ESTABILIDAD POR VELOCIDAD ===
    # Mayor velocidad angular ‚Üí mayor estabilidad girosc√≥pica
    # (efecto de conservaci√≥n del momento angular)
    omega_normalized = abs(self._ema_current)
    inertia_factor = 1.0 - math.exp(-2.0 * omega_normalized)
    
    # === T√âRMINO DE NUTACI√ìN ===
    # Oscilaci√≥n r√°pida del eje - detectada por cambio en signo de dI/dt
    if not hasattr(self, '_last_dI_dt'):
        self._last_dI_dt = dI_dt
        nutation_factor = 1.0
    else:
        # Detecci√≥n de cambio de signo (nutaci√≥n)
        if self._last_dI_dt * dI_dt < 0:
            nutation_factor = 0.8  # Penalizar nutaci√≥n
        else:
            nutation_factor = 1.0
        self._last_dI_dt = dI_dt
    
    # === C√ÅLCULO FINAL DE ESTABILIDAD ===
    sensitivity = SystemConstants.GYRO_SENSITIVITY
    
    # Componente exponencial por tasa de cambio
    exp_term = math.exp(-sensitivity * abs(dI_dt))
    
    # Componente de precesi√≥n
    precession_term = math.cos(precession_angle)
    
    # Estabilidad combinada
    Sg = exp_term * precession_term * inertia_factor * nutation_factor
    
    # Normalizar a [0, 1] con suavizado
    Sg_normalized = (Sg + 1.0) / 2.0
    Sg_normalized = max(0.0, min(1.0, Sg_normalized))
    
    # Actualizar estado
    self._last_current = current_I
    self._last_time = current_time
    
    return Sg_normalized


### DataFluxCondenser - M√©todos Refinados

def _estimate_cache_hits(self, batch: List, cache: Dict) -> int:
    """
    Estimaci√≥n probabil√≠stica de cache hits usando modelo Bayesiano
    con prior basado en historial de hits anteriores.
    """
    if not batch:
        return 0
    
    if not cache:
        # Sin cach√©: asumir miss rate alto
        return max(1, len(batch) // 4)
    
    # === PRIOR BASADO EN HISTORIAL ===
    if not hasattr(self, '_cache_hit_history'):
        self._cache_hit_history = deque(maxlen=50)
    
    if self._cache_hit_history:
        # Media m√≥vil del hit rate hist√≥rico como prior
        prior_hit_rate = sum(self._cache_hit_history) / len(self._cache_hit_history)
    else:
        prior_hit_rate = 0.5  # Prior no informativo
    
    # === LIKELIHOOD POR MUESTREO ===
    sample_size = min(50, len(batch))
    sample_indices = range(0, len(batch), max(1, len(batch) // sample_size))
    
    cache_field_set = set(cache.keys())
    sample_hits = 0
    
    for idx in sample_indices:
        if idx < len(batch):
            record = batch[idx]
            if isinstance(record, dict):
                # Hit si hay intersecci√≥n significativa con campos de cach√©
                record_fields = set(record.keys())
                overlap = len(record_fields & cache_field_set)
                total_fields = len(record_fields | cache_field_set)
                
                if total_fields > 0:
                    # Jaccard similarity como proxy de hit
                    jaccard = overlap / total_fields
                    if jaccard > 0.3:  # Umbral de similitud
                        sample_hits += 1
    
    # Likelihood del sample
    actual_sample_size = len(list(sample_indices))
    if actual_sample_size > 0:
        sample_hit_rate = sample_hits / actual_sample_size
    else:
        sample_hit_rate = prior_hit_rate
    
    # === POSTERIOR BAYESIANO ===
    # Combinar prior y likelihood con pesos
    prior_weight = min(len(self._cache_hit_history) / 20, 0.5)  # Max 50% prior
    posterior_hit_rate = prior_weight * prior_hit_rate + (1 - prior_weight) * sample_hit_rate
    
    # Calcular hits estimados
    estimated_hits = int(posterior_hit_rate * len(batch))
    
    # Actualizar historial
    self._cache_hit_history.append(sample_hit_rate)
    
    return max(1, estimated_hits)


def _predict_next_saturation(self, history: List[float]) -> float:
    """
    Predicci√≥n de saturaci√≥n usando filtro de Kalman simplificado
    para mejor estimaci√≥n en presencia de ruido.
    """
    if len(history) < 2:
        return history[-1] if history else 0.5
    
    # === INICIALIZACI√ìN DEL FILTRO ===
    if not hasattr(self, '_kalman_state'):
        self._kalman_state = {
            'x': history[-1],      # Estado estimado
            'P': 1.0,              # Covarianza del error
            'Q': 0.01,             # Ruido del proceso
            'R': 0.1               # Ruido de medici√≥n
        }
    
    ks = self._kalman_state
    
    # === PREDICCI√ìN ===
    # Modelo: x(k+1) = x(k) + v(k) donde v(k) es tendencia
    # Estimar tendencia de las √∫ltimas muestras
    n = min(5, len(history))
    if n >= 2:
        trend = (history[-1] - history[-n]) / (n - 1)
    else:
        trend = 0.0
    
    # Limitar tendencia para estabilidad
    trend = max(-0.2, min(0.2, trend))
    
    x_pred = ks['x'] + trend
    P_pred = ks['P'] + ks['Q']
    
    # === ACTUALIZACI√ìN (Correcci√≥n) ===
    z = history[-1]  # Medici√≥n actual
    
    # Ganancia de Kalman
    K = P_pred / (P_pred + ks['R'])
    
    # Actualizar estado
    x_new = x_pred + K * (z - x_pred)
    P_new = (1 - K) * P_pred
    
    # Guardar estado
    ks['x'] = x_new
    ks['P'] = P_new
    
    # Adaptar ruido del proceso basado en error de predicci√≥n
    prediction_error = abs(z - x_pred)
    ks['Q'] = 0.9 * ks['Q'] + 0.1 * prediction_error**2
    
    # Predicci√≥n del pr√≥ximo valor
    next_prediction = x_new + trend
    
    # Saturar al rango v√°lido
    return max(0.0, min(1.0, next_prediction))


def _process_single_batch_with_recovery(
    self, 
    batch: List, 
    cache: Dict, 
    consecutive_failures: int
) -> BatchResult:
    """
    Procesamiento con estrategia de recuperaci√≥n multinivel:
    1. Intento normal
    2. Divisi√≥n binaria recursiva
    3. Procesamiento elemento por elemento
    4. Skip con logging
    """
    if not batch:
        return BatchResult(success=True, records_processed=0, dataframe=pd.DataFrame())
    
    batch_size = len(batch)
    
    # === NIVEL 0: INTENTO NORMAL ===
    if consecutive_failures == 0:
        try:
            parsed_data = ParsedData(batch, cache)
            df = self._rectify_signal(parsed_data)
            
            if df is None:
                df = pd.DataFrame()
            
            return BatchResult(
                success=True,
                dataframe=df,
                records_processed=len(df)
            )
        except Exception as e:
            self.logger.debug(f"Intento normal fall√≥: {e}")
            # Continuar a recuperaci√≥n
    
    # === NIVEL 1: DIVISI√ìN BINARIA ===
    if consecutive_failures <= 2 and batch_size > 10:
        try:
            mid = batch_size // 2
            result_left = self._process_single_batch_with_recovery(
                batch[:mid], cache, consecutive_failures + 1
            )
            result_right = self._process_single_batch_with_recovery(
                batch[mid:], cache, consecutive_failures + 1
            )
            
            dfs = []
            records = 0
            
            if result_left.success and result_left.dataframe is not None:
                dfs.append(result_left.dataframe)
                records += result_left.records_processed
            
            if result_right.success and result_right.dataframe is not None:
                dfs.append(result_right.dataframe)
                records += result_right.records_processed
            
            if dfs:
                combined = pd.concat(dfs, ignore_index=True) if len(dfs) > 1 else dfs[0]
                return BatchResult(
                    success=True,
                    dataframe=combined,
                    records_processed=records
                )
        except Exception as e:
            self.logger.debug(f"Divisi√≥n binaria fall√≥: {e}")
    
    # === NIVEL 2: PROCESAMIENTO INDIVIDUAL ===
    if batch_size <= 100:
        successful_records = []
        
        for i, record in enumerate(batch):
            try:
                parsed_single = ParsedData([record], cache)
                df_single = self._rectify_signal(parsed_single)
                
                if df_single is not None and not df_single.empty:
                    successful_records.append(df_single)
            except Exception:
                # Skip silencioso en modo recuperaci√≥n
                continue
        
        if successful_records:
            combined = pd.concat(successful_records, ignore_index=True)
            return BatchResult(
                success=True,
                dataframe=combined,
                records_processed=len(combined),
                error_message=f"Recuperaci√≥n parcial: {len(combined)}/{batch_size}"
            )
    
    # === NIVEL 3: FALLO TOTAL ===
    return BatchResult(
        success=False,
        error_message=f"Recuperaci√≥n fallida para batch de {batch_size} registros",
        records_processed=0
    )


def _process_batches_with_pid(
    self,
    raw_records: List,
    cache: Dict,
    total_records: int,
    on_progress: Optional[Callable],
    progress_callback: Optional[Callable],
    telemetry: Optional[TelemetryContext],
) -> List[pd.DataFrame]:
    """
    Procesamiento con control PID mejorado:
    1. Predicci√≥n anticipativa con Kalman
    2. Control feedforward basado en complejidad estimada
    3. Detecci√≥n de r√©gimen estacionario para optimizaci√≥n
    """
    processed_batches = []
    failed_batches_count = 0
    current_index = 0
    current_batch_size = self.condenser_config.min_batch_size
    iteration = 0
    max_iterations = total_records * SystemConstants.MAX_ITERATIONS_MULTIPLIER

    # Historiales para an√°lisis
    saturation_history = []
    batch_size_history = []
    
    # Detecci√≥n de r√©gimen estacionario
    steady_state_counter = 0
    steady_state_threshold = 5
    
    # Control feedforward
    last_complexity = 0.5

    while current_index < total_records and iteration < max_iterations:
        iteration += 1

        # Determinar rango del batch
        end_index = min(current_index + current_batch_size, total_records)
        batch = raw_records[current_index:end_index]
        batch_size = len(batch)

        if batch_size == 0:
            break

        elapsed_time = time.time() - self._start_time

        # === VERIFICACI√ìN DE TIMEOUT ===
        time_remaining = SystemConstants.PROCESSING_TIMEOUT - elapsed_time
        if time_remaining <= 0:
            self.logger.error("‚è∞ Timeout de procesamiento alcanzado")
            break

        # === ESTIMACI√ìN DE CACHE HITS ===
        cache_hits_est = self._estimate_cache_hits(batch, cache)

        # === M√âTRICAS F√çSICAS ===
        metrics = self.physics.calculate_metrics(
            total_records=batch_size,
            cache_hits=cache_hits_est,
            error_count=failed_batches_count,
            processing_time=elapsed_time
        )

        saturation = metrics.get("saturation", 0.5)
        complexity = metrics.get("complexity", 0.5)
        power = metrics.get("dissipated_power", 0)
        flyback = metrics.get("flyback_voltage", 0)
        gyro_stability = metrics.get("gyroscopic_stability", 1.0)

        # === PREDICCI√ìN ANTICIPATIVA ===
        saturation_history.append(saturation)
        if len(saturation_history) >= 3:
            predicted_sat = self._predict_next_saturation(saturation_history)
        else:
            predicted_sat = saturation

        # === CONTROL FEEDFORWARD ===
        # Ajuste anticipativo basado en cambio de complejidad
        complexity_delta = complexity - last_complexity
        feedforward_adjustment = 1.0
        
        if complexity_delta > 0.1:
            # Complejidad aumentando: reducir batch preventivamente
            feedforward_adjustment = 0.85
        elif complexity_delta < -0.1:
            # Complejidad disminuyendo: podemos aumentar batch
            feedforward_adjustment = 1.1
        
        last_complexity = complexity

        # === DETECCI√ìN DE R√âGIMEN ESTACIONARIO ===
        if len(saturation_history) >= 3:
            recent_var = sum(
                (s - saturation)**2 
                for s in saturation_history[-3:]
            ) / 3
            
            if recent_var < 0.01:  # Baja varianza
                steady_state_counter += 1
            else:
                steady_state_counter = 0
        
        in_steady_state = steady_state_counter >= steady_state_threshold

        # === CALLBACK DE M√âTRICAS ===
        if progress_callback:
            try:
                progress_callback({
                    **metrics,
                    "predicted_saturation": predicted_sat,
                    "in_steady_state": in_steady_state,
                    "feedforward_adjustment": feedforward_adjustment
                })
            except Exception as e:
                self.logger.warning(f"Error en progress_callback: {e}")

        # === CONTROL PID CON AJUSTES ===
        # Usar saturaci√≥n efectiva considerando estabilidad girosc√≥pica
        if gyro_stability < 0.5:
            effective_saturation = min(saturation + 0.2, 0.9)
            self.logger.debug(
                f"Baja estabilidad girosc√≥pica ({gyro_stability:.2f}): "
                f"inflando saturaci√≥n {saturation:.2f} ‚Üí {effective_saturation:.2f}"
            )
        else:
            effective_saturation = saturation

        pid_output = self.controller.compute(effective_saturation)

        # Aplicar feedforward
        pid_output = int(pid_output * feedforward_adjustment)

        # === FRENOS DE EMERGENCIA ===
        emergency_brake = False
        brake_reason = ""

        if power > SystemConstants.OVERHEAT_POWER_THRESHOLD:
            brake_factor = 0.3
            pid_output = max(SystemConstants.MIN_BATCH_SIZE_FLOOR,
                           int(pid_output * brake_factor))
            emergency_brake = True
            brake_reason = f"OVERHEAT P={power:.1f}W"

        if flyback > SystemConstants.MAX_FLYBACK_VOLTAGE * 0.7:
            pid_output = max(SystemConstants.MIN_BATCH_SIZE_FLOOR,
                           int(pid_output * 0.5))
            emergency_brake = True
            brake_reason = f"FLYBACK V={flyback:.2f}V"

        if predicted_sat > 0.9 and not in_steady_state:
            pid_output = max(SystemConstants.MIN_BATCH_SIZE_FLOOR,
                           int(pid_output * 0.7))
            emergency_brake = True
            brake_reason = f"PREDICTED_SAT={predicted_sat:.2f}"

        if emergency_brake:
            self._emergency_brake_count += 1
            self._stats.emergency_brakes_triggered += 1
            self.logger.warning(f"üõë EMERGENCY BRAKE: {brake_reason}")

        # === PROCESAMIENTO DEL BATCH ===
        result = self._process_single_batch_with_recovery(
            batch, cache, failed_batches_count
        )

        # Actualizar estad√≠sticas
        if result.success and result.dataframe is not None:
            if not result.dataframe.empty:
                processed_batches.append(result.dataframe)
            
            self._stats.add_batch_stats(
                batch_size=result.records_processed,
                saturation=saturation,
                power=power,
                flyback=flyback,
                kinetic=metrics.get("kinetic_energy", 0),
                success=True
            )
            failed_batches_count = max(0, failed_batches_count - 1)
        else:
            failed_batches_count += 1
            self._stats.add_batch_stats(
                batch_size=batch_size,
                saturation=saturation,
                power=power,
                flyback=flyback,
                kinetic=metrics.get("kinetic_energy", 0),
                success=False
            )
            
            if failed_batches_count >= self.condenser_config.max_failed_batches:
                if not self.condenser_config.enable_partial_recovery:
                    raise ProcessingError(
                        f"L√≠mite de batches fallidos: {failed_batches_count}"
                    )
                # Recuperaci√≥n extrema
                pid_output = SystemConstants.MIN_BATCH_SIZE_FLOOR
                self.logger.warning("Activando recuperaci√≥n extrema")

        # === CALLBACK DE PROGRESO ===
        if on_progress:
            try:
                on_progress(self._stats)
            except Exception as e:
                self.logger.warning(f"Error en on_progress: {e}")

        # === TELEMETR√çA ===
        if telemetry and (iteration % 10 == 0 or emergency_brake):
            telemetry.record_event("batch_iteration", {
                "iteration": iteration,
                "progress": current_index / total_records,
                "batch_size": batch_size,
                "pid_output": pid_output,
                "saturation": saturation,
                "predicted_saturation": predicted_sat,
                "in_steady_state": in_steady_state,
                "emergency_brake": emergency_brake
            })

        # === ACTUALIZAR PARA SIGUIENTE ITERACI√ìN ===
        current_index = end_index
        batch_size_history.append(current_batch_size)

        # Suavizado inercial del tama√±o de batch
        # Mayor inercia en estado estacionario
        inertia = 0.8 if in_steady_state else 0.6
        current_batch_size = int(
            inertia * current_batch_size + 
            (1 - inertia) * pid_output
        )
        
        # Aplicar l√≠mites
        current_batch_size = max(
            SystemConstants.MIN_BATCH_SIZE_FLOOR,
            min(current_batch_size, self.condenser_config.max_batch_size)
        )

    return processed_batches


### 