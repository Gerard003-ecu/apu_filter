# ==================== LinearInteractionMatrix REFINADO ====================

class LinearInteractionMatrix:
    """
    Implementaci√≥n algebraica rigurosa de la MIC como Operador Diagonal.
    
    Refinamiento: Incorpora m√©trica de Killing-Cartan para verificaci√≥n
    de ortogonalidad y c√≥mputo espectral basado en teor√≠a de representaciones.
    """

    def __init__(self):
        self._basis: Dict[str, BasisVector] = {}
        self._dimension = 0
        self._gram_matrix: Optional[np.ndarray] = None
        self._orthonormal_basis_computed = False
        self._killing_form_cache: Optional[np.ndarray] = None

    def get_rank(self) -> int:
        """Retorna el rango de la matriz (Dimensi√≥n del espacio imagen)."""
        if self._gram_matrix is not None:
            return int(np.linalg.matrix_rank(self._gram_matrix))
        return self._dimension

    def add_basis_vector(
        self, 
        label: str, 
        step_class: Type[ProcessingStep], 
        stratum: Stratum
    ):
        """
        Expande el espacio vectorial con verificaci√≥n de independencia lineal.

        Invariante: ‚àÄe_i, e_j ‚àà B, i ‚â† j ‚Üí <e_i, e_j>_K = 0
        donde <¬∑,¬∑>_K es la forma de Killing.
        """
        if not label or not isinstance(label, str):
            raise ValueError("Label debe ser una cadena no vac√≠a")
            
        if label in self._basis:
            raise ValueError(
                f"Dependencia Lineal: '{label}' viola independencia. "
                f"Base actual: {list(self._basis.keys())}"
            )

        if not (isinstance(step_class, type) and issubclass(step_class, ProcessingStep)):
            raise TypeError(
                f"El operador {step_class} no es lineal "
                f"(debe ser subclase de ProcessingStep)"
            )

        vector = BasisVector(
            index=self._dimension,
            label=label,
            operator_class=step_class,
            stratum=stratum
        )

        self._verify_orthogonality_killing(vector)

        self._basis[label] = vector
        self._dimension += 1
        self._invalidate_caches()

        logger.debug(
            f"üìê Vector base a√±adido: {label} (dim={self._dimension}, "
            f"estrato={stratum.name})"
        )

    def _invalidate_caches(self):
        """Invalida caches tras modificaci√≥n de la base."""
        self._orthonormal_basis_computed = False
        self._gram_matrix = None
        self._killing_form_cache = None

    def _verify_orthogonality_killing(self, new_vector: BasisVector):
        """
        Verifica ortogonalidad usando forma de Killing generalizada.
        
        La forma de Killing K(X,Y) = Tr(ad_X ‚àò ad_Y) se aproxima aqu√≠
        mediante una m√©trica funcional basada en:
        - Coincidencia de operador (colinealidad directa)
        - Coincidencia de estrato con diferente operador (interferencia)
        - Solapamiento de dominio/codominio funcional
        """
        for existing_label, existing_vector in self._basis.items():
            # Colinealidad directa: mismo operador
            if existing_vector.operator_class == new_vector.operator_class:
                raise ValueError(
                    f"Colinealidad funcional: '{new_vector.label}' usa mismo "
                    f"operador que '{existing_label}'"
                )
            
            # Verificar interferencia de estrato con an√°lisis de firma
            killing_value = self._compute_killing_pairing(existing_vector, new_vector)
            
            if abs(killing_value) > 0.95:  # Umbral de cuasi-colinealidad
                raise ValueError(
                    f"Cuasi-colinealidad detectada: K({new_vector.label}, "
                    f"{existing_label}) = {killing_value:.3f}"
                )

    def _compute_killing_pairing(
        self, 
        v1: BasisVector, 
        v2: BasisVector
    ) -> float:
        """
        Computa el apareamiento de Killing entre dos vectores base.
        
        Retorna valor en [-1, 1] donde:
        - 0: Ortogonales (independientes)
        - ¬±1: Paralelos (dependientes)
        """
        # Factor por coincidencia de estrato
        stratum_factor = 1.0 if v1.stratum == v2.stratum else 0.3
        
        # Factor por proximidad de √≠ndice (localidad en la filtraci√≥n)
        index_distance = abs(v1.index - v2.index)
        locality_factor = 1.0 / (1.0 + index_distance)
        
        # Factor por an√°lisis de firma del operador
        signature_similarity = self._compute_operator_signature_similarity(
            v1.operator_class, v2.operator_class
        )
        
        killing_value = stratum_factor * locality_factor * signature_similarity
        return np.clip(killing_value, -1.0, 1.0)

    def _compute_operator_signature_similarity(
        self,
        op1: Type[ProcessingStep],
        op2: Type[ProcessingStep]
    ) -> float:
        """
        Calcula similitud de firma entre operadores usando introspecci√≥n.
        """
        try:
            import inspect
            
            sig1 = inspect.signature(op1.execute)
            sig2 = inspect.signature(op2.execute)
            
            params1 = set(sig1.parameters.keys()) - {'self'}
            params2 = set(sig2.parameters.keys()) - {'self'}
            
            if not params1 or not params2:
                return 0.0
                
            intersection = len(params1 & params2)
            union = len(params1 | params2)
            
            return intersection / union if union > 0 else 0.0
            
        except Exception:
            return 0.0

    def project_intent(self, intent_label: str) -> BasisVector:
        """
        Proyecci√≥n ortogonal del vector de intenci√≥n sobre la base E.
        
        Implementa: proj_E(q) = Œ£_i <q, e_i> e_i / ||e_i||¬≤
        En espacio discreto se reduce a b√∫squeda exacta con validaci√≥n.
        """
        if not intent_label:
            raise ValueError("Vector de intenci√≥n vac√≠o (norma cero)")

        vector = self._basis.get(intent_label)
        if vector is None:
            available = list(self._basis.keys())
            # Intentar match parcial para sugerencias
            suggestions = [k for k in available if intent_label.lower() in k.lower()]
            
            raise ValueError(
                f"Vector '{intent_label}' ‚àà Ker(œÄ) (n√∫cleo de proyecci√≥n). "
                f"Base disponible: {available}. "
                f"Sugerencias: {suggestions if suggestions else 'ninguna'}"
            )

        if not self._orthonormal_basis_computed:
            self._orthonormalize_basis()

        return vector

    def _orthonormalize_basis(self):
        """
        Aplica Gram-Schmidt modificado para estabilidad num√©rica.
        
        Algoritmo: MGS (Modified Gram-Schmidt)
        Para k = 1, ..., n:
            q_k = v_k
            Para j = 1, ..., k-1:
                q_k = q_k - <q_k, q_j> q_j
            q_k = q_k / ||q_k||
        """
        if self._orthonormal_basis_computed:
            return

        n = self._dimension
        if n == 0:
            self._gram_matrix = np.array([[]])
            self._orthonormal_basis_computed = True
            return

        # Construir matriz de Gram usando forma de Killing
        self._gram_matrix = np.eye(n)
        
        vectors = list(self._basis.values())
        for i in range(n):
            for j in range(i + 1, n):
                killing_ij = self._compute_killing_pairing(vectors[i], vectors[j])
                self._gram_matrix[i, j] = killing_ij
                self._gram_matrix[j, i] = killing_ij

        # Verificar definici√≥n positiva (espacio m√©trico v√°lido)
        try:
            eigenvalues = np.linalg.eigvalsh(self._gram_matrix)
            if np.any(eigenvalues < -1e-10):
                logger.warning(
                    f"‚ö†Ô∏è Matriz de Gram no definida positiva. "
                    f"Eigenvalores negativos: {eigenvalues[eigenvalues < 0]}"
                )
                # Regularizaci√≥n de Tikhonov
                self._gram_matrix += np.eye(n) * abs(min(eigenvalues)) * 1.1
        except np.linalg.LinAlgError:
            logger.warning("‚ö†Ô∏è Error en descomposici√≥n espectral, usando identidad")
            self._gram_matrix = np.eye(n)

        self._orthonormal_basis_computed = True
        logger.debug(f"üßÆ Gram-Schmidt completado. Condici√≥n: {np.linalg.cond(self._gram_matrix):.2f}")

    def get_spectrum(self) -> Dict[str, float]:
        """
        Calcula el espectro del operador basado en la matriz de Gram.
        
        Los valores propios indican la 'inercia' de cada direcci√≥n base.
        """
        if not self._orthonormal_basis_computed:
            self._orthonormalize_basis()

        if self._gram_matrix is None or self._gram_matrix.size == 0:
            return {}

        try:
            eigenvalues = np.linalg.eigvalsh(self._gram_matrix)
            eigenvalues = np.sort(eigenvalues)[::-1]  # Descendente
            
            spectrum = {}
            for i, (label, vector) in enumerate(self._basis.items()):
                if i < len(eigenvalues):
                    # Ponderar por nivel de estrato
                    stratum_weight = {
                        Stratum.PHYSICS: 1.0,
                        Stratum.TACTICS: 1.2,
                        Stratum.STRATEGY: 1.5,
                        Stratum.WISDOM: 2.0
                    }.get(vector.stratum, 1.0)
                    
                    spectrum[label] = float(eigenvalues[i]) * stratum_weight
                else:
                    spectrum[label] = 1.0
                    
            return spectrum
            
        except np.linalg.LinAlgError as e:
            logger.error(f"‚ùå Error computando espectro: {e}")
            return {label: 1.0 for label in self._basis.keys()}

    def get_condition_number(self) -> float:
        """Retorna n√∫mero de condici√≥n de la base (estabilidad num√©rica)."""
        if not self._orthonormal_basis_computed:
            self._orthonormalize_basis()
            
        if self._gram_matrix is None or self._gram_matrix.size == 0:
            return 1.0
            
        return float(np.linalg.cond(self._gram_matrix))


# ==================== PipelineDirector._compute_homology_groups REFINADO ====================

def _compute_homology_groups(self, context: dict):
    """
    Computa grupos de homolog√≠a del complejo simplicial de datos.
    
    Usa el Laplaciano combinatorio para calcular n√∫meros de Betti:
    - Œ≤‚ÇÄ = dim(ker(L‚ÇÄ)) = componentes conexas
    - Œ≤‚ÇÅ = dim(ker(L‚ÇÅ)) - dim(im(L‚ÇÄ)) = ciclos independientes
    
    Refinamiento: Usa descomposici√≥n sparse y manejo robusto de casos degenerados.
    """
    try:
        df_keys = [k for k in context.keys() 
                   if isinstance(context.get(k), pd.DataFrame) 
                   and not context[k].empty]

        n = len(df_keys)
        if n < 2:
            self._homology_groups = {"H0": 1, "H1": 0, "Betti": [1, 0]}
            return

        # Construir matriz de adyacencia ponderada
        adj_matrix = sparse.lil_matrix((n, n), dtype=np.float64)

        for i, key_i in enumerate(df_keys):
            df_i = context[key_i]
            cols_i = set(df_i.columns)
            
            for j in range(i + 1, n):
                key_j = df_keys[j]
                df_j = context[key_j]
                cols_j = set(df_j.columns)
                
                # Peso = Jaccard similarity de columnas
                intersection = len(cols_i & cols_j)
                union = len(cols_i | cols_j)
                
                if intersection > 0 and union > 0:
                    weight = intersection / union
                    adj_matrix[i, j] = weight
                    adj_matrix[j, i] = weight

        adj_csr = adj_matrix.tocsr()
        
        # Construir Laplaciano combinatorio L = D - A
        degrees = np.array(adj_csr.sum(axis=1)).flatten()
        degree_matrix = sparse.diags(degrees)
        laplacian = degree_matrix - adj_csr

        # Calcular Œ≤‚ÇÄ (componentes conexas) via eigenvalores cercanos a 0
        h0 = self._count_zero_eigenvalues(laplacian, n)
        
        # Calcular Œ≤‚ÇÅ usando f√≥rmula de Euler: œá = Œ≤‚ÇÄ - Œ≤‚ÇÅ + Œ≤‚ÇÇ - ...
        # Para 1-complejo: œá = V - E, entonces Œ≤‚ÇÅ = E - V + Œ≤‚ÇÄ
        num_edges = adj_csr.nnz // 2
        num_vertices = n
        h1 = max(0, num_edges - num_vertices + h0)

        # Verificar consistencia topol√≥gica
        euler_char = h0 - h1
        expected_euler = num_vertices - num_edges
        
        if euler_char != expected_euler:
            logger.warning(
                f"‚ö†Ô∏è Inconsistencia en caracter√≠stica de Euler: "
                f"calculada={euler_char}, esperada={expected_euler}"
            )

        self._homology_groups = {
            "H0": h0,
            "H1": h1,
            "Betti_numbers": [h0, h1],
            "Euler_characteristic": euler_char,
            "vertices": num_vertices,
            "edges": num_edges
        }
        
        logger.debug(f"üßÆ Homolog√≠a: Œ≤‚ÇÄ={h0}, Œ≤‚ÇÅ={h1}, œá={euler_char}")

    except Exception as e:
        logger.warning(f"‚ö†Ô∏è Error computando homolog√≠a: {e}")
        self._homology_groups = {"H0": 1, "H1": 0, "error": str(e)}

def _count_zero_eigenvalues(
    self, 
    laplacian: sparse.spmatrix, 
    n: int,
    tol: float = 1e-10
) -> int:
    """
    Cuenta eigenvalores cercanos a cero del Laplaciano.
    
    Usa shift-invert para estabilidad con eigenvalores peque√±os.
    """
    if n <= 1:
        return n
    
    if n <= 10:
        # Para matrices peque√±as, usar m√©todo denso
        try:
            L_dense = laplacian.toarray()
            eigenvalues = np.linalg.eigvalsh(L_dense)
            return int(np.sum(np.abs(eigenvalues) < tol))
        except np.linalg.LinAlgError:
            return 1

    # Para matrices grandes, usar m√©todo iterativo
    try:
        k = min(n - 1, max(1, n // 4))  # N√∫mero de eigenvalores a calcular
        
        # Shift-invert mode para eigenvalores cerca de 0
        eigenvalues = sparse.linalg.eigsh(
            laplacian, 
            k=k, 
            sigma=0.0,  # Target eigenvalue
            which='LM',  # Largest magnitude after shift-invert = smallest
            return_eigenvectors=False,
            tol=1e-6,
            maxiter=1000
        )
        
        return int(np.sum(np.abs(eigenvalues) < tol))
        
    except (sparse.linalg.ArpackNoConvergence, sparse.linalg.ArpackError) as e:
        logger.debug(f"ARPACK no convergi√≥, usando estimaci√≥n: {e}")
        # Fallback: estimar componentes por BFS
        return self._estimate_components_bfs(laplacian, n)
    except Exception:
        return 1

def _estimate_components_bfs(
    self, 
    laplacian: sparse.spmatrix, 
    n: int
) -> int:
    """Estima componentes conexas por BFS cuando eigsh falla."""
    adj = (laplacian != laplacian.diagonal()).astype(bool)
    visited = np.zeros(n, dtype=bool)
    components = 0
    
    for start in range(n):
        if not visited[start]:
            components += 1
            # BFS
            queue = [start]
            while queue:
                node = queue.pop(0)
                if not visited[node]:
                    visited[node] = True
                    neighbors = adj[node].nonzero()[1]
                    queue.extend(neighbors[~visited[neighbors]])
    
    return components


# ==================== InformationGeometry REFINADO ====================

class InformationGeometry:
    """
    Geometr√≠a de la informaci√≥n para espacios de datos.
    
    Refinamiento: Implementa m√©trica de Fisher-Rao y divergencia de 
    Kullback-Leibler con estimadores robustos.
    """

    def __init__(self, n_components_pca: int = 10):
        self.n_components_pca = n_components_pca
        self._entropy_cache: Dict[int, float] = {}

    def compute_entropy(self, df: pd.DataFrame) -> Dict[str, float]:
        """
        Calcula medidas de entrop√≠a e informaci√≥n con estimadores robustos.
        """
        if df is None or df.empty:
            return self._empty_metrics()

        result = {
            "shannon_entropy": 0.0,
            "intrinsic_dimension": 0.0,
            "fisher_information": 0.0,
            "effective_rank": 0.0
        }

        # Entrop√≠a de Shannon para columnas categ√≥ricas
        categorical_cols = df.select_dtypes(exclude=[np.number]).columns
        total_entropy = 0.0
        
        for col in categorical_cols:
            col_entropy = self._compute_column_entropy(df[col])
            total_entropy += col_entropy
            
        result["shannon_entropy"] = total_entropy

        # An√°lisis de columnas num√©ricas
        numeric_df = df.select_dtypes(include=[np.number])
        if not numeric_df.empty and len(numeric_df) > 1:
            # Limpiar datos
            numeric_data = numeric_df.fillna(0).values
            numeric_data = np.nan_to_num(numeric_data, nan=0, posinf=0, neginf=0)
            
            # Dimensi√≥n intr√≠nseca via PCA
            result["intrinsic_dimension"] = self._compute_intrinsic_dimension(numeric_data)
            
            # Informaci√≥n de Fisher (aproximaci√≥n diagonal)
            result["fisher_information"] = self._compute_fisher_information(numeric_data)
            
            # Rango efectivo (diversidad espectral)
            result["effective_rank"] = self._compute_effective_rank(numeric_data)

        return result

    def _empty_metrics(self) -> Dict[str, float]:
        return {
            "shannon_entropy": 0.0,
            "intrinsic_dimension": 0.0,
            "fisher_information": 0.0,
            "effective_rank": 0.0
        }

    def _compute_column_entropy(self, series: pd.Series) -> float:
        """Calcula entrop√≠a de Shannon para una columna."""
        try:
            value_counts = series.value_counts(normalize=True, dropna=True)
            if value_counts.empty:
                return 0.0
                
            probabilities = value_counts.values
            # Filtrar probabilidades cero
            probabilities = probabilities[probabilities > 0]
            
            return float(-np.sum(probabilities * np.log2(probabilities)))
        except Exception:
            return 0.0

    def _compute_intrinsic_dimension(self, data: np.ndarray) -> float:
        """
        Calcula dimensi√≥n intr√≠nseca usando PCA con criterio de energ√≠a.
        
        Umbral: 95% de varianza explicada.
        """
        if data.shape[0] < 2 or data.shape[1] < 1:
            return 0.0

        try:
            # Estandarizar datos
            mean = np.mean(data, axis=0)
            std = np.std(data, axis=0)
            std[std == 0] = 1  # Evitar divisi√≥n por cero
            data_std = (data - mean) / std

            n_components = min(self.n_components_pca, data.shape[0] - 1, data.shape[1])
            if n_components < 1:
                return 1.0

            # Usar TruncatedSVD para eficiencia con datos grandes
            if data.shape[0] > 1000 or data.shape[1] > 100:
                svd = TruncatedSVD(n_components=n_components, random_state=42)
                svd.fit(data_std)
                explained_variance = svd.explained_variance_ratio_
            else:
                pca = PCA(n_components=n_components)
                pca.fit(data_std)
                explained_variance = pca.explained_variance_ratio_

            # Dimensi√≥n = primer k donde varianza acumulada >= 95%
            cumulative = np.cumsum(explained_variance)
            threshold_idx = np.searchsorted(cumulative, 0.95)
            
            return float(threshold_idx + 1)

        except Exception as e:
            logger.debug(f"Error en PCA: {e}")
            return 1.0

    def _compute_fisher_information(self, data: np.ndarray) -> float:
        """
        Aproxima la informaci√≥n de Fisher como traza de la inversa de covarianza.
        
        I(Œ∏) ‚âà Tr(Œ£‚Åª¬π) donde Œ£ es la matriz de covarianza.
        """
        if data.shape[0] < 2:
            return 0.0

        try:
            # Covarianza con regularizaci√≥n
            cov = np.cov(data.T)
            if cov.ndim == 0:
                cov = np.array([[cov]])
            
            # Regularizaci√≥n de Tikhonov para invertibilidad
            reg_lambda = 1e-6 * np.trace(cov) / cov.shape[0] if cov.shape[0] > 0 else 1e-6
            cov_reg = cov + reg_lambda * np.eye(cov.shape[0])
            
            # Traza de la inversa
            cov_inv = np.linalg.inv(cov_reg)
            fisher = np.trace(cov_inv)
            
            return float(np.clip(fisher, 0, 1e10))

        except np.linalg.LinAlgError:
            return 0.0
        except Exception:
            return 0.0

    def _compute_effective_rank(self, data: np.ndarray) -> float:
        """
        Calcula rango efectivo basado en entrop√≠a espectral.
        
        eff_rank = exp(H(œÉ)) donde H es entrop√≠a de valores singulares normalizados.
        """
        if data.shape[0] < 2 or data.shape[1] < 1:
            return 0.0

        try:
            # SVD parcial para eficiencia
            k = min(50, data.shape[0] - 1, data.shape[1])
            if k < 1:
                return 1.0
                
            _, singular_values, _ = np.linalg.svd(data, full_matrices=False)
            singular_values = singular_values[:k]
            
            # Normalizar a distribuci√≥n de probabilidad
            sv_sum = np.sum(singular_values)
            if sv_sum == 0:
                return 0.0
                
            probs = singular_values / sv_sum
            probs = probs[probs > 1e-10]  # Filtrar valores muy peque√±os
            
            # Entrop√≠a espectral
            entropy = -np.sum(probs * np.log(probs))
            
            return float(np.exp(entropy))

        except Exception:
            return 1.0

    def kl_divergence(self, df1: pd.DataFrame, df2: pd.DataFrame) -> float:
        """
        Calcula divergencia de Kullback-Leibler aproximada entre dos DataFrames.
        """
        if df1.empty or df2.empty:
            return float('inf')

        # Comparar distribuciones de columnas comunes
        common_cols = set(df1.columns) & set(df2.columns)
        if not common_cols:
            return float('inf')

        total_kl = 0.0
        for col in common_cols:
            if col in df1.select_dtypes(exclude=[np.number]).columns:
                kl = self._categorical_kl(df1[col], df2[col])
            else:
                kl = self._numeric_kl(df1[col], df2[col])
            total_kl += kl

        return total_kl / len(common_cols)

    def _categorical_kl(self, s1: pd.Series, s2: pd.Series) -> float:
        """KL divergencia para series categ√≥ricas."""
        try:
            p = s1.value_counts(normalize=True)
            q = s2.value_counts(normalize=True)
            
            all_categories = set(p.index) | set(q.index)
            
            kl = 0.0
            for cat in all_categories:
                p_val = p.get(cat, 1e-10)
                q_val = q.get(cat, 1e-10)
                if p_val > 0:
                    kl += p_val * np.log(p_val / q_val)
                    
            return float(kl)
        except Exception:
            return 0.0

    def _numeric_kl(self, s1: pd.Series, s2: pd.Series) -> float:
        """KL divergencia para series num√©ricas (asumiendo Gaussianas)."""
        try:
            mu1, var1 = s1.mean(), s1.var() + 1e-10
            mu2, var2 = s2.mean(), s2.var() + 1e-10
            
            kl = np.log(np.sqrt(var2/var1)) + (var1 + (mu1-mu2)**2)/(2*var2) - 0.5
            return float(np.clip(kl, 0, 100))
        except Exception:
            return 0.0


# ==================== ProcrustesAnalyzer REFINADO ====================

class ProcrustesAnalyzer:
    """
    Analizador de alineamiento Procrustes con soporte multi-modal.
    
    Refinamiento: Manejo robusto de dimensiones heterog√©neas y
    m√©tricas de calidad de alineamiento.
    """

    def __init__(self, padding_strategy: str = "zero"):
        """
        Args:
            padding_strategy: 'zero', 'mean', o 'noise' para padding dimensional.
        """
        self.padding_strategy = padding_strategy
        self._last_alignment_quality: Optional[float] = None

    def isometric_align(
        self, 
        X: np.ndarray, 
        Y: np.ndarray,
        return_quality: bool = False
    ) -> Tuple[np.ndarray, np.ndarray, Optional[np.ndarray]]:
        """
        Alineamiento isom√©trico (r√≠gido) preservando distancias.
        
        Minimiza ||X - Y @ R||_F s.t. R^T @ R = I
        """
        X, Y = self._validate_and_prepare(X, Y)
        
        if X.shape[0] != Y.shape[0]:
            X, Y = self._match_rows(X, Y)

        if X.shape[1] != Y.shape[1]:
            X, Y = self._match_columns(X, Y)

        # Centrar datos
        X_centered = X - X.mean(axis=0)
        Y_centered = Y - Y.mean(axis=0)

        try:
            # SVD para encontrar rotaci√≥n √≥ptima
            H = Y_centered.T @ X_centered
            U, S, Vt = np.linalg.svd(H)
            
            # Rotaci√≥n √≥ptima (ortogonal)
            R = U @ Vt
            
            # Manejar reflexiones (det(R) = -1)
            if np.linalg.det(R) < 0:
                Vt[-1, :] *= -1
                R = U @ Vt

            Y_aligned = Y_centered @ R

            # Calcular calidad de alineamiento
            self._last_alignment_quality = self._compute_alignment_quality(
                X_centered, Y_aligned
            )

            if return_quality:
                return X_centered, Y_aligned, R, self._last_alignment_quality
                
            return X_centered, Y_aligned, R

        except np.linalg.LinAlgError as e:
            logger.warning(f"‚ö†Ô∏è SVD fall√≥ en Procrustes: {e}")
            return X, Y, np.eye(Y.shape[1])

    def conformal_align(
        self, 
        X: np.ndarray, 
        Y: np.ndarray
    ) -> Tuple[np.ndarray, np.ndarray, Tuple[float, np.ndarray]]:
        """
        Alineamiento conforme: preserva √°ngulos, permite escala uniforme.
        
        Minimiza ||X - s * Y @ R||_F
        """
        X, Y = self._validate_and_prepare(X, Y)
        
        if X.shape != Y.shape:
            X, Y = self._match_dimensions(X, Y)

        X_centered = X - X.mean(axis=0)
        Y_centered = Y - Y.mean(axis=0)

        # Primero obtener rotaci√≥n √≥ptima
        _, Y_rotated, R = self.isometric_align(X_centered, Y_centered)

        # Calcular escala √≥ptima
        numerator = np.trace(X_centered.T @ Y_rotated)
        denominator = np.trace(Y_rotated.T @ Y_rotated)
        
        scale = numerator / denominator if denominator > 1e-10 else 1.0
        scale = np.clip(scale, 0.01, 100.0)  # L√≠mites razonables

        Y_scaled = scale * Y_rotated

        self._last_alignment_quality = self._compute_alignment_quality(
            X_centered, Y_scaled
        )

        return X_centered, Y_scaled, (scale, R)

    def affine_align(
        self, 
        X: np.ndarray, 
        Y: np.ndarray
    ) -> Tuple[np.ndarray, np.ndarray, Optional[np.ndarray]]:
        """
        Alineamiento af√≠n general: permite rotaci√≥n, escala y sesgo.
        
        Encuentra A tal que ||X - Y @ A||_F es m√≠nimo.
        """
        X, Y = self._validate_and_prepare(X, Y)
        
        if X.shape[0] != Y.shape[0]:
            X, Y = self._match_rows(X, Y)

        try:
            # Soluci√≥n de m√≠nimos cuadrados: A = (Y^T Y)^{-1} Y^T X
            A, residuals, rank, s = np.linalg.lstsq(Y, X, rcond=None)
            Y_aligned = Y @ A
            
            self._last_alignment_quality = self._compute_alignment_quality(X, Y_aligned)
            
            return X, Y_aligned, A

        except np.linalg.LinAlgError:
            return X, Y, None

    def _validate_and_prepare(
        self, 
        X: np.ndarray, 
        Y: np.ndarray
    ) -> Tuple[np.ndarray, np.ndarray]:
        """Valida y prepara arrays para alineamiento."""
        X = np.atleast_2d(np.asarray(X, dtype=np.float64))
        Y = np.atleast_2d(np.asarray(Y, dtype=np.float64))
        
        # Reemplazar NaN/Inf
        X = np.nan_to_num(X, nan=0.0, posinf=0.0, neginf=0.0)
        Y = np.nan_to_num(Y, nan=0.0, posinf=0.0, neginf=0.0)
        
        return X, Y

    def _match_rows(
        self, 
        X: np.ndarray, 
        Y: np.ndarray
    ) -> Tuple[np.ndarray, np.ndarray]:
        """Iguala n√∫mero de filas truncando al m√≠nimo."""
        min_rows = min(X.shape[0], Y.shape[0])
        return X[:min_rows], Y[:min_rows]

    def _match_columns(
        self, 
        X: np.ndarray, 
        Y: np.ndarray
    ) -> Tuple[np.ndarray, np.ndarray]:
        """Iguala n√∫mero de columnas mediante padding."""
        max_cols = max(X.shape[1], Y.shape[1])
        
        X_padded = self._pad_columns(X, max_cols)
        Y_padded = self._pad_columns(Y, max_cols)
        
        return X_padded, Y_padded

    def _match_dimensions(
        self, 
        X: np.ndarray, 
        Y: np.ndarray
    ) -> Tuple[np.ndarray, np.ndarray]:
        """Iguala ambas dimensiones."""
        X, Y = self._match_rows(X, Y)
        X, Y = self._match_columns(X, Y)
        return X, Y

    def _pad_columns(self, arr: np.ndarray, target_cols: int) -> np.ndarray:
        """Aplica padding de columnas seg√∫n estrategia."""
        if arr.shape[1] >= target_cols:
            return arr
            
        padding_size = target_cols - arr.shape[1]
        
        if self.padding_strategy == "zero":
            padding = np.zeros((arr.shape[0], padding_size))
        elif self.padding_strategy == "mean":
            col_mean = np.mean(arr)
            padding = np.full((arr.shape[0], padding_size), col_mean)
        elif self.padding_strategy == "noise":
            std = np.std(arr) * 0.01
            padding = np.random.normal(0, std, (arr.shape[0], padding_size))
        else:
            padding = np.zeros((arr.shape[0], padding_size))
            
        return np.hstack([arr, padding])

    def _compute_alignment_quality(
        self, 
        X: np.ndarray, 
        Y_aligned: np.ndarray
    ) -> float:
        """
        Calcula calidad de alineamiento como 1 - error_relativo.
        """
        residual = np.linalg.norm(X - Y_aligned, 'fro')
        baseline = np.linalg.norm(X, 'fro')
        
        if baseline < 1e-10:
            return 1.0 if residual < 1e-10 else 0.0
            
        relative_error = residual / baseline
        quality = max(0.0, 1.0 - relative_error)
        
        return float(quality)

    def get_last_alignment_quality(self) -> Optional[float]:
        """Retorna calidad del √∫ltimo alineamiento realizado."""
        return self._last_alignment_quality


# ==================== DataMerger REFINADO ====================

class DataMerger(BaseCostProcessor):
    """
    Fusionador con m√©trica de informaci√≥n y preservaci√≥n topol√≥gica.
    
    Refinamiento: M√∫ltiples estrategias de merge con votaci√≥n ponderada
    y validaci√≥n de inmersi√≥n algebraica.
    """

    def __init__(self, thresholds: ProcessingThresholds):
        super().__init__({}, thresholds)
        self._match_stats: Dict[str, float] = {}
        self._information_geometry = InformationGeometry()
        self._procrustes_analyzer = ProcrustesAnalyzer()
        self._merge_quality_threshold = 0.6

    def merge_apus_with_insumos(
        self,
        df_apus: pd.DataFrame,
        df_insumos: pd.DataFrame,
        alignment_strategy: str = "isometric",
        preserve_topology: bool = True
    ) -> pd.DataFrame:
        """
        Merge con an√°lisis de geometr√≠a de la informaci√≥n.
        """
        # Validaci√≥n de entrada
        if not self._validate_input(df_apus, "merge_apus"):
            return pd.DataFrame()
        if not self._validate_input(df_insumos, "merge_insumos"):
            return df_apus.copy()

        # M√©tricas de informaci√≥n pre-merge
        info_apus = self._information_geometry.compute_entropy(df_apus)
        info_insumos = self._information_geometry.compute_entropy(df_insumos)

        logger.info(
            f"üßÆ Entrop√≠a pre-merge: APUs={info_apus['shannon_entropy']:.3f}, "
            f"Insumos={info_insumos['shannon_entropy']:.3f}"
        )

        # Ejecutar estrategias de merge y evaluar
        candidates = self._execute_merge_strategies(df_apus, df_insumos)

        if not candidates:
            logger.error("‚ùå Todas las estrategias de merge fallaron")
            return self._fallback_merge(df_apus, df_insumos)

        # Seleccionar mejor candidato
        candidates.sort(key=lambda x: x[0], reverse=True)
        best_quality, best_result, best_strategy = candidates[0]

        if best_quality < self._merge_quality_threshold:
            logger.warning(
                f"‚ö†Ô∏è Calidad de merge sub√≥ptima: {best_quality:.3f} "
                f"(umbral: {self._merge_quality_threshold})"
            )

        logger.info(f"‚úÖ Merge √≥ptimo: {best_strategy} (calidad={best_quality:.3f})")

        # Validar preservaci√≥n de informaci√≥n
        if preserve_topology:
            self._validate_information_preservation(
                info_apus, info_insumos, best_result
            )

        self._log_merge_statistics(best_result)

        return best_result

    def _execute_merge_strategies(
        self,
        df_apus: pd.DataFrame,
        df_insumos: pd.DataFrame
    ) -> List[Tuple[float, pd.DataFrame, str]]:
        """Ejecuta m√∫ltiples estrategias de merge."""
        candidates = []

        strategies = [
            ("exact", self._exact_merge),
            ("fuzzy", self._fuzzy_merge),
            ("hierarchical", self._hierarchical_merge),
        ]

        for name, strategy in strategies:
            try:
                result = strategy(df_apus.copy(), df_insumos.copy())
                if not result.empty:
                    quality = self._evaluate_merge_quality(result)
                    candidates.append((quality, result, name))
                    logger.debug(f"Estrategia '{name}': calidad={quality:.3f}")
            except Exception as e:
                logger.debug(f"Estrategia '{name}' fall√≥: {e}")

        return candidates

    def _exact_merge(
        self,
        df_apus: pd.DataFrame,
        df_insumos: pd.DataFrame
    ) -> pd.DataFrame:
        """Merge exacto por descripci√≥n normalizada."""
        # Asegurar columnas normalizadas
        if ColumnNames.NORMALIZED_DESC not in df_apus.columns:
            if ColumnNames.DESCRIPCION_INSUMO in df_apus.columns:
                df_apus[ColumnNames.NORMALIZED_DESC] = normalize_text_series(
                    df_apus[ColumnNames.DESCRIPCION_INSUMO]
                )
            else:
                return pd.DataFrame()

        if ColumnNames.DESCRIPCION_INSUMO_NORM not in df_insumos.columns:
            if ColumnNames.DESCRIPCION_INSUMO in df_insumos.columns:
                df_insumos[ColumnNames.DESCRIPCION_INSUMO_NORM] = normalize_text_series(
                    df_insumos[ColumnNames.DESCRIPCION_INSUMO]
                )
            else:
                return pd.DataFrame()

        df_merged = pd.merge(
            df_apus,
            df_insumos,
            left_on=ColumnNames.NORMALIZED_DESC,
            right_on=ColumnNames.DESCRIPCION_INSUMO_NORM,
            how="left",
            suffixes=("_apu", "_insumo"),
            indicator="_merge"
        )

        return self._consolidate_columns(df_merged)

    def _fuzzy_merge(
        self,
        df_apus: pd.DataFrame,
        df_insumos: pd.DataFrame
    ) -> pd.DataFrame:
        """Merge fuzzy por similitud de texto."""
        try:
            from difflib import SequenceMatcher

            # Obtener descripciones
            desc_col_apu = ColumnNames.DESCRIPCION_INSUMO
            desc_col_insumo = ColumnNames.DESCRIPCION_INSUMO

            if desc_col_apu not in df_apus.columns:
                return pd.DataFrame()

            df_merged = df_apus.copy()
            df_merged["_fuzzy_match_idx"] = None
            df_merged["_fuzzy_score"] = 0.0

            insumo_descs = df_insumos[desc_col_insumo].fillna("").str.lower().tolist()

            for idx, row in df_merged.iterrows():
                apu_desc = str(row.get(desc_col_apu, "")).lower()
                if not apu_desc:
                    continue

                best_score = 0.0
                best_idx = None

                for i, insumo_desc in enumerate(insumo_descs):
                    score = SequenceMatcher(None, apu_desc, insumo_desc).ratio()
                    if score > best_score and score > 0.7:  # Umbral m√≠nimo
                        best_score = score
                        best_idx = i

                if best_idx is not None:
                    df_merged.at[idx, "_fuzzy_match_idx"] = best_idx
                    df_merged.at[idx, "_fuzzy_score"] = best_score

            # Aplicar matches
            matched_mask = df_merged["_fuzzy_match_idx"].notna()
            for idx in df_merged[matched_mask].index:
                insumo_idx = int(df_merged.at[idx, "_fuzzy_match_idx"])
                for col in df_insumos.columns:
                    if col not in df_merged.columns:
                        df_merged.at[idx, col] = df_insumos.iloc[insumo_idx][col]

            return df_merged.drop(columns=["_fuzzy_match_idx", "_fuzzy_score"])

        except ImportError:
            return pd.DataFrame()
        except Exception as e:
            logger.debug(f"Fuzzy merge fall√≥: {e}")
            return pd.DataFrame()

    def _hierarchical_merge(
        self,
        df_apus: pd.DataFrame,
        df_insumos: pd.DataFrame
    ) -> pd.DataFrame:
        """Merge jer√°rquico por grupo de insumo."""
        if ColumnNames.GRUPO_INSUMO not in df_insumos.columns:
            return pd.DataFrame()

        if ColumnNames.TIPO_INSUMO not in df_apus.columns:
            return pd.DataFrame()

        # Mapear grupos a tipos
        group_type_map = {
            "MATERIALES": InsumoType.MATERIAL,
            "MATERIAL": InsumoType.MATERIAL,
            "MANO DE OBRA": InsumoType.MANO_DE_OBRA,
            "CUADRILLAS": InsumoType.MANO_DE_OBRA,
            "EQUIPOS": InsumoType.EQUIPO,
            "HERRAMIENTAS": InsumoType.HERRAMIENTA,
            "TRANSPORTE": InsumoType.TRANSPORTE,
        }

        df_insumos = df_insumos.copy()
        df_insumos["_tipo_mapped"] = df_insumos[ColumnNames.GRUPO_INSUMO].str.upper().map(
            lambda x: group_type_map.get(x, InsumoType.OTROS)
        )

        # Merge por tipo
        df_merged = pd.merge(
            df_apus,
            df_insumos,
            left_on=ColumnNames.TIPO_INSUMO,
            right_on="_tipo_mapped",
            how="left",
            suffixes=("_apu", "_insumo")
        )

        return self._consolidate_columns(df_merged.drop(columns=["_tipo_mapped"], errors="ignore"))

    def _consolidate_columns(self, df: pd.DataFrame) -> pd.DataFrame:
        """Consolida columnas duplicadas del merge."""
        df = df.copy()

        # Consolidar descripci√≥n
        if f"{ColumnNames.DESCRIPCION_INSUMO}_insumo" in df.columns:
            df[ColumnNames.DESCRIPCION_INSUMO] = (
                df[f"{ColumnNames.DESCRIPCION_INSUMO}_insumo"]
                .fillna(df.get(f"{ColumnNames.DESCRIPCION_INSUMO}_apu", ""))
                .fillna(df.get(ColumnNames.NORMALIZED_DESC, ""))
            )

        # Consolidar valores num√©ricos
        for col in [ColumnNames.VR_UNITARIO_INSUMO, ColumnNames.CANTIDAD_APU]:
            insumo_col = f"{col}_insumo"
            apu_col = f"{col}_apu"

            if insumo_col in df.columns and apu_col in df.columns:
                df[col] = df[insumo_col].fillna(df[apu_col])
            elif insumo_col in df.columns:
                df[col] = df[insumo_col]
            elif apu_col in df.columns:
                df[col] = df[apu_col]

        return df

    def _fallback_merge(
        self,
        df_apus: pd.DataFrame,
        df_insumos: pd.DataFrame
    ) -> pd.DataFrame:
        """Merge de fallback: retorna APUs con columnas vac√≠as de insumos."""
        logger.warning("‚ö†Ô∏è Usando merge de fallback (sin enriquecimiento)")
        df = df_apus.copy()
        
        for col in [ColumnNames.VR_UNITARIO_INSUMO, ColumnNames.GRUPO_INSUMO]:
            if col not in df.columns:
                df[col] = None
                
        return df

    def _evaluate_merge_quality(self, df_merged: pd.DataFrame) -> float:
        """Eval√∫a calidad del merge con m√©tricas compuestas."""
        if df_merged.empty:
            return 0.0

        metrics = []

        # 1. Completitud (1 - ratio de NaN)
        nan_ratio = df_merged.isnull().mean().mean()
        completeness = 1.0 - nan_ratio
        metrics.append(("completeness", completeness, 0.3))

        # 2. Tasa de match (si hay indicador)
        if "_merge" in df_merged.columns:
            match_rate = (df_merged["_merge"] == "both").mean()
            metrics.append(("match_rate", match_rate, 0.4))

        # 3. Consistencia de tipos
        type_consistency = self._compute_type_consistency(df_merged)
        metrics.append(("type_consistency", type_consistency, 0.15))

        # 4. Cobertura de columnas clave
        key_cols = [
            ColumnNames.CODIGO_APU,
            ColumnNames.DESCRIPCION_INSUMO,
            ColumnNames.VR_UNITARIO_INSUMO
        ]
        coverage = sum(1 for c in key_cols if c in df_merged.columns) / len(key_cols)
        metrics.append(("key_coverage", coverage, 0.15))

        # Promedio ponderado
        weighted_sum = sum(value * weight for _, value, weight in metrics)
        total_weight = sum(weight for _, _, weight in metrics)

        return weighted_sum / total_weight if total_weight > 0 else 0.0

    def _compute_type_consistency(self, df: pd.DataFrame) -> float:
        """Calcula consistencia de tipos usando entrop√≠a normalizada."""
        type_counts = {}
        for col in df.columns:
            dtype = str(df[col].dtype)
            type_counts[dtype] = type_counts.get(dtype, 0) + 1

        if not type_counts:
            return 1.0

        total = sum(type_counts.values())
        proportions = [count / total for count in type_counts.values()]
        
        # Entrop√≠a normalizada (inversa = consistencia)
        entropy = -sum(p * np.log(p) for p in proportions if p > 0)
        max_entropy = np.log(len(type_counts)) if len(type_counts) > 1 else 1

        if max_entropy == 0:
            return 1.0

        # Invertir: baja entrop√≠a = alta consistencia
        return 1.0 - (entropy / max_entropy)

    def _validate_information_preservation(
        self,
        info_before_a: Dict[str, float],
        info_before_b: Dict[str, float],
        df_merged: pd.DataFrame
    ):
        """Valida que el merge preserve informaci√≥n."""
        info_after = self._information_geometry.compute_entropy(df_merged)

        # Verificar no colapso dimensional
        dim_before = (
            info_before_a.get("intrinsic_dimension", 0) +
            info_before_b.get("intrinsic_dimension", 0)
        )
        dim_after = info_after.get("intrinsic_dimension", 0)

        if dim_before > 0:
            preservation_ratio = dim_after / dim_before
            if preservation_ratio < 0.5:
                logger.warning(
                    f"‚ö†Ô∏è Colapso dimensional: {preservation_ratio:.1%} preservado"
                )

        # Verificar no p√©rdida de entrop√≠a excesiva
        entropy_before = (
            info_before_a.get("shannon_entropy", 0) +
            info_before_b.get("shannon_entropy", 0)
        )
        entropy_after = info_after.get("shannon_entropy", 0)

        if entropy_before > 0:
            entropy_ratio = entropy_after / entropy_before
            if entropy_ratio < 0.3:
                logger.warning(
                    f"‚ö†Ô∏è P√©rdida de entrop√≠a significativa: {entropy_ratio:.1%}"
                )

    def merge_with_presupuesto(
        self, 
        df_presupuesto: pd.DataFrame, 
        df_apu_costos: pd.DataFrame
    ) -> pd.DataFrame:
        """Fusiona presupuesto con costos APU."""
        if not self._validate_input(df_presupuesto, "merge_presupuesto_left"):
            return pd.DataFrame()

        if not self._validate_input(df_apu_costos, "merge_presupuesto_right"):
            return df_presupuesto.copy()

        try:
            # Intentar merge 1:1 primero
            df_merged = pd.merge(
                df_presupuesto,
                df_apu_costos,
                on=ColumnNames.CODIGO_APU,
                how="left",
                validate="1:1",
            )
            self.logger.info(f"‚úÖ Merge 1:1 exitoso: {len(df_merged)} filas")
            return df_merged

        except pd.errors.MergeError as e:
            self.logger.warning(f"‚ö†Ô∏è Merge 1:1 fall√≥, usando many-to-one: {e}")

            # Deduplicar df_apu_costos antes de merge
            df_apu_dedup = df_apu_costos.drop_duplicates(
                subset=[ColumnNames.CODIGO_APU], 
                keep="first"
            )

            df_merged = pd.merge(
                df_presupuesto,
                df_apu_dedup,
                on=ColumnNames.CODIGO_APU,
                how="left",
            )
            return df_merged

        except Exception as e:
            self.logger.error(f"‚ùå Error en merge con presupuesto: {e}")
            raise

    def _log_merge_statistics(self, df: pd.DataFrame):
        """Registra estad√≠sticas del merge."""
        if "_merge" in df.columns:
            stats = df["_merge"].value_counts(normalize=True) * 100
            self._match_stats = {
                str(k): float(v) for k, v in stats.to_dict().items()
            }
            self.logger.info(f"üìä Estad√≠sticas merge: {self._match_stats}")
        
        # Estad√≠sticas adicionales
        self._match_stats["total_rows"] = len(df)
        self._match_stats["null_ratio"] = float(df.isnull().mean().mean())

    def calculate(self, *args, **kwargs):
        """Implementaci√≥n requerida por clase base."""
        pass


# ==================== PipelineDirector._validate_stratum_transition REFINADO ====================

def _validate_stratum_transition(
    self, 
    current: Optional[Stratum], 
    next_stratum: Stratum
):
    """
    Valida transici√≥n entre estratos usando teor√≠a de √≥rdenes parciales.
    
    Reglas:
    1. Transiciones hacia arriba: Siempre permitidas
    2. Transiciones laterales (mismo nivel): Permitidas
    3. Transiciones hacia abajo: Solo si se completa ciclo o reinicio expl√≠cito
    4. Saltos de m√°s de un nivel: Warning pero permitido
    """
    if current is None:
        # Primera ejecuci√≥n, cualquier estrato es v√°lido
        logger.debug(f"üöÄ Iniciando en estrato {next_stratum.name}")
        return

    current_level = self._stratum_to_filtration(current)
    next_level = self._stratum_to_filtration(next_stratum)

    # Caso 1: Avance o mismo nivel (normal)
    if next_level >= current_level:
        # Detectar salto de estratos
        if next_level > current_level + 1:
            skipped = next_level - current_level - 1
            skipped_strata = [
                s.name for s in Stratum 
                if current_level < self._stratum_to_filtration(s) < next_level
            ]
            logger.warning(
                f"‚ö†Ô∏è Salto de {skipped} estrato(s): {current.name} ‚Üí {next_stratum.name}. "
                f"Estratos omitidos: {skipped_strata}"
            )
            
            # Registrar en telemetr√≠a
            self.telemetry.record_metric(
                "stratum_transition", 
                "skipped_strata", 
                skipped
            )
        return

    # Caso 2: Retroceso (potencial reinicio de ciclo)
    if next_level < current_level:
        # Verificar si es reinicio v√°lido (volver a PHYSICS desde WISDOM)
        is_valid_cycle_restart = (
            current == Stratum.WISDOM and 
            next_stratum == Stratum.PHYSICS
        )
        
        if is_valid_cycle_restart:
            logger.info(
                f"üîÑ Reinicio de ciclo detectado: {current.name} ‚Üí {next_stratum.name}"
            )
            self._filtration_level = 0  # Resetear nivel
            return

        # Retroceso parcial (potencialmente problem√°tico)
        regression_depth = current_level - next_level
        logger.warning(
            f"‚ö†Ô∏è Regresi√≥n de estrato detectada: {current.name} ‚Üí {next_stratum.name} "
            f"(profundidad: {regression_depth})"
        )
        
        # Registrar para auditor√≠a
        self.telemetry.record_metric(
            "stratum_transition",
            "regression_depth",
            regression_depth
        )
        
        # Permitir pero marcar contexto
        return


# ==================== PresupuestoProcessor._clean_phantom_rows REFINADO ====================

def _clean_phantom_rows(self, df: pd.DataFrame) -> pd.DataFrame:
    """
    Elimina filas fantasma con detecci√≥n multi-patr√≥n.
    
    Patrones detectados:
    1. Filas completamente vac√≠as
    2. Filas con solo NaN/None
    3. Filas con patrones de placeholder
    4. Filas de metadatos (totales, subtotales)
    """
    if df is None:
        return pd.DataFrame()

    if not isinstance(df, pd.DataFrame):
        logger.error(f"‚ùå _clean_phantom_rows recibi√≥ tipo: {type(df).__name__}")
        return pd.DataFrame()

    if df.empty:
        return df

    initial_rows = len(df)
    removal_reasons: Dict[str, int] = {}

    # 1. Eliminar filas completamente vac√≠as
    empty_mask = df.isna().all(axis=1)
    removal_reasons["completely_empty"] = empty_mask.sum()
    df_clean = df[~empty_mask].copy()

    # 2. Eliminar filas con solo valores "vac√≠os" (string patterns)
    str_df = df_clean.astype(str).apply(lambda x: x.str.strip().str.lower())
    
    empty_patterns = {
        "", "nan", "none", "nat", "<na>", "null", "n/a", "na", 
        "-", "--", "---", "...", ".", "undefined", "sin dato"
    }
    
    is_empty_mask = str_df.isin(empty_patterns).all(axis=1)
    removal_reasons["pattern_empty"] = is_empty_mask.sum()
    df_clean = df_clean[~is_empty_mask].copy()

    # 3. Eliminar filas de metadatos (totales, subtotales, encabezados)
    if not df_clean.empty and len(df_clean.columns) > 0:
        first_col = df_clean.iloc[:, 0].astype(str).str.strip().str.lower()
        
        metadata_patterns = [
            r"^total\b", r"^subtotal\b", r"^suma\b", r"^promedio\b",
            r"^gran\s*total", r"^item\b", r"^codigo\b", r"^descripcion\b",
            r"^\d+\.\s*total", r"^resumen\b"
        ]
        
        metadata_mask = pd.Series(False, index=df_clean.index)
        for pattern in metadata_patterns:
            metadata_mask |= first_col.str.contains(pattern, regex=True, na=False)
        
        removal_reasons["metadata_rows"] = metadata_mask.sum()
        df_clean = df_clean[~metadata_mask].copy()

    # 4. Eliminar filas donde columnas cr√≠ticas est√°n vac√≠as
    critical_columns = [col for col in df_clean.columns 
                       if any(kw in col.upper() for kw in ["CODIGO", "DESCRIPCION", "APU"])]
    
    if critical_columns:
        critical_empty_mask = df_clean[critical_columns].isna().all(axis=1)
        removal_reasons["critical_empty"] = critical_empty_mask.sum()
        df_clean = df_clean[~critical_empty_mask].copy()

    # Log detallado
    total_removed = initial_rows - len(df_clean)
    if total_removed > 0:
        logger.info(f"üëª Filas fantasma eliminadas: {total_removed}")
        for reason, count in removal_reasons.items():
            if count > 0:
                logger.debug(f"   - {reason}: {count}")

    return df_clean


# ==================== APUCostCalculator._normalize_tipo_insumo REFINADO ====================

def _normalize_tipo_insumo(self, df: pd.DataFrame) -> pd.DataFrame:
    """
    Normalizaci√≥n robusta de tipos de insumo usando patrones regex.
    """
    df = df.copy()

    if ColumnNames.TIPO_INSUMO not in df.columns:
        df[ColumnNames.TIPO_INSUMO] = InsumoType.OTROS
        df["_CATEGORIA_COSTO"] = ColumnNames.OTROS
        return df

    # Patrones de clasificaci√≥n con prioridad (el primero que coincide gana)
    classification_patterns = [
        # (patr√≥n regex, InsumoType)
        (r"\b(mano\s*de?\s*obra|cuadrilla|jornale?s?|operario|obrero|maestro)\b", 
         InsumoType.MANO_DE_OBRA),
        (r"\b(equipo|maquinaria|herramienta|compresor|mezcladora|vibrador)\b", 
         InsumoType.EQUIPO),
        (r"\b(transporte|flete|acarreo|traslado)\b", 
         InsumoType.TRANSPORTE),
        (r"\b(subcontrat|terceriza|outsourc)\b", 
         InsumoType.SUBCONTRATO),
        (r"\b(material|suministro|cemento|arena|grava|acero|madera|pvc|tuber[i√≠]a)\b", 
         InsumoType.MATERIAL),
    ]

    def classify_insumo(val) -> InsumoType:
        if pd.isna(val):
            return InsumoType.OTROS
            
        val_str = str(val).lower().strip()
        
        if not val_str or val_str in ("nan", "none", ""):
            return InsumoType.OTROS

        for pattern, insumo_type in classification_patterns:
            if re.search(pattern, val_str, re.IGNORECASE):
                return insumo_type

        return InsumoType.OTROS

    # Aplicar clasificaci√≥n
    df[ColumnNames.TIPO_INSUMO] = df[ColumnNames.TIPO_INSUMO].apply(classify_insumo)

    # Mapear a categor√≠a de costo
    df["_CATEGORIA_COSTO"] = df[ColumnNames.TIPO_INSUMO].map(
        lambda x: self._tipo_to_categoria.get(x, ColumnNames.OTROS)
    )

    # Estad√≠sticas de clasificaci√≥n
    if not df.empty:
        stats = df["_CATEGORIA_COSTO"].value_counts(normalize=True) * 100
        coverage = 100 - stats.get(ColumnNames.OTROS, 0)
        
        self.logger.info(
            f"üìä Clasificaci√≥n de insumos: {coverage:.1f}% cubiertos. "
            f"Distribuci√≥n: {stats.to_dict()}"
        )
        
        # Alerta si muchos "OTROS"
        if stats.get(ColumnNames.OTROS, 0) > 30:
            self.logger.warning(
                f"‚ö†Ô∏è Alta proporci√≥n de insumos sin clasificar: "
                f"{stats.get(ColumnNames.OTROS, 0):.1f}%"
            )

    return df


