### A. Nuevas Estructuras de Soporte

# ==================== CONSTANTES DE FILTRACI√ìN ====================

_STRATUM_ORDER: Dict[Stratum, int] = {
    Stratum.PHYSICS: 0,
    Stratum.TACTICS: 1,
    Stratum.STRATEGY: 2,
    Stratum.WISDOM: 3,
}

# Evidencia m√≠nima para considerar un estrato "validado".
# Un estrato S est√° validado ‚ü∫ TODAS las claves de evidencia est√°n
# presentes en el contexto con valor no-None y no-vac√≠o.
_STRATUM_EVIDENCE: Dict[Stratum, Tuple[str, ...]] = {
    Stratum.PHYSICS:  ("df_presupuesto", "df_insumos", "df_apus_raw"),
    Stratum.TACTICS:  ("df_apu_costos", "df_tiempo"),
    Stratum.STRATEGY: ("df_final",),
    Stratum.WISDOM:   ("final_result",),
}

# Valores por defecto para m√©tricas de materializaci√≥n
_DEFAULT_PYRAMID_STABILITY: float = 10.0
_DEFAULT_AVG_SATURATION: float = 0.0

# Versi√≥n del sobre de persistencia para evoluci√≥n futura del formato
_SESSION_ENVELOPE_VERSION: int = 1


def stratum_level(s: Stratum) -> int:
    """Retorna el nivel ordinal de un estrato en la filtraci√≥n DIKW."""
    return _STRATUM_ORDER.get(s, -1)


### B. MICRegistry ‚Äî M√©todos Refinados

Cambios clave:

    _normalize_validated_strata ahora se consume en project_intent.
    project_intent aplica validaci√≥n de filtraci√≥n antes de invocar el handler.


class MICRegistry:

    # ... (constructor y registros sin cambios) ...

    def _normalize_validated_strata(self, raw: Any) -> set:
        """Normaliza validated_strata de cualquier iterable a set[Stratum]."""
        if isinstance(raw, set):
            return {s for s in raw if isinstance(s, Stratum)}
        if isinstance(raw, (list, tuple)):
            return {s for s in raw if isinstance(s, Stratum)}
        return set()

    def _check_stratum_prerequisites(
        self,
        target_stratum: Stratum,
        validated_strata: set,
    ) -> Tuple[bool, List[str]]:
        """
        Verifica la clausura transitiva de la filtraci√≥n.

        Para ejecutar en estrato S, todo estrato con nivel < nivel(S)
        debe estar en `validated_strata`.

        Returns:
            (is_valid, list_of_missing_stratum_names)
        """
        target_level = stratum_level(target_stratum)
        missing = [
            s.name
            for s, level in _STRATUM_ORDER.items()
            if level < target_level and s not in validated_strata
        ]
        return (len(missing) == 0, missing)

    def project_intent(
        self,
        service_name: str,
        payload: Dict[str, Any],
        context: Dict[str, Any],
    ) -> Dict[str, Any]:
        """
        Proyecta una intenci√≥n sobre el espacio vectorial invocando un handler
        registrado, con validaci√≥n opcional de filtraci√≥n.

        Refinamiento respecto a V2:
        - Normaliza `validated_strata` desde el contexto (antes se ignoraba).
        - Emite advertencia si la filtraci√≥n no se cumple (soft-check por defecto;
          el hard-check queda en el Director).
        """
        if service_name not in self._vectors:
            available = self.registered_services
            raise ValueError(
                f"Unknown vector: '{service_name}'. "
                f"Available: {available if available else 'none registered'}"
            )

        target_stratum, handler = self._vectors[service_name]

        # ‚îÄ‚îÄ Soft-check de filtraci√≥n ‚îÄ‚îÄ
        raw_strata = context.get("validated_strata")
        validated = self._normalize_validated_strata(raw_strata)
        is_valid, missing = self._check_stratum_prerequisites(
            target_stratum, validated
        )
        if not is_valid:
            self.logger.warning(
                f"‚ö†Ô∏è Filtration soft-check: vector '{service_name}' targets "
                f"{target_stratum.name} but strata {missing} not yet validated."
            )

        try:
            result = handler(**payload)
            if not isinstance(result, dict):
                result = {"success": True, "result": result}
            if result.get("success", False):
                result["_mic_stratum"] = target_stratum.name
            return result
        except TypeError as e:
            self.logger.error(
                f"Handler signature mismatch for '{service_name}': {e}",
                exc_info=True,
            )
            return {"success": False, "error": str(e), "error_type": "signature"}
        except Exception as e:
            self.logger.error(
                f"Error executing vector '{service_name}': {e}",
                exc_info=True,
            )
            return {"success": False, "error": str(e), "error_type": "runtime"}


### C. PipelineDirector ‚Äî M√©todos Refinados
C.1 _compute_validated_strata (reemplaza _infer_current_stratum_from_context)

def _compute_validated_strata(self, context: dict) -> set:
    """
    Calcula el conjunto de estratos validados verificando evidencia
    en el contexto.

    Diferencia con V2 (`_infer_current_stratum_from_context`):
    - Retorna un *conjunto* de estratos, no un √∫nico m√°ximo.
    - Verifica que los valores no sean None ni DataFrames vac√≠os.
    - Detecta gaps en la filtraci√≥n (ej. STRATEGY presente sin TACTICS).

    Complejidad: O(|Estratos| √ó max|evidencia_i|) ‚Äî constante en la pr√°ctica.
    """
    validated = set()
    for stratum, evidence_keys in _STRATUM_EVIDENCE.items():
        all_present = True
        for key in evidence_keys:
            value = context.get(key)
            if value is None:
                all_present = False
                break
            # DataFrames vac√≠os no constituyen evidencia v√°lida
            if isinstance(value, pd.DataFrame) and value.empty:
                all_present = False
                break
        if all_present:
            validated.add(stratum)
    return validated


### C.2 _enforce_filtration_invariant (nuevo)

def _enforce_filtration_invariant(
    self,
    step_name: str,
    target_stratum: Stratum,
    validated_strata: set,
    strict: bool = False,
) -> bool:
    """
    Impone la restricci√≥n topol√≥gica de filtraci√≥n de subespacios:
        V_PHYSICS ‚äÇ V_TACTICS ‚äÇ V_STRATEGY ‚äÇ V_WISDOM

    Para ejecutar un operador en estrato S (nivel n), todos los estratos
    con nivel < n deben estar en `validated_strata`.

    Args:
        step_name: Etiqueta del paso (para logging).
        target_stratum: Estrato objetivo del paso a ejecutar.
        validated_strata: Conjunto de estratos ya validados.
        strict: Si True, lanza ValueError en violaci√≥n.
                Si False, emite warning y retorna False.

    Returns:
        True si la filtraci√≥n se cumple, False si hay violaci√≥n (modo no estricto).

    Raises:
        ValueError: En modo estricto cuando hay violaci√≥n.
    """
    target_level = stratum_level(target_stratum)
    missing = [
        s for s in _STRATUM_ORDER
        if stratum_level(s) < target_level and s not in validated_strata
    ]

    if not missing:
        return True

    missing_names = [s.name for s in missing]
    validated_names = sorted(
        [s.name for s in validated_strata],
        key=lambda name: stratum_level(Stratum[name]),
    )

    msg = (
        f"Filtration violation for step '{step_name}' "
        f"targeting {target_stratum.name} (level {target_level}): "
        f"prerequisite strata not validated: {missing_names}. "
        f"Currently validated: {validated_names if validated_names else '‚àÖ'}"
    )

    if strict:
        raise ValueError(msg)

    self.logger.warning(f"‚ö†Ô∏è {msg}")
    return False


### C.3 run_single_step (refinado)


def run_single_step(
    self,
    step_name: str,
    session_id: str,
    initial_context: Optional[Dict[str, Any]] = None,
    validate_stratum: bool = True,
    strict_filtration: bool = False,
) -> Dict[str, Any]:
    """
    Ejecuta un √∫nico paso del pipeline con validaci√≥n de filtraci√≥n.

    Refinamientos V3:
    - Usa `_compute_validated_strata` (conjunto) en lugar de
      `_infer_current_stratum_from_context` (escalar).
    - Impone clausura transitiva v√≠a `_enforce_filtration_invariant`.
    - Propaga `validated_strata` al contexto para consumo downstream.
    - Inyecci√≥n de MIC como atributo de instancia (expl√≠cita, sin try/except).
    """
    self.logger.info(
        f"Executing step: {step_name} (Session: {session_id[:8]}...)"
    )

    # 1. Cargar contexto de sesi√≥n
    context = self._load_context_state(session_id) or {}

    # 2. Fusionar initial_context (sesi√≥n tiene precedencia)
    if initial_context:
        context = {**initial_context, **context}

    try:
        # 3. Resolver vector base
        basis_vector = self.mic.get_basis_vector(step_name)
        if not basis_vector:
            available = self.mic.get_available_labels()
            raise ValueError(
                f"Step '{step_name}' not found in MIC. Available: {available}"
            )

        # 4. Validar filtraci√≥n de estratos (clausura transitiva)
        if validate_stratum:
            validated_strata = self._compute_validated_strata(context)
            target_stratum = basis_vector.stratum

            self._enforce_filtration_invariant(
                step_name=step_name,
                target_stratum=target_stratum,
                validated_strata=validated_strata,
                strict=strict_filtration,
            )

            # Propagar el conjunto validado al contexto
            context["validated_strata"] = validated_strata

        # 5. Instanciar y ejecutar
        step_instance = basis_vector.operator_class(self.config, self.thresholds)
        step_instance.mic = self.mic  # Inyecci√≥n expl√≠cita de MIC

        updated_context = step_instance.execute(context, self.telemetry)

        if updated_context is None:
            raise ValueError(
                f"Step '{step_name}' returned None context. "
                f"All steps must return the (possibly modified) context dict."
            )

        # 6. Recalcular y persistir estratos validados tras ejecuci√≥n
        updated_context["validated_strata"] = self._compute_validated_strata(
            updated_context
        )
        self._save_context_state(session_id, updated_context)

        self.logger.info(f"Step '{step_name}' completed successfully.")
        return {
            "status": "success",
            "step": step_name,
            "stratum": basis_vector.stratum.name,
            "session_id": session_id,
            "context_keys": list(updated_context.keys()),
            "validated_strata": [
                s.name for s in updated_context["validated_strata"]
            ],
        }

    except Exception as e:
        self.logger.error(
            f"Error executing step '{step_name}': {e}", exc_info=True
        )
        self.telemetry.record_error(step_name, str(e))
        return {
            "status": "error",
            "step": step_name,
            "error": str(e),
            "session_id": session_id,
        }


### C.4 execute_pipeline_orchestrated (refinado)

def execute_pipeline_orchestrated(self, initial_context: dict) -> dict:
    """
    Ejecuta el pipeline completo de forma orquestada.

    Refinamientos V3:
    - Elimina patr√≥n fr√°gil `first_step`: pasa `initial_context` en
      TODOS los pasos. La sem√°ntica de merge en `run_single_step`
      (sesi√≥n tiene precedencia) garantiza que no hay regresi√≥n.
    - Activaci√≥n de filtraci√≥n estricta configurable.
    - Registro de progreso como fracci√≥n completada.
    """
    session_id = str(uuid.uuid4())
    self.logger.info(
        f"Starting orchestrated pipeline (Session ID: {session_id})"
    )

    # Obtener receta de ejecuci√≥n
    default_recipe = self.mic.get_execution_sequence()
    recipe = self.config.get("pipeline_recipe", default_recipe)

    # Filtrar solo pasos habilitados para calcular total real
    enabled_steps = [
        s for s in recipe
        if s.get("step") and s.get("enabled", True)
    ]
    total_enabled = len(enabled_steps)

    if total_enabled == 0:
        self.logger.warning("‚ö†Ô∏è Pipeline recipe has no enabled steps.")
        return initial_context

    # Verificar persistencia funcional antes de iniciar
    self._save_context_state(session_id, initial_context)
    if self._load_context_state(session_id) is None:
        raise IOError(
            f"Failed to persist initial context for session {session_id}. "
            f"Check disk permissions on {self.session_dir}."
        )

    strict_filtration = self.config.get("strict_filtration", False)

    for step_idx, step_config in enumerate(enabled_steps):
        step_name = step_config["step"]

        self.logger.info(
            f"Orchestrating step [{step_idx + 1}/{total_enabled}]: {step_name}"
        )

        result = self.run_single_step(
            step_name,
            session_id,
            initial_context=initial_context,
            strict_filtration=strict_filtration,
        )

        if result["status"] == "error":
            error_msg = (
                f"Pipeline failed at step '{step_name}' "
                f"[{step_idx + 1}/{total_enabled}]: {result.get('error')}"
            )
            self.logger.critical(error_msg)
            # No limpiar sesi√≥n en error: permite an√°lisis forense
            raise RuntimeError(error_msg)

        self.telemetry.record_metric(
            "pipeline_progress",
            "fraction_complete",
            (step_idx + 1) / total_enabled,
        )

    final_context = self._load_context_state(session_id) or {}
    self._cleanup_session(session_id)

    self.logger.info(
        f"Pipeline completed successfully (Session: {session_id})"
    )
    return final_context


### C.5 _save_context_state / _load_context_state (con integridad)

def _save_context_state(self, session_id: str, context: dict):
    """
    Guarda el estado de una sesi√≥n con escritura at√≥mica y sobre
    de integridad (SHA-256 sobre el payload serializado).
    """
    try:
        session_file = self.session_dir / f"{session_id}.pkl"
        tmp_file = session_file.with_suffix(".pkl.tmp")

        payload = pickle.dumps(context, protocol=pickle.HIGHEST_PROTOCOL)
        checksum = hashlib.sha256(payload).hexdigest()

        envelope = {
            "_v": _SESSION_ENVELOPE_VERSION,
            "_checksum": checksum,
            "_ts": datetime.datetime.now(datetime.timezone.utc).isoformat(),
            "payload": payload,
        }

        with open(tmp_file, "wb") as f:
            pickle.dump(envelope, f, protocol=pickle.HIGHEST_PROTOCOL)
        tmp_file.replace(session_file)  # At√≥mica en POSIX

        self.logger.debug(
            f"Context saved for session {session_id} "
            f"(checksum: {checksum[:12]}‚Ä¶)"
        )
    except Exception as e:
        self.logger.error(
            f"Failed to save context for session {session_id}: {e}"
        )
        try:
            tmp_file = self.session_dir / f"{session_id}.pkl.tmp"
            if tmp_file.exists():
                tmp_file.unlink()
        except Exception:
            pass


def _load_context_state(self, session_id: str) -> Optional[dict]:
    """
    Carga y verifica integridad del estado de sesi√≥n.

    Refinamientos V3:
    - Verifica versi√≥n del sobre.
    - Valida SHA-256 del payload antes de deserializar.
    - Retrocompatibilidad con sesiones V2 (sin sobre).
    """
    if not session_id:
        return None
    try:
        session_file = self.session_dir / f"{session_id}.pkl"
        if not session_file.exists():
            return None

        with open(session_file, "rb") as f:
            raw = pickle.load(f)

        # ‚îÄ‚îÄ Retrocompatibilidad: sesiones V2 almacenan dict directo ‚îÄ‚îÄ
        if isinstance(raw, dict) and "_v" not in raw:
            self.logger.debug(
                f"Session {session_id}: legacy V2 format (no envelope)."
            )
            return raw

        # ‚îÄ‚îÄ Validar sobre V3 ‚îÄ‚îÄ
        if not isinstance(raw, dict) or "_v" not in raw:
            self.logger.error(
                f"Corrupted session {session_id}: unrecognized format."
            )
            return None

        envelope_version = raw.get("_v", 0)
        if envelope_version != _SESSION_ENVELOPE_VERSION:
            self.logger.warning(
                f"Session {session_id}: envelope version mismatch "
                f"(expected {_SESSION_ENVELOPE_VERSION}, got {envelope_version})."
            )

        payload_bytes = raw.get("payload")
        expected_checksum = raw.get("_checksum")

        if payload_bytes is None or expected_checksum is None:
            self.logger.error(
                f"Corrupted envelope for session {session_id}: "
                f"missing payload or checksum."
            )
            return None

        actual_checksum = hashlib.sha256(payload_bytes).hexdigest()
        if actual_checksum != expected_checksum:
            self.logger.error(
                f"Integrity violation in session {session_id}: "
                f"expected {expected_checksum[:12]}‚Ä¶, "
                f"got {actual_checksum[:12]}‚Ä¶"
            )
            return None

        context = pickle.loads(payload_bytes)
        if not isinstance(context, dict):
            self.logger.error(
                f"Corrupted session data {session_id}: "
                f"expected dict, got {type(context).__name__}"
            )
            return None

        return context

    except (pickle.UnpicklingError, EOFError, ModuleNotFoundError) as e:
        self.logger.error(
            f"Failed to deserialize session {session_id}: {e}"
        )
    except Exception as e:
        self.logger.error(
            f"Failed to load context for session {session_id}: {e}"
        )
    return None


### D. Pasos del Pipeline ‚Äî M√©todos Refinados
D.1 LoadDataStep.execute (contexto inmutable + consistencia)

class LoadDataStep(ProcessingStep):
    """
    Paso de Carga de Datos.

    Refinamiento V3: El contexto de entrada nunca se muta;
    se construye un nuevo dict con los artefactos producidos.
    """

    def __init__(self, config: dict, thresholds: ProcessingThresholds):
        if not config or not isinstance(config, dict):
            raise ValueError("Configuraci√≥n inv√°lida para LoadDataStep")
        self.config = config
        self.thresholds = thresholds or ProcessingThresholds()

    def _load_file_profiles(self) -> dict:
        """Extrae perfiles de archivo con defaults seguros."""
        profiles = self.config.get("file_profiles", {})
        if not profiles:
            logger.warning(
                "‚ö†Ô∏è 'file_profiles' no encontrado en config, usando defaults."
            )
            return {
                "presupuesto_default": {},
                "insumos_default": {},
                "apus_default": {},
            }
        return profiles

    def _validate_paths(
        self, context: dict, telemetry: TelemetryContext
    ) -> Dict[str, str]:
        """Valida existencia de rutas requeridas en el contexto."""
        required = ("presupuesto_path", "apus_path", "insumos_path")
        paths = {}
        file_validator = FileValidator()

        for key in required:
            value = context.get(key)
            if not value:
                error = f"Ruta requerida '{key}' no encontrada en contexto"
                telemetry.record_error("load_data", error)
                raise ValueError(error)
            paths[key] = value

        label_map = {
            "presupuesto_path": "presupuesto",
            "apus_path": "APUs",
            "insumos_path": "insumos",
        }
        for key, file_type in label_map.items():
            is_valid, error = file_validator.validate_file_exists(
                paths[key], file_type
            )
            if not is_valid:
                telemetry.record_error("load_data", error)
                raise ValueError(error)

        return paths

    def execute(self, context: dict, telemetry: TelemetryContext) -> dict:
        telemetry.start_step("load_data")
        try:
            paths = self._validate_paths(context, telemetry)
            profiles = self._load_file_profiles()

            # ‚îÄ‚îÄ Presupuesto ‚îÄ‚îÄ
            p_profile = profiles.get("presupuesto_default", {})
            p_processor = PresupuestoProcessor(
                self.config, self.thresholds, p_profile
            )
            df_presupuesto = p_processor.process(paths["presupuesto_path"])

            if df_presupuesto is None or df_presupuesto.empty:
                error = "Procesamiento de presupuesto retorn√≥ DataFrame vac√≠o"
                telemetry.record_error("load_data", error)
                raise ValueError(error)
            telemetry.record_metric(
                "load_data", "presupuesto_rows", len(df_presupuesto)
            )

            # ‚îÄ‚îÄ Insumos ‚îÄ‚îÄ
            i_profile = profiles.get("insumos_default", {})
            i_processor = InsumosProcessor(self.thresholds, i_profile)
            df_insumos = i_processor.process(paths["insumos_path"])

            if df_insumos is None or df_insumos.empty:
                error = "Procesamiento de insumos retorn√≥ DataFrame vac√≠o"
                telemetry.record_error("load_data", error)
                raise ValueError(error)
            telemetry.record_metric(
                "load_data", "insumos_rows", len(df_insumos)
            )

            # ‚îÄ‚îÄ APUs: Vector de estabilizaci√≥n (MIC) ‚îÄ‚îÄ
            flux_result = self.mic.project_intent(
                "stabilize_flux",
                {"file_path": str(paths["apus_path"]), "config": self.config},
                context,
            )
            if not flux_result.get("success"):
                error = flux_result.get(
                    "error", "Unknown error in stabilize_flux"
                )
                telemetry.record_error("load_data", error)
                raise ValueError(error)

            df_apus_raw = pd.DataFrame(flux_result["data"])
            if df_apus_raw.empty:
                error = "DataFluxCondenser vector returned empty data"
                telemetry.record_error("load_data", error)
                raise ValueError(error)
            telemetry.record_metric(
                "load_data", "apus_raw_rows", len(df_apus_raw)
            )
            logger.info("‚úÖ Vector stabilize_flux completado.")

            # ‚îÄ‚îÄ APUs: Vector de parsing topol√≥gico (MIC) ‚îÄ‚îÄ
            apus_profile = profiles.get("apus_default", {})
            parse_result = self.mic.project_intent(
                "parse_raw",
                {
                    "file_path": str(paths["apus_path"]),
                    "profile": apus_profile,
                },
                context,
            )
            if not parse_result.get("success"):
                error = parse_result.get(
                    "error", "Unknown error in parse_raw"
                )
                telemetry.record_error("load_data", error)
                raise ValueError(error)

            raw_records = parse_result["raw_records"]
            parse_cache = parse_result["parse_cache"]
            telemetry.record_metric(
                "load_data", "raw_records_count", len(raw_records)
            )
            logger.info("‚úÖ Vector parse_raw completado.")

            # ‚îÄ‚îÄ Validaci√≥n post-carga ‚îÄ‚îÄ
            data_validator = DataValidator()
            for df, name in [
                (df_presupuesto, "presupuesto"),
                (df_insumos, "insumos"),
                (df_apus_raw, "APUs"),
            ]:
                is_valid, error = data_validator.validate_dataframe_not_empty(
                    df, name
                )
                if not is_valid:
                    telemetry.record_error("load_data", error)
                    raise ValueError(error)

            # ‚îÄ‚îÄ Producir nuevo contexto (copy-on-write) ‚îÄ‚îÄ
            new_context = {**context}
            new_context.update(
                {
                    "df_presupuesto": df_presupuesto,
                    "df_insumos": df_insumos,
                    "df_apus_raw": df_apus_raw,
                    "raw_records": raw_records,
                    "parse_cache": parse_cache,
                }
            )

            telemetry.end_step("load_data", "success")
            return new_context

        except Exception as e:
            logger.error(f"‚ùå Error en LoadDataStep: {e}", exc_info=True)
            telemetry.record_error("load_data", str(e))
            telemetry.end_step("load_data", "error")
            raise


### D.2 CalculateCostsStep.execute (corrige dead code path)

class CalculateCostsStep(ProcessingStep):
    """
    Paso de C√°lculo de Costos.

    Refinamiento V3:
    - Protocolo de dos fases con fallback expl√≠cito:
      Fase 1 (MIC): Si `structure_logic` provee los 3 DataFrames tipados,
                     se adoptan como resultado primario.
      Fase 2 (Cl√°sico): Fallback a APUProcessor.process_vectors si MIC no
                         proporcion√≥ artefactos completos.
    - Se elimina la ejecuci√≥n incondicional de process_vectors que
      descartaba el resultado MIC (defecto V2).
    """

    def __init__(self, config: dict, thresholds: ProcessingThresholds):
        self.config = config
        self.thresholds = thresholds

    def _try_mic_structured_logic(
        self, context: dict, telemetry: TelemetryContext
    ) -> Optional[Dict[str, pd.DataFrame]]:
        """
        Intenta obtener los vectores de costo desde el vector MIC
        `structure_logic`. Retorna dict con los 3 DataFrames si exitoso,
        None en caso contrario (degradaci√≥n controlada).
        """
        raw_records = context.get("raw_records", [])
        parse_cache = context.get("parse_cache", {})

        if not self.mic or not raw_records:
            return None

        try:
            logic_result = self.mic.project_intent(
                "structure_logic",
                {
                    "raw_records": raw_records,
                    "parse_cache": parse_cache,
                    "config": self.config,
                },
                context,
            )

            if not logic_result.get("success"):
                logger.warning(
                    f"‚ö†Ô∏è MIC structure_logic failed: "
                    f"{logic_result.get('error', 'unknown')}"
                )
                return None

            # Extraer los 3 DataFrames esperados
            candidates = {
                "df_apu_costos": logic_result.get("df_apu_costos"),
                "df_tiempo": logic_result.get("df_tiempo"),
                "df_rendimiento": logic_result.get("df_rendimiento"),
            }

            # Validar que TODOS sean DataFrames no vac√≠os
            for key, df in candidates.items():
                if not isinstance(df, pd.DataFrame) or df.empty:
                    logger.info(
                        f"‚ÑπÔ∏è MIC structure_logic: '{key}' not provided or empty. "
                        f"Falling back to classical computation."
                    )
                    return None

            # Capturar quality report si disponible
            quality_report = logic_result.get("quality_report")
            if quality_report:
                context["quality_report"] = quality_report

            logger.info("‚úÖ Cost vectors produced by MIC structure_logic.")
            return candidates

        except Exception as e:
            logger.warning(f"‚ö†Ô∏è MIC structure_logic degraded: {e}")
            telemetry.record_error("calculate_costs_mic", str(e))
            return None

    def execute(self, context: dict, telemetry: TelemetryContext) -> dict:
        telemetry.start_step("calculate_costs")
        try:
            df_merged = context.get("df_merged")
            if df_merged is None or df_merged.empty:
                error = "df_merged is missing or empty: cannot calculate costs."
                telemetry.record_error("calculate_costs", error)
                raise ValueError(error)

            # Fase 1: Intentar v√≠a MIC (soft dependency)
            mic_result = self._try_mic_structured_logic(context, telemetry)
            source = "mic"

            if mic_result is not None:
                # MIC proporcion√≥ los 3 vectores tipados
                context["df_apu_costos"] = mic_result["df_apu_costos"]
                context["df_tiempo"] = mic_result["df_tiempo"]
                context["df_rendimiento"] = mic_result["df_rendimiento"]
            else:
                # Fase 2: Fallback a c√≥mputo cl√°sico
                source = "classical"
                logger.info(
                    "üõ†Ô∏è Using classical APUProcessor.process_vectors"
                )
                processor = APUProcessor(self.config)
                df_apu_costos, df_tiempo, df_rendimiento = (
                    processor.process_vectors(df_merged)
                )
                context["df_apu_costos"] = df_apu_costos
                context["df_tiempo"] = df_tiempo
                context["df_rendimiento"] = df_rendimiento

            # ‚îÄ‚îÄ M√©tricas ‚îÄ‚îÄ
            telemetry.record_metric(
                "calculate_costs", "costos_rows",
                len(context["df_apu_costos"]),
            )
            telemetry.record_metric(
                "calculate_costs", "tiempo_rows",
                len(context["df_tiempo"]),
            )
            telemetry.record_metric("calculate_costs", "source", source)

            telemetry.end_step("calculate_costs", "success")
            return context

        except Exception as e:
            telemetry.record_error("calculate_costs", str(e))
            telemetry.end_step("calculate_costs", "error")
            raise


### D.3 BusinessTopologyStep.execute (corrige strata hardcodeados + MIC)

class BusinessTopologyStep(ProcessingStep):
    """
    Paso de An√°lisis de Negocio.

    Refinamientos V3:
    - `validated_strata` se computa desde evidencia contextual (antes
      se hardcodeaba {PHYSICS, TACTICS}).
    - Se usa `self.mic` inyectado antes de recurrir al contexto Flask.
    - Evaluaci√≥n BusinessAgent como fase degradable documentada.
    """

    def __init__(self, config: dict, thresholds: ProcessingThresholds):
        self.config = config
        self.thresholds = thresholds

    def _compute_local_validated_strata(self, context: dict) -> set:
        """
        Calcula estratos validados desde evidencia contextual.
        Reutiliza la misma l√≥gica que PipelineDirector pero localizada
        para evitar dependencia circular.
        """
        validated = set()
        for stratum, evidence_keys in _STRATUM_EVIDENCE.items():
            if all(
                context.get(k) is not None
                and not (isinstance(context[k], pd.DataFrame) and context[k].empty)
                for k in evidence_keys
            ):
                validated.add(stratum)
        return validated

    def _resolve_mic(self) -> Optional['MICRegistry']:
        """
        Resuelve MIC con prioridad:
          1. self.mic (inyectado por Director)
          2. current_app.mic (contexto Flask)
          3. None (degradaci√≥n controlada)
        """
        if self.mic is not None:
            return self.mic

        try:
            mic_from_app = getattr(current_app, "mic", None)
            if mic_from_app is not None:
                return mic_from_app
        except RuntimeError:
            logger.debug(
                "No Flask app context available for MIC resolution."
            )

        return None

    def execute(self, context: dict, telemetry: TelemetryContext) -> dict:
        telemetry.start_step("business_topology")
        try:
            df_final = context.get("df_final")
            df_merged = context.get("df_merged")

            if df_final is None:
                error = "df_final is required for BusinessTopologyStep."
                telemetry.record_error("business_topology", error)
                raise ValueError(error)

            # ‚îÄ‚îÄ Fase 1: Materializaci√≥n del grafo topol√≥gico ‚îÄ‚îÄ
            builder = BudgetGraphBuilder()
            graph = builder.build(
                df_final,
                df_merged if df_merged is not None else pd.DataFrame(),
            )
            context["graph"] = graph
            logger.info(
                f"üï∏Ô∏è Grafo de negocio: "
                f"{graph.number_of_nodes()} nodos, "
                f"{graph.number_of_edges()} aristas"
            )
            telemetry.record_metric(
                "business_topology", "graph_nodes",
                graph.number_of_nodes(),
            )
            telemetry.record_metric(
                "business_topology", "graph_edges",
                graph.number_of_edges(),
            )

            # ‚îÄ‚îÄ Fase 2: C√≥mputo de estratos validados (sin hardcoding) ‚îÄ‚îÄ
            validated = self._compute_local_validated_strata(context)
            context["validated_strata"] = validated
            logger.info(
                f"üìä Strata validated from evidence: "
                f"{[s.name for s in sorted(validated, key=stratum_level)]}"
            )

            # ‚îÄ‚îÄ Fase 3: Evaluaci√≥n BusinessAgent (degradable) ‚îÄ‚îÄ
            mic_instance = self._resolve_mic()
            if mic_instance:
                try:
                    agent = BusinessAgent(
                        config=self.config,
                        mic=mic_instance,
                        telemetry=telemetry,
                    )
                    report = agent.evaluate_project(context)
                    if report:
                        context["business_topology_report"] = report
                        logger.info("‚úÖ BusinessAgent complet√≥ la evaluaci√≥n.")
                    else:
                        logger.warning(
                            "‚ö†Ô∏è BusinessAgent retorn√≥ reporte vac√≠o."
                        )
                except Exception as ba_error:
                    logger.warning(
                        f"‚ö†Ô∏è BusinessAgent evaluation degraded: {ba_error}",
                        exc_info=True,
                    )
                    telemetry.record_error("business_agent", str(ba_error))
            else:
                logger.warning(
                    "‚ö†Ô∏è Sin instancia MIC. "
                    "Evaluaci√≥n limitada a grafo topol√≥gico."
                )

            telemetry.end_step("business_topology", "success")
            return context

        except Exception as e:
            logger.error(
                f"‚ùå Error en BusinessTopologyStep: {e}", exc_info=True
            )
            telemetry.record_error("business_topology", str(e))
            telemetry.end_step("business_topology", "error")
            raise