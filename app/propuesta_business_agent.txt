### 1. TopologicalMetricsBundle.structural_coherence ‚Äî Refinado

@property
def structural_coherence(self) -> float:
    """
    Calcula un √≠ndice de coherencia estructural mediante invariantes topol√≥gicos.

    Fundamento Matem√°tico (Topolog√≠a Algebraica):
    =============================================
    Sea K un complejo simplicial asociado al presupuesto. Definimos:
    
    C(K) = exp(-Œª‚ÇÄ¬∑max(0, Œ≤‚ÇÄ-1)) √ó exp(-Œª‚ÇÅ¬∑Œ≤‚ÇÅ/n) √ó Œ®
    
    donde:
    - Œ≤‚ÇÄ: N√∫mero de componentes conexas (H‚ÇÄ). Ideal: Œ≤‚ÇÄ = 1 (conexidad)
    - Œ≤‚ÇÅ: Primer n√∫mero de Betti (H‚ÇÅ). Ciclos independientes ‚âà dependencias circulares
    - n: N√∫mero de v√©rtices (normalizaci√≥n por escala)
    - Œ®: √çndice de estabilidad piramidal ‚àà [0, 1]
    - Œª‚ÇÄ, Œª‚ÇÅ: Tasas de decaimiento (derivadas de an√°lisis de sensibilidad)

    La exponencial garantiza:
    1. Monotonicidad decreciente en patolog√≠as
    2. Composici√≥n multiplicativa (log-aditiva en el espacio de riesgos)
    3. Rango natural en [0, 1] sin truncamiento artificial

    Returns:
        float: √çndice de coherencia ‚àà [0, 1], donde 1 = m√°xima coherencia topol√≥gica.
    """
    import math
    
    beta_0 = self.betti_numbers.get("beta_0", 1)
    beta_1 = self.betti_numbers.get("beta_1", 0)
    
    # Obtener cardinalidad del complejo para normalizaci√≥n
    n_vertices = 1  # Default seguro
    if hasattr(self.graph, 'number_of_nodes'):
        n_vertices = max(self.graph.number_of_nodes(), 1)
    elif hasattr(self.graph, '__len__'):
        n_vertices = max(len(self.graph), 1)
    
    # Tasas de decaimiento fundamentadas en an√°lisis de sensibilidad
    # Œª‚ÇÄ = ln(2) ‚Üí cada componente adicional reduce coherencia a la mitad
    # Œª‚ÇÅ ajustado por densidad del grafo para evitar penalizaci√≥n excesiva en grafos densos
    lambda_0 = math.log(2)  # ‚âà 0.693
    lambda_1 = math.log(2) / max(1, math.sqrt(n_vertices))  # Escala con ‚àön
    
    # Penalizaci√≥n por fragmentaci√≥n (Œ≤‚ÇÄ > 1 indica desconexi√≥n)
    # exp(-Œª‚ÇÄ¬∑(Œ≤‚ÇÄ-1)): Œ≤‚ÇÄ=1‚Üí1, Œ≤‚ÇÄ=2‚Üí0.5, Œ≤‚ÇÄ=3‚Üí0.25, ...
    excess_components = max(0, beta_0 - 1)
    fragmentation_factor = math.exp(-lambda_0 * excess_components)
    
    # Penalizaci√≥n por ciclos, normalizada por tama√±o
    # Densidad de ciclos: Œ≤‚ÇÅ/n evita penalizar grafos grandes injustamente
    cycle_density = beta_1 / n_vertices
    cycle_factor = math.exp(-lambda_1 * beta_1) if beta_1 < n_vertices else math.exp(-lambda_1 * n_vertices)
    
    # Composici√≥n multiplicativa en el grupo ([0,1], √ó)
    raw_coherence = fragmentation_factor * cycle_factor * self.pyramid_stability
    
    # Clamp por seguridad num√©rica (aunque matem√°ticamente ya est√° en [0,1])
    return max(0.0, min(1.0, raw_coherence))


### 2. RiskChallenger ‚Äî Refinado con L√≥gica de Umbrales Graduales

class RiskChallenger:
    """
    Motor de Auditor√≠a Adversarial basado en L√≥gica Fuzzy y Reglas de Consistencia.

    Implementa un sistema de veto multi-nivel que detecta contradicciones entre
    los espacios financiero (Œ¶) y topol√≥gico (T) mediante reglas de inferencia:

    R‚ÇÅ: (Œ¶ ‚àà SAFE) ‚àß (Œ® < Œ∏_cr√≠tico) ‚Üí VETO_ESTRUCTURAL
    R‚ÇÇ: (Œ¶ ‚àà SAFE) ‚àß (C < Œ∏_coherencia) ‚Üí ALERTA_COHERENCIA
    R‚ÇÉ: (Œ≤‚ÇÅ > n/3) ‚àß (Œ¶ ‚àà PROFITABLE) ‚Üí RIESGO_CICLOS

    donde Œ∏ son umbrales configurables por dominio.
    """

    # Umbrales por defecto (calibrados emp√≠ricamente)
    DEFAULT_THRESHOLDS = {
        "critical_stability": 0.70,      # Œ® < 0.70 ‚Üí Veto inmediato
        "warning_stability": 0.85,       # 0.70 ‚â§ Œ® < 0.85 ‚Üí Alerta severa
        "coherence_minimum": 0.60,       # C < 0.60 ‚Üí Degradaci√≥n de score
        "cycle_density_limit": 0.33,     # Œ≤‚ÇÅ/n > 1/3 ‚Üí Advertencia de ciclos
        "integrity_penalty_veto": 0.30,  # Penalizaci√≥n por veto estructural
        "integrity_penalty_warn": 0.15,  # Penalizaci√≥n por alerta
    }

    def __init__(self, config: Optional[Dict[str, float]] = None):
        """
        Inicializa el Challenger con umbrales configurables.

        Args:
            config: Diccionario con umbrales personalizados. Claves v√°lidas:
                    - critical_stability, warning_stability, coherence_minimum,
                    - cycle_density_limit, integrity_penalty_veto, integrity_penalty_warn
        """
        self.thresholds = {**self.DEFAULT_THRESHOLDS}
        if config:
            # Solo aceptar claves v√°lidas
            for key in self.DEFAULT_THRESHOLDS:
                if key in config:
                    self.thresholds[key] = float(config[key])

    def _extract_stability_metrics(
        self, details: Dict[str, Any]
    ) -> Tuple[Optional[float], Optional[float], Optional[int], Optional[int]]:
        """
        Extrae m√©tricas de estabilidad de la estructura anidada del reporte.

        Returns:
            Tupla (Œ®, coherencia, Œ≤‚ÇÅ, n_nodos) con None para valores no encontrados.
        """
        stability = details.get("pyramid_stability")
        coherence = details.get("structural_coherence")
        beta_1 = None
        n_nodes = None

        # Buscar en estructura anidada
        topo_inv = details.get("topological_invariants", {})
        if stability is None:
            stability = topo_inv.get("pyramid_stability")
        if coherence is None:
            coherence = topo_inv.get("structural_coherence")

        betti = topo_inv.get("betti_numbers", {})
        beta_1 = betti.get("beta_1")

        # Intentar obtener n√∫mero de nodos del grafo
        if "graph_order" in details:
            n_nodes = details["graph_order"]
        elif "n_nodes" in topo_inv:
            n_nodes = topo_inv["n_nodes"]

        return stability, coherence, beta_1, n_nodes

    def _classify_financial_risk(self, risk_level: Any) -> str:
        """
        Normaliza el nivel de riesgo financiero a categor√≠as est√°ndar.

        Returns:
            Una de: "SAFE", "MODERATE", "HIGH", "UNKNOWN"
        """
        risk_str = str(risk_level).upper().strip()
        
        safe_keywords = {"LOW", "BAJO", "SAFE", "SEGURO", "MINIMAL", "M√çNIMO"}
        moderate_keywords = {"MODERATE", "MODERADO", "MEDIUM", "MEDIO"}
        high_keywords = {"HIGH", "ALTO", "CRITICAL", "CR√çTICO", "SEVERE", "SEVERO"}

        if any(kw in risk_str for kw in safe_keywords):
            return "SAFE"
        elif any(kw in risk_str for kw in moderate_keywords):
            return "MODERATE"
        elif any(kw in risk_str for kw in high_keywords):
            return "HIGH"
        return "UNKNOWN"

    def challenge_verdict(
        self, report: ConstructionRiskReport
    ) -> ConstructionRiskReport:
        """
        Ejecuta auditor√≠a adversarial multi-nivel sobre el reporte.

        Aplica un sistema de reglas de inferencia para detectar contradicciones
        l√≥gicas entre m√©tricas financieras y estructurales, emitiendo vetos
        graduados seg√∫n la severidad de la inconsistencia.

        Args:
            report: Reporte preliminar a auditar.

        Returns:
            ConstructionRiskReport auditado con posibles modificaciones en:
            - financial_risk_level (si hay veto)
            - integrity_score (penalizado si hay contradicciones)
            - strategic_narrative (con acta de deliberaci√≥n adjunta)
            - details (con registro del challenge)
        """
        logger.info("‚öñÔ∏è  Risk Challenger: Iniciando auditor√≠a adversarial...")

        details = report.details or {}
        stability, coherence, beta_1, n_nodes = self._extract_stability_metrics(details)
        financial_class = self._classify_financial_risk(report.financial_risk_level)

        # Si no hay m√©tricas suficientes, no podemos auditar
        if stability is None:
            logger.warning(
                "‚ö†Ô∏è  Risk Challenger: M√©tricas de estabilidad no disponibles. "
                "Auditor√≠a omitida."
            )
            return report

        # === REGLA 1: Veto por Estabilidad Cr√≠tica ===
        if stability < self.thresholds["critical_stability"]:
            if financial_class in ("SAFE", "MODERATE"):
                return self._emit_veto(
                    report=report,
                    veto_type="VETO_CRITICAL_INSTABILITY",
                    stability=stability,
                    financial_class=financial_class,
                    severity="CR√çTICO",
                    penalty=self.thresholds["integrity_penalty_veto"],
                    reason=(
                        f"Estabilidad piramidal Œ®={stability:.3f} est√° por debajo del "
                        f"umbral cr√≠tico ({self.thresholds['critical_stability']:.2f}). "
                        "El proyecto presenta riesgo de colapso log√≠stico."
                    ),
                )

        # === REGLA 2: Alerta por Estabilidad Sub√≥ptima ===
        if stability < self.thresholds["warning_stability"]:
            if financial_class == "SAFE":
                return self._emit_veto(
                    report=report,
                    veto_type="ALERTA_STRUCTURAL_WARNING",
                    stability=stability,
                    financial_class=financial_class,
                    severity="SEVERO",
                    penalty=self.thresholds["integrity_penalty_warn"],
                    reason=(
                        f"Estabilidad piramidal Œ®={stability:.3f} es sub√≥ptima "
                        f"(umbral de alerta: {self.thresholds['warning_stability']:.2f}). "
                        "Financieramente sano pero estructuralmente fr√°gil."
                    ),
                )

        # === REGLA 3: Alerta por Densidad de Ciclos ===
        if beta_1 is not None and n_nodes is not None and n_nodes > 0:
            cycle_density = beta_1 / n_nodes
            if cycle_density > self.thresholds["cycle_density_limit"]:
                if financial_class in ("SAFE", "MODERATE"):
                    logger.warning(
                        f"‚ö†Ô∏è  Densidad de ciclos Œ≤‚ÇÅ/n = {cycle_density:.3f} excede "
                        f"el l√≠mite {self.thresholds['cycle_density_limit']:.2f}"
                    )
                    # No es veto, pero registrar en detalles
                    new_details = details.copy()
                    new_details["challenger_cycle_warning"] = {
                        "beta_1": beta_1,
                        "n_nodes": n_nodes,
                        "cycle_density": cycle_density,
                        "threshold": self.thresholds["cycle_density_limit"],
                    }
                    return ConstructionRiskReport(
                        integrity_score=report.integrity_score * 0.95,  # Penalizaci√≥n leve
                        waste_alerts=report.waste_alerts,
                        circular_risks=report.circular_risks,
                        complexity_level=report.complexity_level,
                        financial_risk_level=report.financial_risk_level,
                        details=new_details,
                        strategic_narrative=report.strategic_narrative,
                    )

        logger.info("‚úÖ Risk Challenger: Coherencia verificada. Sin contradicciones.")
        return report

    def _emit_veto(
        self,
        report: ConstructionRiskReport,
        veto_type: str,
        stability: float,
        financial_class: str,
        severity: str,
        penalty: float,
        reason: str,
    ) -> ConstructionRiskReport:
        """
        Emite un veto estructurado con acta de deliberaci√≥n.

        Args:
            report: Reporte original.
            veto_type: C√≥digo del tipo de veto.
            stability: Valor de Œ® que dispar√≥ el veto.
            financial_class: Clasificaci√≥n financiera original.
            severity: Nivel de severidad ("CR√çTICO", "SEVERO", "MODERADO").
            penalty: Factor de penalizaci√≥n ‚àà [0, 1].
            reason: Justificaci√≥n textual del veto.

        Returns:
            Reporte modificado con el veto aplicado.
        """
        logger.warning(f"üö® Risk Challenger: {veto_type} - {reason}")

        original_integrity = report.integrity_score
        new_integrity = max(0.0, original_integrity * (1.0 - penalty))

        # Acta de deliberaci√≥n formal
        debate_log = (
            "‚îÅ" * 60 + "\n"
            "üèõÔ∏è **ACTA DE DELIBERACI√ìN DEL CONSEJO DE RIESGO**\n"
            "‚îÅ" * 60 + "\n\n"
            f"üìã **Tipo de Veto:** {veto_type}\n"
            f"‚ö†Ô∏è  **Severidad:** {severity}\n\n"
            "**Posiciones de los Agentes:**\n\n"
            f"1. ü§µ **Gestor Financiero:** ¬´El proyecto es financieramente {financial_class}. "
            "Los indicadores de rentabilidad son favorables.¬ª\n\n"
            f"2. üë∑ **Ingeniero Estructural:** ¬´OBJECI√ìN. {reason}¬ª\n\n"
            f"3. ‚öñÔ∏è  **Fiscal de Riesgos:** ¬´Se detecta contradicci√≥n l√≥gica entre "
            f"viabilidad financiera (Œ¶={financial_class}) y estabilidad estructural "
            f"(Œ®={stability:.3f}).¬ª\n\n"
            "**VEREDICTO FINAL:**\n"
            f"Se emite **{veto_type}**. La integridad del proyecto se degrada de "
            f"{original_integrity:.1f} a {new_integrity:.1f} puntos.\n\n"
            "‚îÅ" * 60
        )

        new_narrative = f"{debate_log}\n\n{report.strategic_narrative}"

        new_details = report.details.copy() if report.details else {}
        new_details["challenger_verdict"] = {
            "type": veto_type,
            "severity": severity,
            "stability_at_veto": stability,
            "financial_class_at_veto": financial_class,
            "original_integrity": original_integrity,
            "penalty_applied": penalty,
            "reason": reason,
        }

        return ConstructionRiskReport(
            integrity_score=new_integrity,
            waste_alerts=report.waste_alerts,
            circular_risks=report.circular_risks,
            complexity_level=report.complexity_level,
            financial_risk_level=f"RIESGO ESTRUCTURAL ({severity})",
            details=new_details,
            strategic_narrative=new_narrative,
        )


### 3. _validate_dataframes ‚Äî Refinado con Verificaci√≥n Topol√≥gica

def _validate_dataframes(
    self,
    df_presupuesto: Optional[pd.DataFrame],
    df_apus_detail: Optional[pd.DataFrame],
) -> Tuple[bool, str, Optional[Dict[str, Any]]]:
    """
    Validaci√≥n estructural y topol√≥gica de DataFrames de entrada.

    Implementa verificaci√≥n en tres niveles:
    1. **Existencia**: DataFrames no nulos y no vac√≠os
    2. **Esquema**: Columnas requeridas presentes con tipos correctos
    3. **Consistencia Referencial**: Integridad de claves for√°neas entre DFs
    4. **Distribuci√≥n**: Detecci√≥n de anomal√≠as estad√≠sticas

    Args:
        df_presupuesto: DataFrame del presupuesto general.
        df_apus_detail: DataFrame con detalle de APUs (merged).

    Returns:
        Tupla (es_v√°lido, mensaje, diagn√≥stico) donde diagn√≥stico contiene
        m√©tricas adicionales de calidad de datos si la validaci√≥n es exitosa.
    """
    diagnostics: Dict[str, Any] = {
        "validation_timestamp": pd.Timestamp.now().isoformat(),
        "warnings": [],
        "schema_compatibility": {},
    }

    # ‚îÅ‚îÅ‚îÅ Nivel 1: Existencia B√°sica ‚îÅ‚îÅ‚îÅ
    if df_presupuesto is None:
        return False, "DataFrame 'df_presupuesto' es None", None
    if df_apus_detail is None:
        return False, "DataFrame 'df_apus_detail' es None", None
    if df_presupuesto.empty:
        return False, "DataFrame 'df_presupuesto' est√° vac√≠o", None
    if df_apus_detail.empty:
        return False, "DataFrame 'df_apus_detail' est√° vac√≠o", None

    diagnostics["row_counts"] = {
        "presupuesto": len(df_presupuesto),
        "apus_detail": len(df_apus_detail),
    }

    # ‚îÅ‚îÅ‚îÅ Nivel 2: Validaci√≥n de Esquema con √Ålgebra de Columnas ‚îÅ‚îÅ‚îÅ
    # Espacios vectoriales de columnas requeridas
    budget_schema = {
        ColumnNames.CODIGO_APU: {"type": "categorical", "required": True},
        ColumnNames.DESCRIPCION_APU: {"type": "string", "required": True},
        ColumnNames.VALOR_TOTAL: {"type": "numeric", "required": False, "min": 0},
    }

    detail_schema = {
        ColumnNames.CODIGO_APU: {"type": "categorical", "required": True},
        ColumnNames.DESCRIPCION_INSUMO: {"type": "string", "required": True},
        ColumnNames.CANTIDAD_APU: {"type": "numeric", "required": True, "min": 0},
        ColumnNames.COSTO_INSUMO_EN_APU: {"type": "numeric", "required": True, "min": 0},
    }

    # Mapeo de compatibilidad con esquemas legacy
    legacy_mappings = {
        "item": ColumnNames.CODIGO_APU,
        "descripcion": ColumnNames.DESCRIPCION_APU,
        "total": ColumnNames.VALOR_TOTAL,
        "codigo": ColumnNames.CODIGO_APU,
        "desc_insumo": ColumnNames.DESCRIPCION_INSUMO,
        "cantidad": ColumnNames.CANTIDAD_APU,
        "costo": ColumnNames.COSTO_INSUMO_EN_APU,
    }

    def find_column(df: pd.DataFrame, target: str, mappings: Dict) -> Optional[str]:
        """Busca una columna por nombre moderno o legacy."""
        if target in df.columns:
            return target
        for legacy, modern in mappings.items():
            if modern == target and legacy in df.columns:
                return legacy
        return None

    def validate_schema(
        df: pd.DataFrame, schema: Dict, df_name: str
    ) -> Tuple[bool, List[str]]:
        """Valida un DataFrame contra su esquema."""
        errors = []
        for col_name, spec in schema.items():
            actual_col = find_column(df, col_name, legacy_mappings)

            if actual_col is None:
                if spec["required"]:
                    errors.append(f"{df_name}: Columna requerida '{col_name}' no encontrada")
                continue

            # Registrar mapeo para diagn√≥stico
            if actual_col != col_name:
                diagnostics["schema_compatibility"][actual_col] = col_name

            # Validar tipo
            if spec["type"] == "numeric":
                if not pd.api.types.is_numeric_dtype(df[actual_col]):
                    errors.append(
                        f"{df_name}: Columna '{actual_col}' debe ser num√©rica, "
                        f"es {df[actual_col].dtype}"
                    )
                elif "min" in spec:
                    invalid_count = (df[actual_col] < spec["min"]).sum()
                    if invalid_count > 0:
                        errors.append(
                            f"{df_name}: '{actual_col}' tiene {invalid_count} valores "
                            f"< {spec['min']}"
                        )

        return len(errors) == 0, errors

    # Validar ambos DataFrames
    budget_valid, budget_errors = validate_schema(
        df_presupuesto, budget_schema, "Presupuesto"
    )
    detail_valid, detail_errors = validate_schema(
        df_apus_detail, detail_schema, "APUs Detail"
    )

    all_errors = budget_errors + detail_errors
    if all_errors:
        return False, "; ".join(all_errors), diagnostics

    # ‚îÅ‚îÅ‚îÅ Nivel 3: Consistencia Referencial (Integridad de FK) ‚îÅ‚îÅ‚îÅ
    budget_apu_col = find_column(df_presupuesto, ColumnNames.CODIGO_APU, legacy_mappings)
    detail_apu_col = find_column(df_apus_detail, ColumnNames.CODIGO_APU, legacy_mappings)

    if budget_apu_col and detail_apu_col:
        budget_codes = set(df_presupuesto[budget_apu_col].dropna().unique())
        detail_codes = set(df_apus_detail[detail_apu_col].dropna().unique())

        orphan_details = detail_codes - budget_codes
        missing_details = budget_codes - detail_codes

        if orphan_details:
            diagnostics["warnings"].append(
                f"APUs en detalle sin referencia en presupuesto: {len(orphan_details)}"
            )
        if missing_details:
            diagnostics["warnings"].append(
                f"APUs en presupuesto sin detalle: {len(missing_details)}"
            )

        diagnostics["referential_integrity"] = {
            "budget_codes": len(budget_codes),
            "detail_codes": len(detail_codes),
            "orphan_details": len(orphan_details),
            "missing_details": len(missing_details),
            "coverage_ratio": len(budget_codes & detail_codes) / max(len(budget_codes), 1),
        }

    # ‚îÅ‚îÅ‚îÅ Nivel 4: An√°lisis Distribucional (Detecci√≥n de Outliers) ‚îÅ‚îÅ‚îÅ
    valor_col = find_column(df_presupuesto, ColumnNames.VALOR_TOTAL, legacy_mappings)
    if valor_col and len(df_presupuesto) >= 10:
        values = df_presupuesto[valor_col].dropna()
        if len(values) > 0:
            q1, q3 = values.quantile(0.25), values.quantile(0.75)
            iqr = q3 - q1
            lower_bound = q1 - 1.5 * iqr
            upper_bound = q3 + 1.5 * iqr

            outliers = values[(values < lower_bound) | (values > upper_bound)]
            outlier_ratio = len(outliers) / len(values)

            diagnostics["distribution_analysis"] = {
                "total_values": len(values),
                "mean": float(values.mean()),
                "std": float(values.std()),
                "q1": float(q1),
                "q3": float(q3),
                "iqr": float(iqr),
                "outlier_count": len(outliers),
                "outlier_ratio": float(outlier_ratio),
            }

            if outlier_ratio > 0.10:
                diagnostics["warnings"].append(
                    f"Alta proporci√≥n de outliers: {outlier_ratio:.1%} ({len(outliers)} valores)"
                )

    # Loguear advertencias
    for warning in diagnostics["warnings"]:
        logger.warning(f"‚ö†Ô∏è  Validaci√≥n: {warning}")

    return True, "Validaci√≥n estructural exitosa", diagnostics


### 4. _build_topological_model ‚Äî Refinado con Teoremas Formales

def _build_topological_model(
    self,
    df_presupuesto: pd.DataFrame,
    df_apus_detail: pd.DataFrame,
) -> TopologicalMetricsBundle:
    """
    Construye el modelo topol√≥gico del presupuesto como complejo simplicial.

    Fundamentos de Topolog√≠a Algebraica:
    ====================================
    Modelamos el presupuesto como un grafo dirigido G = (V, E) donde:
    - V: Conjunto de partidas/APUs
    - E: Relaciones de dependencia (flujo de costos)

    Invariantes calculados:
    - Œ≤‚ÇÄ = dim(H‚ÇÄ): Componentes conexas. Un presupuesto sano tiene Œ≤‚ÇÄ = 1
    - Œ≤‚ÇÅ = dim(H‚ÇÅ): Ciclos independientes. Indican dependencias circulares
    - Œ®: Estabilidad piramidal (proporci√≥n de flujo hacia arriba)

    Teorema de Viabilidad (heur√≠stico):
    Si el grafo G subyacente es dirigido ac√≠clico (DAG), entonces Œ≤‚ÇÅ = 0.
    Ciclos en H‚ÇÅ indican dependencias circulares que pueden causar:
    - Loops de costos infinitos
    - Indeterminaci√≥n en la propagaci√≥n de precios

    Cota emp√≠rica: Para un presupuesto con n partidas, se espera Œ≤‚ÇÅ ‚â§ ‚àön
    (m√°s ciclos sugieren modelado deficiente o circularidades patol√≥gicas).

    Args:
        df_presupuesto: DataFrame del presupuesto.
        df_apus_detail: DataFrame con detalle de APUs.

    Returns:
        TopologicalMetricsBundle con invariantes homol√≥gicos.

    Raises:
        TopologicalAnomalyError: Si la estructura viola restricciones de viabilidad.
        RuntimeError: Si la construcci√≥n del grafo falla.
    """
    logger.info("üèóÔ∏è  Construyendo topolog√≠a del presupuesto...")

    try:
        # Fase 1: Construcci√≥n del complejo simplicial (grafo)
        graph = self.graph_builder.build(df_presupuesto, df_apus_detail)

        # Validaci√≥n post-construcci√≥n
        if graph is None:
            raise RuntimeError("El constructor de grafos retorn√≥ None")

        n_nodes = graph.number_of_nodes()
        n_edges = graph.number_of_edges()

        if n_nodes == 0:
            raise TopologicalAnomalyError(
                "El grafo construido no tiene v√©rtices. "
                "Verifique que los DataFrames contengan datos v√°lidos."
            )

        logger.debug(f"Grafo construido: |V|={n_nodes}, |E|={n_edges}")

        # Fase 2: An√°lisis de conectividad (H‚ÇÄ)
        undirected = graph.to_undirected()
        is_connected = nx.is_connected(undirected)
        n_components = nx.number_connected_components(undirected)

        if not is_connected:
            logger.warning(
                f"‚ö†Ô∏è  Grafo no conexo: {n_components} componentes (Œ≤‚ÇÄ = {n_components}). "
                "Esto puede indicar partidas aisladas o datos incompletos."
            )

        # Fase 3: C√°lculo de invariantes algebraicos
        betti_raw = self.topological_analyzer.calculate_betti_numbers(graph)
        betti_numbers = asdict(betti_raw) if hasattr(betti_raw, '__dataclass_fields__') else dict(betti_raw)

        pyramid_stability = self.topological_analyzer.calculate_pyramid_stability(graph)

        # Fase 4: Verificaci√≥n de cotas de viabilidad
        beta_1 = betti_numbers.get("beta_1", 0)

        # Cota emp√≠rica: Œ≤‚ÇÅ ‚â§ ‚àön para presupuestos bien estructurados
        # Esta cota es m√°s laxa que n/2 y tiene mejor fundamento estad√≠stico
        import math
        cycle_bound = math.ceil(math.sqrt(n_nodes))

        if beta_1 > cycle_bound:
            # No es un error fatal, pero merece advertencia severa
            logger.warning(
                f"‚ö†Ô∏è  Alto n√∫mero de ciclos independientes: Œ≤‚ÇÅ={beta_1} > ‚àön‚âà{cycle_bound}. "
                "Esto sugiere dependencias circulares excesivas."
            )

        # Cota dura: Si Œ≤‚ÇÅ > n, hay m√°s ciclos que nodos (patolog√≠a severa)
        if beta_1 > n_nodes:
            raise TopologicalAnomalyError(
                f"Patolog√≠a topol√≥gica cr√≠tica: Œ≤‚ÇÅ={beta_1} > |V|={n_nodes}. "
                "El presupuesto tiene m√°s ciclos independientes que partidas."
            )

        # Fase 5: Homolog√≠a persistente (opcional)
        persistence: Optional[List[Tuple[float, float]]] = None
        try:
            if hasattr(self.topological_analyzer, "calculate_persistence"):
                raw_persistence = self.topological_analyzer.calculate_persistence(graph)
                if raw_persistence:
                    # Filtrar caracter√≠sticas con muerte infinita y normalizar
                    persistence = []
                    for item in raw_persistence:
                        if isinstance(item, (tuple, list)) and len(item) >= 2:
                            birth, death = item[0], item[1]
                            # Reemplazar infinito por un valor grande pero finito
                            if not math.isfinite(death):
                                death = birth + 10.0  # Vida m√°xima artificial
                            if math.isfinite(birth):
                                persistence.append((float(birth), float(death)))

                    if persistence:
                        lifetimes = [abs(d - b) for b, d in persistence]
                        min_life = min(lifetimes)
                        avg_life = sum(lifetimes) / len(lifetimes)

                        if min_life < 0.01 and avg_life < 0.1:
                            logger.warning(
                                "‚ö†Ô∏è  Homolog√≠a persistente revela caracter√≠sticas ef√≠meras "
                                f"(vida m√≠nima={min_life:.4f}, promedio={avg_life:.4f})"
                            )

        except Exception as e:
            logger.debug(f"Homolog√≠a persistente no disponible: {e}")

        logger.info(
            f"M√©tricas topol√≥gicas: Œ≤‚ÇÄ={betti_numbers.get('beta_0')}, "
            f"Œ≤‚ÇÅ={betti_numbers.get('beta_1')}, Œ®={pyramid_stability:.3f}, "
            f"Conexo={is_connected}"
        )

        return TopologicalMetricsBundle(
            betti_numbers=betti_numbers,
            pyramid_stability=pyramid_stability,
            graph=graph,
            persistence_diagram=persistence,
        )

    except TopologicalAnomalyError:
        raise
    except Exception as e:
        self.telemetry.record_error("business_agent.topology_build", str(e))
        raise RuntimeError(f"Error construyendo modelo topol√≥gico: {e}") from e


### 5. _compose_enriched_report ‚Äî Refinado con √Ålgebra Lineal Robusta

def _compose_enriched_report(
    self,
    topological_bundle: TopologicalMetricsBundle,
    financial_metrics: Dict[str, Any],
    thermal_metrics: Dict[str, Any],
    entropy: float = 0.5,
    exergy: float = 0.6,
) -> ConstructionRiskReport:
    """
    Genera reporte ejecutivo mediante √°lgebra de decisiones multicriterio.

    Marco Matem√°tico (√Ålgebra Lineal Aplicada):
    ===========================================
    Sea el espacio de decisi√≥n D = ‚Ñù‚Åø. Definimos tres subespacios:
    - T ‚äÇ D: Espacio topol√≥gico (coherencia, estabilidad, Betti)
    - F ‚äÇ D: Espacio financiero (VPN, TIR, VaR, Sharpe)
    - Œò ‚äÇ D: Espacio termodin√°mico (temperatura, entrop√≠a, exerg√≠a)

    El vector de decisi√≥n final es una combinaci√≥n convexa:
    
    d = Œ±¬∑œÄ_T(v) + Œ≤¬∑œÄ_F(v) + Œ≥¬∑œÄ_Œò(v)
    
    donde:
    - œÄ_X: Proyecci√≥n ortogonal sobre el subespacio X
    - Œ± + Œ≤ + Œ≥ = 1 (normalizaci√≥n convexa)
    - Los vectores se normalizan en la esfera unitaria S^(n-1)

    El score integrado usa media geom√©trica ponderada para reflejar
    que un fallo en cualquier dimensi√≥n compromete todo el proyecto.

    Args:
        topological_bundle: Bundle de m√©tricas topol√≥gicas.
        financial_metrics: Diccionario con m√©tricas financieras.
        thermal_metrics: Diccionario con m√©tricas t√©rmicas.
        entropy: Entrop√≠a del sistema ‚àà [0, 1].
        exergy: Exerg√≠a (trabajo √∫til disponible) ‚àà [0, 1].

    Returns:
        ConstructionRiskReport con √°lgebra de decisiones aplicada.

    Raises:
        SynthesisAlgebraError: Si la fusi√≥n de espacios vectoriales falla.
    """
    logger.info("üß† Sintetizando reporte con √°lgebra de decisiones multicriterio...")

    # ‚îÅ‚îÅ‚îÅ Fase 1: Generaci√≥n del reporte base ‚îÅ‚îÅ‚îÅ
    base_report = self.topological_analyzer.generate_executive_report(
        topological_bundle.graph, financial_metrics
    )

    if base_report is None:
        raise SynthesisAlgebraError(
            "El analizador topol√≥gico gener√≥ un reporte nulo. "
            "Verifique la integridad del grafo de entrada."
        )

    # ‚îÅ‚îÅ‚îÅ Fase 2: Construcci√≥n de vectores caracter√≠sticos ‚îÅ‚îÅ‚îÅ
    def safe_get(d: Dict, key: str, default: float = 0.0) -> float:
        """Extrae valor num√©rico con fallback seguro."""
        val = d.get(key, default)
        if isinstance(val, (int, float)) and np.isfinite(val):
            return float(val)
        return default

    # Vector topol√≥gico T ‚àà ‚Ñù‚Å¥
    topo_vector = np.array([
        topological_bundle.structural_coherence,
        topological_bundle.pyramid_stability,
        1.0 / (topological_bundle.betti_numbers.get("beta_0", 1) + 1.0),  # Inversi√≥n suave
        np.exp(-0.1 * topological_bundle.betti_numbers.get("beta_1", 0)),  # Decaimiento
    ], dtype=np.float64)

    # Vector financiero F ‚àà ‚Ñù‚Å¥
    # Normalizar VPN por inversi√≥n inicial para escala comparable
    initial_inv = abs(safe_get(financial_metrics, "initial_investment", 1e6))
    if initial_inv < 1.0:
        initial_inv = 1e6

    npv_normalized = safe_get(financial_metrics, "npv", 0.0) / initial_inv
    irr = safe_get(financial_metrics, "irr", 0.0)
    payback = safe_get(financial_metrics, "payback_period", 10.0)
    sharpe = safe_get(financial_metrics, "sharpe_ratio", 0.0)

    finance_vector = np.array([
        np.tanh(npv_normalized),  # Compresi√≥n a [-1, 1]
        np.clip(irr, -1.0, 1.0),  # TIR ya es ratio
        np.exp(-payback / 10.0),  # Decaimiento (menor payback = mejor)
        np.tanh(sharpe),  # Sharpe comprimido
    ], dtype=np.float64)

    # Vector termodin√°mico Œò ‚àà ‚Ñù‚Å¥
    thermo_vector = np.array([
        1.0 - np.clip(thermal_metrics.get("system_temperature", 0.0), 0, 1),  # Inverso de T
        1.0 - np.clip(entropy, 0, 1),  # Negentrop√≠a
        np.clip(exergy, 0, 1),  # Exerg√≠a normalizada
        np.clip(thermal_metrics.get("heat_capacity", 0.5), 0, 1),  # Capacidad t√©rmica
    ], dtype=np.float64)

    # ‚îÅ‚îÅ‚îÅ Fase 3: Normalizaci√≥n en esfera unitaria ‚îÅ‚îÅ‚îÅ
    def normalize_to_sphere(v: np.ndarray, epsilon: float = 1e-10) -> np.ndarray:
        """
        Proyecta vector a la esfera unitaria S^(n-1).
        
        Si ‚Äñv‚Äñ < Œµ, retorna vector uniforme en la esfera.
        """
        norm = np.linalg.norm(v)
        if norm < epsilon:
            # Vector degenerado ‚Üí direcci√≥n uniforme
            n = len(v)
            return np.ones(n) / np.sqrt(n)
        return v / norm

    topo_normalized = normalize_to_sphere(topo_vector)
    finance_normalized = normalize_to_sphere(finance_vector)
    thermo_normalized = normalize_to_sphere(thermo_vector)

    # ‚îÅ‚îÅ‚îÅ Fase 4: Combinaci√≥n convexa con pesos configurables ‚îÅ‚îÅ‚îÅ
    # Pesos por defecto (pueden venir de config)
    weights = self.config.get("decision_weights", {})
    alpha = weights.get("topology", 0.40)
    beta = weights.get("finance", 0.40)
    gamma = weights.get("thermodynamics", 0.20)

    # Normalizar pesos a combinaci√≥n convexa
    weight_sum = alpha + beta + gamma
    if weight_sum > 0:
        alpha, beta, gamma = alpha / weight_sum, beta / weight_sum, gamma / weight_sum
    else:
        alpha, beta, gamma = 1/3, 1/3, 1/3

    # Vector de decisi√≥n final
    decision_vector = (
        alpha * topo_normalized +
        beta * finance_normalized +
        gamma * thermo_normalized
    )
    decision_magnitude = float(np.linalg.norm(decision_vector))

    # ‚îÅ‚îÅ‚îÅ Fase 5: C√°lculo de score integrado (media geom√©trica ponderada) ‚îÅ‚îÅ‚îÅ
    def weighted_geometric_mean(
        factors: List[float],
        weights: List[float],
        epsilon: float = 1e-8,
    ) -> float:
        """
        Media geom√©trica ponderada: (‚àè x·µ¢^w·µ¢)^(1/Œ£w·µ¢)
        
        Robusta ante factores no positivos.
        """
        if not factors or not weights:
            return 0.0

        # Sanitizar factores
        clean_factors = [max(f, epsilon) for f in factors]
        clean_weights = [max(w, 0) for w in weights]

        weight_sum = sum(clean_weights)
        if weight_sum < epsilon:
            return 0.0

        # Calcular en espacio logar√≠tmico para estabilidad num√©rica
        log_sum = sum(w * np.log(f) for f, w in zip(clean_factors, clean_weights))
        return float(np.exp(log_sum / weight_sum))

    # Factores de calidad para cada dimensi√≥n [0, 1]
    topo_quality = (
        topological_bundle.structural_coherence * topological_bundle.pyramid_stability
    ) ** 0.5  # Media geom√©trica de coherencia y estabilidad

    # Calidad financiera basada en VPN normalizado
    finance_quality = (np.tanh(npv_normalized) + 1.0) / 2.0  # Mapeo a [0, 1]

    # Calidad termodin√°mica: balance entre orden (negentrop√≠a) y capacidad de trabajo (exerg√≠a)
    thermo_quality = ((1.0 - entropy) + exergy) / 2.0

    integrated_score = weighted_geometric_mean(
        factors=[topo_quality, finance_quality, thermo_quality],
        weights=[alpha, beta, gamma],
    )

    # Escalar a [0, 100]
    integrated_score_100 = float(np.clip(integrated_score * 100.0, 0.0, 100.0))

    # ‚îÅ‚îÅ‚îÅ Fase 6: Generaci√≥n de narrativa estrat√©gica ‚îÅ‚îÅ‚îÅ
    decision_algebra_summary = {
        "decision_vector": decision_vector.tolist(),
        "magnitude": decision_magnitude,
        "dimension": len(decision_vector),
        "weights": {"alpha": alpha, "beta": beta, "gamma": gamma},
        "contributions": {
            "topology": float(alpha * np.linalg.norm(topo_normalized)),
            "finance": float(beta * np.linalg.norm(finance_normalized)),
            "thermodynamics": float(gamma * np.linalg.norm(thermo_normalized)),
        },
        "quality_factors": {
            "topology": float(topo_quality),
            "finance": float(finance_quality),
            "thermodynamics": float(thermo_quality),
        },
    }

    try:
        strategic_report = self.translator.compose_strategic_narrative(
            topological_metrics=topological_bundle.betti_numbers,
            financial_metrics=financial_metrics,
            stability=topological_bundle.pyramid_stability,
            synergy_risk=base_report.details.get("synergy_risk"),
            spectral=base_report.details.get("spectral_analysis"),
            thermal_metrics=thermal_metrics,
            decision_algebra=decision_algebra_summary,
        )
        narrative = getattr(strategic_report, "raw_narrative", str(strategic_report))
    except Exception as e:
        logger.warning(f"‚ö†Ô∏è  Generaci√≥n de narrativa fall√≥: {e}")
        narrative = (
            f"Reporte base con score de integridad {integrated_score_100:.1f}/100. "
            f"Coherencia topol√≥gica: {topo_quality:.2%}. "
            f"Salud financiera: {finance_quality:.2%}. "
            f"Calidad termodin√°mica: {thermo_quality:.2%}."
        )

    # ‚îÅ‚îÅ‚îÅ Fase 7: Construcci√≥n del reporte enriquecido ‚îÅ‚îÅ‚îÅ
    enriched_details = {
        **base_report.details,
        "strategic_narrative": narrative,
        "financial_metrics": financial_metrics,
        "thermal_metrics": thermal_metrics,
        "thermodynamics": {
            "entropy": float(entropy),
            "exergy": float(exergy),
            "negentropy": float(1.0 - entropy),
            "system_temperature": float(thermal_metrics.get("system_temperature", 0.0)),
        },
        "topological_invariants": {
            "betti_numbers": topological_bundle.betti_numbers,
            "pyramid_stability": float(topological_bundle.pyramid_stability),
            "structural_coherence": float(topological_bundle.structural_coherence),
            "is_connected": nx.is_connected(topological_bundle.graph.to_undirected()),
            "n_nodes": topological_bundle.graph.number_of_nodes(),
        },
        "decision_algebra": decision_algebra_summary,
    }

    report = ConstructionRiskReport(
        integrity_score=integrated_score_100,
        waste_alerts=base_report.waste_alerts,
        circular_risks=base_report.circular_risks,
        complexity_level=base_report.complexity_level,
        financial_risk_level=base_report.financial_risk_level,
        details=enriched_details,
        strategic_narrative=narrative,
    )

    # ‚îÅ‚îÅ‚îÅ Fase 8: Auditor√≠a adversarial ‚îÅ‚îÅ‚îÅ
    audited_report = self.risk_challenger.challenge_verdict(report)

    # Verificaci√≥n de integridad num√©rica final
    if not np.isfinite(audited_report.integrity_score):
        logger.error(
            f"‚ùå Score de integridad no finito: {audited_report.integrity_score}"
        )
        self.telemetry.record_error(
            "business_agent.non_finite_score",
            f"Score: {audited_report.integrity_score}",
        )
        # Fallback a un valor seguro
        audited_report = ConstructionRiskReport(
            integrity_score=0.0,
            waste_alerts=audited_report.waste_alerts,
            circular_risks=audited_report.circular_risks,
            complexity_level=audited_report.complexity_level,
            financial_risk_level="ERROR NUM√âRICO",
            details=audited_report.details,
            strategic_narrative=audited_report.strategic_narrative,
        )

    return audited_report


### 6. M√©todos Auxiliares de √Ålgebra ‚Äî Nuevos

class AlgebraicOperations:
    """
    Operaciones algebraicas auxiliares para el BusinessAgent.
    
    Encapsula funciones de √°lgebra lineal y estad√≠stica robustas
    para uso en el pipeline de decisi√≥n.
    """
    
    @staticmethod
    def safe_normalize(
        vector: np.ndarray,
        epsilon: float = 1e-10
    ) -> np.ndarray:
        """
        Normaliza un vector a norma unitaria de forma segura.
        
        Si el vector es casi nulo, retorna un vector uniforme
        en la esfera unitaria S^(n-1).
        
        Args:
            vector: Vector a normalizar.
            epsilon: Umbral de norma m√≠nima.
        
        Returns:
            Vector normalizado en S^(n-1).
        """
        norm = np.linalg.norm(vector)
        if norm < epsilon:
            n = len(vector)
            return np.ones(n) / np.sqrt(n)
        return vector / norm
    
    @staticmethod
    def weighted_geometric_mean(
        factors: List[float],
        weights: Optional[List[float]] = None,
        epsilon: float = 1e-8
    ) -> float:
        """
        Calcula la media geom√©trica ponderada de forma robusta.
        
        F√≥rmula: (‚àè·µ¢ x·µ¢^w·µ¢)^(1/Œ£w·µ¢)
        
        Maneja:
        - Factores no positivos (reemplaza por epsilon)
        - Pesos nulos o faltantes (usa pesos uniformes)
        - C√°lculo en espacio log para estabilidad num√©rica
        
        Args:
            factors: Lista de factores positivos.
            weights: Lista de pesos (opcional, default uniforme).
            epsilon: Valor m√≠nimo para factores.
        
        Returns:
            Media geom√©trica ponderada.
        """
        if not factors:
            return 0.0
        
        n = len(factors)
        if weights is None:
            weights = [1.0 / n] * n
        
        # Sanitizar
        clean_factors = [max(f, epsilon) for f in factors]
        clean_weights = [max(w, 0.0) for w in weights]
        
        weight_sum = sum(clean_weights)
        if weight_sum < epsilon:
            return 0.0
        
        # Calcular en log-space
        log_sum = sum(w * np.log(f) for f, w in zip(clean_factors, clean_weights))
        return float(np.exp(log_sum / weight_sum))
    
    @staticmethod
    def convex_combination(
        vectors: List[np.ndarray],
        weights: List[float],
        normalize_weights: bool = True
    ) -> np.ndarray:
        """
        Calcula combinaci√≥n convexa de vectores.
        
        d = Œ£·µ¢ Œ±·µ¢¬∑v·µ¢  donde Œ£Œ±·µ¢ = 1
        
        Args:
            vectors: Lista de vectores de igual dimensi√≥n.
            weights: Pesos para cada vector.
            normalize_weights: Si True, normaliza pesos a suma 1.
        
        Returns:
            Vector resultante de la combinaci√≥n.
        
        Raises:
            ValueError: Si las dimensiones no coinciden.
        """
        if not vectors:
            raise ValueError("Lista de vectores vac√≠a")
        
        dim = len(vectors[0])
        for v in vectors:
            if len(v) != dim:
                raise ValueError(f"Dimensiones inconsistentes: {len(v)} vs {dim}")
        
        if normalize_weights:
            weight_sum = sum(weights)
            if weight_sum > 0:
                weights = [w / weight_sum for w in weights]
            else:
                n = len(weights)
                weights = [1.0 / n] * n
        
        result = np.zeros(dim)
        for v, w in zip(vectors, weights):
            result += w * np.array(v)
        
        return result
    
    @staticmethod
    def cosine_similarity(v1: np.ndarray, v2: np.ndarray) -> float:
        """
        Calcula similitud coseno entre dos vectores.
        
        cos(Œ∏) = (v‚ÇÅ¬∑v‚ÇÇ) / (‚Äñv‚ÇÅ‚Äñ¬∑‚Äñv‚ÇÇ‚Äñ)
        
        Args:
            v1, v2: Vectores a comparar.
        
        Returns:
            Similitud en [-1, 1].
        """
        norm1 = np.linalg.norm(v1)
        norm2 = np.linalg.norm(v2)
        
        if norm1 < 1e-10 or norm2 < 1e-10:
            return 0.0
        
        return float(np.dot(v1, v2) / (norm1 * norm2))


### 