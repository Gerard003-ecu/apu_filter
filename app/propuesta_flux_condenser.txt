### Propuesta 1

import logging
import os
from pathlib import Path
from typing import Dict, Any, List, Tuple, Optional
import pandas as pd

# Se asume que estos m√≥dulos existen y est√°n en el path correcto.
# Si InsumoProcesado no se usa expl√≠citamente aqu√≠, se elimina para mantener el namespace limpio.
from .report_parser_crudo import ReportParserCrudo
from .apu_processor import APUProcessor

class DataFluxCondenser:
    """
    Implementaci√≥n de Capacitancia L√≥gica para el procesamiento de APUs.
    
    Patr√≥n Facade/Orquestador que:
    1. Ingiere archivos crudos (CSV/Excel).
    2. Filtra y valida la estructura inicial (Parsing).
    3. Transforma y normaliza la l√≥gica de negocio (Processing).
    4. Retorna un DataFrame consolidado y seguro.
    """
    
    def __init__(self, config: Dict[str, Any], profile: Dict[str, Any]):
        """
        Inicializa el condensador con la configuraci√≥n y perfil del proyecto.
        
        Args:
            config: Configuraci√≥n global del sistema.
            profile: Perfil espec√≠fico del proyecto/usuario.
        """
        self.config = config or {}
        self.profile = profile or {}
        # Es buena pr√°ctica nombrar el logger con la clase para facilitar el rastreo
        self.logger = logging.getLogger(self.__class__.__name__)
        
        self._validate_configuration()

    def _validate_configuration(self):
        """Valida que las configuraciones cr√≠ticas existan antes de procesar."""
        if not self.profile:
            self.logger.warning("‚ö†Ô∏è El perfil de procesamiento est√° vac√≠o. Esto podr√≠a generar resultados gen√©ricos.")

    def stabilize(self, file_path: str) -> pd.DataFrame:
        """
        Orquestador principal: Proceso de Carga y Descarga.
        
        Toma una ruta de archivo, maneja excepciones de E/S y l√≥gica,
        y garantiza el retorno de un DataFrame (lleno o vac√≠o).
        
        Args:
            file_path: Ruta absoluta o relativa al archivo a procesar.
            
        Returns:
            pd.DataFrame: Datos procesados o DataFrame vac√≠o en caso de error/falta de datos.
        """
        path_obj = Path(file_path)
        
        self.logger.info(f"‚ö° Iniciando ciclo de estabilizaci√≥n para: {path_obj.name}")

        if not path_obj.exists() or not path_obj.is_file():
            self.logger.error(f"‚õî El archivo no existe o no es v√°lido: {path_obj}")
            return pd.DataFrame()

        try:
            # --- FASE 1: FILTRADO (Parsing & Validaci√≥n) ---
            raw_records, parse_cache = self._absorb_and_filter(str(path_obj))
            
            if not raw_records:
                self.logger.warning(f"‚ö†Ô∏è La carga del archivo {path_obj.name} no gener√≥ registros v√°lidos.")
                return pd.DataFrame()

            # --- FASE 2: RECTIFICACI√ìN (Procesamiento L√≥gico) ---
            df_stabilized = self._rectify_signal(raw_records, parse_cache)
            
            if df_stabilized.empty:
                self.logger.warning(f"‚ö†Ô∏è El procesamiento finaliz√≥ sin filas resultantes para: {path_obj.name}")
                return pd.DataFrame()

            self.logger.info(f"‚úÖ Flujo estabilizado: {len(df_stabilized)} registros listos desde {path_obj.name}.")
            return df_stabilized

        except Exception as e:
            # Captura cualquier error no controlado en el pipeline para evitar crash de la app
            self.logger.error(f"üî• Error cr√≠tico estabilizando {path_obj.name}: {str(e)}", exc_info=True)
            return pd.DataFrame()

    def _absorb_and_filter(self, file_path: str) -> Tuple[List[Dict], Dict]:
        """
        Instancia el parser y extrae la data cruda.
        
        Returns:
            Tuple[List[Dict], Dict]: (Lista de registros crudos, Cache de metadatos de parseo)
        """
        self.logger.debug("... Filtrando ruido de entrada (ReportParserCrudo) ...")
        
        # Se asume que ReportParserCrudo puede lanzar excepciones si el formato es inv√°lido
        parser = ReportParserCrudo(file_path, self.profile, self.config)
        
        try:
            raw_records = parser.parse_to_raw()
            parse_cache = parser.get_parse_cache()
            return raw_records, parse_cache
        except Exception as e:
            self.logger.error(f"‚ùå Error durante la fase de Parsing: {e}")
            raise # Re-lanzamos para que stabilize maneje el flujo de salida

    def _rectify_signal(self, raw_records: List[Dict], parse_cache: Dict) -> pd.DataFrame:
        """
        Instancia el procesador de APUs y transforma la data cruda en estructura tabular.
        """
        self.logger.debug("... Rectificando se√±al (APUProcessor) ...")
        
        try:
            processor = APUProcessor(
                raw_records=raw_records,
                config=self.config,
                profile=self.profile,
                parse_cache=parse_cache
            )
            return processor.process_all()
        except Exception as e:
            self.logger.error(f"‚ùå Error durante la fase de Procesamiento de APUs: {e}")
            raise # Re-lanzamos para mantener la coherencia del manejo de errores en stabilize


### Propuesta 2

import logging
from typing import Dict, Any, List, Optional, Tuple, NamedTuple
from pathlib import Path
from dataclasses import dataclass
import pandas as pd

from .report_parser_crudo import ReportParserCrudo
from .apu_processor import APUProcessor
from .schemas import InsumoProcesado

logger = logging.getLogger(__name__)


class ParsedData(NamedTuple):
    """Estructura de datos resultante del parseo."""
    raw_records: List[Dict[str, Any]]
    parse_cache: Dict[str, Any]


class DataFluxCondenserError(Exception):
    """Excepci√≥n base para errores del condensador."""
    pass


class InvalidInputError(DataFluxCondenserError):
    """Error de validaci√≥n de entrada."""
    pass


class ProcessingError(DataFluxCondenserError):
    """Error durante el procesamiento."""
    pass


@dataclass(frozen=True)
class CondenserConfig:
    """Configuraci√≥n validada del condensador."""
    min_records_threshold: int = 1
    enable_strict_validation: bool = True
    log_level: str = "INFO"


class DataFluxCondenser:
    """
    Implementaci√≥n de Capacitancia L√≥gica para el procesamiento de APUs.
    
    Act√∫a como un condensador y estabilizador de flujo que:
    1. Absorbe la carga bruta (archivos CSV crudos).
    2. Filtra el ruido (usando ReportParserCrudo como filtro pasa-bajos).
    3. Rectifica la se√±al (usando APUProcessor para estructurar datos).
    4. Descarga un flujo limpio y estable (DataFrame) al sistema principal.
    
    Attributes:
        config (Dict[str, Any]): Configuraci√≥n del sistema.
        profile (Dict[str, Any]): Perfil de procesamiento.
        condenser_config (CondenserConfig): Configuraci√≥n espec√≠fica del condensador.
        
    Raises:
        InvalidInputError: Si los par√°metros de entrada son inv√°lidos.
        ProcessingError: Si falla el procesamiento de datos.
    """
    
    # Constantes
    REQUIRED_CONFIG_KEYS = {'parser_settings', 'processor_settings'}
    REQUIRED_PROFILE_KEYS = {'columns_mapping', 'validation_rules'}
    
    def __init__(
        self, 
        config: Dict[str, Any], 
        profile: Dict[str, Any],
        condenser_config: Optional[CondenserConfig] = None
    ):
        """
        Inicializa el condensador con validaci√≥n de dependencias.
        
        Args:
            config: Configuraci√≥n general del sistema.
            profile: Perfil de procesamiento de APUs.
            condenser_config: Configuraci√≥n espec√≠fica del condensador.
            
        Raises:
            InvalidInputError: Si config o profile son inv√°lidos.
        """
        self._validate_initialization_params(config, profile)
        
        self.config = config
        self.profile = profile
        self.condenser_config = condenser_config or CondenserConfig()
        self.logger = logging.getLogger(self.__class__.__name__)
        self.logger.setLevel(self.condenser_config.log_level)
        
        self.logger.info("DataFluxCondenser inicializado correctamente")

    def _validate_initialization_params(
        self, 
        config: Dict[str, Any], 
        profile: Dict[str, Any]
    ) -> None:
        """
        Valida que los par√°metros de inicializaci√≥n sean coherentes.
        
        Args:
            config: Configuraci√≥n a validar.
            profile: Perfil a validar.
            
        Raises:
            InvalidInputError: Si falta alguna clave requerida.
        """
        if not isinstance(config, dict) or not isinstance(profile, dict):
            raise InvalidInputError(
                "config y profile deben ser diccionarios v√°lidos"
            )
        
        # Validaci√≥n flexible: advertir sobre claves faltantes sin bloquear
        missing_config_keys = self.REQUIRED_CONFIG_KEYS - set(config.keys())
        if missing_config_keys:
            logger.warning(
                f"Claves faltantes en config (modo tolerante): {missing_config_keys}"
            )
        
        missing_profile_keys = self.REQUIRED_PROFILE_KEYS - set(profile.keys())
        if missing_profile_keys:
            logger.warning(
                f"Claves faltantes en profile (modo tolerante): {missing_profile_keys}"
            )

    def stabilize(self, file_path: str) -> pd.DataFrame:
        """
        Proceso de Carga y Descarga del Condensador.
        
        Ejecuta el ciclo completo de estabilizaci√≥n:
        1. Validaci√≥n de entrada
        2. Filtrado de se√±al (parsing)
        3. Rectificaci√≥n (procesamiento)
        4. Validaci√≥n de salida
        
        Args:
            file_path: Ruta al archivo CSV a procesar.
            
        Returns:
            DataFrame estabilizado con datos procesados.
            
        Raises:
            InvalidInputError: Si el archivo no existe o es inv√°lido.
            ProcessingError: Si falla el procesamiento.
        """
        self.logger.info(
            f"[INICIO] Ciclo de estabilizaci√≥n para: {file_path}"
        )
        
        try:
            # Validaci√≥n de entrada
            validated_path = self._validate_input_file(file_path)
            
            # FASE 1: FILTRADO (El Guardia)
            parsed_data = self._absorb_and_filter(validated_path)
            
            if not self._validate_parsed_data(parsed_data):
                self.logger.warning(
                    "[ADVERTENCIA] La carga no gener√≥ se√±al v√°lida (0 registros)"
                )
                return pd.DataFrame()

            # FASE 2: RECTIFICACI√ìN (El Cirujano)
            df_stabilized = self._rectify_signal(parsed_data)
            
            # Validaci√≥n de salida
            self._validate_output(df_stabilized)
            
            self.logger.info(
                f"[√âXITO] Flujo estabilizado: {len(df_stabilized)} registros procesados"
            )
            return df_stabilized
            
        except InvalidInputError as e:
            self.logger.error(f"[ERROR] Entrada inv√°lida: {e}")
            raise
        except ProcessingError as e:
            self.logger.error(f"[ERROR] Fallo en procesamiento: {e}")
            raise
        except Exception as e:
            self.logger.exception(f"[ERROR CR√çTICO] Error inesperado: {e}")
            raise ProcessingError(f"Error inesperado durante estabilizaci√≥n: {e}") from e

    def _validate_input_file(self, file_path: str) -> Path:
        """
        Valida que el archivo de entrada exista y sea accesible.
        
        Args:
            file_path: Ruta al archivo a validar.
            
        Returns:
            Path validado.
            
        Raises:
            InvalidInputError: Si el archivo no existe o no es accesible.
        """
        if not file_path or not isinstance(file_path, str):
            raise InvalidInputError(
                f"file_path debe ser una cadena no vac√≠a, recibido: {type(file_path)}"
            )
        
        path = Path(file_path)
        
        if not path.exists():
            raise InvalidInputError(f"El archivo no existe: {file_path}")
        
        if not path.is_file():
            raise InvalidInputError(f"La ruta no es un archivo: {file_path}")
        
        if not path.suffix.lower() in {'.csv', '.txt'}:
            self.logger.warning(
                f"Extensi√≥n inusual detectada: {path.suffix}. "
                "Se esperaba .csv o .txt"
            )
        
        self.logger.debug(f"[VALIDACI√ìN] Archivo validado: {path}")
        return path

    def _absorb_and_filter(self, file_path: Path) -> ParsedData:
        """
        Usa ReportParserCrudo para filtrar el ruido de entrada.
        
        Args:
            file_path: Ruta validada al archivo.
            
        Returns:
            ParsedData con registros crudos y cach√© de parseo.
            
        Raises:
            ProcessingError: Si el parseo falla.
        """
        self.logger.debug("[FASE 1] Filtrando ruido con ReportParserCrudo...")
        
        try:
            parser = ReportParserCrudo(
                str(file_path), 
                self.profile, 
                self.config
            )
            raw_records = parser.parse_to_raw()
            parse_cache = parser.get_parse_cache()
            
            # Validaci√≥n de consistencia
            if raw_records is None:
                raw_records = []
                self.logger.warning(
                    "[FASE 1] Parser retorn√≥ None, se asume lista vac√≠a"
                )
            
            if parse_cache is None:
                parse_cache = {}
                self.logger.warning(
                    "[FASE 1] Cache retorn√≥ None, se asume diccionario vac√≠o"
                )
            
            parsed_data = ParsedData(
                raw_records=raw_records,
                parse_cache=parse_cache
            )
            
            self.logger.debug(
                f"[FASE 1] Filtrado completado: {len(raw_records)} registros extra√≠dos"
            )
            return parsed_data
            
        except Exception as e:
            raise ProcessingError(
                f"Error durante el filtrado con ReportParserCrudo: {e}"
            ) from e

    def _validate_parsed_data(self, parsed_data: ParsedData) -> bool:
        """
        Valida que los datos parseados sean coherentes y suficientes.
        
        Args:
            parsed_data: Datos parseados a validar.
            
        Returns:
            True si los datos son v√°lidos, False si est√°n vac√≠os pero v√°lidos.
            
        Raises:
            ProcessingError: Si los datos est√°n corruptos.
        """
        if not isinstance(parsed_data.raw_records, list):
            raise ProcessingError(
                f"raw_records debe ser lista, recibido: {type(parsed_data.raw_records)}"
            )
        
        if not isinstance(parsed_data.parse_cache, dict):
            raise ProcessingError(
                f"parse_cache debe ser dict, recibido: {type(parsed_data.parse_cache)}"
            )
        
        records_count = len(parsed_data.raw_records)
        
        if records_count < self.condenser_config.min_records_threshold:
            self.logger.warning(
                f"[VALIDACI√ìN] Registros insuficientes: {records_count} < "
                f"{self.condenser_config.min_records_threshold}"
            )
            return False
        
        self.logger.debug(f"[VALIDACI√ìN] Datos parseados v√°lidos: {records_count} registros")
        return True

    def _rectify_signal(self, parsed_data: ParsedData) -> pd.DataFrame:
        """
        Usa APUProcessor para convertir la se√±al filtrada en datos utilizables.
        
        Args:
            parsed_data: Datos parseados a procesar.
            
        Returns:
            DataFrame con datos procesados.
            
        Raises:
            ProcessingError: Si el procesamiento falla.
        """
        self.logger.debug("[FASE 2] Rectificando se√±al con APUProcessor...")
        
        try:
            processor = APUProcessor(
                raw_records=parsed_data.raw_records,
                config=self.config,
                profile=self.profile,
                parse_cache=parsed_data.parse_cache
            )
            
            df_result = processor.process_all()
            
            if not isinstance(df_result, pd.DataFrame):
                raise ProcessingError(
                    f"APUProcessor.process_all() debe retornar DataFrame, "
                    f"recibido: {type(df_result)}"
                )
            
            self.logger.debug(
                f"[FASE 2] Rectificaci√≥n completada: {len(df_result)} registros procesados"
            )
            return df_result
            
        except Exception as e:
            raise ProcessingError(
                f"Error durante la rectificaci√≥n con APUProcessor: {e}"
            ) from e

    def _validate_output(self, df: pd.DataFrame) -> None:
        """
        Valida el DataFrame de salida antes de retornarlo.
        
        Args:
            df: DataFrame a validar.
            
        Raises:
            ProcessingError: Si el DataFrame es inv√°lido.
        """
        if not isinstance(df, pd.DataFrame):
            raise ProcessingError(
                f"La salida debe ser DataFrame, recibido: {type(df)}"
            )
        
        if self.condenser_config.enable_strict_validation:
            if df.empty:
                self.logger.warning(
                    "[VALIDACI√ìN] DataFrame vac√≠o generado (puede ser v√°lido)"
                )
            
            # Validar que no haya columnas completamente nulas
            null_columns = df.columns[df.isnull().all()].tolist()
            if null_columns:
                self.logger.warning(
                    f"[VALIDACI√ìN] Columnas completamente nulas: {null_columns}"
                )
        
        self.logger.debug(
            f"[VALIDACI√ìN] Salida validada: {df.shape[0]} filas, {df.shape[1]} columnas"
        )

    def get_processing_stats(self) -> Dict[str, Any]:
        """
        Retorna estad√≠sticas del √∫ltimo procesamiento.
        
        Returns:
            Diccionario con estad√≠sticas de procesamiento.
        """
        return {
            "condenser_config": {
                "min_records_threshold": self.condenser_config.min_records_threshold,
                "strict_validation": self.condenser_config.enable_strict_validation,
                "log_level": self.condenser_config.log_level
            },
            "config_keys": list(self.config.keys()),
            "profile_keys": list(self.profile.keys())
        }