### Clase FluxPhysicsEngine

# 1. _validate_parameters (Robustecido)

def _validate_parameters(
    self, capacitance: float, resistance: float, inductance: float
) -> None:
    """
    Valida parÃ¡metros fÃ­sicos con rangos plausibles.

    Rangos industriales tÃ­picos:
    - Capacitancia: 1e-12 F (pF) a 10 F (supercapacitores)
    - Resistencia: 1e-6 Î© (Î¼Î©) a 1e9 Î© (GÎ©)
    - Inductancia: 1e-9 H (nH) a 1e3 H (kH)
    
    Raises:
        ConfigurationError: Si los parÃ¡metros son invÃ¡lidos.
    """
    errors: List[str] = []
    warnings: List[str] = []

    # DefiniciÃ³n con rangos plausibles
    params_spec = [
        ("capacitance", capacitance, 1e-15, 1e2),
        ("resistance", resistance, 1e-6, 1e12),
        ("inductance", inductance, 1e-12, 1e6),
    ]

    for name, value, min_plausible, max_plausible in params_spec:
        # ValidaciÃ³n de tipo
        if not isinstance(value, (int, float)):
            errors.append(
                f"{name} debe ser numÃ©rico, recibido: {type(value).__name__}"
            )
            continue

        # ConversiÃ³n segura a float para comparaciones
        try:
            float_val = float(value)
        except (ValueError, TypeError) as e:
            errors.append(f"{name} no convertible a float: {e}")
            continue

        # ValidaciÃ³n de valores especiales IEEE 754
        if math.isnan(float_val):
            errors.append(f"{name} no puede ser NaN")
            continue
        if math.isinf(float_val):
            errors.append(f"{name} no puede ser Inf: {float_val}")
            continue

        # ValidaciÃ³n de positividad estricta
        if float_val <= 0:
            errors.append(f"{name} debe ser > 0, recibido: {float_val}")
            continue

        # Advertencias de plausibilidad fÃ­sica (no bloquean)
        if float_val < min_plausible:
            warnings.append(
                f"{name}={float_val:.2e} < mÃ­nimo plausible {min_plausible:.2e}"
            )
        elif float_val > max_plausible:
            warnings.append(
                f"{name}={float_val:.2e} > mÃ¡ximo plausible {max_plausible:.2e}"
            )

    # ValidaciÃ³n cruzada: factor de calidad Q razonable
    if not errors:
        try:
            Q = (1.0 / float(resistance)) * math.sqrt(float(inductance) / float(capacitance))
            if Q > 1000:
                warnings.append(f"Factor Q={Q:.1f} extremadamente alto (posible inestabilidad)")
            elif Q < 0.01:
                warnings.append(f"Factor Q={Q:.4f} extremadamente bajo (sistema sobreamortiguado)")
        except (ZeroDivisionError, ValueError):
            pass  # Ya capturado en validaciones anteriores

    if errors:
        raise ConfigurationError(
            "ParÃ¡metros fÃ­sicos invÃ¡lidos:\n" + "\n".join(f"  - {e}" for e in errors)
        )

    # Almacenar warnings para el logger (se procesarÃ¡n post-init)
    self._init_warnings = warnings


# 2. __init__ (Ajuste menor)

def __init__(self, capacitance: float, resistance: float, inductance: float):
    """
    Inicializa el motor de fÃ­sica con validaciÃ³n exhaustiva.

    Args:
        capacitance: Capacitancia (Faradios).
        resistance: Resistencia (Ohmios).
        inductance: Inductancia (Henrios).
    """
    # ValidaciÃ³n ANTES de asignaciones
    self._init_warnings: List[str] = []
    self._validate_parameters(capacitance, resistance, inductance)

    self.C = float(capacitance)
    self.R = float(resistance)
    self.L = float(inductance)

    # Constantes derivadas pre-calculadas con protecciÃ³n
    self._tau_base = self.R * self.C
    
    # Frecuencia resonante: f_0 = 1 / (2Ï€âˆš(LC))
    lc_product = self.L * self.C
    self._resonant_freq = 1.0 / (2.0 * math.pi * math.sqrt(lc_product))
    
    # Factor de calidad del circuito: Q = (1/R) * âˆš(L/C)
    self._quality_factor = (1.0 / self.R) * math.sqrt(self.L / self.C)

    # Estado temporal para derivadas
    self._last_current: float = 0.0
    self._last_time: float = time.time()
    self._initialized_temporal: bool = False

    # Historial para anÃ¡lisis de tendencias (usar deque para O(1) en ambos extremos)
    self._metrics_history: deque = deque(maxlen=self._MAX_METRICS_HISTORY)

    self.logger = logging.getLogger(f"{self.__class__.__name__}")
    
    # Procesar warnings acumulados durante validaciÃ³n
    for warning in self._init_warnings:
        self.logger.warning(f"Plausibilidad: {warning}")

    self.logger.info(
        f"Motor RLC inicializado: C={self.C:.2e}F, R={self.R:.2e}Î©, L={self.L:.2e}H | "
        f"Ï„={self._tau_base:.4f}s, fâ‚€={self._resonant_freq:.4f}Hz, Q={self._quality_factor:.2f}"
    )


# 3. calculate_metrics (Robustecido - Correcciones CrÃ­ticas)

def calculate_metrics(self, total_records: int, cache_hits: int) -> Dict[str, float]:
    """
    Modelo fÃ­sico RLC de segundo orden corregido.

    El sistema modela:
    - total_records: AnÃ¡logo a "carga total" o demanda del sistema
    - cache_hits: AnÃ¡logo a "flujo efectivo" o corriente Ãºtil
    
    Returns:
        Dict[str, float]: MÃ©tricas fÃ­sicas del sistema.
    """
    # ================ VALIDACIÃ“N DE ENTRADA ================
    if not isinstance(total_records, (int, float)) or total_records <= 0:
        self.logger.debug(f"total_records invÃ¡lido: {total_records}")
        return self._get_zero_metrics()

    if not isinstance(cache_hits, (int, float)):
        cache_hits = 0

    # SanitizaciÃ³n de cache_hits
    cache_hits = int(max(0, min(total_records, cache_hits)))
    total_records = int(total_records)

    try:
        # ================ PARÃMETROS DEL SISTEMA ================
        # Corriente normalizada (eficiencia del flujo) âˆˆ [0, 1]
        current_I = cache_hits / total_records

        # Factor de complejidad (pÃ©rdidas/ineficiencia) âˆˆ [0, 1]
        complexity = 1.0 - current_I

        # Resistencia dinÃ¡mica: aumenta con complejidad
        R_dyn = self.R * (
            1.0 + complexity * SystemConstants.COMPLEXITY_RESISTANCE_FACTOR
        )

        # ================ PARÃMETROS RLC CORREGIDOS ================
        # Factor de amortiguamiento: Î¶ = R/(2) * âˆš(C/L) = R / (2âˆš(L/C))
        # Equivalente a: Î¶ = R / (2 * Zâ‚€) donde Zâ‚€ = âˆš(L/C) es impedancia caracterÃ­stica
        impedance_char = math.sqrt(self.L / self.C)
        damping_ratio = R_dyn / (2.0 * impedance_char)

        # Frecuencia natural angular: Ï‰â‚™ = 1/âˆš(LC)
        omega_n = 1.0 / math.sqrt(self.L * self.C)

        # Frecuencia amortiguada: Ï‰d = Ï‰â‚™âˆš(1-Î¶Â²) solo si Î¶ < 1
        if damping_ratio < 1.0:
            omega_d = omega_n * math.sqrt(max(0.0, 1.0 - damping_ratio**2))
        else:
            omega_d = 0.0

        # ================ RESPUESTA AL ESCALÃ“N (SATURACIÃ“N) ================
        # Tiempo caracterÃ­stico del sistema basado en registros procesados
        # InterpretaciÃ³n: cada registro representa un "quantum" temporal
        tau_effective = R_dyn * self.C
        t_normalized = float(total_records) / max(1.0, tau_effective * 1000.0)

        # Limitar t_normalized para evitar overflow en exp()
        t_normalized = min(t_normalized, 50.0)

        if damping_ratio < 1.0:
            # SUBAMORTIGUADO: oscilaciones decrecientes
            # y(t) = 1 - (e^(-Î¶Ï‰â‚™t) / âˆš(1-Î¶Â²)) * sin(Ï‰d*t + Ï†)
            # donde Ï† = arccos(Î¶)
            zeta_omega_t = damping_ratio * omega_n * t_normalized
            
            # ProtecciÃ³n contra overflow
            if zeta_omega_t > 700:
                exp_term = 0.0
            else:
                exp_term = math.exp(-zeta_omega_t)

            sqrt_term = math.sqrt(max(1e-10, 1.0 - damping_ratio**2))
            
            # Fase correcta: Ï† = arccos(Î¶)
            phase = math.acos(min(1.0, max(-1.0, damping_ratio)))
            
            sin_arg = omega_d * t_normalized + phase
            sin_term = math.sin(sin_arg)
            
            saturation_V = 1.0 - (exp_term / sqrt_term) * sin_term

        elif abs(damping_ratio - 1.0) < 0.05:
            # CRÃTICAMENTE AMORTIGUADO: respuesta mÃ¡s rÃ¡pida sin oscilaciÃ³n
            # y(t) = 1 - (1 + Ï‰â‚™t) * e^(-Ï‰â‚™t)
            omega_t = omega_n * t_normalized
            
            if omega_t > 700:
                saturation_V = 1.0
            else:
                exp_term = math.exp(-omega_t)
                saturation_V = 1.0 - (1.0 + omega_t) * exp_term

        else:
            # SOBREAMORTIGUADO: respuesta lenta sin oscilaciÃ³n
            # y(t) = 1 - (sâ‚‚/(sâ‚‚-sâ‚))e^(sâ‚t) + (sâ‚/(sâ‚‚-sâ‚))e^(sâ‚‚t)
            discriminant = math.sqrt(max(0.0, damping_ratio**2 - 1.0))
            s1 = -omega_n * (damping_ratio - discriminant)
            s2 = -omega_n * (damping_ratio + discriminant)

            # ProtecciÃ³n divisiÃ³n por cero
            s_diff = s2 - s1
            if abs(s_diff) < 1e-10:
                saturation_V = 1.0 - math.exp(s1 * t_normalized)
            else:
                # ProtecciÃ³n overflow
                exp_s1 = math.exp(max(-700, min(700, s1 * t_normalized)))
                exp_s2 = math.exp(max(-700, min(700, s2 * t_normalized)))
                
                A = s2 / s_diff
                B = -s1 / s_diff
                saturation_V = 1.0 - (A * exp_s1 + B * exp_s2)

        # Clamp saturaciÃ³n a [0, 1]
        saturation_V = max(0.0, min(1.0, saturation_V))

        # ================ ENERGÃAS FÃSICAS REALES ================
        # EnergÃ­a almacenada en capacitor: E_C = Â½CVÂ²
        E_capacitor = 0.5 * self.C * (saturation_V ** 2)

        # EnergÃ­a almacenada en inductor: E_L = Â½LIÂ²
        E_inductor = 0.5 * self.L * (current_I ** 2)

        # EnergÃ­a total del sistema
        E_total = E_capacitor + E_inductor

        # Potencia disipada instantÃ¡nea: P = IÂ²R
        P_dissipated = (current_I ** 2) * R_dyn

        # Factor de potencia: cos(Ï†) = R/Z = P_real/P_aparente
        impedance_total = math.sqrt(R_dyn**2 + (omega_n * self.L - 1/(omega_n * self.C))**2)
        power_factor = R_dyn / max(1e-10, impedance_total)
        power_factor = max(0.0, min(1.0, power_factor))

        # ================ TENSIÃ“N DE FLYBACK (di/dt) ================
        current_time = time.time()
        
        if not self._initialized_temporal:
            self._last_current = current_I
            self._last_time = current_time
            self._initialized_temporal = True
            di_dt = 0.0
        else:
            dt = current_time - self._last_time
            dt = max(1e-6, dt)  # MÃ­nimo 1Î¼s para evitar divisiÃ³n por cero
            
            di_dt = (current_I - self._last_current) / dt
            
            self._last_current = current_I
            self._last_time = current_time

        # TensiÃ³n inductiva: V_L = L * (di/dt)
        V_flyback = abs(self.L * di_dt)
        V_flyback = min(V_flyback, SystemConstants.MAX_FLYBACK_VOLTAGE)

        # ================ MÃ‰TRICAS DE ESTABILIDAD CORREGIDAS ================
        # Factor de estabilidad: MAYOR amortiguamiento = MÃS estable
        # Normalizado: 0 = inestable (Î¶â†’0), 1 = muy estable (Î¶â‰¥1)
        if damping_ratio >= 1.0:
            stability_factor = 1.0
        else:
            stability_factor = damping_ratio  # Lineal para Î¶ âˆˆ [0,1)

        # Margen de fase aproximado para sistema de 2do orden
        # PM â‰ˆ arctan(2Î¶ / âˆš(âˆš(1+4Î¶â´) - 2Î¶Â²)) en grados
        if damping_ratio > 0:
            inner = math.sqrt(max(0.0, math.sqrt(1 + 4*damping_ratio**4) - 2*damping_ratio**2))
            if inner > 1e-10:
                phase_margin = math.degrees(math.atan(2 * damping_ratio / inner))
            else:
                phase_margin = 90.0  # Sistema muy amortiguado
        else:
            phase_margin = 0.0  # Sistema marginal
        
        phase_margin = max(0.0, min(90.0, phase_margin))

        # ================ CLASIFICACIÃ“N DEL SISTEMA ================
        if damping_ratio < 0.7:
            system_type = "UNDERDAMPED"
        elif damping_ratio < 1.1:
            system_type = "CRITICALLY_DAMPED"
        else:
            system_type = "OVERDAMPED"

        # ================ CONSTRUIR RESULTADOS ================
        metrics = {
            # MÃ©tricas principales normalizadas
            "saturation": self._sanitize_metric(saturation_V, 0.0, 1.0),
            "complexity": self._sanitize_metric(complexity, 0.0, 1.0),
            "current_I": self._sanitize_metric(current_I, 0.0, 1.0),
            
            # EnergÃ­as (en Joules)
            "potential_energy": self._sanitize_metric(E_capacitor, 0.0, 1e10),
            "kinetic_energy": self._sanitize_metric(E_inductor, 0.0, 1e10),
            "total_energy": self._sanitize_metric(E_total, 0.0, 1e10),
            
            # Potencia y voltaje
            "dissipated_power": self._sanitize_metric(P_dissipated, 0.0, 1e6),
            "flyback_voltage": self._sanitize_metric(V_flyback, 0.0, SystemConstants.MAX_FLYBACK_VOLTAGE),
            
            # ParÃ¡metros del sistema RLC
            "dynamic_resistance": self._sanitize_metric(R_dyn, 0.0, 1e9),
            "damping_ratio": self._sanitize_metric(damping_ratio, 0.0, 100.0),
            "natural_frequency": self._sanitize_metric(omega_n, 0.0, 1e9),
            "damped_frequency": self._sanitize_metric(omega_d, 0.0, 1e9),
            
            # MÃ©tricas de calidad
            "power_factor": self._sanitize_metric(power_factor, 0.0, 1.0),
            "stability_factor": self._sanitize_metric(stability_factor, 0.0, 1.0),
            "phase_margin": self._sanitize_metric(phase_margin, 0.0, 90.0),
            
            # ClasificaciÃ³n
            "system_type": system_type,
        }

        # Almacenar en historial
        self._store_metrics(metrics)

        # DiagnÃ³stico selectivo
        if damping_ratio < 0.3:
            self.logger.warning(
                f"Sistema muy subamortiguado (Î¶={damping_ratio:.3f}) - riesgo de oscilaciones"
            )
        elif damping_ratio > 5.0:
            self.logger.debug(
                f"Sistema muy sobreamortiguado (Î¶={damping_ratio:.2f}) - respuesta lenta"
            )

        return metrics

    except OverflowError as e:
        self.logger.error(f"Overflow en cÃ¡lculo fÃ­sico: {e}")
        return self._get_zero_metrics()
    except ValueError as e:
        self.logger.error(f"ValueError en modelo fÃ­sico: {e}")
        return self._get_zero_metrics()
    except ZeroDivisionError as e:
        self.logger.error(f"DivisiÃ³n por cero en modelo fÃ­sico: {e}")
        return self._get_zero_metrics()
    except Exception as e:
        self.logger.error(f"Error inesperado en modelo fÃ­sico: {type(e).__name__}: {e}")
        return self._get_zero_metrics()


# 4. _get_zero_metrics (Corregido)

def _get_zero_metrics(self) -> Dict[str, float]:
    """
    Retorna mÃ©tricas en estado cero/inicial (seguro y neutral).
    
    Returns:
        Dict[str, float]: Diccionario con valores por defecto seguros.
    """
    return {
        # MÃ©tricas principales
        "saturation": 0.0,
        "complexity": 1.0,  # Sin datos = mÃ¡xima incertidumbre
        "current_I": 0.0,
        
        # EnergÃ­as
        "potential_energy": 0.0,
        "kinetic_energy": 0.0,
        "total_energy": 0.0,
        
        # Potencia y voltaje
        "dissipated_power": 0.0,
        "flyback_voltage": 0.0,
        
        # ParÃ¡metros del sistema (valores nominales)
        "dynamic_resistance": self.R,
        "damping_ratio": 1.0,  # CrÃ­ticamente amortiguado = estable por defecto
        "natural_frequency": self._resonant_freq * 2.0 * math.pi,
        "damped_frequency": 0.0,
        
        # MÃ©tricas de calidad (valores conservadores)
        "power_factor": 1.0,  # Ideal por defecto
        "stability_factor": 1.0,  # Estable por defecto
        "phase_margin": 45.0,  # Margen razonable
        
        # ClasificaciÃ³n
        "system_type": "INITIAL",
    }


# 5. _store_metrics (Optimizado)

def _store_metrics(self, metrics: Dict[str, float]) -> None:
    """
    Almacena mÃ©tricas en historial con buffer circular eficiente.
    
    Usa deque con maxlen para O(1) en inserciÃ³n y eliminaciÃ³n automÃ¡tica.
    
    Args:
        metrics: Diccionario de mÃ©tricas a almacenar.
    """
    # Crear copia con timestamp para evitar mutaciones externas
    timestamped = {
        **{k: v for k, v in metrics.items() if k != "system_type"},
        "_timestamp": time.time(),
        "_system_type": metrics.get("system_type", "UNKNOWN"),
    }
    
    # deque con maxlen maneja automÃ¡ticamente el lÃ­mite
    self._metrics_history.append(timestamped)


# 6. get_system_diagnosis (Robustecido)

def get_system_diagnosis(self, metrics: Dict[str, float]) -> str:
    """
    Genera diagnÃ³stico jerÃ¡rquico del sistema.

    Args:
        metrics: Diccionario de mÃ©tricas actuales.

    Returns:
        str: DiagnÃ³stico textual con emoji indicador de severidad.
    """
    # ValidaciÃ³n de entrada robusta
    if not isinstance(metrics, dict) or not metrics:
        return "â“ DIAGNÃ“STICO NO DISPONIBLE (mÃ©tricas invÃ¡lidas o vacÃ­as)"

    try:
        # ExtracciÃ³n segura con valores por defecto conservadores
        def safe_get(key: str, default: float) -> float:
            val = metrics.get(key, default)
            if not isinstance(val, (int, float)):
                return default
            if math.isnan(val) or math.isinf(val):
                return default
            return float(val)

        ec = safe_get("potential_energy", 0.0)
        el = safe_get("kinetic_energy", 0.0)
        flyback = safe_get("flyback_voltage", 0.0)
        power = safe_get("dissipated_power", 0.0)
        saturation = safe_get("saturation", 0.0)
        damping = safe_get("damping_ratio", 1.0)
        total_energy = safe_get("total_energy", ec + el)

        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        # DIAGNÃ“STICO JERÃRQUICO (orden de criticidad descendente)
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

        # NIVEL CRÃTICO (ğŸ”´)
        
        # 1. Sobrecalentamiento tÃ©rmico
        if power > SystemConstants.OVERHEAT_POWER_THRESHOLD:
            return f"ğŸ”´ SOBRECALENTAMIENTO CRÃTICO (P={power:.2f}W > {SystemConstants.OVERHEAT_POWER_THRESHOLD}W)"

        # 2. Inestabilidad dinÃ¡mica severa
        if damping < 0.1:
            return f"ğŸ”´ INESTABILIDAD SEVERA (Î¶={damping:.3f} â†’ oscilaciones divergentes)"

        # 3. Sistema sin energÃ­a cinÃ©tica (estancado)
        if el < SystemConstants.MIN_ENERGY_THRESHOLD and saturation < 0.1:
            return "ğŸ”´ SISTEMA ESTANCADO (sin inercia ni saturaciÃ³n)"

        # NIVEL ADVERTENCIA (ğŸŸ )
        
        # 4. Inestabilidad moderada
        if damping < 0.3:
            return f"ğŸŸ  INESTABILIDAD MODERADA (Î¶={damping:.2f} â†’ oscilaciones persistentes)"

        # 5. SobrepresiÃ³n energÃ©tica (mÃ¡s potencial que cinÃ©tica)
        if el > SystemConstants.MIN_ENERGY_THRESHOLD:
            energy_ratio = ec / el
            if energy_ratio > SystemConstants.HIGH_PRESSURE_RATIO:
                return f"ğŸŸ  SOBRECARGA POTENCIAL (Ec/El={energy_ratio:.1f}x)"

        # 6. Pico inductivo significativo
        if flyback > SystemConstants.HIGH_FLYBACK_THRESHOLD:
            return f"âš¡ PICO INDUCTIVO ALTO (V_L={flyback:.2f}V)"

        # NIVEL PRECAUCIÃ“N (ğŸŸ¡)

        # 7. SaturaciÃ³n muy alta
        if saturation > 0.95:
            return f"ğŸŸ¡ SATURACIÃ“N LÃMITE ({saturation:.1%})"

        # 8. Baja inercia operativa
        if el < SystemConstants.LOW_INERTIA_THRESHOLD and el > 0:
            return f"ğŸŸ¡ INERCIA BAJA (El={el:.4f}J)"

        # 9. Sistema subamortiguado (oscilatorio)
        if damping < 0.7:
            return f"ğŸ”µ RÃ‰GIMEN OSCILATORIO (Î¶={damping:.2f}, respuesta con overshoot)"

        # 10. Sistema sobreamortiguado (lento)
        if damping > 2.0:
            return f"ğŸ”µ RÃ‰GIMEN LENTO (Î¶={damping:.2f}, respuesta amortiguada)"

        # NIVEL OK (ğŸŸ¢)
        
        # Sistema en equilibrio saludable
        efficiency = saturation * 100
        return f"ğŸŸ¢ EQUILIBRIO NOMINAL (Î·={efficiency:.1f}%, Î¶={damping:.2f})"

    except Exception as e:
        self.logger.error(f"Error en diagnÃ³stico: {type(e).__name__}: {e}")
        return f"â“ ERROR DE DIAGNÃ“STICO ({type(e).__name__})"


# 7. get_trend_analysis (Robustecido)

def get_trend_analysis(self) -> Dict[str, Any]:
    """
    Analiza tendencias basadas en historial de mÃ©tricas.

    Calcula estadÃ­sticas mÃ³viles y detecta patrones de comportamiento.

    Returns:
        Dict[str, Any]: AnÃ¡lisis de tendencias con estadÃ­sticas.
    """
    history_len = len(self._metrics_history)
    
    if history_len < 5:
        return {
            "status": "INSUFFICIENT_DATA",
            "samples": history_len,
            "message": f"Se requieren al menos 5 muestras, hay {history_len}",
        }

    try:
        # Tomar las Ãºltimas N muestras (mÃ¡ximo 20)
        sample_size = min(20, history_len)
        recent = list(self._metrics_history)[-sample_size:]

        def extract_series(key: str, default: float = 0.0) -> List[float]:
            """Extrae serie temporal de una mÃ©trica con manejo de errores."""
            series = []
            for m in recent:
                val = m.get(key, default)
                if isinstance(val, (int, float)) and not (math.isnan(val) or math.isinf(val)):
                    series.append(float(val))
                else:
                    series.append(default)
            return series

        def calc_stats(series: List[float]) -> Dict[str, float]:
            """Calcula estadÃ­sticas de una serie."""
            if not series:
                return {"current": 0.0, "average": 0.0, "min": 0.0, "max": 0.0, "std": 0.0}
            
            n = len(series)
            avg = sum(series) / n
            variance = sum((x - avg) ** 2 for x in series) / n
            std = math.sqrt(variance)
            
            return {
                "current": series[-1],
                "average": avg,
                "min": min(series),
                "max": max(series),
                "std": std,
            }

        def detect_trend(series: List[float], threshold: float = 0.05) -> str:
            """Detecta tendencia usando regresiÃ³n lineal simple."""
            if len(series) < 3:
                return "INSUFFICIENT"
            
            n = len(series)
            x_mean = (n - 1) / 2.0
            y_mean = sum(series) / n
            
            # Pendiente: Î£(x-xÌ„)(y-È³) / Î£(x-xÌ„)Â²
            numerator = sum((i - x_mean) * (series[i] - y_mean) for i in range(n))
            denominator = sum((i - x_mean) ** 2 for i in range(n))
            
            if abs(denominator) < 1e-10:
                return "STABLE"
            
            slope = numerator / denominator
            
            # Normalizar pendiente por el rango de valores
            value_range = max(series) - min(series) if max(series) != min(series) else 1.0
            normalized_slope = slope * n / value_range
            
            if normalized_slope > threshold:
                return "INCREASING"
            elif normalized_slope < -threshold:
                return "DECREASING"
            else:
                return "STABLE"

        # Extraer series
        saturations = extract_series("saturation", 0.0)
        powers = extract_series("dissipated_power", 0.0)
        dampings = extract_series("damping_ratio", 1.0)
        energies = extract_series("total_energy", 0.0)

        # Calcular timestamps para tasa de muestreo
        timestamps = extract_series("_timestamp", time.time())
        if len(timestamps) >= 2:
            time_span = timestamps[-1] - timestamps[0]
            sample_rate = (len(timestamps) - 1) / max(0.001, time_span)
        else:
            time_span = 0.0
            sample_rate = 0.0

        return {
            "status": "OK",
            "samples": history_len,
            "window_size": sample_size,
            "time_span_seconds": round(time_span, 2),
            "sample_rate_hz": round(sample_rate, 3),
            
            "saturation": {
                **calc_stats(saturations),
                "trend": detect_trend(saturations, 0.03),
            },
            "power": {
                **calc_stats(powers),
                "trend": detect_trend(powers, 0.1),
            },
            "damping": {
                **calc_stats(dampings),
                "trend": detect_trend(dampings, 0.05),
            },
            "energy": {
                **calc_stats(energies),
                "trend": detect_trend(energies, 0.05),
            },
            
            # Alertas basadas en tendencias
            "alerts": self._generate_trend_alerts(saturations, powers, dampings),
        }

    except Exception as e:
        self.logger.error(f"Error en anÃ¡lisis de tendencias: {type(e).__name__}: {e}")
        return {
            "status": "ERROR",
            "samples": history_len,
            "error": str(e),
        }

def _generate_trend_alerts(
    self, 
    saturations: List[float], 
    powers: List[float], 
    dampings: List[float]
) -> List[str]:
    """Genera alertas basadas en anÃ¡lisis de tendencias."""
    alerts = []
    
    if len(saturations) >= 3:
        # Alerta si saturaciÃ³n estÃ¡ cayendo consistentemente
        if all(saturations[i] > saturations[i+1] for i in range(-3, -1)):
            alerts.append("âš ï¸ SaturaciÃ³n en descenso sostenido")
        
        # Alerta si saturaciÃ³n muy alta y estable (posible cuello de botella)
        if min(saturations[-5:]) > 0.9:
            alerts.append("âš ï¸ SaturaciÃ³n persistentemente alta (>90%)")

    if len(powers) >= 3:
        # Alerta si potencia disipada creciendo
        if all(powers[i] < powers[i+1] for i in range(-3, -1)):
            alerts.append("ğŸ”¥ Potencia disipada en aumento")

    if len(dampings) >= 3:
        # Alerta si amortiguamiento cayendo (hacia inestabilidad)
        if all(dampings[i] > dampings[i+1] for i in range(-3, -1)):
            if dampings[-1] < 0.5:
                alerts.append("âš¡ Sistema aproximÃ¡ndose a inestabilidad")

    return alerts


### Clase DataFluxCondenser

# 1. _validate_initialization_params (Robustecido)

def _validate_initialization_params(
    self, config: Dict[str, Any], profile: Dict[str, Any]
) -> None:
    """
    Valida parÃ¡metros de inicializaciÃ³n con mensajes detallados.

    Args:
        config: ConfiguraciÃ³n global.
        profile: Perfil de procesamiento.

    Raises:
        InvalidInputError: Si hay errores crÃ­ticos en los parÃ¡metros.
    """
    errors: List[str] = []
    self._init_warnings: List[str] = []

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # VALIDACIÃ“N DE CONFIG
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    if config is None:
        errors.append("config no puede ser None")
    elif not isinstance(config, dict):
        errors.append(f"config debe ser dict, recibido: {type(config).__name__}")
    else:
        # Verificar claves requeridas
        missing_config = self.REQUIRED_CONFIG_KEYS - set(config.keys())
        if missing_config:
            self._init_warnings.append(
                f"Claves faltantes en config: {missing_config}"
            )

        # Validar tipos de valores crÃ­ticos
        if "parser_settings" in config:
            if not isinstance(config["parser_settings"], dict):
                self._init_warnings.append(
                    f"parser_settings debe ser dict, recibido: "
                    f"{type(config['parser_settings']).__name__}"
                )

        if "processor_settings" in config:
            if not isinstance(config["processor_settings"], dict):
                self._init_warnings.append(
                    f"processor_settings debe ser dict, recibido: "
                    f"{type(config['processor_settings']).__name__}"
                )

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # VALIDACIÃ“N DE PROFILE
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    if profile is None:
        errors.append("profile no puede ser None")
    elif not isinstance(profile, dict):
        errors.append(f"profile debe ser dict, recibido: {type(profile).__name__}")
    else:
        # Verificar claves requeridas
        missing_profile = self.REQUIRED_PROFILE_KEYS - set(profile.keys())
        if missing_profile:
            self._init_warnings.append(
                f"Claves faltantes en profile: {missing_profile}"
            )

        # Validar estructura de columns_mapping
        if "columns_mapping" in profile:
            mapping = profile["columns_mapping"]
            if not isinstance(mapping, dict):
                self._init_warnings.append(
                    f"columns_mapping debe ser dict, recibido: {type(mapping).__name__}"
                )
            elif not mapping:
                self._init_warnings.append("columns_mapping estÃ¡ vacÃ­o")

        # Validar estructura de validation_rules
        if "validation_rules" in profile:
            rules = profile["validation_rules"]
            if not isinstance(rules, (dict, list)):
                self._init_warnings.append(
                    f"validation_rules debe ser dict o list, recibido: {type(rules).__name__}"
                )

    # Lanzar errores crÃ­ticos
    if errors:
        raise InvalidInputError(
            "Errores de inicializaciÃ³n:\n" + "\n".join(f"  - {e}" for e in errors)
        )

    # Registrar warnings (el logger ya existe en este punto)
    for warning in self._init_warnings:
        self.logger.warning(f"[INIT] {warning}")


# 2. _cleanup_after_processing (Implementado)

def _cleanup_after_processing(self) -> None:
    """
    Limpieza despuÃ©s del procesamiento (exitoso o fallido).
    
    Libera referencias a objetos grandes y resetea estado temporal
    para permitir reutilizaciÃ³n eficiente de memoria.
    """
    # Limpiar cachÃ© de claves normalizadas (puede ser grande)
    if hasattr(self, "_cache_keys_normalized"):
        self._cache_keys_normalized.clear()
        del self._cache_keys_normalized

    # Limpiar estado temporal del controlador de saturaciÃ³n
    if hasattr(self, "_last_saturation"):
        del self._last_saturation

    # Forzar garbage collection para objetos grandes si el procesamiento fue extenso
    if self._stats.total_records > 10000:
        import gc
        gc.collect()

    self.logger.debug("[CLEANUP] Limpieza post-procesamiento completada")


# 3. _create_empty_result (Mejorado)

def _create_empty_result(self) -> pd.DataFrame:
    """
    Crea un DataFrame vacÃ­o con metadatos de diagnÃ³stico.
    
    Returns:
        pd.DataFrame: DataFrame vacÃ­o con atributos de contexto.
    """
    elapsed = time.time() - (self._start_time or time.time())
    self._stats.processing_time = elapsed

    # Crear DataFrame vacÃ­o con estructura base esperada
    df_empty = pd.DataFrame()

    # Agregar metadatos como atributos (accesibles vÃ­a df.attrs)
    df_empty.attrs["_condenser_metadata"] = {
        "status": "EMPTY_RESULT",
        "processing_time_seconds": round(elapsed, 3),
        "reason": self._determine_empty_reason(),
        "timestamp": time.time(),
        "condenser_version": getattr(self, "_version", "1.0.0"),
    }

    self.logger.info(
        f"[EMPTY_RESULT] Retornando DataFrame vacÃ­o. "
        f"RazÃ³n: {df_empty.attrs['_condenser_metadata']['reason']}"
    )

    return df_empty

def _determine_empty_reason(self) -> str:
    """Determina la razÃ³n del resultado vacÃ­o basÃ¡ndose en el estado."""
    if self._stats.total_records == 0:
        return "NO_RECORDS_EXTRACTED"
    elif self._stats.total_records < self.condenser_config.min_records_threshold:
        return f"BELOW_THRESHOLD ({self._stats.total_records} < {self.condenser_config.min_records_threshold})"
    elif self._stats.failed_batches > 0 and self._stats.processed_records == 0:
        return "ALL_BATCHES_FAILED"
    else:
        return "UNKNOWN"


# 4. _validate_input_file (Robustecido)

def _validate_input_file(self, file_path: str) -> Path:
    """
    Valida el archivo de entrada exhaustivamente.

    Args:
        file_path: Ruta al archivo.

    Returns:
        Path: Objeto Path validado y resuelto.

    Raises:
        InvalidInputError: Si el archivo es invÃ¡lido o inaccesible.
    """
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # VALIDACIÃ“N DE TIPO Y FORMATO
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    if not isinstance(file_path, (str, Path)):
        raise InvalidInputError(
            f"file_path debe ser str o Path, recibido: {type(file_path).__name__}"
        )

    if isinstance(file_path, str):
        stripped = file_path.strip()
        if not stripped:
            raise InvalidInputError("file_path no puede ser una cadena vacÃ­a")
        # Detectar caracteres problemÃ¡ticos
        if any(c in stripped for c in ["\x00", "\n", "\r"]):
            raise InvalidInputError("file_path contiene caracteres de control invÃ¡lidos")
        path = Path(stripped)
    else:
        path = file_path

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # RESOLUCIÃ“N Y EXISTENCIA
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    try:
        # Resolver enlaces simbÃ³licos para obtener ruta real
        resolved_path = path.resolve()
    except (OSError, RuntimeError) as e:
        raise InvalidInputError(f"Error resolviendo ruta: {e}") from e

    if not resolved_path.exists():
        raise InvalidInputError(f"El archivo no existe: {path}")

    # Verificar que es archivo (no directorio, dispositivo, etc.)
    if not resolved_path.is_file():
        if resolved_path.is_dir():
            raise InvalidInputError(f"La ruta es un directorio, no un archivo: {path}")
        elif resolved_path.is_symlink():
            raise InvalidInputError(f"Enlace simbÃ³lico roto: {path}")
        else:
            raise InvalidInputError(f"La ruta no es un archivo regular: {path}")

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # PERMISOS DE ACCESO
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    try:
        if not os.access(resolved_path, os.R_OK):
            raise InvalidInputError(f"Sin permisos de lectura: {path}")
    except OSError as e:
        raise InvalidInputError(f"Error verificando permisos: {e}") from e

    # Verificar que no estÃ¡ bloqueado (intento de apertura)
    try:
        with open(resolved_path, "rb") as f:
            # Leer solo los primeros bytes para verificar acceso
            header = f.read(16)
    except PermissionError as e:
        raise InvalidInputError(f"Archivo bloqueado o sin permisos: {e}") from e
    except IOError as e:
        raise InvalidInputError(f"Error de I/O al verificar archivo: {e}") from e

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # VALIDACIÃ“N DE TAMAÃ‘O
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    try:
        file_stat = resolved_path.stat()
        file_size = file_stat.st_size
        file_size_mb = file_size / (1024 * 1024)

        if file_size < SystemConstants.MIN_FILE_SIZE_BYTES:
            raise InvalidInputError(
                f"Archivo demasiado pequeÃ±o ({file_size} bytes < "
                f"{SystemConstants.MIN_FILE_SIZE_BYTES} bytes): {path}"
            )

        if file_size_mb > SystemConstants.MAX_FILE_SIZE_MB:
            self.logger.warning(
                f"[VALIDATE] Archivo grande: {file_size_mb:.1f} MB > "
                f"{SystemConstants.MAX_FILE_SIZE_MB} MB lÃ­mite recomendado"
            )

        # Verificar que el archivo no fue modificado recientemente (posible escritura en curso)
        mtime = file_stat.st_mtime
        if time.time() - mtime < 1.0:  # Modificado hace menos de 1 segundo
            self.logger.warning(
                f"[VALIDATE] Archivo modificado recientemente, "
                f"posible escritura en curso: {path}"
            )

    except OSError as e:
        raise InvalidInputError(f"Error obteniendo informaciÃ³n del archivo: {e}") from e

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # VALIDACIÃ“N DE EXTENSIÃ“N Y CONTENIDO
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    suffix_lower = resolved_path.suffix.lower()
    if suffix_lower not in SystemConstants.VALID_FILE_EXTENSIONS:
        self.logger.warning(
            f"[VALIDATE] ExtensiÃ³n no estÃ¡ndar: '{suffix_lower}'. "
            f"Esperadas: {SystemConstants.VALID_FILE_EXTENSIONS}"
        )

    # Detectar si es archivo binario (puede indicar formato incorrecto)
    if header:
        # Bytes nulos indican archivo binario
        null_ratio = header.count(b"\x00") / len(header)
        if null_ratio > 0.3:
            self.logger.warning(
                f"[VALIDATE] Archivo posiblemente binario (null ratio: {null_ratio:.1%})"
            )

    self.logger.debug(f"[VALIDATE] Archivo validado: {resolved_path} ({file_size_mb:.2f} MB)")
    return resolved_path


# 5. _process_batches_with_pid (Correcciones CrÃ­ticas

def _process_batches_with_pid(
    self,
    raw_records: List[Dict[str, Any]],
    cache: Dict[str, Any],
    total_records: int,
) -> List[pd.DataFrame]:
    """
    Procesamiento por lotes con control PID adaptativo.

    CaracterÃ­sticas:
    - Backoff exponencial para fallos con recuperaciÃ³n gradual.
    - Balanceo de carga adaptativo basado en throughput.
    - PredicciÃ³n de tiempo restante con ventana mÃ³vil.
    - ProtecciÃ³n contra pÃ©rdida de datos.

    Args:
        raw_records: Lista de registros crudos.
        cache: CachÃ© de parseo.
        total_records: NÃºmero total de registros.

    Returns:
        List[pd.DataFrame]: Lista de DataFrames procesados.
    """
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # INICIALIZACIÃ“N DE ESTADO
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    processed_batches: List[pd.DataFrame] = []
    current_index = 0
    current_batch_size = self.condenser_config.min_batch_size
    
    # Contadores de fallos
    failed_batches_count = 0
    consecutive_failures = 0
    skipped_records = 0

    # Historial para estadÃ­sticas (ventana fija)
    HISTORY_WINDOW = 10
    batch_times: deque = deque(maxlen=HISTORY_WINDOW)
    batch_sizes: deque = deque(maxlen=HISTORY_WINDOW)
    complexity_history: deque = deque(maxlen=HISTORY_WINDOW)

    # Inicializar estado de saturaciÃ³n
    self._last_saturation: float = 0.5  # Valor neutral inicial

    # Algoritmo de backoff
    backoff_factor = 1.0
    max_backoff_factor = 8.0
    min_backoff_batch = max(1, self.condenser_config.min_batch_size // 4)

    # Tiempo de inicio para mÃ©tricas
    total_start_time = time.time()

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # CÃLCULO DE LÃMITE DE ITERACIONES DINÃMICO
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # LÃ­mite base: permite cierto overhead por reintentos
    base_iterations = int(
        math.ceil(total_records / max(1, self.condenser_config.min_batch_size))
    )
    # Factor de seguridad para reintentos y batches pequeÃ±os
    max_iterations = int(base_iterations * SystemConstants.MAX_ITERATIONS_MULTIPLIER * 1.5)
    # LÃ­mite absoluto para prevenir loops infinitos
    absolute_max = max(max_iterations, total_records * 3)

    self.logger.info(
        f"[PID_LOOP] Iniciando | Registros: {total_records:,} | "
        f"Batch inicial: {current_batch_size} | "
        f"LÃ­mite iteraciones: {max_iterations:,}"
    )

    iteration = 0
    while current_index < total_records and iteration < absolute_max:
        iteration += 1
        batch_start_time = time.time()

        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        # VERIFICACIÃ“N DE TIMEOUT GLOBAL
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        self._check_timeout(f"batch {iteration}")

        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        # PREDICCIÃ“N DE TIEMPO RESTANTE
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        if len(batch_times) >= 3 and len(batch_sizes) >= 3:
            # Calcular tiempo promedio por registro
            total_batch_time = sum(batch_times)
            total_batch_records = sum(batch_sizes)
            
            if total_batch_records > 0:
                avg_time_per_record = total_batch_time / total_batch_records
                records_remaining = total_records - current_index
                estimated_time_remaining = records_remaining * avg_time_per_record

                if iteration % 10 == 0:
                    progress_pct = (current_index / total_records) * 100
                    self.logger.info(
                        f"ğŸ“ˆ Progreso: {current_index:,}/{total_records:,} "
                        f"({progress_pct:.1f}%) | ETA: {estimated_time_remaining:.1f}s"
                    )

        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        # AJUSTE DINÃMICO DE BATCH SIZE BASADO EN RENDIMIENTO
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        if len(batch_times) >= 5:
            recent_times = list(batch_times)[-5:]
            recent_sizes = list(batch_sizes)[-5:]
            
            # Eficiencia = registros por segundo
            efficiencies = [
                s / max(0.001, t) for t, s in zip(recent_times, recent_sizes)
            ]
            avg_efficiency = sum(efficiencies) / len(efficiencies)

            # Ajustar batch size segÃºn eficiencia
            if avg_efficiency < 10:  # < 10 rec/s: reducir
                reduction = max(0.7, 1.0 - (10 - avg_efficiency) / 100)
                current_batch_size = max(
                    min_backoff_batch, 
                    int(current_batch_size * reduction)
                )
                self.logger.debug(
                    f"Eficiencia baja ({avg_efficiency:.1f} rec/s), "
                    f"reduciendo batch a {current_batch_size}"
                )
            elif avg_efficiency > 500 and consecutive_failures == 0:  # > 500 rec/s: aumentar
                current_batch_size = min(
                    self.condenser_config.max_batch_size,
                    int(current_batch_size * 1.2)
                )

        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        # EXTRAER LOTE
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        end_index = min(current_index + current_batch_size, total_records)
        batch_records = raw_records[current_index:end_index]

        if not batch_records:
            self.logger.warning(f"[PID_LOOP] Batch vacÃ­o en Ã­ndice {current_index}")
            current_index = end_index
            continue

        actual_batch_size = len(batch_records)

        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        # CÃLCULO DE MÃ‰TRICAS FÃSICAS
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        cache_hits = self._calculate_cache_hits(batch_records, cache)

        # MÃ©tricas sin factor de posiciÃ³n artificial
        metrics = self.physics.calculate_metrics(actual_batch_size, cache_hits)

        # Actualizar historial de complejidad
        current_complexity = metrics.get("complexity", 0.5)
        complexity_history.append(current_complexity)

        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        # CONTROL PID CON SUAVIZADO
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        saturation = metrics.get("saturation", 0.5)

        # Suavizado exponencial para evitar cambios bruscos
        alpha = 0.3  # Factor de suavizado (0 = sin cambio, 1 = cambio completo)
        smoothed_saturation = alpha * saturation + (1 - alpha) * self._last_saturation

        # Limitar cambio mÃ¡ximo por iteraciÃ³n
        max_change = 0.2
        if abs(smoothed_saturation - self._last_saturation) > max_change:
            smoothed_saturation = self._last_saturation + math.copysign(
                max_change, smoothed_saturation - self._last_saturation
            )

        pid_output = self.controller.compute(smoothed_saturation)
        self._last_saturation = smoothed_saturation

        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        # FRENO DE EMERGENCIA POR SOBRECALENTAMIENTO
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        dissipated_power = metrics.get("dissipated_power", 0.0)
        if dissipated_power > SystemConstants.OVERHEAT_POWER_THRESHOLD:
            self.logger.warning(
                f"ğŸ”¥ [OVERHEAT] P={dissipated_power:.1f}W > "
                f"{SystemConstants.OVERHEAT_POWER_THRESHOLD}W - Aplicando freno"
            )
            pid_output = max(
                self.condenser_config.min_batch_size,
                int(pid_output * SystemConstants.EMERGENCY_BRAKE_FACTOR)
            )
            self._stats.emergency_brakes_triggered += 1

        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        # PROCESAR BATCH
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        batch_result = self._process_single_batch(
            batch_records, cache, current_index, end_index
        )

        batch_time = time.time() - batch_start_time

        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        # MANEJO DE RESULTADO
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        if batch_result.success:
            # Reset de estado de fallo
            consecutive_failures = 0
            backoff_factor = max(1.0, backoff_factor * 0.5)  # RecuperaciÃ³n gradual

            if batch_result.dataframe is not None and not batch_result.dataframe.empty:
                processed_batches.append(batch_result.dataframe)
                self._stats.processed_records += batch_result.records_processed

            # Actualizar historial
            batch_times.append(batch_time)
            batch_sizes.append(actual_batch_size)

            # Calcular nuevo batch size basado en PID
            new_batch_size = int(pid_output)

        else:
            consecutive_failures += 1
            failed_batches_count += 1

            # Backoff exponencial con lÃ­mite
            backoff_factor = min(max_backoff_factor, backoff_factor * 2.0)
            
            # Reducir batch size con jitter para evitar patrones repetitivos
            jitter = 0.9 + (hash(str(iteration)) % 20) / 100.0  # 0.9-1.09
            reduced_size = int(current_batch_size / backoff_factor * jitter)
            new_batch_size = max(min_backoff_batch, reduced_size)

            self.logger.warning(
                f"[BATCH_FAIL] #{consecutive_failures} en Ã­ndice {current_index}. "
                f"Backoff: {backoff_factor:.1f}x, prÃ³ximo batch: {new_batch_size}. "
                f"Error: {batch_result.error_message[:100] if batch_result.error_message else 'N/A'}"
            )

            # Estrategia de recuperaciÃ³n por fallos mÃºltiples
            if consecutive_failures >= 5:
                # Registrar registros que serÃ¡n saltados
                skip_count = min(actual_batch_size, 10)
                skipped_records += skip_count
                self.logger.error(
                    f"[RECOVERY] {consecutive_failures} fallos consecutivos. "
                    f"Saltando {skip_count} registros problemÃ¡ticos."
                )
                
                # Avanzar parcialmente (no saltar todo el batch)
                current_index += skip_count
                new_batch_size = min_backoff_batch
                consecutive_failures = 3  # Reducir pero no resetear

        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        # ACTUALIZAR ESTADÃSTICAS
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        self._stats.add_batch_stats(
            batch_size=actual_batch_size,
            saturation=smoothed_saturation,
            power=dissipated_power,
            flyback=metrics.get("flyback_voltage", 0.0),
            kinetic=metrics.get("kinetic_energy", 0.0),
            success=batch_result.success,
        )

        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        # LOGGING ADAPTATIVO
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        log_frequency = max(1, min(50, total_records // 100))
        if iteration % log_frequency == 0 or iteration <= 3 or consecutive_failures > 0:
            throughput = actual_batch_size / max(0.001, batch_time)
            self.logger.info(
                f"ğŸ”„ [{iteration}] Idx: {current_index:,}-{end_index:,} | "
                f"Sat: {smoothed_saturation:.1%} | "
                f"Time: {batch_time:.3f}s | "
                f"Speed: {throughput:.0f} rec/s | "
                f"Next: {new_batch_size:,}"
            )

        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        # PREPARAR SIGUIENTE ITERACIÃ“N
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        current_index = end_index
        current_batch_size = max(
            self.condenser_config.min_batch_size,
            min(self.condenser_config.max_batch_size, new_batch_size)
        )

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # VALIDACIÃ“N DE TERMINACIÃ“N
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    if iteration >= absolute_max:
        self.logger.error(
            f"[PID_LOOP] LÃ­mite de iteraciones alcanzado: {iteration}. "
            f"Procesados: {current_index}/{total_records}"
        )

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # RESUMEN FINAL
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    total_time = time.time() - total_start_time
    overall_throughput = self._stats.processed_records / max(0.001, total_time)
    
    # Calcular complejidad promedio real
    avg_complexity = (
        sum(complexity_history) / len(complexity_history)
        if complexity_history else 0.5
    )

    self.logger.info(
        f"âœ… [PID_LOOP] Completado en {total_time:.1f}s | "
        f"Throughput: {overall_throughput:.1f} rec/s | "
        f"Batches: {self._stats.total_batches} (fallidos: {failed_batches_count}) | "
        f"Saltados: {skipped_records} | "
        f"Complejidad promedio: {avg_complexity:.2f}"
    )

    return processed_batches


# 6. _calculate_cache_hits (Optimizado y Corregido)

def _calculate_cache_hits(
    self,
    batch_records: List[Dict[str, Any]],
    cache: Dict[str, Any],
) -> int:
    """
    CÃ¡lculo eficiente de cache hits con mÃºltiples estrategias.

    Estrategias de bÃºsqueda (en orden de prioridad):
    1. Coincidencia exacta por clave canÃ³nica
    2. Coincidencia por hash de contenido
    3. Coincidencia parcial limitada (solo si cache es pequeÃ±o)

    Args:
        batch_records: Registros del batch actual.
        cache: CachÃ© disponible.

    Returns:
        int: NÃºmero de hits en cachÃ© (entero, redondeado hacia arriba).
    """
    # Validaciones tempranas
    if not cache or not isinstance(cache, dict):
        return 0

    if not batch_records or not isinstance(batch_records, list):
        return 0

    cache_size = len(cache)
    batch_size = len(batch_records)

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # PREPARAR ÃNDICE DE CACHE (con invalidaciÃ³n)
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # Verificar si necesitamos reconstruir el Ã­ndice
    cache_hash = hash(frozenset(cache.keys())) if cache_size < 10000 else id(cache)
    
    if (
        not hasattr(self, "_cache_index") 
        or not hasattr(self, "_cache_hash")
        or self._cache_hash != cache_hash
    ):
        self._cache_hash = cache_hash
        self._cache_index = {
            "exact": set(),      # Claves normalizadas para bÃºsqueda exacta
            "hashes": set(),     # Hashes conocidos
            "prefixes": set(),   # Prefijos de 20 chars (para bÃºsqueda rÃ¡pida)
        }

        for key, value in cache.items():
            if isinstance(key, str) and len(key) > 3:
                normalized = key.lower().strip()
                self._cache_index["exact"].add(normalized)
                
                # Almacenar prefijo para bÃºsqueda rÃ¡pida
                if len(normalized) >= 20:
                    self._cache_index["prefixes"].add(normalized[:20])

            # Indexar valores hash si existen
            if isinstance(value, dict) and "hash" in value:
                h = value["hash"]
                if isinstance(h, (str, int)):
                    self._cache_index["hashes"].add(str(h))

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # CONTAR HITS
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    exact_hits = 0
    partial_hits = 0.0
    
    # Claves a buscar en registros
    search_keys = ("insumo_line", "line", "raw_line", "content", "text", "data", "key")

    for record in batch_records:
        if not isinstance(record, dict):
            continue

        found_hit = False

        # Estrategia 1: BÃºsqueda exacta por claves conocidas
        for key in search_keys:
            if key not in record:
                continue
                
            content = record[key]
            if not isinstance(content, str) or len(content) <= 3:
                continue

            normalized = content.lower().strip()
            
            if normalized in self._cache_index["exact"]:
                exact_hits += 1
                found_hit = True
                break

        if found_hit:
            continue

        # Estrategia 2: BÃºsqueda por hash
        record_hash = record.get("hash")
        if record_hash is not None:
            str_hash = str(record_hash)
            if str_hash in self._cache_index["hashes"]:
                exact_hits += 1
                continue

        # Estrategia 3: BÃºsqueda por prefijo (solo si cache pequeÃ±o)
        if cache_size < 1000 and self._cache_index["prefixes"]:
            for key in search_keys:
                if key not in record:
                    continue
                    
                content = record[key]
                if not isinstance(content, str) or len(content) < 20:
                    continue

                prefix = content.lower().strip()[:20]
                if prefix in self._cache_index["prefixes"]:
                    partial_hits += 0.5
                    break

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # CALCULAR RESULTADO FINAL
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    total_hits = exact_hits + partial_hits
    
    # Logging de diagnÃ³stico para ratios anÃ³malos
    if batch_size > 50:
        hit_ratio = total_hits / batch_size
        if hit_ratio < 0.05:
            self.logger.debug(
                f"[CACHE] Hit ratio bajo: {hit_ratio:.1%} "
                f"({total_hits:.1f}/{batch_size})"
            )
        elif hit_ratio > 0.95:
            self.logger.debug(
                f"[CACHE] Hit ratio muy alto: {hit_ratio:.1%} - verificar cache"
            )

    # Redondear hacia arriba para no subestimar hits
    return int(math.ceil(total_hits))


# 7. _consolidate_results (Robustecido)

def _consolidate_results(self, processed_batches: List[pd.DataFrame]) -> pd.DataFrame:
    """
    Consolida mÃºltiples DataFrames con validaciÃ³n de esquema.

    Args:
        processed_batches: Lista de DataFrames de los lotes procesados.

    Returns:
        pd.DataFrame: DataFrame consolidado y validado.

    Raises:
        ProcessingError: Si hay error de consolidaciÃ³n irrecuperable.
    """
    if not processed_batches:
        self.logger.warning("[CONSOLIDATE] Sin batches para consolidar")
        return pd.DataFrame()

    if not isinstance(processed_batches, list):
        raise ProcessingError(
            f"processed_batches debe ser list, recibido: {type(processed_batches).__name__}"
        )

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # FILTRAR Y VALIDAR BATCHES
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    valid_batches: List[pd.DataFrame] = []
    total_rows_before = 0
    column_sets: List[Set[str]] = []
    skipped_reasons: Dict[str, int] = {}

    for i, batch in enumerate(processed_batches):
        # Validar tipo
        if not isinstance(batch, pd.DataFrame):
            reason = f"no_dataframe_{type(batch).__name__}"
            skipped_reasons[reason] = skipped_reasons.get(reason, 0) + 1
            continue

        # Saltar vacÃ­os
        if batch.empty:
            skipped_reasons["empty"] = skipped_reasons.get("empty", 0) + 1
            continue

        # Validar que tiene columnas
        if len(batch.columns) == 0:
            skipped_reasons["no_columns"] = skipped_reasons.get("no_columns", 0) + 1
            continue

        valid_batches.append(batch)
        total_rows_before += len(batch)
        column_sets.append(set(batch.columns))

    if skipped_reasons:
        self.logger.warning(f"[CONSOLIDATE] Batches saltados: {skipped_reasons}")

    if not valid_batches:
        self.logger.warning("[CONSOLIDATE] Todos los batches invÃ¡lidos o vacÃ­os")
        return pd.DataFrame()

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # VERIFICAR LÃMITE DE BATCHES
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    if len(valid_batches) > SystemConstants.MAX_BATCHES_TO_CONSOLIDATE:
        self.logger.warning(
            f"[CONSOLIDATE] Demasiados batches ({len(valid_batches)}). "
            f"Submuestreando uniformemente a {SystemConstants.MAX_BATCHES_TO_CONSOLIDATE}"
        )
        # Submuestreo uniforme en lugar de truncar al final
        step = len(valid_batches) / SystemConstants.MAX_BATCHES_TO_CONSOLIDATE
        indices = [int(i * step) for i in range(SystemConstants.MAX_BATCHES_TO_CONSOLIDATE)]
        valid_batches = [valid_batches[i] for i in indices]

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # ANALIZAR CONSISTENCIA DE ESQUEMAS
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    if len(column_sets) > 1:
        # Encontrar columnas comunes y divergentes
        common_columns = column_sets[0].intersection(*column_sets[1:])
        all_columns = column_sets[0].union(*column_sets[1:])
        extra_columns = all_columns - common_columns

        if extra_columns:
            self.logger.info(
                f"[CONSOLIDATE] Esquemas divergentes. "
                f"Comunes: {len(common_columns)}, Extras: {len(extra_columns)}"
            )
            
            # Las columnas extras serÃ¡n NaN donde no existan
            if len(extra_columns) > len(common_columns):
                self.logger.warning(
                    "[CONSOLIDATE] MÃ¡s columnas extras que comunes, "
                    "posible inconsistencia de datos"
                )

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # CONCATENAR CON MANEJO DE MEMORIA
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    try:
        # Estimar memoria requerida
        estimated_rows = sum(len(b) for b in valid_batches)
        avg_cols = sum(len(b.columns) for b in valid_batches) / len(valid_batches)
        
        # Si es muy grande, concatenar en chunks
        CHUNK_SIZE = 50
        if len(valid_batches) > CHUNK_SIZE:
            self.logger.info(
                f"[CONSOLIDATE] ConcatenaciÃ³n por chunks ({len(valid_batches)} batches)"
            )
            
            intermediate_results = []
            for i in range(0, len(valid_batches), CHUNK_SIZE):
                chunk = valid_batches[i:i + CHUNK_SIZE]
                chunk_df = pd.concat(chunk, ignore_index=True, sort=False)
                intermediate_results.append(chunk_df)
            
            df_final = pd.concat(intermediate_results, ignore_index=True, sort=False)
        else:
            df_final = pd.concat(valid_batches, ignore_index=True, sort=False)

        # Validar resultado
        total_rows_after = len(df_final)
        
        if total_rows_after < total_rows_before * 0.95:
            self.logger.warning(
                f"[CONSOLIDATE] PÃ©rdida de filas durante concatenaciÃ³n: "
                f"{total_rows_before} â†’ {total_rows_after} "
                f"({(1 - total_rows_after/total_rows_before)*100:.1f}% perdido)"
            )

        self.logger.info(
            f"[CONSOLIDATE] {len(valid_batches)} batches â†’ "
            f"{len(df_final):,} registros, {len(df_final.columns)} columnas"
        )

        return df_final

    except MemoryError as e:
        raise ProcessingError(
            f"Error de memoria consolidando {len(valid_batches)} batches "
            f"(~{estimated_rows:,} filas): {e}"
        ) from e
    except Exception as e:
        raise ProcessingError(
            f"Error consolidando resultados: {type(e).__name__}: {e}"
        ) from e


# 8. _validate_output (Mejorado)

def _validate_output(self, df: pd.DataFrame) -> None:
    """
    Valida el DataFrame de salida con mÃºltiples criterios de calidad.

    Args:
        df: DataFrame a validar.

    Raises:
        ProcessingError: Si el DataFrame no pasa validaciÃ³n estricta.
    """
    if not isinstance(df, pd.DataFrame):
        raise ProcessingError(
            f"Salida debe ser DataFrame, recibido: {type(df).__name__}"
        )

    # DataFrame vacÃ­o es vÃ¡lido (pero se registra)
    if df.empty:
        self.logger.warning("[VALIDATE_OUTPUT] DataFrame de salida vacÃ­o")
        return

    total_rows = len(df)
    total_cols = len(df.columns)
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # ANÃLISIS DE CALIDAD DE DATOS
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    quality_issues: List[str] = []
    warnings: List[str] = []

    # AnÃ¡lisis por columna
    null_analysis = {}
    dtype_analysis = {}
    
    for col in df.columns:
        try:
            col_data = df[col]
            null_count = col_data.isnull().sum()
            null_ratio = null_count / total_rows

            null_analysis[col] = {
                "count": int(null_count),
                "ratio": round(null_ratio, 4),
            }

            # Clasificar problemas
            if null_ratio == 1.0:
                quality_issues.append(f"Columna '{col}': 100% nulos")
            elif null_ratio > 0.9:
                warnings.append(f"Columna '{col}': {null_ratio:.1%} nulos")

            # Analizar tipos de datos
            dtype_analysis[col] = str(col_data.dtype)

            # Detectar columnas con un solo valor Ãºnico (posible constante o error)
            if null_ratio < 1.0:
                nunique = col_data.nunique(dropna=True)
                if nunique == 1 and total_rows > 10:
                    warnings.append(f"Columna '{col}': solo 1 valor Ãºnico (constante)")
                elif nunique == total_rows and total_rows > 100:
                    # Todos valores Ãºnicos - posible ID o timestamp
                    pass  # Esto es esperado para algunas columnas

        except Exception as e:
            warnings.append(f"Error analizando columna '{col}': {e}")

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # ANÃLISIS GLOBAL
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    
    # Verificar filas duplicadas
    try:
        duplicate_count = df.duplicated().sum()
        duplicate_ratio = duplicate_count / total_rows
        if duplicate_ratio > 0.5:
            warnings.append(f"Alto ratio de duplicados: {duplicate_ratio:.1%}")
    except Exception:
        pass  # Algunos DataFrames no soportan duplicated()

    # Verificar uso de memoria
    try:
        memory_mb = df.memory_usage(deep=True).sum() / (1024 * 1024)
        if memory_mb > 500:
            warnings.append(f"DataFrame grande en memoria: {memory_mb:.1f} MB")
    except Exception:
        pass

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # REPORTAR RESULTADOS
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    if warnings:
        self.logger.warning(
            f"[VALIDATE_OUTPUT] {len(warnings)} advertencias:\n  - " 
            + "\n  - ".join(warnings[:10])  # Limitar a 10
        )

    if quality_issues:
        issue_msg = f"[VALIDATE_OUTPUT] {len(quality_issues)} problemas de calidad:\n  - " + \
                    "\n  - ".join(quality_issues[:5])
        
        if self.condenser_config.enable_strict_validation:
            self.logger.error(issue_msg)
            raise ProcessingError(
                f"ValidaciÃ³n estricta fallida: {len(quality_issues)} problemas de calidad"
            )
        else:
            self.logger.warning(issue_msg)

    # Resumen positivo si todo estÃ¡ bien
    if not quality_issues and not warnings:
        self.logger.debug(
            f"[VALIDATE_OUTPUT] OK: {total_rows:,} filas Ã— {total_cols} columnas"
        )


# 9. reset (Corregido)

def reset(self) -> None:
    """
    Resetea completamente el estado del condensador para reutilizaciÃ³n.
    
    Limpia:
    - Estado del controlador PID
    - EstadÃ­sticas de procesamiento
    - Caches internos
    - Variables temporales
    """
    # Reset controlador PID
    self.controller.reset()
    
    # Reset estadÃ­sticas
    self._stats = ProcessingStats()
    self._start_time = None

    # Limpiar caches
    if hasattr(self, "_cache_keys_normalized"):
        self._cache_keys_normalized.clear()
        del self._cache_keys_normalized

    if hasattr(self, "_cache_index"):
        del self._cache_index
        
    if hasattr(self, "_cache_hash"):
        del self._cache_hash

    # Limpiar estado temporal
    if hasattr(self, "_last_saturation"):
        del self._last_saturation

    # Reset del motor de fÃ­sica tambiÃ©n
    if hasattr(self.physics, "_last_current"):
        self.physics._last_current = 0.0
        self.physics._last_time = time.time()
        self.physics._initialized_temporal = False

    self.logger.info("[RESET] Condensador reseteado completamente")
