# En app/apu_processor.py

import pandas as pd
from typing import List, Dict, Any, Optional, Tuple
from dataclasses import dataclass, field
from collections import defaultdict
from lark.exceptions import LarkError
import logging

logger = logging.getLogger(__name__)


@dataclass
class ParsingStats:
    """Estad√≠sticas detalladas del proceso de parsing."""
    total_lines: int = 0
    successful_parses: int = 0
    lark_parse_errors: int = 0
    lark_unexpected_input: int = 0
    lark_unexpected_chars: int = 0
    transformer_errors: int = 0
    empty_results: int = 0
    fallback_attempts: int = 0
    fallback_successes: int = 0
    cache_hits: int = 0
    failed_lines: List[Dict[str, Any]] = field(default_factory=list)


class APUProcessor:
    """
    Procesador de APUs con soporte para m√∫ltiples formatos de entrada.
    
    Soporta dos formatos:
    1. Formato agrupado (legacy): [{"codigo_apu": "X", "lines": [...]}]
    2. Formato plano (nuevo): [{"apu_code": "X", "insumo_line": "...", "_lark_tree": ...}]
    """
    
    def __init__(
        self, 
        config, 
        profile: Optional[Dict[str, Any]] = None, 
        parse_cache: Optional[Dict[str, Any]] = None
    ):
        """
        Inicializa el procesador con cache opcional de parsing.
        
        Args:
            config: Configuraci√≥n del sistema.
            profile: Perfil de parsing.
            parse_cache: Cache de √°rboles Lark pre-parseados.
        """
        self.config = config
        self.profile = profile or {}
        self.parser = self._initialize_parser()
        self.keyword_cache = {}
        
        # Cache de parsing (optimizaci√≥n)
        self.parse_cache = parse_cache or {}
        
        # Estad√≠sticas globales
        self.global_stats = {
            "total_apus": 0,
            "total_insumos": 0,
            "format_detected": None,
        }
        
        self.parsing_stats = ParsingStats()
        self.debug_mode = self.config.get("debug_mode", False)
        
        # Registros crudos (se establecer√°n externamente)
        self.raw_records = []
        
        if self.parse_cache:
            logger.info(
                f"‚úì APUProcessor inicializado con cache de {len(self.parse_cache)} "
                f"l√≠neas pre-parseadas"
            )
    
    def _detect_record_format(
        self, 
        records: List[Dict[str, Any]]
    ) -> Tuple[str, str]:
        """
        Detecta autom√°ticamente el formato de los registros de entrada.
        
        Args:
            records: Lista de registros a analizar.
        
        Returns:
            Tupla (formato, descripci√≥n) donde formato es "grouped" o "flat".
        """
        if not records:
            return ("unknown", "No hay registros para analizar")
        
        first_record = records[0]
        
        # Formato agrupado (legacy): tiene clave "lines"
        if "lines" in first_record:
            return (
                "grouped", 
                "Formato agrupado (legacy): cada registro es un APU con lista de l√≠neas"
            )
        
        # Formato plano (nuevo): tiene claves "insumo_line" y "apu_code"
        if "insumo_line" in first_record and "apu_code" in first_record:
            return (
                "flat", 
                "Formato plano (nuevo): cada registro es un insumo individual"
            )
        
        # Formato desconocido
        logger.warning(
            f"Formato de registro desconocido. Claves encontradas: {first_record.keys()}"
        )
        return ("unknown", "Formato no reconocido")
    
    def _group_flat_records(
        self, 
        flat_records: List[Dict[str, Any]]
    ) -> List[Dict[str, Any]]:
        """
        Agrupa registros planos por APU.
        
        Convierte el formato plano (nuevo) al formato agrupado que el resto
        del procesador puede manejar, pero preservando optimizaciones como
        el √°rbol Lark pre-parseado.
        
        Args:
            flat_records: Lista de registros en formato plano.
        
        Returns:
            Lista de registros en formato agrupado.
        """
        logger.info(f"Agrupando {len(flat_records)} registros planos por APU...")
        
        # Agrupar por apu_code
        grouped = defaultdict(lambda: {
            "lines": [],
            "_lark_trees": [],  # Preservar √°rboles pre-parseados
            "metadata": {}
        })
        
        for record in flat_records:
            apu_code = record.get("apu_code", "UNKNOWN")
            
            # Agregar l√≠nea de insumo
            insumo_line = record.get("insumo_line", "")
            if insumo_line:
                grouped[apu_code]["lines"].append(insumo_line)
                
                # Preservar √°rbol Lark si existe
                lark_tree = record.get("_lark_tree")
                grouped[apu_code]["_lark_trees"].append(lark_tree)
            
            # Preservar metadata del APU (solo la primera vez)
            if not grouped[apu_code]["metadata"]:
                grouped[apu_code]["metadata"] = {
                    "apu_code": apu_code,
                    "apu_desc": record.get("apu_desc", ""),
                    "apu_unit": record.get("apu_unit", ""),
                    "category": record.get("category", "INDEFINIDO"),
                    "source_line": record.get("source_line", 0),
                }
        
        # Convertir a lista de registros agrupados
        result = []
        for apu_code, data in grouped.items():
            record = {
                "codigo_apu": apu_code,  # Usar nombre legacy para compatibilidad
                "descripcion_apu": data["metadata"].get("apu_desc", ""),
                "unidad_apu": data["metadata"].get("apu_unit", ""),
                "lines": data["lines"],
                "_lark_trees": data["_lark_trees"],  # Nueva clave para optimizaci√≥n
                "category": data["metadata"].get("category", "INDEFINIDO"),
                "source_line": data["metadata"].get("source_line", 0),
            }
            result.append(record)
        
        logger.info(f"‚úì Agrupados en {len(result)} APUs distintos")
        
        return result
    
    def process_all(self) -> pd.DataFrame:
        """
        Procesa todos los registros de APU crudos y devuelve un DataFrame.
        
        Este m√©todo ahora es ADAPTATIVO:
        - Detecta autom√°ticamente el formato de entrada
        - Convierte formato plano a agrupado si es necesario
        - Reutiliza √°rboles Lark pre-parseados cuando est√°n disponibles
        - Mantiene compatibilidad con formato legacy
        
        Returns:
            DataFrame con todos los insumos procesados y estructurados.
        """
        if not self.raw_records:
            logger.warning("No hay registros crudos para procesar")
            return pd.DataFrame()
        
        logger.info(f"Iniciando procesamiento de {len(self.raw_records)} registros")
        
        # üî• PASO 1: Detectar formato de entrada
        format_type, format_desc = self._detect_record_format(self.raw_records)
        self.global_stats["format_detected"] = format_type
        
        logger.info(f"üìã Formato detectado: {format_desc}")
        
        # üî• PASO 2: Normalizar a formato agrupado si es necesario
        if format_type == "flat":
            processed_records = self._group_flat_records(self.raw_records)
        elif format_type == "grouped":
            processed_records = self.raw_records
            logger.info("‚úì Formato ya est√° agrupado, no se requiere conversi√≥n")
        else:
            logger.error(
                "‚ùå Formato de entrada no reconocido. "
                "No se puede procesar sin formato conocido."
            )
            return pd.DataFrame()
        
        # üî• PASO 3: Procesar cada APU
        all_results = []
        self.global_stats["total_apus"] = len(processed_records)
        
        for i, record in enumerate(processed_records):
            try:
                apu_context = self._extract_apu_context(record)
                
                if "lines" in record and record["lines"]:
                    # Preparar cache espec√≠fico para este APU
                    apu_cache = self._prepare_apu_cache(record)
                    
                    insumos = self._process_apu_lines(
                        record["lines"], 
                        apu_context,
                        apu_cache
                    )
                    
                    if insumos:
                        all_results.extend(insumos)
                else:
                    logger.debug(
                        f"APU {apu_context.get('codigo_apu')} no tiene l√≠neas para procesar"
                    )
                
                # Log de progreso
                if (i + 1) % 50 == 0:
                    logger.info(
                        f"Progreso: {i + 1}/{len(processed_records)} APUs procesados "
                        f"({len(all_results)} insumos extra√≠dos hasta ahora)"
                    )
            
            except Exception as e:
                logger.error(
                    f"Error procesando APU {i} "
                    f"[{record.get('codigo_apu', 'UNKNOWN')}]: {e}"
                )
                if self.debug_mode:
                    import traceback
                    logger.debug(f"Traceback:\n{traceback.format_exc()}")
                continue
        
        # üî• PASO 4: Log de resultados finales
        self.global_stats["total_insumos"] = len(all_results)
        self._log_global_stats()
        
        # üî• PASO 5: Convertir a DataFrame
        if all_results:
            return self._convert_to_dataframe(all_results)
        else:
            logger.warning("‚ö†Ô∏è  No se encontraron insumos v√°lidos en ning√∫n APU")
            return pd.DataFrame()
    
    def _prepare_apu_cache(self, record: Dict[str, Any]) -> Dict[str, Any]:
        """
        Prepara el cache de parsing espec√≠fico para un APU.
        
        Si el registro tiene √°rboles Lark pre-parseados (_lark_trees),
        crea un mapeo l√≠nea -> √°rbol para ese APU espec√≠fico.
        
        Args:
            record: Registro del APU con posibles √°rboles pre-parseados.
        
        Returns:
            Diccionario de cache l√≠nea -> √°rbol para este APU.
        """
        apu_cache = {}
        
        # Si el registro tiene √°rboles pre-parseados, mapearlos
        if "_lark_trees" in record and record["_lark_trees"]:
            lines = record.get("lines", [])
            trees = record["_lark_trees"]
            
            # Crear mapeo l√≠nea -> √°rbol
            for line, tree in zip(lines, trees):
                if tree is not None:
                    apu_cache[line.strip()] = tree
            
            if apu_cache:
                logger.debug(
                    f"‚úì Cache espec√≠fico de APU preparado: {len(apu_cache)} √°rboles"
                )
        
        # Combinar con cache global
        combined_cache = {**self.parse_cache, **apu_cache}
        
        return combined_cache
    
    def _extract_apu_context(self, record: Dict[str, Any]) -> Dict[str, Any]:
        """
        Extrae el contexto relevante de un registro de APU.
        
        Soporta tanto nombres de claves legacy como nuevos.
        
        Args:
            record: Registro de APU (formato agrupado).
        
        Returns:
            Diccionario con contexto del APU normalizado.
        """
        # Intentar claves nuevas primero, luego legacy
        return {
            "codigo_apu": record.get("codigo_apu") or record.get("apu_code", ""),
            "descripcion_apu": (
                record.get("descripcion_apu") or 
                record.get("apu_desc", "")
            ),
            "unidad_apu": record.get("unidad_apu") or record.get("apu_unit", ""),
            "cantidad_apu": record.get("cantidad_apu", 1.0),
            "precio_unitario_apu": record.get("precio_unitario_apu", 0.0),
            "category": record.get("category", "INDEFINIDO"),
        }
    
    def _process_apu_lines(
        self, 
        lines: List[str], 
        apu_context: Dict[str, Any],
        line_cache: Optional[Dict[str, Any]] = None
    ) -> List['InsumoProcesado']:
        """
        Procesa l√≠neas de APU con reutilizaci√≥n de cache de parsing.
        
        Args:
            lines: Lista de l√≠neas a procesar.
            apu_context: Contexto del APU.
            line_cache: Cache de √°rboles Lark para estas l√≠neas espec√≠ficas.
        
        Returns:
            Lista de insumos procesados.
        """
        if not lines:
            return []
        
        results = []
        stats = ParsingStats()
        
        # Usar cache combinado (espec√≠fico del APU + global)
        active_cache = line_cache if line_cache is not None else self.parse_cache
        
        apu_code = apu_context.get("codigo_apu", "UNKNOWN")
        
        logger.debug(
            f"Procesando {len(lines)} l√≠neas para APU: {apu_code} "
            f"(cache: {len(active_cache)} entradas)"
        )
        
        for line_num, line in enumerate(lines, start=1):
            if not line or not line.strip():
                continue
            
            stats.total_lines += 1
            line_clean = line.strip()
            insumo = None
            
            try:
                if self.parser:
                    # üî• OPTIMIZACI√ìN: Usar cache si est√° disponible
                    tree = None
                    used_cache = False
                    
                    if line_clean in active_cache:
                        tree = active_cache[line_clean]
                        used_cache = True
                        stats.cache_hits += 1
                        logger.debug(
                            f"  ‚ö° L√≠nea {line_num}: Usando √°rbol Lark del cache"
                        )
                    
                    if tree is None:
                        # Parsear normalmente
                        try:
                            tree = self.parser.parse(line_clean)
                        except LarkError as lark_error:
                            # Si falla aqu√≠ con validaci√≥n unificada, es inesperado
                            logger.warning(
                                f"  ‚ö†Ô∏è  L√≠nea {line_num}: Fall√≥ Lark pero pas√≥ validaci√≥n previa\n"
                                f"      Error: {lark_error}\n"
                                f"      L√≠nea: {line_clean[:100]}"
                            )
                            stats.lark_parse_errors += 1
                            continue
                    
                    # Transformar √°rbol a insumo
                    try:
                        transformer = APUTransformer(
                            apu_context, 
                            self.config, 
                            self.profile, 
                            self.keyword_cache
                        )
                        insumo = transformer.transform(tree)
                        
                        if isinstance(insumo, list):
                            if insumo:
                                insumo = insumo[0]
                                stats.successful_parses += 1
                            else:
                                stats.empty_results += 1
                                logger.debug(
                                    f"  ‚ö†Ô∏è  L√≠nea {line_num}: Transformer devolvi√≥ lista vac√≠a"
                                )
                                insumo = None
                        else:
                            stats.successful_parses += 1
                    
                    except Exception as transform_error:
                        stats.transformer_errors += 1
                        logger.error(
                            f"  ‚úó L√≠nea {line_num}: Error en transformer\n"
                            f"    Error: {type(transform_error).__name__}: {transform_error}\n"
                            f"    L√≠nea: {line_clean[:100]}"
                        )
                        
                        if self.debug_mode:
                            import traceback
                            logger.debug(f"Traceback:\n{traceback.format_exc()}")
                        
                        continue
                
                # Agregar resultado si es v√°lido
                if insumo:
                    insumo.line_number = line_num
                    results.append(insumo)
                else:
                    stats.failed_lines.append({
                        "line_number": line_num,
                        "content": line_clean,
                        "apu_code": apu_code
                    })
            
            except Exception as unexpected_error:
                logger.error(
                    f"  üö® L√≠nea {line_num}: Error inesperado\n"
                    f"    Tipo: {type(unexpected_error).__name__}\n"
                    f"    Error: {unexpected_error}\n"
                    f"    L√≠nea: {line_clean}"
                )
                
                if self.debug_mode:
                    import traceback
                    logger.debug(f"Traceback completo:\n{traceback.format_exc()}")
                
                stats.failed_lines.append({
                    "line_number": line_num,
                    "content": line_clean,
                    "error": str(unexpected_error),
                    "apu_code": apu_code
                })
                continue
        
        # Log de estad√≠sticas del APU
        self._log_parsing_stats(apu_code, stats)
        
        # Actualizar estad√≠sticas globales
        self._merge_stats(stats)
        
        return results
    
    def _merge_stats(self, apu_stats: ParsingStats):
        """Combina estad√≠sticas de un APU con las globales."""
        self.parsing_stats.total_lines += apu_stats.total_lines
        self.parsing_stats.successful_parses += apu_stats.successful_parses
        self.parsing_stats.lark_parse_errors += apu_stats.lark_parse_errors
        self.parsing_stats.transformer_errors += apu_stats.transformer_errors
        self.parsing_stats.empty_results += apu_stats.empty_results
        self.parsing_stats.cache_hits += apu_stats.cache_hits
        self.parsing_stats.failed_lines.extend(apu_stats.failed_lines)
    
    def _log_parsing_stats(self, apu_code: str, stats: ParsingStats):
        """
        Registra estad√≠sticas detalladas del parsing de un APU.
        
        Args:
            apu_code: C√≥digo del APU procesado.
            stats: Estad√≠sticas del procesamiento.
        """
        if stats.total_lines == 0:
            return
        
        success_rate = (
            (stats.successful_parses / stats.total_lines * 100) 
            if stats.total_lines > 0 else 0
        )
        cache_rate = (
            (stats.cache_hits / stats.total_lines * 100)
            if stats.total_lines > 0 else 0
        )
        
        # Solo mostrar detalles si hay problemas o en modo debug
        if success_rate < 100 or self.debug_mode:
            logger.info("-" * 70)
            logger.info(f"üìà APU: {apu_code}")
            logger.info(f"   L√≠neas procesadas:  {stats.total_lines}")
            logger.info(f"   ‚úì Exitosos:         {stats.successful_parses} ({success_rate:.1f}%)")
            logger.info(f"   ‚ö° Cache hits:       {stats.cache_hits} ({cache_rate:.1f}%)")
            
            if stats.lark_parse_errors > 0:
                logger.info(f"   ‚úó Errores Lark:     {stats.lark_parse_errors}")
            if stats.transformer_errors > 0:
                logger.info(f"   ‚úó Errores Trans.:   {stats.transformer_errors}")
            if stats.empty_results > 0:
                logger.info(f"   ‚ö†Ô∏è  Resultados vac√≠os: {stats.empty_results}")
            
            logger.info("-" * 70)
    
    def _log_global_stats(self):
        """Registra estad√≠sticas globales del procesamiento."""
        logger.info("=" * 80)
        logger.info("üìä RESUMEN GLOBAL DE PROCESAMIENTO")
        logger.info("=" * 80)
        logger.info(f"Formato detectado:           {self.global_stats['format_detected']}")
        logger.info(f"Total APUs procesados:       {self.global_stats['total_apus']}")
        logger.info(f"Total insumos extra√≠dos:     {self.global_stats['total_insumos']}")
        logger.info(f"Total l√≠neas procesadas:     {self.parsing_stats.total_lines}")
        logger.info("")
        logger.info("Resultados de parsing:")
        logger.info(f"  ‚úì Exitosos:                {self.parsing_stats.successful_parses}")
        logger.info(f"  ‚ö° Cache hits:              {self.parsing_stats.cache_hits}")
        logger.info(f"  ‚úó Errores Lark:            {self.parsing_stats.lark_parse_errors}")
        logger.info(f"  ‚úó Errores Transformer:     {self.parsing_stats.transformer_errors}")
        logger.info(f"  ‚ö†Ô∏è  Resultados vac√≠os:      {self.parsing_stats.empty_results}")
        logger.info("")
        
        if self.parsing_stats.total_lines > 0:
            success_rate = (
                self.parsing_stats.successful_parses / 
                self.parsing_stats.total_lines * 100
            )
            cache_efficiency = (
                self.parsing_stats.cache_hits / 
                self.parsing_stats.total_lines * 100
            )
            
            logger.info(f"Tasa de √©xito:               {success_rate:.2f}%")
            logger.info(f"Eficiencia de cache:         {cache_efficiency:.2f}%")
        
        logger.info("=" * 80)
        
        # Alertas
        if self.global_stats['total_insumos'] == 0:
            logger.error(
                "üö® CR√çTICO: 0 insumos extra√≠dos.\n"
                "   Posibles causas:\n"
                "   1. Formato de datos incompatible con gram√°tica\n"
                "   2. Errores en el transformer\n"
                "   3. Configuraci√≥n de perfil incorrecta\n"
                "   ‚Üí Revise los logs detallados arriba"
            )
        elif success_rate < 50:
            logger.warning(
                f"‚ö†Ô∏è  Tasa de √©xito baja ({success_rate:.1f}%).\n"
                f"   Considere revisar la gram√°tica o el formato de datos."
            )
    
    def _initialize_parser(self):
        """Inicializa el parser Lark (implementar seg√∫n tu c√≥digo existente)."""
        # Placeholder - implementar seg√∫n tu l√≥gica
        try:
            from app.grammars.apu_grammar import APU_GRAMMAR
            from lark import Lark
            
            return Lark(
                APU_GRAMMAR,
                start='line',
                parser='lalr',
                maybe_placeholders=False,
                cache=True,
            )
        except Exception as e:
            logger.error(f"Error inicializando parser Lark: {e}")
            return None
    
    def _convert_to_dataframe(self, insumos: List['InsumoProcesado']) -> pd.DataFrame:
        """
        Convierte lista de insumos a DataFrame.
        
        Args:
            insumos: Lista de objetos InsumoProcesado.
        
        Returns:
            DataFrame de pandas.
        """
        # Placeholder - implementar seg√∫n tu l√≥gica
        try:
            data = [vars(insumo) for insumo in insumos]
            df = pd.DataFrame(data)
            
            logger.info(f"‚úì DataFrame creado: {len(df)} filas, {len(df.columns)} columnas")
            
            return df
        except Exception as e:
            logger.error(f"Error convirtiendo a DataFrame: {e}")
            return pd.DataFrame()


# ============================================================================
# CLASE AUXILIAR PARA TESTING (si no existe)
# ============================================================================

class APUTransformer:
    """Transformer de Lark para convertir √°rbol de parsing a InsumoProcesado."""
    
    def __init__(self, apu_context, config, profile, keyword_cache):
        self.apu_context = apu_context
        self.config = config
        self.profile = profile
        self.keyword_cache = keyword_cache
    
    def transform(self, tree):
        """
        Transforma √°rbol Lark a InsumoProcesado.
        Implementar seg√∫n tu l√≥gica espec√≠fica.
        """
        # Placeholder
        raise NotImplementedError("Implementar seg√∫n l√≥gica del proyecto")


@dataclass
class InsumoProcesado:
    """Clase de datos para un insumo procesado."""
    codigo_apu: str = ""
    descripcion: str = ""
    unidad: str = ""
    cantidad: float = 0.0
    precio_unitario: float = 0.0
    precio_total: float = 0.0
    category: str = "INDEFINIDO"
    line_number: int = 0