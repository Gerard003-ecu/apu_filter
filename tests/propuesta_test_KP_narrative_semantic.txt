class ComplexityAnalysisRefined:
    """
    Análisis refinado de complejidad computacional con teoría asintótica.
    """
    
    def test_big_o_analysis_narrator(self, narrator: TelemetryNarrator):
        """
        Análisis riguroso de complejidad O() del TelemetryNarrator.
        Usa regresión logarítmica para determinar complejidad real.
        """
        import numpy as np
        from scipy import stats
        
        # Configurar tamaños de entrada (escala logarítmica)
        sizes = [10, 20, 50, 100, 200, 500, 1000]
        times = []
        
        for n in sizes:
            context = create_test_context(num_spans=n, span_depth=2)
            
            # Medir tiempo con precisión estadística
            measurements = []
            for _ in range(10):
                start = time.perf_counter_ns()
                _ = narrator.summarize_execution(context)
                measurements.append((time.perf_counter_ns() - start) / 1_000_000)  # ms
            
            # Usar percentil 50 (mediana) para reducir efecto de outliers
            times.append(np.median(measurements))
        
        # Transformación logarítmica para análisis de regresión
        log_sizes = np.log(sizes)
        log_times = np.log(times)
        
        # Ajustar modelo lineal: log(T) = a + b*log(N)
        slope, intercept, r_value, p_value, std_err = stats.linregress(log_sizes, log_times)
        
        # Determinar complejidad asintótica
        exponent = slope
        
        complexity_map = {
            (0.5, 1.5): "O(n) - Lineal",
            (1.5, 2.5): "O(n²) - Cuadrático",
            (0.8, 1.2): "O(n log n) - Log-lineal",
            (0.1, 0.5): "O(log n) - Logarítmico",
            (2.5, float('inf')): "O(n³) o peor - Exponencial"
        }
        
        for (low, high), complexity in complexity_map.items():
            if low <= exponent < high:
                detected_complexity = complexity
                break
        else:
            detected_complexity = f"O(n^{exponent:.2f}) - Potencia extraña"
        
        # Análisis de residuales para verificar modelo
        predicted_log_times = intercept + slope * log_sizes
        residuals = log_times - predicted_log_times
        residuals_std = np.std(residuals)
        
        # Criterios de aceptación
        assert r_value**2 > 0.95, f"Modelo pobre: R²={r_value**2:.3f}"
        assert residuals_std < 0.1, f"Residuales altos: {residuals_std:.3f}"
        assert 0.5 <= exponent < 2.0, f"Complejidad inaceptable: {exponent:.2f} -> {detected_complexity}"
        
        return {
            "exponent": exponent,
            "complexity": detected_complexity,
            "r_squared": r_value**2,
            "residual_std": residuals_std,
            "sizes": sizes,
            "times": times,
            "predicted_times": np.exp(predicted_log_times).tolist()
        }
    
    def test_memory_complexity_analysis(self, narrator: TelemetryNarrator):
        """
        Análisis de complejidad de memoria usando teoría de grafos de objetos.
        """
        import gc
        import objgraph
        
        sizes = [10, 50, 100]
        memory_growth = []
        
        for n in sizes:
            gc.collect()
            initial_count = len(gc.get_objects())
            
            context = create_test_context(num_spans=n, span_depth=2)
            report = narrator.summarize_execution(context)
            
            # Forzar recolección y contar objetos nuevos
            gc.collect()
            current_count = len(gc.get_objects())
            
            # Contar objetos por tipo para análisis detallado
            type_counts = {}
            for obj in gc.get_objects():
                obj_type = type(obj).__name__
                type_counts[obj_type] = type_counts.get(obj_type, 0) + 1
            
            # Filtrar solo tipos relevantes (aquellos que crecen con n)
            relevant_growth = {}
            for obj_type, count in type_counts.items():
                if count > n * 0.1:  # Tipos que crecen significativamente
                    relevant_growth[obj_type] = count
            
            memory_growth.append({
                "size": n,
                "total_objects": current_count - initial_count,
                "per_span": (current_count - initial_count) / n if n > 0 else 0,
                "relevant_types": relevant_growth
            })
        
        # Análisis de crecimiento de memoria
        growth_ratios = []
        for i in range(1, len(memory_growth)):
            ratio = memory_growth[i]["total_objects"] / memory_growth[i-1]["total_objects"]
            growth_ratios.append(ratio)
        
        # El crecimiento debe ser lineal O(n) o mejor
        avg_growth_ratio = np.mean(growth_ratios) if growth_ratios else 1
        
        # Verificar que no haya fugas (crecimiento constante por objeto)
        assert avg_growth_ratio < 5.0, f"Crecimiento de memoria exponencial: {avg_growth_ratio:.2f}x"
        
        # Verificar que objetos por span sea constante (O(1) por span)
        per_span_values = [m["per_span"] for m in memory_growth]
        per_span_std = np.std(per_span_values) / np.mean(per_span_values) if per_span_values else 0
        
        assert per_span_std < 0.3, f"Memoria por span no constante: CV={per_span_std:.2f}"
        
        return {
            "memory_growth": memory_growth,
            "avg_growth_ratio": avg_growth_ratio,
            "per_span_std": per_span_std,
            "memory_complexity": "O(n)" if avg_growth_ratio < 2.0 else "O(n²) o peor"
        }


class AmortizationAnalysis:
    """
    Análisis de costos amortizados y efectos de caching.
    """
    
    def test_amortized_cost_lattice_operations(self):
        """
        Análisis de costo amortizado para operaciones de lattice.
        Basado en teoría de análisis amortizado (método agregado).
        """
        from collections import Counter
        
        # Secuencia de operaciones típicas
        operations = []
        levels = list(SeverityLevel)
        
        # Generar secuencia realista (mezcla de operaciones)
        for _ in range(1000):
            op_type = np.random.choice(['join', 'meet', 'supremum', 'infimum'], 
                                      p=[0.3, 0.3, 0.2, 0.2])
            operations.append((op_type, np.random.choice(levels, size=2)))
        
        # Medir costo individual y secuencial
        individual_costs = []
        for op_type, (a, b) in operations[:100]:  # Muestra
            start = time.perf_counter_ns()
            if op_type == 'join':
                _ = a | b
            elif op_type == 'meet':
                _ = a & b
            elif op_type == 'supremum':
                _ = SeverityLevel.supremum(a, b)
            else:
                _ = SeverityLevel.infimum(a, b)
            individual_costs.append((time.perf_counter_ns() - start) / 1000)  # µs
        
        # Medir costo de la secuencia completa
        start = time.perf_counter_ns()
        for op_type, (a, b) in operations:
            if op_type == 'join':
                _ = a | b
            elif op_type == 'meet':
                _ = a & b
            elif op_type == 'supremum':
                _ = SeverityLevel.supremum(a, b)
            else:
                _ = SeverityLevel.infimum(a, b)
        total_time = (time.perf_counter_ns() - start) / 1_000_000  # ms
        
        # Calcular costo amortizado
        total_ops = len(operations)
        amortized_cost_ms = total_time / total_ops
        worst_individual_cost = max(individual_costs) / 1000  # Convertir a ms
        
        # Ratio de amortización (debería ser < 1 si hay mejoras por lote)
        amortization_ratio = amortized_cost_ms / (worst_individual_cost + 1e-10)
        
        # Teorema: costo amortizado ≤ 2 * costo individual promedio
        assert amortized_cost_ms < 2 * np.mean(individual_costs) / 1000, \
            f"Mala amortización: {amortized_cost_ms:.3f}ms vs {np.mean(individual_costs)/1000:.3f}ms"
        
        # Verificar consistencia (costo total ≈ suma de individuales)
        expected_total = np.sum(individual_costs[:100]) * (total_ops / 100) / 1000  # ms
        efficiency_ratio = total_time / expected_total
        
        assert 0.8 < efficiency_ratio < 1.2, f"Comportamiento inconsistente: ratio={efficiency_ratio:.2f}"
        
        return {
            "total_operations": total_ops,
            "total_time_ms": total_time,
            "amortized_cost_ms": amortized_cost_ms,
            "worst_individual_cost_ms": worst_individual_cost,
            "amortization_ratio": amortization_ratio,
            "efficiency_ratio": efficiency_ratio,
            "operation_distribution": dict(Counter(op for op, _ in operations))
        }
    
    def test_caching_effectiveness_narrator(self, narrator: TelemetryNarrator):
        """
        Evalúa efectividad del caching en análisis repetitivos.
        """
        # Crear contexto con patrones repetitivos
        context = TelemetryContext()
        
        # Patrones que deberían beneficiarse de caching
        patterns = [
            {"name": "load_data", "stratum": Stratum.PHYSICS, "status": StepStatus.SUCCESS},
            {"name": "calculate_costs", "stratum": Stratum.TACTICS, "status": StepStatus.WARNING},
            {"name": "financial_analysis", "stratum": Stratum.STRATEGY, "status": StepStatus.SUCCESS},
        ]
        
        for i in range(100):
            pattern = patterns[i % len(patterns)]
            with context.span(pattern["name"] + f"_{i}"):
                pass
        
        # Primera ejecución (cold cache)
        start = time.perf_counter_ns()
        report1 = narrator.summarize_execution(context)
        cold_time = (time.perf_counter_ns() - start) / 1_000_000  # ms
        
        # Segunda ejecución (warm cache, misma estructura)
        start = time.perf_counter_ns()
        report2 = narrator.summarize_execution(context)
        warm_time = (time.perf_counter_ns() - start) / 1_000_000
        
        # Tercera ejecución con contexto idéntico
        start = time.perf_counter_ns()
        report3 = narrator.summarize_execution(context)
        hot_time = (time.perf_counter_ns() - start) / 1_000_000
        
        # Calcular speedup por caching
        speedup_cold_warm = cold_time / max(warm_time, 1e-10)
        speedup_cold_hot = cold_time / max(hot_time, 1e-10)
        
        # Efectividad de caching (debería mejorar al menos 20% en ejecuciones repetidas)
        assert speedup_cold_warm > 1.2, f"Caching inefectivo: speedup={speedup_cold_warm:.2f}"
        
        # Consistencia de resultados (caching no debe afectar resultados)
        assert report1["verdict_code"] == report2["verdict_code"] == report3["verdict_code"], \
            "Caching afecta resultados"
        
        # Análisis de patrón de acceso (locality of reference)
        time_pattern = [cold_time, warm_time, hot_time]
        time_reduction_ratio = (cold_time - hot_time) / cold_time
        
        assert time_reduction_ratio > 0.1, f"Poca mejora por caching: {time_reduction_ratio:.1%}"
        
        return {
            "cold_time_ms": cold_time,
            "warm_time_ms": warm_time,
            "hot_time_ms": hot_time,
            "speedup_cold_warm": speedup_cold_warm,
            "speedup_cold_hot": speedup_cold_hot,
            "time_reduction_ratio": time_reduction_ratio,
            "caching_effectiveness": "Bueno" if speedup_cold_hot > 1.5 else "Moderado"
        }


class StochasticPerformanceAnalysis:
    """
    Análisis estocástico de rendimiento usando teoría de probabilidad.
    """
    
    def test_performance_distribution_analysis(self, narrator: TelemetryNarrator):
        """
        Analiza distribución estadística de tiempos de ejecución.
        Usa pruebas de normalidad y análisis de colas.
        """
        from scipy import stats
        
        # Generar muestra de tiempos
        context = create_test_context(num_spans=50, span_depth=2)
        sample_size = 1000
        
        times = []
        for _ in range(sample_size):
            start = time.perf_counter_ns()
            _ = narrator.summarize_execution(context)
            times.append((time.perf_counter_ns() - start) / 1_000_000)  # ms
        
        # Estadísticas descriptivas
        mean_time = np.mean(times)
        std_time = np.std(times)
        cv = std_time / mean_time if mean_time > 0 else 0  # Coeficiente de variación
        
        # Prueba de normalidad (Shapiro-Wilk)
        shapiro_stat, shapiro_p = stats.shapiro(times[:min(500, sample_size)])
        is_normal = shapiro_p > 0.05
        
        # Análisis de colas (extremos)
        percentile_95 = np.percentile(times, 95)
        percentile_99 = np.percentile(times, 99)
        percentile_999 = np.percentile(times, 99.9)
        
        # Ratio cola/mediana (tail latency)
        median_time = np.median(times)
        tail_ratio_99 = percentile_99 / median_time
        tail_ratio_999 = percentile_999 / median_time
        
        # Criterios de calidad de servicio (SLA)
        sla_violations_95 = sum(1 for t in times if t > THRESHOLDS.MAX_REPORT_GENERATION_MS)
        sla_violation_rate = sla_violations_95 / sample_size
        
        # Análisis de autocorrelación (para detectar patrones temporales)
        if len(times) > 10:
            autocorr_lag1 = np.corrcoef(times[:-1], times[1:])[0, 1]
        else:
            autocorr_lag1 = 0
        
        # Verificaciones de calidad
        assert cv < 0.5, f"Alta variabilidad: CV={cv:.3f}"
        assert tail_ratio_99 < 3.0, f"Colas pesadas (99%): {tail_ratio_99:.2f}x mediana"
        assert sla_violation_rate < 0.05, f"Violaciones SLA: {sla_violation_rate:.1%}"
        assert abs(autocorr_lag1) < 0.3, f"Autocorrelación alta: {autocorr_lag1:.3f}"
        
        return {
            "sample_size": sample_size,
            "mean_ms": mean_time,
            "std_ms": std_time,
            "cv": cv,
            "is_normal": is_normal,
            "shapiro_p": shapiro_p,
            "percentiles": {
                "50": median_time,
                "95": percentile_95,
                "99": percentile_99,
                "99.9": percentile_999
            },
            "tail_ratios": {
                "99": tail_ratio_99,
                "999": tail_ratio_999
            },
            "sla_violation_rate": sla_violation_rate,
            "autocorrelation_lag1": autocorr_lag1,
            "distribution_type": "Normal" if is_normal else "No-normal"
        }
    
    def test_monte_carlo_performance_simulation(self, narrator: TelemetryNarrator):
        """
        Simulación Monte Carlo de rendimiento bajo condiciones aleatorias.
        """
        np.random.seed(42)  # Reproducibilidad
        
        num_simulations = 1000
        performance_results = []
        
        for sim in range(num_simulations):
            # Parámetros aleatorios para cada simulación
            num_spans = np.random.randint(10, 200)
            span_depth = np.random.randint(1, 5)
            error_rate = np.random.beta(2, 5)  # Sesgado hacia pocos errores
            
            # Crear contexto con parámetros aleatorios
            num_errors = int(num_spans * error_rate)
            context = create_test_context(
                num_spans=num_spans,
                span_depth=span_depth,
                num_errors=num_errors
            )
            
            # Medir tiempo
            start = time.perf_counter_ns()
            report = narrator.summarize_execution(context)
            elapsed_ms = (time.perf_counter_ns() - start) / 1_000_000
            
            # Registrar resultados
            performance_results.append({
                "simulation": sim,
                "num_spans": num_spans,
                "span_depth": span_depth,
                "error_rate": error_rate,
                "elapsed_ms": elapsed_ms,
                "verdict": report["verdict_code"]
            })
        
        # Análisis de regresión múltiple
        import pandas as pd
        from sklearn.linear_model import LinearRegression
        
        df = pd.DataFrame(performance_results)
        
        # Variables predictoras
        X = df[['num_spans', 'span_depth', 'error_rate']]
        y = df['elapsed_ms']
        
        # Modelo lineal
        model = LinearRegression()
        model.fit(X, y)
        
        # Coeficientes (impacto de cada variable)
        coefficients = dict(zip(['num_spans', 'span_depth', 'error_rate'], model.coef_))
        r_squared = model.score(X, y)
        
        # Análisis de sensibilidad
        sensitivity = {
            'num_spans': abs(coefficients['num_spans']) * 100 / y.mean(),  # % impacto por unidad
            'span_depth': abs(coefficients['span_depth']) * 10 / y.mean(),  # Normalizado
            'error_rate': abs(coefficients['error_rate']) * 1.0 / y.mean()
        }
        
        # Verificar que número de spans sea el factor dominante
        assert sensitivity['num_spans'] > sensitivity['span_depth'], \
            f"Profundidad más impactante que spans: {sensitivity}"
        
        assert sensitivity['num_spans'] > sensitivity['error_rate'], \
            f"Error rate más impactante que spans: {sensitivity}"
        
        # Modelo debe explicar al menos 80% de varianza
        assert r_squared > 0.8, f"Modelo predictivo pobre: R²={r_squared:.3f}"
        
        return {
            "num_simulations": num_simulations,
            "mean_time_ms": y.mean(),
            "std_time_ms": y.std(),
            "regression_coefficients": coefficients,
            "r_squared": r_squared,
            "sensitivity_analysis": sensitivity,
            "most_influential": max(sensitivity.items(), key=lambda x: x[1])[0],
            "performance_predictor": f"Time ≈ {model.intercept_:.2f} + " +
                                    " + ".join(f"{coef:.3f}*{var}" for var, coef in coefficients.items())
        }



class BoundaryAndEdgeCaseAnalysis:
    """
    Análisis de casos límite y comportamiento en extremos.
    """
    
    def test_narrator_extreme_inputs(self, narrator: TelemetryNarrator):
        """
        Pruebas con entradas extremas para validar robustez.
        """
        test_cases = [
            {
                "name": "empty_context",
                "context": TelemetryContext(),
                "expected_behavior": "empty_report"
            },
            {
                "name": "single_span_no_metrics",
                "context": create_test_context(num_spans=1, num_metrics=0),
                "expected_behavior": "fast_processing"
            },
            {
                "name": "max_spans_practical",
                "context": create_test_context(num_spans=10000, span_depth=1),
                "expected_behavior": "scalable_but_slow"
            },
            {
                "name": "deep_nesting_extreme",
                "context": create_test_context(num_spans=1, span_depth=50),
                "expected_behavior": "handle_recursion"
            },
            {
                "name": "all_spans_failed",
                "context": create_test_context(num_spans=100, num_errors=100),
                "expected_behavior": "aggregate_errors"
            },
            {
                "name": "mixed_status_complex",
                "context": self._create_mixed_status_context(),
                "expected_behavior": "accurate_analysis"
            }
        ]
        
        results = []
        
        for test_case in test_cases:
            context = test_case["context"]
            
            # Medir tiempo y memoria
            with measure_time() as times:
                report = narrator.summarize_execution(context)
            
            with measure_memory() as mem:
                _ = narrator.summarize_execution(context)
            
            # Validar según comportamiento esperado
            if test_case["expected_behavior"] == "empty_report":
                assert report["verdict_code"] in ["APPROVED", "EMPTY"], \
                    f"Empty context unexpected verdict: {report['verdict_code']}"
                assert times[0] < 5.0, f"Empty context too slow: {times[0]:.2f}ms"
            
            elif test_case["expected_behavior"] == "fast_processing":
                assert times[0] < THRESHOLDS.MAX_SINGLE_SPAN_ANALYSIS_MS, \
                    f"Single span too slow: {times[0]:.2f}ms"
            
            elif test_case["expected_behavior"] == "scalable_but_slow":
                # Verificar que no crashea y escala razonablemente
                assert times[0] < 10000, f"10000 spans timeout: {times[0]:.2f}ms"
                assert mem.peak_mb < 100, f"10000 spans memory explosion: {mem.peak_mb:.1f}MB"
            
            elif test_case["expected_behavior"] == "handle_recursion":
                # Profundidad extrema pero sin stack overflow
                assert times[0] < 1000, f"Deep nesting timeout: {times[0]:.2f}ms"
                assert "max_depth" in str(report) or "verdict" in report
            
            elif test_case["expected_behavior"] == "aggregate_errors":
                assert "REJECTED" in report["verdict_code"], \
                    f"All failed but verdict: {report['verdict_code']}"
                assert len(report["forensic_evidence"]) > 0, \
                    "No forensic evidence for all-failed case"
            
            elif test_case["expected_behavior"] == "accurate_analysis":
                # Verificar que mezcla correctamente
                has_success = any(s["status"] == "SUCCESS" 
                                 for s in report.get("span_analysis", []))
                has_failure = any(s["status"] == "FAILURE" 
                                 for s in report.get("span_analysis", []))
                assert has_success and has_failure, "Mixed status not reflected"
            
            results.append({
                "test_case": test_case["name"],
                "time_ms": times[0],
                "memory_mb": mem.peak_mb,
                "verdict": report["verdict_code"],
                "passed": True
            })
        
        return results
    
    def _create_mixed_status_context(self) -> TelemetryContext:
        """Crea contexto con mezcla compleja de estados."""
        context = TelemetryContext()
        
        # Patrón: éxito, warning, fallo cíclico
        patterns = [
            (StepStatus.SUCCESS, "SUCCESS"),
            (StepStatus.WARNING, "WARNING"),
            (StepStatus.FAILURE, "FAILURE"),
            (StepStatus.SUCCESS, "RECOVERY"),
        ]
        
        for i in range(20):
            status, name = patterns[i % len(patterns)]
            with context.span(f"{name}_{i}") as span:
                span.status = status
                if status == StepStatus.FAILURE:
                    span.errors.append({
                        "message": f"Controlled failure {i}",
                        "type": "TestFailure"
                    })
        
        return context
    
    def test_translator_boundary_conditions(self, translator: SemanticTranslator):
        """
        Prueba condiciones límite del translator.
        """
        boundary_cases = [
            {
                "name": "zero_topology",
                "topology": TopologyMetricsDTO(beta_0=0, beta_1=0, euler_characteristic=0),
                "financials": {},
                "stability": 0.0,
                "expect": "empty_or_error"
            },
            {
                "name": "negative_betti",
                "topology": TopologyMetricsDTO(beta_0=-1, beta_1=-1),
                "financials": {"performance": {"recommendation": "ACEPTAR"}},
                "stability": 10.0,
                "expect": "handle_negative"
            },
            {
                "name": "extreme_stability",
                "topology": TopologyMetricsDTO(beta_0=1, beta_1=0),
                "financials": {"performance": {"recommendation": "ACEPTAR"}},
                "stability": 1000000.0,
                "expect": "cap_extremes"
            },
            {
                "name": "nan_metrics",
                "topology": TopologyMetricsDTO(beta_0=float('nan'), beta_1=float('nan')),
                "financials": {"wacc": float('nan'), "performance": {"recommendation": "ACEPTAR"}},
                "stability": float('nan'),
                "expect": "handle_nan"
            },
            {
                "name": "infinite_metrics",
                "topology": TopologyMetricsDTO(beta_0=float('inf'), beta_1=float('inf')),
                "financials": {"wacc": float('inf')},
                "stability": float('inf'),
                "expect": "handle_infinity"
            }
        ]
        
        results = []
        
        for case in boundary_cases:
            try:
                report = translator.compose_strategic_narrative(
                    topological_metrics=case["topology"],
                    financial_metrics=case["financials"],
                    stability=case["stability"]
                )
                
                status = "success"
                verdict = report.verdict.name if hasattr(report, 'verdict') else "unknown"
                
            except Exception as e:
                status = f"exception: {type(e).__name__}"
                verdict = "error"
            
            # Validar según expectativa
            if case["expect"] == "empty_or_error":
                assert status != "success" or verdict in ["VIABLE", "PRECAUCION"], \
                    f"Zero topology unexpected: {status}, {verdict}"
            
            elif case["expect"] == "handle_negative":
                # Debería manejar valores negativos sin crashear
                assert status != "exception: ValueError", "Negative values caused crash"
            
            elif case["expect"] == "cap_extremes":
                assert status == "success", f"Extreme stability crashed: {status}"
                # Debería normalizar valores extremos
            
            elif case["expect"] == "handle_nan":
                # Debería manejar NaN sin propagar
                assert status == "success", f"NaN caused crash: {status}"
            
            elif case["expect"] == "handle_infinity":
                # Debería manejar infinito
                assert status == "success", f"Infinity caused crash: {status}"
            
            results.append({
                "case": case["name"],
                "status": status,
                "verdict": verdict,
                "passed": True
            })
        
        return results


class AdvancedComparativeAnalysis:
    """
    Análisis comparativo avanzado con métricas normalizadas.
    """
    
    def test_normalized_performance_metrics(self, narrator: TelemetryNarrator, translator: SemanticTranslator):
        """
        Métricas de rendimiento normalizadas para comparación justa.
        """
        # Definir unidad de trabajo estándar
        STANDARD_UNIT = {
            "narrator": {"spans": 100, "depth": 2, "metrics": 10},
            "translator": {"topology_complexity": 1.0, "financial_items": 5}
        }
        
        # Medir narrator por unidad de trabajo
        context = create_test_context(
            num_spans=STANDARD_UNIT["narrator"]["spans"],
            span_depth=STANDARD_UNIT["narrator"]["depth"],
            num_metrics=STANDARD_UNIT["narrator"]["metrics"]
        )
        
        narrator_times = []
        for _ in range(50):
            start = time.perf_counter_ns()
            _ = narrator.summarize_execution(context)
            narrator_times.append((time.perf_counter_ns() - start) / 1_000_000)
        
        narrator_perf = {
            "mean_ms": np.mean(narrator_times),
            "std_ms": np.std(narrator_times),
            "throughput_ups": 1000 / np.mean(narrator_times) if np.mean(narrator_times) > 0 else 0,
            "efficiency": STANDARD_UNIT["narrator"]["spans"] / np.mean(narrator_times)
        }
        
        # Medir translator por unidad de trabajo
        translator_times = []
        for _ in range(50):
            start = time.perf_counter_ns()
            _ = translator.compose_strategic_narrative(
                topological_metrics=TopologyMetricsDTO(
                    beta_0=STANDARD_UNIT["translator"]["topology_complexity"],
                    beta_1=0
                ),
                financial_metrics={
                    "wacc": 0.12,
                    "performance": {"recommendation": "ACEPTAR", "profitability_index": 1.3},
                    "contingency": {"recommended": 15000.0}
                },
                stability=10.0
            )
            translator_times.append((time.perf_counter_ns() - start) / 1_000_000)
        
        translator_perf = {
            "mean_ms": np.mean(translator_times),
            "std_ms": np.std(translator_times),
            "throughput_tps": 1000 / np.mean(translator_times) if np.mean(translator_times) > 0 else 0,
            "efficiency": STANDARD_UNIT["translator"]["financial_items"] / np.mean(translator_times)
        }
        
        # Métricas comparativas normalizadas
        comparative_metrics = {
            "speed_ratio": narrator_perf["mean_ms"] / translator_perf["mean_ms"],
            "throughput_ratio": translator_perf["throughput_tps"] / narrator_perf["throughput_ups"],
            "stability_ratio": narrator_perf["std_ms"] / translator_perf["std_ms"],
            "efficiency_ratio": narrator_perf["efficiency"] / translator_perf["efficiency"],
            "complexity_adjusted_score": (
                narrator_perf["throughput_ups"] * STANDARD_UNIT["narrator"]["spans"] /
                translator_perf["throughput_tps"] / STANDARD_UNIT["translator"]["financial_items"]
            )
        }
        
        # Verificaciones de equilibrio del sistema
        assert 0.1 < comparative_metrics["speed_ratio"] < 10.0, \
            f"Desbalance extremo en velocidad: {comparative_metrics['speed_ratio']:.2f}"
        
        assert comparative_metrics["stability_ratio"] < 3.0, \
            f"Desbalance en estabilidad: {comparative_metrics['stability_ratio']:.2f}"
        
        # El sistema debe estar balanceado (ningún módulo es cuello de botella extremo)
        bottleneck_threshold = 0.1
        assert comparative_metrics["complexity_adjusted_score"] > bottleneck_threshold, \
            f"Narrator es cuello de botella: score={comparative_metrics['complexity_adjusted_score']:.3f}"
        
        return {
            "standard_unit": STANDARD_UNIT,
            "narrator_performance": narrator_perf,
            "translator_performance": translator_perf,
            "comparative_metrics": comparative_metrics,
            "system_balance": "Bueno" if comparative_metrics["speed_ratio"] > 0.3 and 
                                      comparative_metrics["speed_ratio"] < 3.0 else "Desbalanceado",
            "bottleneck": "Narrator" if comparative_metrics["complexity_adjusted_score"] < 0.5 else 
                         "Translator" if comparative_metrics["complexity_adjusted_score"] > 2.0 else 
                         "Balanceado"
        }
    
    def test_scalability_comparison_matrix(self, narrator: TelemetryNarrator, translator: SemanticTranslator):
        """
        Matriz de escalabilidad comparativa entre módulos.
        """
        scale_factors = [1, 2, 4, 8, 16]
        
        narrator_scaling = []
        translator_scaling = []
        
        for factor in scale_factors:
            # Narrator scaling
            context = create_test_context(num_spans=100 * factor, span_depth=2)
            
            times = []
            for _ in range(10):
                start = time.perf_counter_ns()
                _ = narrator.summarize_execution(context)
                times.append((time.perf_counter_ns() - start) / 1_000_000)
            
            narrator_scaling.append({
                "factor": factor,
                "time_ms": np.median(times),
                "time_per_unit": np.median(times) / (100 * factor)
            })
            
            # Translator scaling
            times = []
            for _ in range(10):
                start = time.perf_counter_ns()
                _ = translator.compose_strategic_narrative(
                    topological_metrics=TopologyMetricsDTO(beta_0=factor, beta_1=factor//2),
                    financial_metrics={"performance": {"recommendation": "ACEPTAR"}},
                    stability=10.0 * factor
                )
                times.append((time.perf_counter_ns() - start) / 1_000_000)
            
            translator_scaling.append({
                "factor": factor,
                "time_ms": np.median(times),
                "time_per_unit": np.median(times) / factor
            })
        
        # Análisis de escalabilidad relativa
        narrator_scaling_coefficient = self._calculate_scaling_coefficient(
            [s["factor"] for s in narrator_scaling],
            [s["time_ms"] for s in narrator_scaling]
        )
        
        translator_scaling_coefficient = self._calculate_scaling_coefficient(
            [s["factor"] for s in translator_scaling],
            [s["time_ms"] for s in translator_scaling]
        )
        
        # Comparar escalabilidad
        scaling_ratio = narrator_scaling_coefficient / translator_scaling_coefficient
        
        # Ideal: ambos escalan similarmente (ratio cerca de 1)
        assert 0.5 < scaling_ratio < 2.0, \
            f"Escalabilidad muy diferente: narrator O(n^{narrator_scaling_coefficient:.2f}) vs " \
            f"translator O(n^{translator_scaling_coefficient:.2f}), ratio={scaling_ratio:.2f}"
        
        return {
            "scale_factors": scale_factors,
            "narrator_scaling": narrator_scaling,
            "translator_scaling": translator_scaling,
            "narrator_complexity": f"O(n^{narrator_scaling_coefficient:.2f})",
            "translator_complexity": f"O(n^{translator_scaling_coefficient:.2f})",
            "scaling_ratio": scaling_ratio,
            "scaling_compatibility": "Buena" if 0.7 < scaling_ratio < 1.3 else "Moderada"
        }
    
    def _calculate_scaling_coefficient(self, sizes: List[float], times: List[float]) -> float:
        """Calcula coeficiente de escalabilidad usando regresión log-log."""
        log_sizes = np.log(sizes)
        log_times = np.log(times)
        
        slope, _, r_value, _, _ = stats.linregress(log_sizes, log_times)
        
        return slope  # Exponente en O(n^slope)


class HeatAndLoadDistributionAnalysis:
    """
    Análisis de distribución de calor computacional y carga.
    """
    
    def test_computational_heat_map_narrator(self, narrator: TelemetryNarrator):
        """
        Genera mapa de calor computacional del narrator.
        Identifica hotspots y cuellos de botella.
        """
        context = create_test_context(num_spans=100, span_depth=3, num_errors=5)
        
        # Instrumentación fina de fases
        phase_timings = {}
        
        # Fase 1: Recorrido y análisis de spans
        start = time.perf_counter_ns()
        phases = []
        for span in context.root_spans:
            phase_start = time.perf_counter_ns()
            phase = narrator._analyze_phase(span)
            phases.append(phase)
            phase_timings.setdefault("analyze_span", []).append(
                (time.perf_counter_ns() - phase_start) / 1_000_000
            )
        phase_timings["total_analyze_spans"] = (time.perf_counter_ns() - start) / 1_000_000
        
        # Fase 2: Agrupación por estrato
        start = time.perf_counter_ns()
        strata_groups = narrator._group_by_stratum(phases)
        phase_timings["group_by_stratum"] = (time.perf_counter_ns() - start) / 1_000_000
        
        # Fase 3: Análisis por estrato
        stratum_analysis = {}
        for stratum, phase_list in strata_groups.items():
            start = time.perf_counter_ns()
            analysis = narrator._analyze_stratum(stratum, phase_list)
            stratum_analysis[stratum] = analysis
            phase_timings.setdefault(f"analyze_stratum_{stratum.name}", []).append(
                (time.perf_counter_ns() - start) / 1_000_000
            )
        
        # Fase 4: Síntesis final
        start = time.perf_counter_ns()
        final_report = narrator._synthesize_report(stratum_analysis)
        phase_timings["synthesize_report"] = (time.perf_counter_ns() - start) / 1_000_000
        
        # Análisis de distribución
        total_time = sum(
            t[0] if isinstance(t, list) else t 
            for t in phase_timings.values() 
            if (isinstance(t, float) or (isinstance(t, list) and len(t) > 0))
        )
        
        heat_distribution = {}
        for phase, timing in phase_timings.items():
            if isinstance(timing, list) and timing:
                phase_total = np.sum(timing)
            elif isinstance(timing, float):
                phase_total = timing
            else:
                continue
            
            percentage = (phase_total / total_time) * 100
            heat_distribution[phase] = {
                "time_ms": phase_total,
                "percentage": percentage,
                "is_hotspot": percentage > 20.0  # Más del 20% es hotspot
            }
        
        # Identificar cuellos de botella
        hotspots = [phase for phase, data in heat_distribution.items() 
                   if data["is_hotspot"]]
        
        # Verificar que no haya hotspots extremos
        max_percentage = max(data["percentage"] for data in heat_distribution.values())
        assert max_percentage < 50.0, f"Hotspot extremo: {max_percentage:.1f}% en {hotspots}"
        
        # Verificar distribución balanceada
        balance_score = np.std(list(data["percentage"] for data in heat_distribution.values()))
        assert balance_score < 15.0, f"Distribución desbalanceada: σ={balance_score:.1f}"
        
        return {
            "total_time_ms": total_time,
            "heat_distribution": heat_distribution,
            "hotspots": hotspots,
            "balance_score": balance_score,
            "recommendations": self._generate_optimization_recommendations(heat_distribution)
        }
    
    def _generate_optimization_recommendations(self, heat_distribution: Dict) -> List[str]:
        """Genera recomendaciones basadas en mapa de calor."""
        recommendations = []
        
        # Ordenar por porcentaje descendente
        sorted_phases = sorted(
            heat_distribution.items(),
            key=lambda x: x[1]["percentage"],
            reverse=True
        )
        
        for phase, data in sorted_phases[:3]:  # Top 3 hotspots
            if data["percentage"] > 30:
                recommendations.append(f"OPTIMIZAR CRÍTICO: {phase} ({data['percentage']:.1f}%)")
            elif data["percentage"] > 15:
                recommendations.append(f"Considerar optimizar: {phase} ({data['percentage']:.1f}%)")
        
        if not recommendations:
            recommendations.append("Distribución balanceada, sin optimizaciones críticas")
        
        return recommendations
    
    def test_load_distribution_concurrent(self, narrator: TelemetryNarrator, translator: SemanticTranslator):
        """
        Analiza distribución de carga en ejecución concurrente.
        """
        from concurrent.futures import ProcessPoolExecutor, as_completed
        import multiprocessing as mp
        
        cpu_count = mp.cpu_count()
        batch_sizes = [1, 2, 4, 8, cpu_count]
        
        results = []
        
        for batch_size in batch_sizes:
            # Crear lotes de trabajo
            num_batches = 100
            batch_load = 50  # Spans por batch
            
            # Medir tiempo secuencial (baseline)
            sequential_times = []
            for i in range(num_batches):
                context = create_test_context(num_spans=batch_load, span_depth=2)
                start = time.perf_counter_ns()
                _ = narrator.summarize_execution(context)
                sequential_times.append((time.perf_counter_ns() - start) / 1_000_000)
            
            sequential_total = np.sum(sequential_times)
            
            # Medir tiempo paralelo
            def process_batch(batch_id):
                context = create_test_context(num_spans=batch_load, span_depth=2)
                start = time.perf_counter_ns()
                result = narrator.summarize_execution(context)
                return (time.perf_counter_ns() - start) / 1_000_000, batch_id
            
            parallel_times = []
            with ProcessPoolExecutor(max_workers=batch_size) as executor:
                futures = [executor.submit(process_batch, i) for i in range(num_batches)]
                for future in as_completed(futures):
                    batch_time, batch_id = future.result()
                    parallel_times.append(batch_time)
            
            parallel_total = np.max(parallel_times) * (num_batches / batch_size)  # Tiempo teórico
            
            # Calcular speedup y eficiencia
            speedup = sequential_total / parallel_total if parallel_total > 0 else 1
            efficiency = (speedup / batch_size) * 100
            
            # Ley de Amdahl: speedup máximo teórico
            parallel_fraction = 0.8  # Estimación
            amdahl_speedup = 1 / ((1 - parallel_fraction) + (parallel_fraction / batch_size))
            
            results.append({
                "batch_size": batch_size,
                "sequential_ms": sequential_total,
                "parallel_ms": parallel_total,
                "speedup": speedup,
                "efficiency_percent": efficiency,
                "amdahl_theoretical": amdahl_speedup,
                "achieved_vs_theoretical": speedup / amdahl_speedup if amdahl_speedup > 0 else 0
            })
        
        # Análisis de escalabilidad paralela
        speedups = [r["speedup"] for r in results]
        efficiencies = [r["efficiency_percent"] for r in results]
        
        # Verificar que speedup mejora (aunque no perfectamente)
        assert speedups[-1] > speedups[0] * 0.5, \
            f"Poco speedup al escalar: {speedups[0]:.2f} -> {speedups[-1]:.2f}"
        
        # Eficiencia no debe caer demasiado rápido
        efficiency_drop = (efficiencies[0] - efficiencies[-1]) / efficiencies[0]
        assert efficiency_drop < 0.8, f"Eficiencia cae demasiado: {efficiency_drop:.1%}"
        
        return {
            "cpu_count": cpu_count,
            "results": results,
            "scalability_summary": {
                "max_speedup": max(speedups),
                "min_efficiency": min(efficiencies),
                "optimal_batch_size": batch_sizes[np.argmax(speedups)],
                "parallel_scalability": "Buena" if speedups[-1] > cpu_count * 0.5 else "Moderada"
            }
        }