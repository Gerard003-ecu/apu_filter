# ============================================================================
# CONTROLADOR PI DISCRETO - M√âTODOS REFINADOS
# ============================================================================
class PIController:
    """
    Controlador PI Discreto con:
    1. Filtro de media m√≥vil exponencial para estabilizaci√≥n
    2. Anti-windup con back-calculation mejorado
    3. An√°lisis de estabilidad basado en Lyapunov discreto mejorado
    4. Ganancia integral adaptativa para evitar windup en r√©gimen transitorio
    """

    _MAX_HISTORY_SIZE: int = 100

    def __init__(
        self,
        kp: float,
        ki: float,
        setpoint: float,
        min_output: int,
        max_output: int,
        integral_limit_factor: float = 2.0,
    ):
        self._validate_control_parameters(kp, ki, setpoint, min_output, max_output)

        self.Kp = float(kp)
        self.Ki = float(ki)
        self.setpoint = float(setpoint)
        self.min_output = int(min_output)
        self.max_output = int(max_output)

        # Espacio de salida normalizado
        self._output_range = max(1, self.max_output - self.min_output)
        self._output_center = (self.max_output + self.min_output) / 2.0

        # Anti-windup: l√≠mite integral basado en rango de salida
        self._integral_limit = self._output_range * max(0.1, integral_limit_factor)
        self._integral_error: float = 0.0

        # Filtro EMA (Exponential Moving Average) para suavizado
        self._ema_alpha: float = 0.3  # Factor de suavizado
        self._filtered_pv: Optional[float] = None

        # Estado temporal
        self._last_time: Optional[float] = None
        self._last_error: Optional[float] = None
        self._last_output: Optional[int] = None
        self._iteration_count: int = 0

        # Historial para an√°lisis de estabilidad
        self._error_history: deque = deque(maxlen=self._MAX_HISTORY_SIZE)
        self._output_history: deque = deque(maxlen=self._MAX_HISTORY_SIZE)

        # M√©tricas de estabilidad de Lyapunov
        self._lyapunov_sum: float = 0.0
        self._lyapunov_count: int = 0
        
        # Adaptaci√≥n de ganancia integral
        self._ki_adaptive: float = ki
        self._windup_detection_window: deque = deque(maxlen=5)

    def _validate_control_parameters(
        self, kp: float, ki: float, setpoint: float,
        min_output: int, max_output: int
    ) -> None:
        """Validaci√≥n de par√°metros con criterios de estabilidad mejorados."""
        errors = []

        if kp <= 0:
            errors.append(f"Kp debe ser positivo para respuesta proporcional, got {kp}")
        if ki < 0:
            errors.append(f"Ki debe ser no-negativo, got {ki}")
        if min_output >= max_output:
            errors.append(
                f"Rango de salida inv√°lido: [{min_output}, {max_output}]"
            )
        if min_output <= 0:
            errors.append(f"min_output debe ser positivo, got {min_output}")
        if not (0.0 < setpoint < 1.0):
            errors.append(f"setpoint debe estar en (0, 1), got {setpoint}")

        # Criterio de estabilidad mejorado para PI discreto:
        # Para sistema de primer orden con tiempo de muestreo T=1, condiciones de estabilidad:
        # 0 < Kp < 2/|G| y 0 < Ki < Kp donde |G| es ganancia del proceso (~1)
        if kp > 2.0:
            logger.warning(f"Kp={kp} puede causar inestabilidad (recomendado < 2.0)")
        
        if ki > kp:
            logger.warning(f"Ki/Kp ratio={ki/kp:.2f} > 1 puede causar oscilaciones")

        if errors:
            raise ConfigurationError(
                "Errores en par√°metros de control:\n" +
                "\n".join(f"  ‚Ä¢ {e}" for e in errors)
            )

    def _apply_ema_filter(self, measurement: float) -> float:
        """Filtro de media m√≥vil exponencial con adaptaci√≥n de alpha."""
        if self._filtered_pv is None:
            self._filtered_pv = measurement
            return measurement
            
        # Adaptar alpha basado en la varianza reciente del error
        if len(self._error_history) > 1:
            recent_errors = list(self._error_history)[-5:]
            error_variance = np.var(recent_errors) if np else sum((e - sum(recent_errors)/len(recent_errors))**2 for e in recent_errors)/len(recent_errors)
            # Alpha m√°s bajo para ruido alto, m√°s alto para se√±al limpia
            adaptive_alpha = max(0.1, min(0.5, 0.3/(1 + error_variance)))
            self._ema_alpha = adaptive_alpha
            
        self._filtered_pv = (
            self._ema_alpha * measurement +
            (1 - self._ema_alpha) * self._filtered_pv
        )
        return self._filtered_pv

    def _update_lyapunov_metric(self, error: float) -> None:
        """
        Actualiza m√©trica de estabilidad de Lyapunov mejorada.
        
        Usa funci√≥n de Lyapunov V(e) = e¬≤ y calcula exponente de Lyapunov
        mediante diferencia finita logar√≠tmica.
        """
        if self._last_error is not None:
            # Para evitar divisi√≥n por cero
            if abs(self._last_error) < 1e-10:
                return
                
            # Tasa de cambio logar√≠tmica (exponente de Lyapunov aproximado)
            lyapunov_rate = math.log(abs(error) + 1e-10) - math.log(abs(self._last_error) + 1e-10)
            
            # Filtrado EMA del exponente
            self._lyapunov_sum += lyapunov_rate
            self._lyapunov_count += 1
            
            # Detecci√≥n de inestabilidad temprana
            if self._lyapunov_count > 10:
                avg_exponent = self._lyapunov_sum / self._lyapunov_count
                if avg_exponent > 0.1:  # Exponente positivo indica inestabilidad
                    logger.warning(f"Posible inestabilidad detectada: Œª ‚âà {avg_exponent:.3f}")

    def _adapt_integral_gain(self, error: float, output_saturated: bool) -> None:
        """Adapta la ganancia integral para prevenir windup."""
        self._windup_detection_window.append((error, output_saturated))
        
        if len(self._windup_detection_window) < 3:
            return
            
        # Detectar windup: error constante con saturaci√≥n
        recent_errors = [e for e, _ in self._windup_detection_window]
        saturated_count = sum(1 for _, s in self._windup_detection_window if s)
        
        error_std = np.std(recent_errors) if np else math.sqrt(sum((e - sum(recent_errors)/len(recent_errors))**2 for e in recent_errors)/len(recent_errors))
        
        # Condiciones para windup: baja variaci√≥n en error con saturaci√≥n frecuente
        if error_std < 0.05 and saturated_count >= 2:
            self._ki_adaptive = self.Ki * 0.5  # Reducir Ki temporalmente
            logger.debug("Windup detectado: reduciendo Ki adaptativamente")
        else:
            self._ki_adaptive = self.Ki  # Restaurar Ki nominal

    def compute(self, process_variable: float) -> int:
        """
        Calcula salida de control PI con anti-windup mejorado.
        """
        self._iteration_count += 1
        current_time = time.time()

        # Filtrado de se√±al adaptativo
        filtered_pv = self._apply_ema_filter(
            max(0.0, min(1.0, process_variable))
        )

        # C√°lculo de error
        error = self.setpoint - filtered_pv
        self._error_history.append(error)

        # C√°lculo de delta tiempo con l√≠mites
        if self._last_time is None:
            dt = SystemConstants.MIN_DELTA_TIME
        else:
            dt = max(
                SystemConstants.MIN_DELTA_TIME,
                min(current_time - self._last_time, SystemConstants.MAX_DELTA_TIME)
            )

        # T√©rmino proporcional
        P = self.Kp * error

        # Adaptaci√≥n de ganancia integral
        output_saturated_precheck = False
        # Pre-c√°lculo para detecci√≥n temprana de saturaci√≥n
        integral_increment = error * dt
        proposed_integral = self._integral_error + integral_increment
        I_proposed = self._ki_adaptive * proposed_integral
        output_unbounded_pre = self._output_center + P + I_proposed
        if output_unbounded_pre > self.max_output or output_unbounded_pre < self.min_output:
            output_saturated_precheck = True
            
        self._adapt_integral_gain(error, output_saturated_precheck)

        # T√©rmino integral con anti-windup mejorado
        integral_increment = error * dt
        proposed_integral = self._integral_error + integral_increment

        # Back-calculation mejorado
        I_proposed = self._ki_adaptive * proposed_integral
        output_unbounded = self._output_center + P + I_proposed

        # Anti-windup con retroalimentaci√≥n de saturaci√≥n
        saturation_error = 0.0
        if output_unbounded > self.max_output:
            saturation_error = output_unbounded - self.max_output
            # Factor de back-calculation ajustado
            back_calc_factor = 0.8
            self._integral_error = proposed_integral - (back_calc_factor * saturation_error / max(self._ki_adaptive, 1e-6))
        elif output_unbounded < self.min_output:
            saturation_error = self.min_output - output_unbounded
            back_calc_factor = 0.8
            self._integral_error = proposed_integral + (back_calc_factor * saturation_error / max(self._ki_adaptive, 1e-6))
        else:
            self._integral_error = proposed_integral

        # Aplicar l√≠mite absoluto de integral con suavizado
        if abs(self._integral_error) > self._integral_limit:
            # Suavizar la limitaci√≥n para evitar discontinuidades
            limit_factor = self._integral_limit / abs(self._integral_error)
            self._integral_error *= limit_factor

        # C√°lculo final de salida
        I = self._ki_adaptive * self._integral_error
        output_raw = self._output_center + P + I
        
        # Suavizado de salida para cambios bruscos
        if self._last_output is not None:
            max_change = self._output_range * 0.1  # M√°ximo 10% de cambio por iteraci√≥n
            output_raw = self._last_output + max(-max_change, min(max_change, output_raw - self._last_output))
        
        output = int(round(max(self.min_output, min(self.max_output, output_raw))))

        # Actualizar m√©tricas de estabilidad
        self._update_lyapunov_metric(error)

        # Guardar estado
        self._last_time = current_time
        self._last_error = error
        self._last_output = output
        self._output_history.append(output)

        return output


# ============================================================================
# MOTOR DE F√çSICA - M√âTODOS REFINADOS
# ============================================================================
class FluxPhysicsEngine:
    """
    Motor de f√≠sica RLC mejorado con:
    1. Integraci√≥n num√©rica m√°s estable (Runge-Kutta de 2do orden)
    2. C√°lculo de n√∫meros de Betti corregido para grafos
    3. Entrop√≠a termodin√°mica con fundamentaci√≥n estad√≠stica rigurosa
    4. Modelo de amortiguamiento no lineal para alta saturaci√≥n
    """

    _MAX_METRICS_HISTORY: int = 100

    def __init__(self, capacitance: float, resistance: float, inductance: float):
        self.logger = logging.getLogger(f"{__name__}.{self.__class__.__name__}")

        self._validate_physical_parameters(capacitance, resistance, inductance)

        self.C = float(capacitance)
        self.R = float(resistance)
        self.L = float(inductance)

        # Par√°metros derivados del circuito RLC
        self._omega_0 = 1.0 / math.sqrt(self.L * self.C)  # Frecuencia natural
        self._alpha = self.R / (2.0 * self.L)  # Factor de amortiguamiento
        self._zeta = self._alpha / self._omega_0  # Ratio de amortiguamiento
        self._Q = math.sqrt(self.L / self.C) / self.R if self.R > 0 else float('inf')

        # Clasificaci√≥n del sistema
        self._update_damping_classification()

        # Estado del sistema: [carga Q, corriente I]
        self._state = [0.0, 0.0]
        self._state_history: deque = deque(maxlen=self._MAX_METRICS_HISTORY)

        # Grafo de conectividad para an√°lisis topol√≥gico
        self._adjacency_list: Dict[int, Set[int]] = {}
        self._vertex_count: int = 0
        self._edge_count: int = 0

        # Historial de m√©tricas
        self._metrics_history: deque = deque(maxlen=self._MAX_METRICS_HISTORY)
        self._entropy_history: deque = deque(maxlen=self._MAX_METRICS_HISTORY)

        # Estado temporal
        self._last_current: float = 0.0
        self._ema_current: float = 0.0
        self._last_time: float = time.time()
        self._initialized: bool = False
        
        # Amortiguamiento no lineal
        self._nonlinear_damping_factor: float = 1.0

    def _update_damping_classification(self) -> None:
        """Actualiza clasificaci√≥n de amortiguamiento del sistema."""
        if self._zeta > 1.0:
            self._damping_type = "OVERDAMPED"
            self._omega_d = self._omega_0 * math.sqrt(self._zeta**2 - 1)
        elif self._zeta < 1.0:
            self._damping_type = "UNDERDAMPED"
            self._omega_d = self._omega_0 * math.sqrt(1 - self._zeta**2)
        else:
            self._damping_type = "CRITICALLY_DAMPED"
            self._omega_d = 0.0

    def _evolve_state_rk2(self, driving_current: float, dt: float) -> Tuple[float, float]:
        """
        Evoluciona el estado del sistema RLC usando Runge-Kutta de 2do orden.
        
        Sistema de ecuaciones:
        dQ/dt = I
        dI/dt = (V - R*I - Q/C) / L
        donde V es proporcional a driving_current
        """
        Q, I = self._state
        
        # Convertir driving_current a voltaje de entrada (normalizado)
        V_in = driving_current * 10.0  # Factor de escala
        
        # Funci√≥n derivada
        def derivatives(q, i):
            dq_dt = i
            di_dt = (V_in - self.R * i - q / self.C) / self.L
            return dq_dt, di_dt
        
        # Runge-Kutta de 2do orden (m√©todo del punto medio)
        k1_q, k1_i = derivatives(Q, I)
        k2_q, k2_i = derivatives(Q + 0.5 * dt * k1_q, I + 0.5 * dt * k1_i)
        
        Q_new = Q + dt * k2_q
        I_new = I + dt * k2_i
        
        # Aplicar amortiguamiento no lineal para alta energ√≠a
        current_energy = 0.5 * self.L * I_new**2 + 0.5 * self.C * (Q_new/self.C)**2
        if current_energy > 10.0:  # Umbral de energ√≠a
            damping_factor = 1.0 / (1.0 + 0.1 * current_energy)
            I_new *= damping_factor
            self._nonlinear_damping_factor = damping_factor
        
        self._state = [Q_new, I_new]
        self._state_history.append({
            'Q': Q_new,
            'I': I_new,
            'time': time.time(),
            'energy': current_energy
        })

        return Q_new, I_new

    def _build_metric_graph(self, metrics: Dict[str, float]) -> None:
        """
        Construye grafo de correlaci√≥n entre m√©tricas usando umbral adaptativo
        basado en percentiles.
        """
        # Seleccionar m√©tricas clave para el grafo
        metric_keys = ['saturation', 'complexity', 'current_I', 'potential_energy', 
                      'kinetic_energy', 'entropy_shannon']
        values = [metrics.get(k, 0.0) for k in metric_keys]

        self._adjacency_list.clear()
        self._vertex_count = len(values)
        self._edge_count = 0

        # Inicializar listas de adyacencia
        for i in range(self._vertex_count):
            self._adjacency_list[i] = set()

        # Crear aristas basadas en correlaci√≥n estad√≠stica
        if len(values) > 1:
            # Usar diferencia de percentiles como umbral adaptativo
            sorted_vals = sorted(values)
            if len(sorted_vals) >= 4:
                q1 = sorted_vals[len(sorted_vals)//4]
                q3 = sorted_vals[3*len(sorted_vals)//4]
                iqr = q3 - q1
                threshold = max(0.1, iqr * 0.5)  # 50% del IQR, m√≠nimo 0.1
            else:
                value_range = max(values) - min(values) if max(values) != min(values) else 1.0
                threshold = 0.3 * value_range

            for i in range(len(values)):
                for j in range(i + 1, len(values)):
                    correlation = 1.0 - abs(values[i] - values[j]) / (abs(values[i]) + abs(values[j]) + 1e-10)
                    if correlation > 0.7:  # Correlaci√≥n fuerte
                        self._adjacency_list[i].add(j)
                        self._adjacency_list[j].add(i)
                        self._edge_count += 1

    def _calculate_betti_numbers(self) -> Dict[int, int]:
        """
        Calcula n√∫meros de Betti para el grafo de m√©tricas.
        
        Para un grafo simple:
        - Œ≤‚ÇÄ = n√∫mero de componentes conexas
        - Œ≤‚ÇÅ = n√∫mero de ciclos independientes = E - V + Œ≤‚ÇÄ
        """
        if self._vertex_count == 0:
            return {0: 0, 1: 0}

        # Calcular componentes conexas con DFS iterativo
        visited = [False] * self._vertex_count
        components = 0
        
        for v in range(self._vertex_count):
            if not visited[v]:
                components += 1
                stack = [v]
                while stack:
                    current = stack.pop()
                    if not visited[current]:
                        visited[current] = True
                        for neighbor in self._adjacency_list.get(current, set()):
                            if not visited[neighbor]:
                                stack.append(neighbor)

        beta_0 = components
        
        # Œ≤‚ÇÅ = E - V + Œ≤‚ÇÄ (caracter√≠stica de Euler-Poincar√© para grafos)
        beta_1 = max(0, self._edge_count - self._vertex_count + beta_0)
        
        # Para grafos, Œ≤_k = 0 para k ‚â• 2
        return {0: beta_0, 1: beta_1, 2: 0}

    def calculate_system_entropy(
        self,
        total_records: int,
        error_count: int,
        processing_time: float
    ) -> Dict[str, float]:
        """
        Calcula entrop√≠a del sistema con fundamentaci√≥n estad√≠stica rigurosa.
        
        1. Entrop√≠a de Shannon con correcci√≥n de peque√±as muestras
        2. Entrop√≠a de Tsallis para no-extensividad
        3. Entrop√≠a relativa (Kullback-Leibler) respecto a distribuci√≥n uniforme
        """
        if total_records <= 0:
            return self._get_zero_entropy()

        success_count = total_records - error_count
        
        # Probabilidades
        p_success = success_count / total_records
        p_error = error_count / total_records
        
        # Entrop√≠a de Shannon con correcci√≥n de Miller-Madow para bias en peque√±as muestras
        shannon_entropy = 0.0
        if p_success > 0:
            shannon_entropy -= p_success * math.log2(p_success)
        if p_error > 0:
            shannon_entropy -= p_error * math.log2(p_error)
            
        # Correcci√≥n de Miller-Madow
        m = 2  # N√∫mero de bins (√©xito/error)
        shannon_entropy_corrected = shannon_entropy + (m - 1) / (2 * total_records * math.log(2))
        
        # Entrop√≠a de Tsallis (q-entrop√≠a) para capturar no-extensividad
        q = 1.5  # Par√°metro de no-extensividad
        tsallis_entropy = 0.0
        if q != 1:
            for p in [p_success, p_error]:
                if p > 0:
                    tsallis_entropy += p**q
            tsallis_entropy = (1 - tsallis_entropy) / (q - 1)
        else:
            tsallis_entropy = shannon_entropy / math.log(2)  # Convertir a nats para consistencia
        
        # Entrop√≠a relativa (divergencia KL) respecto a distribuci√≥n uniforme
        uniform_p = 0.5
        kl_divergence = 0.0
        for p in [p_success, p_error]:
            if p > 0:
                kl_divergence += p * math.log2(p / uniform_p)
                
        # Producci√≥n de entrop√≠a por unidad de tiempo
        entropy_rate = shannon_entropy / max(processing_time, 0.001)
        
        # Diagn√≥stico de estado termodin√°mico
        max_shannon = 1.0  # M√°ximo para distribuci√≥n binomial
        entropy_ratio = shannon_entropy / max_shannon
        
        # Ley de enfriamiento de Newton aplicada a entrop√≠a
        entropy_decay_time = 0.0
        if entropy_rate > 0:
            entropy_decay_time = shannon_entropy / entropy_rate

        result = {
            "shannon_entropy": shannon_entropy,
            "shannon_entropy_corrected": shannon_entropy_corrected,
            "tsallis_entropy": tsallis_entropy,
            "kl_divergence": kl_divergence,
            "entropy_rate": entropy_rate,
            "entropy_decay_time": entropy_decay_time,
            "max_entropy": max_shannon,
            "entropy_ratio": entropy_ratio,
            "is_thermal_death": entropy_ratio > 0.8
        }

        self._entropy_history.append({
            **result,
            'timestamp': time.time(),
            'total_records': total_records,
            'error_rate': error_count / total_records if total_records > 0 else 0
        })

        return result

    def calculate_gyroscopic_stability(self, current_I: float) -> float:
        """
        Calcula la Estabilidad Girosc√≥pica (Sg) con modelo de precesi√≥n.
        
        Modelo: Sg = exp(-Œ∫ * |dI/dt|) * cos(Œ∏)
        donde Œ∫ es sensibilidad y Œ∏ es √°ngulo de precesi√≥n
        """
        if not self._initialized:
            self._ema_current = current_I
            self._initialized = True
            return 1.0
            
        # Calcular derivada de la corriente
        current_time = time.time()
        dt = max(1e-6, current_time - self._last_time)
        dI_dt = (current_I - self._last_current) / dt
        
        # Actualizar EMA del eje de rotaci√≥n
        alpha = SystemConstants.GYRO_EMA_ALPHA
        self._ema_current = alpha * current_I + (1 - alpha) * self._ema_current
        
        # Calcular √°ngulo de precesi√≥n (desviaci√≥n del eje)
        precession_angle = math.atan2(current_I - self._ema_current, 1.0)
        
        # Calcular estabilidad girosc√≥pica
        sensitivity = SystemConstants.GYRO_SENSITIVITY
        stability = math.exp(-sensitivity * abs(dI_dt)) * math.cos(precession_angle)
        
        # Normalizar a [0, 1]
        stability_normalized = (stability + 1) / 2.0
        
        return max(0.0, min(1.0, stability_normalized))


# ============================================================================
# DATA FLUX CONDENSER - M√âTODOS REFINADOS
# ============================================================================
class DataFluxCondenser:
    """
    Orquesta el pipeline de validaci√≥n y procesamiento con control adaptativo mejorado.
    """

    def _process_batches_with_pid(
        self,
        raw_records: List,
        cache: Dict,
        total_records: int,
        on_progress: Optional[Callable],
        progress_callback: Optional[Callable],
        telemetry: Optional[TelemetryContext],
    ) -> List[pd.DataFrame]:
        """
        Procesa lotes con control PID adaptativo mejorado.
        
        Mejoras:
        1. Estimaci√≥n de cache_hits m√°s precisa usando cach√© real
        2. Control de frecuencia de batch adaptativo
        3. Predicci√≥n de saturaci√≥n para pre-ajuste
        """
        processed_batches = []
        failed_batches_count = 0
        current_index = 0
        current_batch_size = self.condenser_config.min_batch_size
        iteration = 0
        max_iterations = total_records * SystemConstants.MAX_ITERATIONS_MULTIPLIER
        
        # Historial para predicci√≥n
        saturation_history = []
        batch_size_history = []

        while current_index < total_records and iteration < max_iterations:
            iteration += 1

            # Determinar rango del batch con predicci√≥n
            end_index = min(current_index + current_batch_size, total_records)
            batch = raw_records[current_index:end_index]
            batch_size = len(batch)

            if batch_size == 0:
                break

            # Calcular tiempo transcurrido
            elapsed_time = time.time() - self._start_time

            # Verificar timeout global con margen de seguridad
            time_remaining = SystemConstants.PROCESSING_TIMEOUT - elapsed_time
            if time_remaining < 0:
                self.logger.error("Timeout de procesamiento alcanzado")
                break
            elif time_remaining < 60:  # √öltimo minuto
                self.logger.warning(f"Quedan {time_remaining:.0f}s para timeout")

            # Estimaci√≥n realista de cache_hits basada en contenido del batch
            cache_hits_est = self._estimate_cache_hits(batch, cache)
            
            # Calcular m√©tricas f√≠sicas con amortiguamiento no lineal
            metrics = self.physics.calculate_metrics(
                total_records=batch_size,
                cache_hits=cache_hits_est,
                error_count=failed_batches_count,
                processing_time=elapsed_time
            )

            # Predicci√≥n de saturaci√≥n para pr√≥xima iteraci√≥n
            if len(saturation_history) >= 2:
                predicted_saturation = self._predict_next_saturation(saturation_history)
                # Pre-ajustar controlador si se predice saturaci√≥n alta
                if predicted_saturation > 0.8:
                    current_batch_size = max(SystemConstants.MIN_BATCH_SIZE_FLOOR,
                                           int(current_batch_size * 0.8))

            saturation_history.append(metrics.get("saturation", 0.5))
            batch_size_history.append(batch_size)

            # Callback de m√©tricas con diagn√≥stico
            if progress_callback:
                try:
                    enhanced_metrics = {**metrics, "predicted_saturation": predicted_saturation if 'predicted_saturation' in locals() else None}
                    progress_callback(enhanced_metrics)
                except Exception as e:
                    self.logger.warning(f"Error en progress_callback: {e}")

            # Control PID con l√≠mite din√°mico basado en estabilidad girosc√≥pica
            saturation = metrics.get("saturation", 0.5)
            gyro_stability = metrics.get("gyroscopic_stability", 1.0)
            
            # Reducir agresividad del control si baja estabilidad girosc√≥pica
            if gyro_stability < 0.5:
                effective_saturation = min(saturation, 0.7)  # Limitar entrada al controlador
                self.logger.debug(f"Baja estabilidad girosc√≥pica ({gyro_stability:.2f}), limitando saturaci√≥n")
            else:
                effective_saturation = saturation
                
            pid_output = self.controller.compute(effective_saturation)

            # Freno de emergencia adaptativo
            power = metrics.get("dissipated_power", 0)
            flyback = metrics.get("flyback_voltage", 0)
            
            emergency_brake = False
            if power > SystemConstants.OVERHEAT_POWER_THRESHOLD:
                brake_factor = min(0.3, power / (2 * SystemConstants.OVERHEAT_POWER_THRESHOLD))
                pid_output = max(SystemConstants.MIN_BATCH_SIZE_FLOOR,
                               int(pid_output * brake_factor))
                emergency_brake = True
                self.logger.warning(f"üî• OVERHEAT: P={power:.2f}W, aplicando freno {brake_factor:.1%}")
                
            if flyback > SystemConstants.MAX_FLYBACK_VOLTAGE * 0.8:
                pid_output = max(SystemConstants.MIN_BATCH_SIZE_FLOOR,
                               int(pid_output * 0.6))
                emergency_brake = True
                self.logger.warning(f"‚ö° HIGH FLYBACK: V={flyback:.2f}V")

            if emergency_brake:
                self._emergency_brake_count += 1
                self._stats.emergency_brakes_triggered += 1

            # Procesar batch con recuperaci√≥n inteligente
            result = self._process_single_batch_with_recovery(batch, cache, failed_batches_count)

            if result.success and result.dataframe is not None:
                processed_batches.append(result.dataframe)
                self._stats.add_batch_stats(
                    batch_size=result.records_processed,
                    saturation=saturation,
                    power=power,
                    flyback=flyback,
                    kinetic=metrics.get("kinetic_energy", 0),
                    success=True
                )
                # Reset de contador de fallos consecutivos
                failed_batches_count = max(0, failed_batches_count - 1)
            else:
                failed_batches_count += 1
                self._stats.add_batch_stats(
                    batch_size=batch_size,
                    saturation=saturation,
                    power=power,
                    flyback=flyback,
                    kinetic=metrics.get("kinetic_energy", 0),
                    success=False
                )
                
                error_msg = result.error_message[:100] + "..." if len(result.error_message) > 100 else result.error_message
                self.logger.error(f"Error procesando batch {iteration}: {error_msg}")
                
                # Estrategia de recuperaci√≥n adaptativa
                if failed_batches_count >= self.condenser_config.max_failed_batches:
                    if not self.condenser_config.enable_partial_recovery:
                        raise ProcessingError(
                            f"L√≠mite de batches fallidos alcanzado: {failed_batches_count}"
                        )
                    else:
                        # Reducir tama√±o de batch dr√°sticamente para recuperaci√≥n
                        pid_output = SystemConstants.MIN_BATCH_SIZE_FLOOR
                        self.logger.warning(
                            f"Recuperaci√≥n parcial activada, reduciendo batch size a {pid_output}"
                        )

            # Callback de progreso con diagn√≥stico
            if on_progress:
                try:
                    enhanced_stats = self._enhance_stats_with_diagnostics(self._stats, metrics)
                    on_progress(enhanced_stats)
                except Exception as e:
                    self.logger.warning(f"Error en on_progress: {e}")

            # Telemetr√≠a de batch con m√©tricas extendidas
            if telemetry and (iteration % 10 == 0 or emergency_brake):
                telemetry.record_event("batch_processed", {
                    "iteration": iteration,
                    "progress": current_index / total_records,
                    "batch_size": batch_size,
                    "saturation": saturation,
                    "gyroscopic_stability": gyro_stability,
                    "emergency_brake": emergency_brake,
                    "failed_batches_consecutive": failed_batches_count
                })

            # Avanzar √≠ndice y actualizar tama√±o de batch con inercia
            current_index = end_index
            
            # Suavizar cambios en batch size
            if iteration > 1:
                inertia_factor = 0.7  # Conservar 70% del tama√±o anterior
                current_batch_size = int(inertia_factor * current_batch_size + 
                                       (1 - inertia_factor) * pid_output)
            else:
                current_batch_size = pid_output
                
            current_batch_size = max(SystemConstants.MIN_BATCH_SIZE_FLOOR,
                                   min(current_batch_size, self.condenser_config.max_batch_size))

        return processed_batches

    def _estimate_cache_hits(self, batch: List, cache: Dict) -> int:
        """
        Estima cache_hits basado en similitud con cach√© existente.
        """
        if not cache or not batch:
            return len(batch) // 2  # Estimaci√≥n conservadora
            
        # M√©trica simple: porcentaje de registros con campos en cach√©
        cache_fields = set(cache.keys())
        hits = 0
        
        for record in batch[:100]:  # Muestra de 100 registros para eficiencia
            if isinstance(record, dict):
                record_fields = set(record.keys())
                if record_fields & cache_fields:  # Intersecci√≥n no vac√≠a
                    hits += 1
                    
        # Extrapolar a todo el batch
        if len(batch) > 100:
            hit_rate = hits / 100
            hits = int(hit_rate * len(batch))
            
        return hits

    def _predict_next_saturation(self, history: List[float]) -> float:
        """
        Predice siguiente valor de saturaci√≥n usando regresi√≥n lineal simple.
        """
        if len(history) < 3:
            return history[-1] if history else 0.5
            
        # Regresi√≥n lineal en las √∫ltimas N muestras
        n = min(5, len(history))
        x = list(range(n))
        y = history[-n:]
        
        # Calcular pendiente
        sum_x = sum(x)
        sum_y = sum(y)
        sum_xy = sum(x[i] * y[i] for i in range(n))
        sum_x2 = sum(x_i * x_i for x_i in x)
        
        if n * sum_x2 - sum_x * sum_x == 0:
            return y[-1]
            
        slope = (n * sum_xy - sum_x * sum_y) / (n * sum_x2 - sum_x * sum_x)
        
        # Predecir pr√≥ximo valor
        prediction = y[-1] + slope
        
        return max(0.0, min(1.0, prediction))

    def _process_single_batch_with_recovery(self, batch: List, cache: Dict, consecutive_failures: int) -> BatchResult:
        """
        Procesa un lote individual con estrategias de recuperaci√≥n mejoradas.
        """
        if not batch:
            return BatchResult(success=False, error_message="Batch vac√≠o")

        # Ajustar estrategia basado en fallos consecutivos
        recovery_mode = consecutive_failures > 0
        
        try:
            parsed_data = ParsedData(batch, cache)
            
            if recovery_mode:
                # Modo recuperaci√≥n: procesar en sub-lotes m√°s peque√±os
                sub_results = []
                sub_batch_size = max(1, len(batch) // (consecutive_failures + 1))
                
                for i in range(0, len(batch), sub_batch_size):
                    sub_batch = batch[i:i + sub_batch_size]
                    sub_parsed = ParsedData(sub_batch, cache)
                    
                    try:
                        df_sub = self._rectify_signal(sub_parsed)
                        if df_sub is not None and not df_sub.empty:
                            sub_results.append(df_sub)
                    except Exception as e:
                        self.logger.debug(f"Sub-lote {i//sub_batch_size} fall√≥: {e}")
                        continue
                
                if sub_results:
                    df = pd.concat(sub_results, ignore_index=True) if len(sub_results) > 1 else sub_results[0]
                    return BatchResult(
                        success=True,
                        dataframe=df,
                        records_processed=len(df)
                    )
                else:
                    return BatchResult(
                        success=False,
                        error_message="Todos los sub-lotes fallaron en modo recuperaci√≥n"
                    )
            else:
                # Modo normal
                df = self._rectify_signal(parsed_data)
                
                if df is None or df.empty:
                    return BatchResult(
                        success=True,
                        dataframe=pd.DataFrame(),
                        records_processed=0
                    )

                return BatchResult(
                    success=True,
                    dataframe=df,
                    records_processed=len(df)
                )

        except Exception as e:
            # An√°lisis de error para diagn√≥stico
            error_type = type(e).__name__
            error_msg = str(e)
            
            # Clasificar error para estrategia de recuperaci√≥n
            if "memory" in error_msg.lower() or "MemoryError" in error_type:
                recovery_hint = "REDUCE_BATCH_SIZE"
            elif "timeout" in error_msg.lower():
                recovery_hint = "INCREASE_TIMEOUT"
            elif "connection" in error_msg.lower() or "network" in error_msg.lower():
                recovery_hint = "RETRY_WITH_BACKOFF"
            else:
                recovery_hint = "UNKNOWN"
                
            enhanced_error = f"{error_type}: {error_msg} [RECOVERY_HINT: {recovery_hint}]"
            
            return BatchResult(
                success=False,
                error_message=enhanced_error
            )

    def _enhance_stats_with_diagnostics(self, stats: ProcessingStats, metrics: Dict[str, float]) -> Dict[str, Any]:
        """
        Mejora las estad√≠sticas con diagn√≥stico del sistema.
        """
        base_stats = asdict(stats)
        
        # Calcular eficiencia
        if stats.total_records > 0:
            efficiency = stats.processed_records / stats.total_records
        else:
            efficiency = 0.0
            
        # Diagn√≥stico del sistema
        system_health = self.get_system_health()
        physics_diagnosis = self.physics.get_system_diagnosis(metrics)
        
        enhanced = {
            **base_stats,
            "efficiency": efficiency,
            "throughput": stats.processed_records / max(stats.processing_time, 0.001),
            "system_health": system_health,
            "physics_diagnosis": physics_diagnosis,
            "current_metrics": {
                "saturation": metrics.get("saturation", 0),
                "complexity": metrics.get("complexity", 0),
                "gyroscopic_stability": metrics.get("gyroscopic_stability", 1.0),
                "entropy_ratio": metrics.get("entropy_ratio", 0)
            }
        }
        
        return enhanced
        