### 1. _safe_float - LÃ³gica de Parsing Simplificada

def _safe_float(self, value: Any, default: float = 0.0) -> float:
    """
    Convierte un valor a float de manera segura con soporte para formatos locales.
    
    LÃ³gica de detecciÃ³n:
    1. Si hay ambos separadores: el Ãºltimo determina el decimal
    2. Si solo hay coma: se evalÃºa por posiciÃ³n y longitud de decimales
    """
    if pd.isna(value) or value is None:
        return default
    
    if isinstance(value, (int, float)):
        return float(value)
    
    try:
        str_value = str(value).strip().replace('\xa0', '').replace(' ', '')
        
        if not str_value:
            return default
        
        has_comma = ',' in str_value
        has_dot = '.' in str_value
        
        if has_comma and has_dot:
            # Determinar cuÃ¡l es el separador decimal por posiciÃ³n
            last_comma = str_value.rfind(',')
            last_dot = str_value.rfind('.')
            
            if last_dot > last_comma:
                # Formato: 1,234,567.89 (inglÃ©s)
                str_value = str_value.replace(',', '')
            else:
                # Formato: 1.234.567,89 (europeo)
                str_value = str_value.replace('.', '').replace(',', '.')
        
        elif has_comma and not has_dot:
            parts = str_value.split(',')
            # HeurÃ­stica: si la Ãºltima parte tiene 1-3 dÃ­gitos, es decimal
            if len(parts) == 2 and 1 <= len(parts[1]) <= 3 and parts[1].isdigit():
                str_value = str_value.replace(',', '.')
            else:
                # Es separador de miles
                str_value = str_value.replace(',', '')
        
        return float(str_value)
    
    except (ValueError, TypeError, AttributeError):
        return default


### 2. analyze_spectral_stability - CorrecciÃ³n de Eigenvalores y Robustez

def analyze_spectral_stability(self, graph: nx.DiGraph) -> Dict[str, Any]:
    """
    Calcula la Estabilidad Espectral y mÃ©tricas derivadas del Laplaciano.

    Fundamentos MatemÃ¡ticos:
    - Fiedler Value (Î»â‚‚): Segundo eigenvalor mÃ¡s pequeÃ±o del Laplaciano.
      Mide la conectividad algebraica. Î»â‚‚ > 0 âŸº grafo conexo.
    - Gap Espectral: Î»â‚‚ / Î»_max. Proxy de sincronizabilidad.
    - EnergÃ­a Espectral: ||L||_FÂ² = Î£Î»áµ¢Â². Mide dispersiÃ³n estructural.
    - Longitud de Onda: 1/Î»_max. Escala caracterÃ­stica de difusiÃ³n.

    Returns:
        Dict con mÃ©tricas espectrales y diagnÃ³stico de resonancia.
    """
    n_nodes = graph.number_of_nodes()
    
    default_result = {
        "fiedler_value": 0.0,
        "spectral_gap": 0.0,
        "spectral_energy": 0.0,
        "wavelength": float('inf'),
        "resonance_risk": False,
        "algebraic_connectivity": 0.0,
        "eigenvalues": [],
        "status": "insufficient_nodes"
    }
    
    if n_nodes < 2:
        return default_result

    try:
        # Convertir a no dirigido para anÃ¡lisis espectral simÃ©trico
        ud_graph = graph.to_undirected()
        
        # Remover nodos aislados para estabilidad numÃ©rica del Laplaciano normalizado
        isolated = list(nx.isolates(ud_graph))
        if isolated:
            ud_graph = ud_graph.copy()
            ud_graph.remove_nodes_from(isolated)
        
        n_effective = ud_graph.number_of_nodes()
        if n_effective < 2:
            default_result["status"] = "degenerate_after_isolation_removal"
            return default_result

        # Laplaciano Normalizado: L_norm = I - D^(-1/2) A D^(-1/2)
        L = nx.normalized_laplacian_matrix(ud_graph).astype(np.float64)

        # EnergÃ­a Espectral vÃ­a norma de Frobenius (eficiente, no requiere eigendecomposiciÃ³n)
        if scipy.sparse.issparse(L):
            spectral_energy = float(scipy.sparse.linalg.norm(L, 'fro') ** 2)
        else:
            spectral_energy = float(np.linalg.norm(L, 'fro') ** 2)

        # CÃ¡lculo de Eigenvalores
        if n_effective <= 50:
            # DecomposiciÃ³n completa para grafos pequeÃ±os
            eigenvalues = np.linalg.eigvalsh(L.toarray())
            eigenvalues = np.sort(np.real(eigenvalues))
            # Filtrar eigenvalores numÃ©ricamente cero (tolerancia)
            eigenvalues = eigenvalues[eigenvalues > 1e-10]
        else:
            # Sparse solver para grafos grandes
            # SM (Smallest Magnitude) para obtener los mÃ¡s pequeÃ±os
            k_small = min(n_effective - 1, 6)
            try:
                eigenvalues_small = scipy.sparse.linalg.eigsh(
                    L, k=k_small, which='SM', return_eigenvectors=False,
                    tol=1e-6, maxiter=n_effective * 100
                )
            except scipy.sparse.linalg.ArpackNoConvergence as e:
                eigenvalues_small = e.eigenvalues
            
            # LM (Largest Magnitude) para Î»_max
            try:
                eigenvalues_large = scipy.sparse.linalg.eigsh(
                    L, k=1, which='LM', return_eigenvectors=False
                )
            except scipy.sparse.linalg.ArpackNoConvergence as e:
                eigenvalues_large = e.eigenvalues if len(e.eigenvalues) > 0 else np.array([2.0])
            
            eigenvalues = np.sort(np.concatenate([eigenvalues_small, eigenvalues_large]))
            eigenvalues = eigenvalues[eigenvalues > 1e-10]

        if len(eigenvalues) < 1:
            default_result["status"] = "no_valid_eigenvalues"
            return default_result

        # Fiedler Value: primer eigenvalor no trivial (Î»â‚‚ para Laplaciano standard)
        # Para Laplaciano normalizado de grafo conexo, Î»â‚ = 0, Î»â‚‚ > 0
        fiedler_value = float(eigenvalues[0]) if len(eigenvalues) >= 1 else 0.0
        
        # Î»_max (para Laplaciano normalizado, estÃ¡ acotado por 2)
        lambda_max = float(eigenvalues[-1]) if len(eigenvalues) > 0 else 2.0
        lambda_max = min(lambda_max, 2.0)  # Cota teÃ³rica

        # Gap Espectral Normalizado
        spectral_gap = fiedler_value / lambda_max if lambda_max > 1e-10 else 0.0

        # Longitud de Onda (escala de difusiÃ³n caracterÃ­stica)
        wavelength = 1.0 / lambda_max if lambda_max > 1e-10 else float('inf')

        # DiagnÃ³stico de Resonancia: espectro degenerado indica vulnerabilidad
        # Coeficiente de variaciÃ³n bajo indica concentraciÃ³n espectral
        if len(eigenvalues) > 2:
            eigen_std = np.std(eigenvalues)
            eigen_mean = np.mean(eigenvalues)
            cv = eigen_std / eigen_mean if eigen_mean > 1e-10 else 0.0
            resonance_risk = cv < 0.15
        else:
            resonance_risk = True  # Muy pocos eigenvalores = alto riesgo

        return {
            "fiedler_value": round(fiedler_value, 6),
            "spectral_gap": round(spectral_gap, 6),
            "spectral_energy": round(spectral_energy, 4),
            "wavelength": round(wavelength, 6),
            "resonance_risk": bool(resonance_risk),
            "algebraic_connectivity": round(fiedler_value, 6),
            "lambda_max": round(lambda_max, 6),
            "isolated_nodes_removed": len(isolated),
            "eigenvalues": [round(float(e), 6) for e in eigenvalues[:8]],
            "status": "success"
        }

    except Exception as e:
        self.logger.error(f"Error en anÃ¡lisis espectral: {e}", exc_info=True)
        default_result["status"] = f"error: {str(e)}"
        return default_result


### 3. calculate_pyramid_stability - NormalizaciÃ³n y Robustez

def calculate_pyramid_stability(self, graph: nx.DiGraph) -> float:
    """
    Calcula el Ãndice de Estabilidad Piramidal (Î¨) normalizado.
    
    Modelo FÃ­sico:
    - Una pirÃ¡mide estable tiene base ancha (muchos insumos) y cÃºspide estrecha (pocos APUs).
    - Î¨ âˆˆ [0, 1] donde 1 = mÃ¡xima estabilidad.
    
    Componentes:
    1. Ratio Base/Altura: logâ‚â‚€(1 + insumos/APUs)
    2. Factor de Densidad: penaliza grafos muy densos (inestables)
    3. Factor de Aciclicidad: ciclos reducen estabilidad estructural
    4. Factor de Conectividad: fragmentaciÃ³n reduce estabilidad
    
    Returns:
        float: Ãndice Î¨ normalizado en [0, 1]
    """
    if graph.number_of_nodes() == 0:
        return 0.0
    
    # Conteo por tipo de nodo
    type_counts = {"APU": 0, "INSUMO": 0, "CAPITULO": 0}
    for _, data in graph.nodes(data=True):
        node_type = data.get("type", "")
        if node_type in type_counts:
            type_counts[node_type] += 1
    
    num_apus = type_counts["APU"]
    num_insumos = type_counts["INSUMO"]
    
    # Caso degenerado: sin APUs o sin insumos
    if num_apus == 0:
        return 0.0 if num_insumos == 0 else 0.5  # Solo insumos: parcialmente estable
    
    if num_insumos == 0:
        return 0.3  # Solo APUs sin detalle: baja estabilidad
    
    # 1. Ratio de Base (normalizado con log para escalar grandes diferencias)
    base_ratio = num_insumos / num_apus
    # Normalizar: ratio ideal ~5-10 insumos por APU
    ratio_score = min(1.0, np.log10(1 + base_ratio) / np.log10(11))  # log10(11) â‰ˆ 1.04
    
    # 2. Factor de Densidad
    density = nx.density(graph)
    # Densidad Ã³ptima para DAG jerÃ¡rquico: ~0.01-0.1
    # Penalizar densidades muy altas (> 0.3) o muy bajas (< 0.001)
    if density < 0.001:
        density_score = 0.5  # Muy disperso
    elif density > 0.5:
        density_score = 0.3  # Muy denso
    else:
        density_score = 1.0 - min(0.7, density)
    
    # 3. Factor de Aciclicidad
    is_dag = nx.is_directed_acyclic_graph(graph)
    if is_dag:
        acyclic_score = 1.0
    else:
        # PenalizaciÃ³n proporcional al nÃºmero de SCCs no triviales
        sccs = [c for c in nx.strongly_connected_components(graph) if len(c) > 1]
        cycle_penalty = min(0.5, len(sccs) * 0.1)
        acyclic_score = 0.5 - cycle_penalty
    
    # 4. Factor de Conectividad
    if nx.is_weakly_connected(graph):
        connectivity_score = 1.0
    else:
        num_components = nx.number_weakly_connected_components(graph)
        connectivity_score = 1.0 / num_components
    
    # CombinaciÃ³n ponderada
    weights = {
        "ratio": 0.35,
        "density": 0.20,
        "acyclic": 0.30,
        "connectivity": 0.15
    }
    
    stability = (
        weights["ratio"] * ratio_score +
        weights["density"] * density_score +
        weights["acyclic"] * acyclic_score +
        weights["connectivity"] * connectivity_score
    )
    
    return round(max(0.0, min(1.0, stability)), 4)


### 4. audit_integration_homology - CorrecciÃ³n de Mayer-Vietoris

def audit_integration_homology(
    self, graph_a: nx.DiGraph, graph_b: nx.DiGraph
) -> Dict[str, Any]:
    """
    Ejecuta el Test de Mayer-Vietoris para auditorÃ­a de fusiÃ³n topolÃ³gica.
    
    Teorema de Mayer-Vietoris (simplificado para grafos):
    Para X = A âˆª B con intersecciÃ³n A âˆ© B:
    
    Î²â‚(X) â‰¤ Î²â‚(A) + Î²â‚(B) + Î²â‚€(Aâˆ©B) - 1  (cota superior)
    
    La emergencia de nuevos ciclos indica conflicto de integraciÃ³n.
    
    Returns:
        Dict con diagnÃ³stico de fusiÃ³n y narrativa.
    """
    # Calcular mÃ©tricas individuales
    metrics_a = self.calculate_betti_numbers(graph_a)
    metrics_b = self.calculate_betti_numbers(graph_b)
    
    # Construir uniÃ³n
    graph_union = nx.compose(graph_a, graph_b)
    metrics_union = self.calculate_betti_numbers(graph_union)
    
    # Construir intersecciÃ³n (subgrafo inducido por nodos comunes)
    nodes_a = set(graph_a.nodes())
    nodes_b = set(graph_b.nodes())
    common_nodes = nodes_a & nodes_b
    
    # IntersecciÃ³n: aristas que existen en AMBOS grafos
    graph_intersection = nx.DiGraph()
    if common_nodes:
        graph_intersection.add_nodes_from(
            (n, graph_a.nodes[n]) for n in common_nodes
        )
        # Solo aristas presentes en ambos grafos
        for u, v in graph_a.edges():
            if u in common_nodes and v in common_nodes and graph_b.has_edge(u, v):
                graph_intersection.add_edge(u, v)
    
    metrics_intersection = self.calculate_betti_numbers(graph_intersection)
    
    # FÃ³rmula de Mayer-Vietoris para Î²â‚
    # Î²â‚(AâˆªB) = Î²â‚(A) + Î²â‚(B) - Î²â‚(Aâˆ©B) + Î´
    # donde Î´ = Î²â‚€(Aâˆ©B) - Î²â‚€(A) - Î²â‚€(B) + Î²â‚€(AâˆªB)
    
    # CÃ¡lculo del tÃ©rmino de conexiÃ³n
    delta_connectivity = (
        metrics_intersection.beta_0 
        - metrics_a.beta_0 
        - metrics_b.beta_0 
        + metrics_union.beta_0
    )
    
    # Valor teÃ³rico esperado de Î²â‚ en la uniÃ³n
    beta1_theoretical = (
        metrics_a.beta_1 
        + metrics_b.beta_1 
        - metrics_intersection.beta_1 
        + max(0, delta_connectivity)
    )
    
    # Emergencia: diferencia entre observado y suma simple
    emergent_observed = metrics_union.beta_1 - (metrics_a.beta_1 + metrics_b.beta_1)
    
    # Discrepancia respecto al modelo teÃ³rico
    discrepancy = abs(metrics_union.beta_1 - beta1_theoretical)
    
    # DeterminaciÃ³n del veredicto
    if discrepancy <= 1:
        if emergent_observed > 0:
            verdict = "INTEGRATION_CONFLICT"
            severity = "warning"
        elif emergent_observed < 0:
            verdict = "TOPOLOGY_SIMPLIFIED"
            severity = "info"
        else:
            verdict = "CLEAN_MERGE"
            severity = "success"
    else:
        verdict = "INCONSISTENT_TOPOLOGY"
        severity = "error"
    
    # AnÃ¡lisis de interfaz (nodos de frontera)
    boundary_nodes = []
    for node in common_nodes:
        # Nodo frontera: tiene aristas hacia nodos exclusivos de A o B
        neighbors_a = set(graph_a.successors(node)) | set(graph_a.predecessors(node))
        neighbors_b = set(graph_b.successors(node)) | set(graph_b.predecessors(node))
        exclusive_neighbors = (neighbors_a - nodes_b) | (neighbors_b - nodes_a)
        if exclusive_neighbors:
            boundary_nodes.append(node)
    
    narrative = self._generate_mayer_vietoris_narrative(
        emergent_observed, discrepancy, len(boundary_nodes)
    )

    return {
        "status": verdict,
        "severity": severity,
        "delta_beta_1": emergent_observed,
        "beta_1_observed": metrics_union.beta_1,
        "beta_1_theoretical": beta1_theoretical,
        "discrepancy": discrepancy,
        "boundary_nodes": boundary_nodes,
        "details": {
            "beta_1_A": metrics_a.beta_1,
            "beta_1_B": metrics_b.beta_1,
            "beta_1_intersection": metrics_intersection.beta_1,
            "beta_1_union": metrics_union.beta_1,
            "beta_0_A": metrics_a.beta_0,
            "beta_0_B": metrics_b.beta_0,
            "beta_0_intersection": metrics_intersection.beta_0,
            "beta_0_union": metrics_union.beta_0,
            "common_nodes_count": len(common_nodes),
            "boundary_nodes_count": len(boundary_nodes),
            "delta_connectivity": delta_connectivity,
        },
        "narrative": narrative,
    }


def _generate_mayer_vietoris_narrative(
    self, observed: int, discrepancy: float, boundary_count: int
) -> str:
    """Genera narrativa contextualizada del anÃ¡lisis Mayer-Vietoris."""
    parts = []
    
    if discrepancy > 2:
        parts.append(
            f"âš ï¸ ANOMALÃA TOPOLÃ“GICA: Discrepancia significativa (Î”={discrepancy:.1f}). "
            f"La estructura combinada no corresponde al modelo teÃ³rico. "
            f"Revisar coherencia de datos en {boundary_count} nodos de frontera."
        )
    elif discrepancy > 1:
        parts.append(
            f"âš ï¸ Discrepancia menor detectada (Î”={discrepancy:.1f}). "
            f"Posible redundancia en la interfaz de fusiÃ³n."
        )
    
    if observed > 0:
        parts.append(
            f"ğŸš¨ CONFLICTO DE INTEGRACIÃ“N: La fusiÃ³n generÃ³ {observed} nuevo(s) ciclo(s) "
            f"de dependencia no presentes en los componentes originales. "
            f"Esto indica incompatibilidad estructural en la interfaz."
        )
    elif observed < 0:
        parts.append(
            f"âœ… SIMPLIFICACIÃ“N TOPOLÃ“GICA: La fusiÃ³n eliminÃ³ {abs(observed)} ciclo(s) "
            f"redundante(s). La estructura combinada es mÃ¡s eficiente."
        )
    elif discrepancy <= 1:
        parts.append(
            "âœ… FUSIÃ“N LIMPIA: No se detectaron conflictos estructurales. "
            "La integraciÃ³n es topolÃ³gicamente neutral."
        )
    
    return " ".join(parts) if parts else "AnÃ¡lisis completado sin observaciones."


### 5. analyze_thermal_flow - PropagaciÃ³n Corregida

def analyze_thermal_flow(self, graph: nx.DiGraph) -> Dict[str, Any]:
    """
    Calcula el Flujo TÃ©rmico Estructural (Modelo de DifusiÃ³n de Riesgo).

    Modelo FÃ­sico:
    1. Temperatura Base: asignada a insumos segÃºn volatilidad histÃ³rica.
    2. ConducciÃ³n TÃ©rmica: el calor fluye de hojas hacia la raÃ­z, ponderado por costo.
    3. Temperatura SistÃ©mica: promedio ponderado en el nodo raÃ­z.

    La "temperatura" es un proxy de la sensibilidad del presupuesto a variaciones
    de precio en insumos volÃ¡tiles (combustibles, acero, etc.).

    Returns:
        Dict con temperatura del sistema, hotspots y gradiente tÃ©rmico completo.
    """
    # DefiniciÃ³n de temperaturas base por tipo de insumo (escala 0-100)
    # Basado en volatilidad histÃ³rica de precios en construcciÃ³n
    BASE_TEMPERATURES = {
        "COMBUSTIBLE": 95.0, "GASOLINA": 95.0, "DIESEL": 95.0, "ACPM": 95.0,
        "ASFALTO": 90.0, "BITUMEN": 90.0,
        "ACERO": 85.0, "HIERRO": 85.0, "VARILLA": 85.0, "ALAMBRE": 80.0,
        "COBRE": 88.0, "ALUMINIO": 82.0,
        "CEMENTO": 60.0, "CONCRETO": 55.0, "AGREGADO": 45.0, "ARENA": 40.0,
        "TRANSPORTE": 75.0, "FLETE": 75.0, "ACARREO": 70.0,
        "MAQUINARIA": 50.0, "EQUIPO": 45.0, "HERRAMIENTA": 35.0,
        "MANO DE OBRA": 25.0, "OFICIAL": 25.0, "AYUDANTE": 20.0,
        "MADERA": 55.0, "FORMALETA": 50.0,
    }
    DEFAULT_TEMP = 30.0

    def get_base_temperature(description: str, tipo: str) -> float:
        """Determina temperatura base por matching de keywords."""
        text = f"{description} {tipo}".upper()
        matched_temp = DEFAULT_TEMP
        for keyword, temp in BASE_TEMPERATURES.items():
            if keyword in text:
                matched_temp = max(matched_temp, temp)
        return matched_temp

    # Inicializar estructuras
    node_temperatures: Dict[str, float] = {}
    node_costs: Dict[str, float] = {}
    
    # Paso 1: Asignar temperatura base a nodos INSUMO (hojas)
    for node, data in graph.nodes(data=True):
        if data.get("type") == "INSUMO":
            desc = str(data.get("description", ""))
            tipo = str(data.get("tipo_insumo", ""))
            node_temperatures[node] = get_base_temperature(desc, tipo)
            
            # Costo total del insumo = suma de aristas entrantes (desde APUs)
            total_cost = sum(
                graph[pred][node].get("total_cost", 0.0)
                for pred in graph.predecessors(node)
            )
            node_costs[node] = max(total_cost, data.get("unit_cost", 0.0))
        else:
            node_temperatures[node] = 0.0
            node_costs[node] = 0.0

    # Paso 2: PropagaciÃ³n hacia arriba (Insumos â†’ APUs â†’ CapÃ­tulos â†’ Root)
    # Usar BFS desde hojas o Topological Sort inverso
    try:
        # Orden topolÃ³gico: raÃ­z primero, hojas al final
        # Invertir para procesar hojas primero
        topo_order = list(reversed(list(nx.topological_sort(graph))))
    except nx.NetworkXUnfeasible:
        # Si hay ciclos, usar ordenamiento por nivel (heurÃ­stico)
        topo_order = sorted(
            graph.nodes(),
            key=lambda n: graph.nodes[n].get("level", 0),
            reverse=True
        )

    for node in topo_order:
        node_type = graph.nodes[node].get("type", "")
        
        # Los insumos ya tienen temperatura asignada
        if node_type == "INSUMO":
            continue
        
        # Calcular temperatura ponderada de hijos (sucesores)
        children = list(graph.successors(node))
        if not children:
            node_temperatures[node] = DEFAULT_TEMP * 0.5
            continue
        
        weighted_sum = 0.0
        cost_sum = 0.0
        
        for child in children:
            # El peso es el costo de la arista padreâ†’hijo
            edge_data = graph[node][child]
            edge_cost = edge_data.get("total_cost", 0.0)
            if edge_cost == 0.0:
                edge_cost = edge_data.get("weight", 0.0)
            if edge_cost == 0.0:
                # Fallback: usar costo del hijo
                edge_cost = node_costs.get(child, 1.0)
            
            child_temp = node_temperatures.get(child, DEFAULT_TEMP)
            
            weighted_sum += child_temp * edge_cost
            cost_sum += edge_cost
        
        if cost_sum > 0:
            node_temperatures[node] = weighted_sum / cost_sum
            node_costs[node] = cost_sum
        else:
            node_temperatures[node] = DEFAULT_TEMP * 0.5

    # Paso 3: Temperatura del Sistema (nodo ROOT o promedio global)
    root_nodes = [n for n, d in graph.nodes(data=True) if d.get("type") == "ROOT"]
    
    if root_nodes:
        system_temp = node_temperatures.get(root_nodes[0], 0.0)
    else:
        # Promedio ponderado de APUs
        apu_nodes = [n for n, d in graph.nodes(data=True) if d.get("type") == "APU"]
        if apu_nodes:
            total_cost = sum(node_costs.get(n, 0) for n in apu_nodes)
            if total_cost > 0:
                system_temp = sum(
                    node_temperatures[n] * node_costs[n] 
                    for n in apu_nodes
                ) / total_cost
            else:
                system_temp = np.mean([node_temperatures[n] for n in apu_nodes])
        else:
            system_temp = 0.0

    # Paso 4: Identificar Hotspots (Top N por temperatura, con costo significativo)
    # Filtrar nodos con costo > percentil 10 para evitar ruido
    cost_threshold = np.percentile(
        [c for c in node_costs.values() if c > 0], 10
    ) if any(c > 0 for c in node_costs.values()) else 0
    
    candidate_hotspots = [
        (node, temp) for node, temp in node_temperatures.items()
        if node_costs.get(node, 0) > cost_threshold
    ]
    candidate_hotspots.sort(key=lambda x: x[1], reverse=True)
    
    hotspots = [
        {
            "id": node,
            "temperature": round(temp, 1),
            "type": graph.nodes[node].get("type", ""),
            "cost": round(node_costs.get(node, 0), 2),
            "description": graph.nodes[node].get("description", "")[:50]
        }
        for node, temp in candidate_hotspots[:10]
    ]

    # ClasificaciÃ³n del riesgo tÃ©rmico
    if system_temp > 70:
        thermal_risk_level = "CRÃTICO"
    elif system_temp > 50:
        thermal_risk_level = "ALTO"
    elif system_temp > 35:
        thermal_risk_level = "MEDIO"
    else:
        thermal_risk_level = "BAJO"

    return {
        "system_temperature": round(system_temp, 2),
        "thermal_risk_level": thermal_risk_level,
        "hotspots": hotspots,
        "thermal_gradient": {k: round(v, 2) for k, v in node_temperatures.items()},
        "cost_distribution": {k: round(v, 2) for k, v in node_costs.items()},
        "stats": {
            "max_temperature": round(max(node_temperatures.values()), 2) if node_temperatures else 0,
            "min_temperature": round(min(node_temperatures.values()), 2) if node_temperatures else 0,
            "std_temperature": round(float(np.std(list(node_temperatures.values()))), 2) if node_temperatures else 0,
        }
    }


### 6. detect_risk_synergy - Score Corregido

def detect_risk_synergy(
    self, graph: nx.DiGraph, raw_cycles: Optional[List[List[str]]] = None
) -> Dict[str, Any]:
    """
    Detecta Sinergia de Riesgo: ciclos que comparten nodos crÃ­ticos.
    
    Concepto:
    Si dos ciclos de dependencia comparten un nodo de alta centralidad,
    un fallo en ese nodo dispara ambos ciclos simultÃ¡neamente (efecto dominÃ³).
    
    MÃ©tricas:
    - Nodos Puente: nodos de alta centralidad en mÃºltiples ciclos.
    - Score de Sinergia: probabilidad relativa de fallo en cascada.
    
    Returns:
        Dict con detecciÃ³n de sinergia, nodos puente y score normalizado.
    """
    if raw_cycles is None:
        raw_cycles, _ = self._get_raw_cycles(graph)
    
    default_result = {
        "synergy_detected": False,
        "shared_nodes": [],
        "intersecting_cycles_count": 0,
        "bridge_nodes": [],
        "synergy_score": 0.0,
        "risk_level": "NINGUNO",
        "details": {}
    }
    
    if len(raw_cycles) < 2:
        return default_result

    # Calcular Betweenness Centrality
    try:
        if graph.number_of_nodes() > 1000:
            # AproximaciÃ³n para grafos grandes
            betweenness = nx.betweenness_centrality(
                graph, normalized=True, k=min(100, graph.number_of_nodes())
            )
        else:
            betweenness = nx.betweenness_centrality(graph, normalized=True)
    except Exception as e:
        self.logger.warning(f"Error calculando betweenness: {e}")
        betweenness = {n: 0.0 for n in graph.nodes()}

    # Umbral adaptativo para nodos crÃ­ticos
    if betweenness:
        bc_values = list(betweenness.values())
        if len(bc_values) >= 4:
            threshold = np.percentile(bc_values, 75)
        else:
            threshold = np.mean(bc_values)
    else:
        threshold = 0.0

    critical_nodes = {n for n, c in betweenness.items() if c >= threshold and c > 0}
    
    # Analizar intersecciones de ciclos
    cycle_sets = [set(c) for c in raw_cycles]
    n_cycles = len(cycle_sets)
    
    synergy_pairs = []
    bridge_node_occurrences: Dict[str, List[Tuple[int, int]]] = {}
    
    for i in range(n_cycles):
        for j in range(i + 1, n_cycles):
            intersection = cycle_sets[i] & cycle_sets[j]
            
            # Sinergia significativa: â‰¥2 nodos compartidos o 1 nodo crÃ­tico
            critical_intersection = intersection & critical_nodes
            
            if len(intersection) >= 2 or critical_intersection:
                synergy_pairs.append((i, j, intersection))
                
                for node in intersection:
                    if node not in bridge_node_occurrences:
                        bridge_node_occurrences[node] = []
                    bridge_node_occurrences[node].append((i, j))
    
    if not synergy_pairs:
        return default_result
    
    # Identificar nodos puente ordenados por impacto
    bridge_nodes = sorted(
        [
            {
                "id": node,
                "cycles_connected": len(occurrences),
                "betweenness": round(betweenness.get(node, 0), 4),
                "is_critical": node in critical_nodes
            }
            for node, occurrences in bridge_node_occurrences.items()
        ],
        key=lambda x: (x["cycles_connected"], x["betweenness"]),
        reverse=True
    )
    
    # Score de Sinergia normalizado [0, 1]
    # Componentes:
    # 1. Ratio de pares con sinergia vs total posible
    total_pairs = n_cycles * (n_cycles - 1) / 2
    pair_ratio = len(synergy_pairs) / total_pairs if total_pairs > 0 else 0
    
    # 2. ConcentraciÃ³n de puentes crÃ­ticos
    critical_bridges = sum(1 for b in bridge_nodes if b["is_critical"])
    critical_ratio = critical_bridges / len(bridge_nodes) if bridge_nodes else 0
    
    # 3. Promedio de conexiones por puente
    avg_connections = np.mean([b["cycles_connected"] for b in bridge_nodes]) if bridge_nodes else 0
    connection_factor = min(1.0, avg_connections / 3)  # Normalizar a 3 conexiones
    
    synergy_score = 0.4 * pair_ratio + 0.35 * critical_ratio + 0.25 * connection_factor
    synergy_score = round(min(1.0, synergy_score), 4)
    
    # Nivel de riesgo
    if synergy_score > 0.6:
        risk_level = "CRÃTICO"
    elif synergy_score > 0.3:
        risk_level = "ALTO"
    elif synergy_score > 0.1:
        risk_level = "MEDIO"
    else:
        risk_level = "BAJO"

    return {
        "synergy_detected": True,
        "shared_nodes": list(bridge_node_occurrences.keys()),
        "intersecting_cycles_count": len(synergy_pairs),
        "bridge_nodes": bridge_nodes[:10],  # Top 10
        "synergy_score": synergy_score,
        "risk_level": risk_level,
        "details": {
            "total_cycles": n_cycles,
            "synergy_pairs": len(synergy_pairs),
            "critical_bridges": critical_bridges,
            "pair_ratio": round(pair_ratio, 4),
        }
    }


### 7. generate_executive_report - IntegraciÃ³n del Factor Espectral

def generate_executive_report(
    self, graph: nx.DiGraph, financial_metrics: Optional[Dict[str, Any]] = None
) -> ConstructionRiskReport:
    """
    Genera reporte ejecutivo con modelo de scoring bayesiano multi-factor.
    
    Factores del Score de Integridad:
    1. Eficiencia de Euler (topologÃ­a)
    2. Estabilidad Piramidal (estructura)
    3. Factor de Densidad (complejidad)
    4. Factor Espectral (resonancia)
    
    Penalizaciones:
    - Ciclos de dependencia
    - Sinergia de riesgo
    - AnomalÃ­as (nodos aislados, huÃ©rfanos)
    - Riesgo de resonancia espectral
    """
    # CÃ¡lculos base
    metrics = self.calculate_betti_numbers(graph)
    raw_cycles, truncated = self._get_raw_cycles(graph)
    cycles = [" â†’ ".join(c + [c[0]]) for c in raw_cycles]
    
    synergy = self.detect_risk_synergy(graph, raw_cycles)
    anomalies = self._classify_anomalous_nodes(graph)
    pyramid_stability = self.calculate_pyramid_stability(graph)
    connectivity = self._compute_connectivity_analysis(graph)
    spectral = self.analyze_spectral_stability(graph)
    
    # DetecciÃ³n de fluidos convectivos
    fluid_keywords = ["TRANSPORTE", "COMBUSTIBLE", "FLETE", "ACARREO", 
                      "GASOLINA", "DIESEL", "ACPM", "ASFALTO"]
    fluid_nodes = [
        n for n, d in graph.nodes(data=True)
        if d.get("type") == "INSUMO"
        and any(k in str(d.get("description", "")).upper() for k in fluid_keywords)
    ]
    convection = self.analyze_inflationary_convection(graph, fluid_nodes)
    thermal = self.analyze_thermal_flow(graph)
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # MODELO DE SCORING BAYESIANO MULTI-FACTOR
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    
    density = nx.density(graph) if graph.number_of_nodes() > 0 else 0.0
    
    # Factor 1: Eficiencia de Euler (0-1)
    euler_factor = metrics.euler_efficiency
    
    # Factor 2: Estabilidad Piramidal (ya normalizado 0-1)
    stability_factor = pyramid_stability
    
    # Factor 3: Densidad Ã“ptima (penaliza extremos)
    if density < 0.01:
        density_factor = 0.7  # Muy disperso
    elif density > 0.5:
        density_factor = 0.5  # Muy denso
    else:
        density_factor = 1.0 - (density * 0.5)  # Escala lineal inversa
    
    # Factor 4: Espectral (basado en conectividad algebraica y resonancia)
    fiedler = spectral.get("fiedler_value", 0.0)
    if spectral.get("resonance_risk", False):
        spectral_factor = 0.6
    elif fiedler > 0.1:
        spectral_factor = 1.0
    elif fiedler > 0.01:
        spectral_factor = 0.85
    else:
        spectral_factor = 0.7
    
    # Pesos del modelo (suman 1.0)
    weights = {
        "euler": 0.30,
        "stability": 0.25,
        "density": 0.20,
        "spectral": 0.25
    }
    
    base_score = 100.0 * (
        weights["euler"] * euler_factor +
        weights["stability"] * stability_factor +
        weights["density"] * density_factor +
        weights["spectral"] * spectral_factor
    )
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # PENALIZACIONES
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    
    penalty = 0.0
    penalty_details = []
    
    # PenalizaciÃ³n por ciclos (hasta -25 puntos)
    if metrics.beta_1 > 0:
        cycle_penalty = min(25.0, metrics.beta_1 * 5.0)
        penalty += cycle_penalty
        penalty_details.append(f"Ciclos ({metrics.beta_1}): -{cycle_penalty:.1f}")
    
    # PenalizaciÃ³n por sinergia de riesgo (hasta -20 puntos)
    if synergy["synergy_detected"]:
        synergy_penalty = min(20.0, synergy["synergy_score"] * 30.0)
        penalty += synergy_penalty
        penalty_details.append(f"Sinergia ({synergy['synergy_score']:.2f}): -{synergy_penalty:.1f}")
    
    # PenalizaciÃ³n por resonancia espectral (hasta -10 puntos)
    if spectral.get("resonance_risk", False):
        penalty += 10.0
        penalty_details.append("Resonancia espectral: -10.0")
    
    # PenalizaciÃ³n por anomalÃ­as (hasta -15 puntos)
    iso_count = len(anomalies["isolated_nodes"])
    orphan_count = len(anomalies["orphan_insumos"])
    empty_count = len(anomalies["empty_apus"])
    
    if iso_count + orphan_count + empty_count > 0:
        anomaly_penalty = min(15.0, (iso_count * 2 + orphan_count * 1.5 + empty_count * 1))
        penalty += anomaly_penalty
        penalty_details.append(f"AnomalÃ­as ({iso_count}+{orphan_count}+{empty_count}): -{anomaly_penalty:.1f}")
    
    # PenalizaciÃ³n por riesgo tÃ©rmico (hasta -10 puntos)
    if thermal.get("thermal_risk_level") == "CRÃTICO":
        penalty += 10.0
        penalty_details.append("Riesgo tÃ©rmico crÃ­tico: -10.0")
    elif thermal.get("thermal_risk_level") == "ALTO":
        penalty += 5.0
        penalty_details.append("Riesgo tÃ©rmico alto: -5.0")
    
    # Score final
    integrity_score = max(0.0, min(100.0, base_score - penalty))
    integrity_score = round(integrity_score, 1)
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # CLASIFICACIÃ“N DE COMPLEJIDAD
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    
    complexity_score = (
        0.30 * min(1.0, metrics.beta_1 / max(1, graph.number_of_nodes()) * 10) +
        0.25 * density +
        0.25 * (1.0 - metrics.euler_efficiency) +
        0.20 * (1.0 - pyramid_stability)
    )
    
    if complexity_score > 0.5:
        complexity_level = "CRÃTICA"
    elif complexity_score > 0.3:
        complexity_level = "Alta"
    elif complexity_score > 0.15:
        complexity_level = "Media"
    else:
        complexity_level = "Baja"
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # ALERTAS Y RIESGOS
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    
    waste_alerts = []
    if iso_count > 0:
        waste_alerts.append(f"ğŸš¨ {iso_count} nodo(s) aislado(s) detectado(s) (recursos sin uso).")
    if orphan_count > 0:
        waste_alerts.append(f"âš ï¸ {orphan_count} insumo(s) huÃ©rfano(s) (sin vinculaciÃ³n a APU).")
    if empty_count > 0:
        waste_alerts.append(f"âš ï¸ {empty_count} APU(s) vacÃ­o(s) (sin detalle de insumos).")
    if metrics.euler_efficiency < 0.6:
        waste_alerts.append(f"âš ï¸ Eficiencia topolÃ³gica baja ({metrics.euler_efficiency:.2%}).")
    if truncated:
        waste_alerts.append(f"â„¹ï¸ AnÃ¡lisis de ciclos truncado a {self.max_cycles} (hay mÃ¡s).")

    circular_risks = []
    if metrics.beta_1 > 0:
        circular_risks.append(f"ğŸš¨ CRÃTICO: {metrics.beta_1} ciclo(s) de dependencia detectado(s).")
        for cycle in cycles[:3]:
            circular_risks.append(f"   â†³ {cycle[:80]}{'...' if len(cycle) > 80 else ''}")
    
    if synergy["synergy_detected"]:
        circular_risks.append(
            f"â˜£ï¸ RIESGO SISTÃ‰MICO: Sinergia de riesgo nivel {synergy['risk_level']} "
            f"(score: {synergy['synergy_score']:.2f})."
        )
    
    if convection["high_risk_nodes"]:
        circular_risks.append(
            f"ğŸ”¥ RIESGO CONVECTIVO: {len(convection['high_risk_nodes'])} nodo(s) "
            f"altamente sensible(s) a variaciÃ³n de transporte/combustible."
        )
    
    if spectral.get("resonance_risk", False):
        circular_risks.append(
            "ğŸ”Š RIESGO DE RESONANCIA: Espectro degenerado detectado. "
            "Alta vulnerabilidad a perturbaciones sistÃ©micas."
        )
    
    if thermal.get("thermal_risk_level") in ["CRÃTICO", "ALTO"]:
        circular_risks.append(
            f"ğŸŒ¡ï¸ TEMPERATURA SISTÃ‰MICA {thermal['thermal_risk_level']}: "
            f"{thermal['system_temperature']:.1f}Â°. Alta sensibilidad a inflaciÃ³n de insumos."
        )
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # RIESGO FINANCIERO INTEGRADO
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    
    financial_risk = None
    if financial_metrics:
        volatility = financial_metrics.get("volatility", 0.0)
        roi = financial_metrics.get("roi", 0.0)
        
        if roi < -0.1:
            financial_risk = "CRÃTICO"
        elif roi < 0:
            financial_risk = "ALTO"
        elif volatility > 0.3:
            financial_risk = "ALTO"
        elif volatility > 0.2:
            financial_risk = "MEDIO"
        else:
            financial_risk = "BAJO"
        
        # Escalamiento por factores topolÃ³gicos
        if financial_risk in ["ALTO", "MEDIO"]:
            if metrics.beta_1 > 2 or synergy["synergy_detected"]:
                financial_risk = "CATÃSTROFICO"
            elif spectral.get("resonance_risk", False):
                if financial_risk == "MEDIO":
                    financial_risk = "ALTO"

    # Narrativa estratÃ©gica
    strategic_narrative = self._generate_strategic_narrative(
        metrics, synergy, pyramid_stability, financial_risk, thermal, spectral
    )

    return ConstructionRiskReport(
        integrity_score=integrity_score,
        waste_alerts=waste_alerts,
        circular_risks=circular_risks,
        complexity_level=complexity_level,
        financial_risk_level=financial_risk,
        strategic_narrative=strategic_narrative,
        details={
            "metrics": asdict(metrics),
            "cycles": cycles,
            "cycles_truncated": truncated,
            "anomalies": anomalies,
            "synergy_risk": synergy,
            "connectivity": connectivity,
            "pyramid_stability": pyramid_stability,
            "density": round(density, 6),
            "convection_risk": convection,
            "spectral_analysis": spectral,
            "thermal_analysis": {
                "system_temperature": thermal["system_temperature"],
                "thermal_risk_level": thermal["thermal_risk_level"],
                "hotspots": thermal["hotspots"][:5]
            },
            "scoring_breakdown": {
                "base_score": round(base_score, 2),
                "total_penalty": round(penalty, 2),
                "penalty_details": penalty_details,
                "factor_weights": weights,
                "factors": {
                    "euler": round(euler_factor, 4),
                    "stability": round(stability_factor, 4),
                    "density": round(density_factor, 4),
                    "spectral": round(spectral_factor, 4)
                }
            }
        },
    )


def _generate_strategic_narrative(
    self,
    metrics: TopologicalMetrics,
    synergy: Dict[str, Any],
    stability: float,
    financial_risk: Optional[str],
    thermal: Dict[str, Any],
    spectral: Dict[str, Any],
) -> str:
    """
    Genera narrativa estratÃ©gica ejecutiva integrando todos los anÃ¡lisis.
    Estilo: 'Consejo de Sabios' - Directo, tÃ©cnico pero accesible.
    """
    sections = []
    
    # 1. DIAGNÃ“STICO ESTRUCTURAL
    if stability > 0.7:
        sections.append(
            "ğŸ—ï¸ **ESTRUCTURA SÃ“LIDA**: La pirÃ¡mide presupuestaria presenta una base "
            "robusta con adecuada distribuciÃ³n de insumos por APU."
        )
    elif stability > 0.4:
        sections.append(
            "âš ï¸ **ESTRUCTURA MODERADA**: La relaciÃ³n base-altura es aceptable pero "
            "existen oportunidades de consolidaciÃ³n en la distribuciÃ³n de recursos."
        )
    else:
        sections.append(
            "ğŸš¨ **ALERTA ESTRUCTURAL (PIRÃMIDE INVERTIDA)**: La base de insumos es "
            "insuficiente para la complejidad de APUs definidos. Riesgo de colapso "
            "ante variaciones de mercado."
        )
    
    # 2. INTEGRIDAD LÃ“GICA
    if metrics.beta_1 == 0:
        sections.append(
            "âœ… **TRAZABILIDAD LIMPIA**: Sin ciclos de dependencia. El flujo de costos "
            "es unidireccional y auditable."
        )
    elif metrics.beta_1 <= 2:
        sections.append(
            f"âš ï¸ **CICLOS DETECTADOS**: {metrics.beta_1} ciclo(s) de dependencia "
            f"identificado(s). Requieren revisiÃ³n para evitar cÃ¡lculos circulares."
        )
    else:
        sections.append(
            f"ğŸš¨ **COMPLEJIDAD CÃCLICA CRÃTICA**: {metrics.beta_1} ciclos de dependencia "
            f"comprometen la integridad del cÃ¡lculo. AuditorÃ­a inmediata requerida."
        )
    
    # 3. RIESGO SISTÃ‰MICO
    if synergy.get("synergy_detected"):
        risk_level = synergy.get("risk_level", "DETECTADO")
        bridge_count = len(synergy.get("bridge_nodes", []))
        sections.append(
            f"â˜£ï¸ **RIESGO DE CONTAGIO ({risk_level})**: {bridge_count} nodo(s) puente "
            f"conectan mÃºltiples ciclos. Un fallo en estos puntos desencadenarÃ­a "
            f"efecto dominÃ³ en cascada."
        )
    
    # 4. SENSIBILIDAD TÃ‰RMICA
    if thermal.get("thermal_risk_level") in ["CRÃTICO", "ALTO"]:
        temp = thermal.get("system_temperature", 0)
        sections.append(
            f"ğŸŒ¡ï¸ **ALTA SENSIBILIDAD INFLACIONARIA**: Temperatura sistÃ©mica de {temp:.0f}Â°. "
            f"El presupuesto es vulnerable a fluctuaciones de precios en insumos volÃ¡tiles."
        )
    
    # 5. ESTABILIDAD ESPECTRAL
    if spectral.get("resonance_risk"):
        sections.append(
            "ğŸ”Š **VULNERABILIDAD ESPECTRAL**: ConcentraciÃ³n anÃ³mala en el espectro "
            "del Laplaciano. El sistema podrÃ­a amplificar perturbaciones externas "
            "(efecto resonancia)."
        )
    
    # 6. VEREDICTO FINANCIERO
    if financial_risk:
        if financial_risk == "CATÃSTROFICO":
            sections.append(
                "ğŸ’€ **ALERTA CRÃTICA DE VIABILIDAD**: El perfil de riesgo financiero "
                "combinado con la estructura topolÃ³gica indica probabilidad significativa "
                "de fracaso del proyecto. Suspender compromisos hasta revisiÃ³n profunda."
            )
        elif financial_risk == "CRÃTICO":
            sections.append(
                "ğŸ“‰ **RIESGO FINANCIERO CRÃTICO**: Los indicadores econÃ³micos "
                "requieren atenciÃ³n inmediata. Considerar reestructuraciÃ³n."
            )
        elif financial_risk == "ALTO":
            sections.append(
                "ğŸ“Š **PRECAUCIÃ“N FINANCIERA**: Volatilidad elevada en componentes "
                "crÃ­ticos. Implementar coberturas o contingencias."
            )
        elif financial_risk == "BAJO":
            sections.append(
                "ğŸ’° **SALUD FINANCIERA**: Los indicadores econÃ³micos respaldan "
                "la viabilidad tÃ©cnica del proyecto."
            )
    
    return " ".join(sections)
