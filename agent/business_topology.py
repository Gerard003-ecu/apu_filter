"""
Este m√≥dulo implementa la l√≥gica para modelar el presupuesto no como una lista de 
precios, sino como un Grafo Dirigido (o Complejo Simplicial Abstracto). Su funci√≥n 
es calcular invariantes topol√≥gicos que revelan la "Salud Estructural" del proyecto 
antes de invertir dinero.

M√©tricas e Invariantes Topol√≥gicos:
-----------------------------------
1. N√∫meros de Betti (Homolog√≠a):
   - Œ≤0 (Componentes Conexas): Mide la fragmentaci√≥n. Un Œ≤0 > 1 indica "Islas" 
     de recursos desconectados o hu√©rfanos.
   - Œ≤1 (Ciclos): Mide la complejidad circular. Un Œ≤1 > 0 revela "Socavones L√≥gicos" 
     (dependencias circulares A->B->A) que impiden el c√°lculo de costos.

2. Estabilidad Piramidal (Œ®):
   Calcula la relaci√≥n entre la base log√≠stica (Insumos) y la carga t√°ctica (APUs).
   - Œ® < 1.0 ("Pir√°mide Invertida"): Alerta sobre una base de proveedores peligrosamente 
     estrecha soportando una estructura masiva, indicando alto riesgo de colapso log√≠stico.

3. Termodin√°mica Estructural (Flujo T√©rmico):
   Simula la difusi√≥n de la volatilidad de precios (calor) desde los insumos hacia 
   el proyecto general, generando un mapa de calor de riesgo inflacionario.

4. Resonancia Espectral:
   Analiza el espectro del Laplaciano del grafo para detectar si la estructura es 
   susceptible a fallos sist√©micos sincronizados (efecto domin√≥).
"""

import logging
from dataclasses import asdict, dataclass, field
from typing import Any, Dict, List, Optional, Tuple

import networkx as nx
import numpy as np
import pandas as pd
import scipy.sparse.linalg

from app.constants import ColumnNames
from app.telemetry import TelemetryContext

logger = logging.getLogger(__name__)


@dataclass(frozen=True)
class TopologicalMetrics:
    """
    M√©tricas Topol√≥gicas Invariantes para el Grafo de Negocio.

    Representa los invariantes topol√≥gicos calculados sobre el grafo
    del presupuesto, fundamentales para entender la complejidad estructural.

    Attributes:
        beta_0 (int): N√∫mero de componentes conexas (fragmentaci√≥n).
            Indica cu√°ntas partes aisladas tiene el proyecto.
        beta_1 (int): N√∫mero de ciclos independientes (complejidad de bucles).
            Representa dependencias circulares y redundancia l√≥gica.
        euler_characteristic (int): Caracter√≠stica de Euler (beta_0 - beta_1).
            Invariante fundamental de la topolog√≠a del grafo.
        euler_efficiency (float): Eficiencia topol√≥gica normalizada (0.0 - 1.0).
            M√©trica sint√©tica de la calidad estructural.
    """

    beta_0: int
    beta_1: int
    euler_characteristic: int
    euler_efficiency: float = 1.0

    @property
    def is_connected(self) -> bool:
        """
        Determina si el grafo est√° conectado.

        Un grafo conectado tiene exactamente una componente conexa (beta_0 = 1).

        Returns:
            bool: Verdadero si beta_0 es 1.
        """
        return self.beta_0 == 1

    @property
    def is_simply_connected(self) -> bool:
        """
        Determina si el grafo es simplemente conexo.

        Un grafo es simplemente conexo si es conexo y no tiene ciclos (beta_1 = 0).

        Returns:
            bool: Verdadero si beta_0 es 1 y beta_1 es 0.
        """
        return self.beta_0 == 1 and self.beta_1 == 0


@dataclass
class ConstructionRiskReport:
    """
    Reporte Ejecutivo de Riesgos de Construcci√≥n.

    Consolida el an√°lisis topol√≥gico, financiero y estructural en un
    informe unificado para la toma de decisiones estrat√©gicas.

    Attributes:
        integrity_score (float): Puntuaci√≥n de integridad estructural (0-100).
        waste_alerts (List[str]): Alertas de posible desperdicio (nodos aislados).
        circular_risks (List[str]): Riesgos de c√°lculo circular (ciclos).
        complexity_level (str): Nivel de complejidad (Baja, Media, Alta, Cr√≠tica).
        details (Dict[str, Any]): Metadatos detallados para serializaci√≥n y auditor√≠a.
        financial_risk_level (Optional[str]): Nivel de riesgo financiero.
            ('Bajo', 'Medio', 'Alto', 'CAT√ÅSTROFICO').
        strategic_narrative (Optional[str]): Narrativa estrat√©gica para decisores.
            Representa "La Voz del Consejo" interpretando los datos t√©cnicos.
    """

    integrity_score: float
    waste_alerts: List[str]
    circular_risks: List[str]
    complexity_level: str
    details: Dict[str, Any] = field(default_factory=dict)
    financial_risk_level: Optional[str] = None
    strategic_narrative: Optional[str] = None


class BudgetGraphBuilder:
    """
    Constructor del Grafo de Presupuesto (Topolog√≠a de Negocio).

    Implementa la construcci√≥n jer√°rquica de la estructura piramidal:
    - Nivel 0: Ra√≠z del Proyecto (Apex).
    - Nivel 1: Cap√≠tulos (Pilares Estructurales).
    - Nivel 2: APUs (Cuerpo T√°ctico).
    - Nivel 3: Insumos (Cimentaci√≥n de Recursos).
    """

    def __init__(self):
        """Inicializa el constructor del grafo."""
        self.logger = logging.getLogger(self.__class__.__name__)
        self.ROOT_NODE = "PROYECTO_TOTAL"

    def _sanitize_code(self, value: Any) -> str:
        """
        Sanitiza el c√≥digo o identificador.

        Limpia espacios y normaliza la cadena para asegurar identificadores
        √∫nicos y consistentes en el grafo.

        Args:
            value: Valor a sanitizar.

        Returns:
            str: Cadena limpia y normalizada.
        """
        if pd.isna(value) or value is None:
            return ""
        sanitized = str(value).strip()
        sanitized = " ".join(sanitized.split())
        return sanitized

    def _safe_float(self, value: Any, default: float = 0.0) -> float:
        """
        Convierte un valor a float de manera segura (localizaci√≥n).

        Maneja formatos num√©ricos internacionales (coma vs punto decimal)
        mediante heur√≠sticas de posici√≥n y conteo para robustez.

        Args:
            value: Valor a convertir.
            default: Valor por defecto si falla la conversi√≥n.

        Returns:
            float: Valor num√©rico flotante.
        """
        if pd.isna(value) or value is None:
            return default

        if isinstance(value, (int, float)):
            return float(value)

        try:
            str_value = str(value).strip().replace("\xa0", "").replace(" ", "")

            if not str_value:
                return default

            has_comma = "," in str_value
            has_dot = "." in str_value

            if has_comma and has_dot:
                # Determinar cu√°l es el separador decimal por posici√≥n
                last_comma = str_value.rfind(",")
                last_dot = str_value.rfind(".")

                if last_dot > last_comma:
                    # Formato: 1,234,567.89 (ingl√©s)
                    str_value = str_value.replace(",", "")
                else:
                    # Formato: 1.234.567,89 (europeo)
                    str_value = str_value.replace(".", "").replace(",", ".")

            elif has_comma and not has_dot:
                parts = str_value.split(",")
                # Heur√≠stica: si la √∫ltima parte tiene 1-3 d√≠gitos, es decimal
                if len(parts) == 2 and 1 <= len(parts[1]) <= 3 and parts[1].isdigit():
                    str_value = str_value.replace(",", ".")
                else:
                    # Es separador de miles
                    str_value = str_value.replace(",", "")

            return float(str_value)

        except (ValueError, TypeError, AttributeError):
            return default

    def _create_node_attributes(
        self,
        node_type: str,
        level: int,
        source: str = "generated",
        idx: int = -1,
        inferred: bool = False,
        **kwargs,
    ) -> Dict[str, Any]:
        """
        Crea el diccionario de atributos estandarizado para un nodo.

        Args:
            node_type: Tipo de nodo (ROOT, CAPITULO, APU, INSUMO).
            level: Nivel jer√°rquico (0-3).
            source: Origen del dato (presupuesto, detail, generated).
            idx: √çndice original en el DataFrame.
            inferred: Si el nodo fue inferido y no expl√≠cito.
            **kwargs: Atributos adicionales.

        Returns:
            Dict: Diccionario de atributos del nodo.
        """
        attrs = {
            "type": node_type,
            "level": level,
            "source": source,
            "original_index": idx,
            "inferred": inferred,
        }
        attrs.update(kwargs)
        return attrs

    def _create_apu_attributes(
        self, row: pd.Series, source: str, idx: int, inferred: bool
    ) -> Dict[str, Any]:
        """
        Genera atributos espec√≠ficos para nodos de tipo APU.

        Args:
            row: Fila del DataFrame.
            source: Origen del dato.
            idx: √çndice de fila.
            inferred: Si es inferido.

        Returns:
            Dict: Atributos del nodo APU.
        """
        attrs = self._create_node_attributes(
            node_type="APU", level=2, source=source, idx=idx, inferred=inferred
        )
        if not inferred:
            attrs["description"] = self._sanitize_code(row.get(ColumnNames.DESCRIPCION_APU))
            attrs["quantity"] = self._safe_float(row.get(ColumnNames.CANTIDAD_PRESUPUESTO))
        return attrs

    def _create_insumo_attributes(
        self, row: pd.Series, insumo_desc: str, source: str, idx: int
    ) -> Dict[str, Any]:
        """
        Genera atributos espec√≠ficos para nodos de tipo INSUMO.

        Args:
            row: Fila del DataFrame.
            insumo_desc: Descripci√≥n del insumo.
            source: Origen del dato.
            idx: √çndice de fila.

        Returns:
            Dict: Atributos del nodo INSUMO.
        """
        return self._create_node_attributes(
            node_type="INSUMO",
            level=3,
            source=source,
            idx=idx,
            description=insumo_desc,
            tipo_insumo=self._sanitize_code(row.get(ColumnNames.TIPO_INSUMO)),
            unit_cost=self._safe_float(row.get(ColumnNames.COSTO_INSUMO_EN_APU)),
        )

    def _upsert_edge(
        self, G: nx.DiGraph, u: str, v: str, unit_cost: float, quantity: float, idx: int
    ) -> bool:
        """
        Inserta o actualiza una arista con agregaci√≥n de costos (Upsert).

        Si la arista ya existe, acumula la cantidad y el costo total,
        permitiendo m√∫ltiples ocurrencias del mismo insumo en un APU.

        Args:
            G: Grafo de NetworkX.
            u: Nodo origen.
            v: Nodo destino.
            unit_cost: Costo unitario.
            quantity: Cantidad.
            idx: √çndice original.

        Returns:
            bool: True si se cre√≥ una nueva arista, False si se actualiz√≥.
        """
        total_cost = unit_cost * quantity

        if G.has_edge(u, v):
            edge = G[u][v]
            edge["quantity"] += quantity
            edge["total_cost"] += total_cost
            edge["occurrence_count"] += 1
            if "original_indices" not in edge:
                edge["original_indices"] = []
            edge["original_indices"].append(idx)
            return False

        G.add_edge(
            u,
            v,
            quantity=quantity,
            unit_cost=unit_cost,
            total_cost=total_cost,
            occurrence_count=1,
            original_indices=[idx],
        )
        return True

    def _compute_graph_statistics(self, G: nx.DiGraph) -> Dict[str, int]:
        """
        Calcula estad√≠sticas b√°sicas del grafo construido.

        Args:
            G: Grafo construido.

        Returns:
            Dict: Conteo de nodos por tipo y totales.
        """
        stats = {
            "chapter_count": 0,
            "apu_count": 0,
            "insumo_count": 0,
            "inferred_count": 0,
            "total_nodes": G.number_of_nodes(),
            "total_edges": G.number_of_edges(),
        }

        type_counters = {
            "CAPITULO": "chapter_count",
            "APU": "apu_count",
            "INSUMO": "insumo_count",
        }

        for _, data in G.nodes(data=True):
            node_type = data.get("type")
            if node_type in type_counters:
                stats[type_counters[node_type]] += 1
                if node_type == "APU" and data.get("inferred", False):
                    stats["inferred_count"] += 1

        return stats

    def _process_presupuesto_row(
        self, G: nx.DiGraph, row: pd.Series, idx: int, chapter_cols: List[str]
    ) -> None:
        """
        Procesa una fila del presupuesto para construir la estructura superior.

        Niveles afectados:
        - Nivel 0 (Ra√≠z) -> Nivel 1 (Cap√≠tulo) -> Nivel 2 (APU).
        """
        apu_code = self._sanitize_code(row.get(ColumnNames.CODIGO_APU))
        if not apu_code:
            return

        # Calcular Costo Total del APU para asignar peso a la arista
        # Intentar obtener VALOR_TOTAL_APU directo
        total_cost = self._safe_float(row.get(ColumnNames.VALOR_TOTAL_APU))
        if total_cost == 0.0:
            # Fallback: Cantidad * Precio Unitario
            qty = self._safe_float(row.get(ColumnNames.CANTIDAD_PRESUPUESTO))
            price = self._safe_float(row.get(ColumnNames.PRECIO_UNIT_APU))
            total_cost = qty * price

        # Crear nodo APU (Nivel 2)
        attrs = self._create_apu_attributes(
            row, source="presupuesto", idx=idx, inferred=False
        )
        G.add_node(apu_code, **attrs)

        # Buscar y establecer jerarqu√≠a de cap√≠tulo
        chapter_name = None
        for col in chapter_cols:
            val = self._sanitize_code(row.get(col))
            if val:
                chapter_name = val
                break

        if chapter_name:
            if chapter_name not in G:
                G.add_node(
                    chapter_name,
                    type="CAPITULO",
                    level=1,
                    description=f"Cap√≠tulo: {chapter_name}",
                )
                # Inicializar arista Root -> Cap√≠tulo con peso 0 (se acumular√°)
                G.add_edge(
                    self.ROOT_NODE,
                    chapter_name,
                    relation="CONTAINS",
                    weight=0.0,
                    total_cost=0.0,
                )

            # Acumular costo en arista Root -> Cap√≠tulo
            if G.has_edge(self.ROOT_NODE, chapter_name):
                edge_rc = G[self.ROOT_NODE][chapter_name]
                edge_rc["weight"] = edge_rc.get("weight", 0.0) + total_cost
                edge_rc["total_cost"] = edge_rc.get("total_cost", 0.0) + total_cost

            # Arista Cap√≠tulo -> APU
            G.add_edge(
                chapter_name,
                apu_code,
                relation="CONTAINS",
                weight=total_cost,
                total_cost=total_cost,
            )
        else:
            # Arista Root -> APU (Hu√©rfano de cap√≠tulo)
            G.add_edge(
                self.ROOT_NODE,
                apu_code,
                relation="CONTAINS",
                weight=total_cost,
                total_cost=total_cost,
            )

    def _process_apu_detail_row(self, G: nx.DiGraph, row: pd.Series, idx: int) -> None:
        """
        Procesa una fila de detalle de APU para construir la base.

        Niveles afectados:
        - Nivel 2 (APU) -> Nivel 3 (Insumo).
        """
        apu_code = self._sanitize_code(row.get(ColumnNames.CODIGO_APU))
        insumo_desc = self._sanitize_code(row.get(ColumnNames.DESCRIPCION_INSUMO))

        if not apu_code or not insumo_desc:
            return

        # Inferir APU si no existe (creado din√°micamente)
        if apu_code not in G:
            attrs = self._create_apu_attributes(row, source="detail", idx=idx, inferred=True)
            G.add_node(apu_code, **attrs)
            G.add_edge(self.ROOT_NODE, apu_code, relation="CONTAINS_INFERRED")

        # Crear o reutilizar nodo Insumo
        if insumo_desc not in G:
            attrs = self._create_insumo_attributes(
                row, insumo_desc, source="detail", idx=idx
            )
            G.add_node(insumo_desc, **attrs)

        # Establecer relaci√≥n APU -> Insumo con agregaci√≥n
        qty = self._safe_float(row.get(ColumnNames.CANTIDAD_APU))
        cost = self._safe_float(row.get(ColumnNames.COSTO_INSUMO_EN_APU))
        self._upsert_edge(G, apu_code, insumo_desc, cost, qty, idx)

    def build(
        self, presupuesto_df: pd.DataFrame, apus_detail_df: pd.DataFrame
    ) -> nx.DiGraph:
        """
        Construye el Grafo Piramidal de Presupuesto.

        Integra la estructura de alto nivel (Presupuesto) con el detalle
        t√©cnico (APUs e Insumos) en un solo grafo dirigido.

        Args:
            presupuesto_df: DataFrame del presupuesto general.
            apus_detail_df: DataFrame con el detalle de APUs (an√°lisis unitarios).

        Returns:
            nx.DiGraph: Grafo dirigido representando la topolog√≠a del proyecto.
        """
        G = nx.DiGraph(name="BudgetTopology")
        self.logger.info("Iniciando construcci√≥n del Grafo Piramidal de Presupuesto...")

        # Nivel 0: Nodo Ra√≠z (El √Åpice)
        G.add_node(self.ROOT_NODE, type="ROOT", level=0, description="Proyecto Completo")

        # Columnas candidatas para identificar cap√≠tulos
        chapter_cols = ["CAPITULO", "CATEGORIA", "TITULO"]

        # Niveles 1 y 2: Procesar Presupuesto (Estructura y T√°ctica)
        if presupuesto_df is not None and not presupuesto_df.empty:
            available_chapter_cols = [c for c in chapter_cols if c in presupuesto_df.columns]
            for idx, row in presupuesto_df.iterrows():
                self._process_presupuesto_row(G, row, idx, available_chapter_cols)

        # Nivel 3: Procesar Detalle de APUs (Cimentaci√≥n)
        if apus_detail_df is not None and not apus_detail_df.empty:
            for idx, row in apus_detail_df.iterrows():
                self._process_apu_detail_row(G, row, idx)

        stats = self._compute_graph_statistics(G)
        self.logger.info(f"Grafo Piramidal construido: {stats}")
        return G


class BusinessTopologicalAnalyzer:
    """
    Analizador de Topolog√≠a de Negocio V2.

    Fusiona el rigor matem√°tico (Topolog√≠a Algebraica, An√°lisis Espectral)
    con una narrativa de ingenier√≠a civil ("La Voz del Consejo").

    Responsabilidades:
    - Calcular invariantes topol√≥gicos (Betti numbers).
    - Analizar estabilidad espectral y resonancia.
    - Detectar ciclos y sinergia de riesgos.
    - Modelar flujo t√©rmico (volatilidad).
    - Generar reportes estrat√©gicos.
    """

    def __init__(self, telemetry: Optional[TelemetryContext] = None, max_cycles: int = 100):
        """
        Inicializa el analizador.

        Args:
            telemetry: Contexto de telemetr√≠a opcional.
            max_cycles: L√≠mite m√°ximo de ciclos a detectar para evitar timeout.
        """
        self.telemetry = telemetry
        self.max_cycles = max_cycles
        self.logger = logging.getLogger(self.__class__.__name__)

    def analyze_spectral_stability(self, graph: nx.DiGraph) -> Dict[str, Any]:
        """
        Calcula la Estabilidad Espectral y m√©tricas del Laplaciano.

        Fundamentos Matem√°ticos:
        - Fiedler Value (Œª‚ÇÇ): Conectividad algebraica. Œª‚ÇÇ > 0 implica conexi√≥n.
        - Gap Espectral: Œª‚ÇÇ / Œª_max. Proxy de la capacidad de sincronizaci√≥n.
        - Energ√≠a Espectral: ||L||_F¬≤. Dispersi√≥n estructural.

        Args:
            graph: Grafo a analizar.

        Returns:
            Dict: M√©tricas espectrales y diagn√≥stico de resonancia.
        """
        n_nodes = graph.number_of_nodes()

        default_result = {
            "fiedler_value": 0.0,
            "spectral_gap": 0.0,
            "spectral_energy": 0.0,
            "wavelength": float("inf"),
            "resonance_risk": False,
            "algebraic_connectivity": 0.0,
            "eigenvalues": [],
            "status": "insufficient_nodes",
        }

        if n_nodes < 2:
            return default_result

        try:
            # Convertir a no dirigido para an√°lisis espectral sim√©trico
            ud_graph = graph.to_undirected()

            # Remover nodos aislados para estabilidad num√©rica
            isolated = list(nx.isolates(ud_graph))
            if isolated:
                ud_graph = ud_graph.copy()
                ud_graph.remove_nodes_from(isolated)

            n_effective = ud_graph.number_of_nodes()
            if n_effective < 2:
                default_result["status"] = "degenerate_after_isolation_removal"
                return default_result

            # Laplaciano Normalizado: L = I - D^(-1/2) A D^(-1/2)
            L = nx.normalized_laplacian_matrix(ud_graph).astype(np.float64)

            # Energ√≠a Espectral (Norma de Frobenius)
            if scipy.sparse.issparse(L):
                spectral_energy = float(scipy.sparse.linalg.norm(L, "fro") ** 2)
            else:
                spectral_energy = float(np.linalg.norm(L, "fro") ** 2)

            # C√°lculo de Eigenvalores
            if n_effective <= 50:
                # M√©todo denso para grafos peque√±os
                eigenvalues = np.linalg.eigvalsh(L.toarray())
                eigenvalues = np.sort(np.real(eigenvalues))
            else:
                # M√©todo sparse para grafos grandes
                k_small = min(n_effective - 1, 6)
                try:
                    eigenvalues_small = scipy.sparse.linalg.eigsh(
                        L,
                        k=k_small,
                        which="SM",
                        return_eigenvectors=False,
                        tol=1e-6,
                        maxiter=n_effective * 100,
                    )
                except scipy.sparse.linalg.ArpackNoConvergence as e:
                    eigenvalues_small = e.eigenvalues

                try:
                    eigenvalues_large = scipy.sparse.linalg.eigsh(
                        L, k=1, which="LM", return_eigenvectors=False
                    )
                except scipy.sparse.linalg.ArpackNoConvergence as e:
                    eigenvalues_large = (
                        e.eigenvalues if len(e.eigenvalues) > 0 else np.array([2.0])
                    )

                eigenvalues = np.sort(np.concatenate([eigenvalues_small, eigenvalues_large]))

            if len(eigenvalues) < 2:
                default_result["status"] = "insufficient_eigenvalues"
                return default_result

            # Fiedler Value: Œª‚ÇÇ
            fiedler_value = float(eigenvalues[1]) if len(eigenvalues) >= 2 else 0.0
            if fiedler_value < 1e-9:
                fiedler_value = 0.0

            # Œª_max (acotado por 2 en Laplaciano normalizado)
            lambda_max = float(eigenvalues[-1]) if len(eigenvalues) > 0 else 2.0
            lambda_max = min(lambda_max, 2.0)

            # Gap Espectral Normalizado
            spectral_gap = fiedler_value / lambda_max if lambda_max > 1e-10 else 0.0

            # Longitud de Onda (escala de difusi√≥n)
            wavelength = 1.0 / lambda_max if lambda_max > 1e-10 else float("inf")

            # Riesgo de Resonancia: baja varianza en eigenvalores
            if len(eigenvalues) > 2:
                eigen_std = np.std(eigenvalues)
                eigen_mean = np.mean(eigenvalues)
                cv = eigen_std / eigen_mean if eigen_mean > 1e-10 else 0.0
                resonance_risk = cv < 0.15
            else:
                resonance_risk = True

            return {
                "fiedler_value": round(fiedler_value, 6),
                "spectral_gap": round(spectral_gap, 6),
                "spectral_energy": round(spectral_energy, 4),
                "wavelength": round(wavelength, 6),
                "resonance_risk": bool(resonance_risk),
                "algebraic_connectivity": round(fiedler_value, 6),
                "lambda_max": round(lambda_max, 6),
                "isolated_nodes_removed": len(isolated),
                "eigenvalues": [round(float(e), 6) for e in eigenvalues[:8]],
                "status": "success",
            }

        except Exception as e:
            self.logger.error(f"Error en an√°lisis espectral: {e}", exc_info=True)
            default_result["status"] = f"error: {str(e)}"
            return default_result

    def calculate_euler_efficiency(self, graph: nx.DiGraph, weighted: bool = False) -> float:
        """
        Calcula la Eficiencia de Euler mediante decaimiento exponencial.

        Args:
            graph: Grafo a analizar.
            weighted: Si es True, considera el peso de las aristas (simulado).

        Returns:
            float: Eficiencia normalizada (0.0 - 1.0).
        """
        n_nodes = graph.number_of_nodes()
        n_edges = graph.number_of_edges()

        if n_nodes == 0:
            return 1.0

        # Si no es d√©bilmente conexo y tiene > 1 nodos, la eficiencia estructural es 0
        # (Excepci√≥n: nodo √∫nico n=1 siempre es conexo y eficiente)
        if n_nodes > 1 and not nx.is_weakly_connected(graph):
            return 0.0

        min_edges = max(0, n_nodes - 1)
        excess_edges = max(0, n_edges - min_edges)

        # Base efficiency
        efficiency = np.exp(-excess_edges / n_nodes) if n_nodes > 0 else 1.0

        if weighted:
            # En modo ponderado, aristas cr√≠ticas (peso > 1) penalizan m√°s
            # Simulamos esto reduciendo la eficiencia si hay aristas pesadas
            has_heavy_edges = any(d.get('weight', 1.0) > 1.0 for u, v, d in graph.edges(data=True))
            if has_heavy_edges:
                efficiency *= 0.8  # Penalizaci√≥n significativa por ruta cr√≠tica
            else:
                efficiency *= 0.95 # Penalizaci√≥n base por weighted flag

        return efficiency # Sin redondeo para precisi√≥n matem√°tica

    def calculate_betti_numbers(self, graph: nx.DiGraph) -> TopologicalMetrics:
        """
        Calcula los N√∫meros de Betti (Invariantes Topol√≥gicos).

        Args:
            graph: Grafo a analizar.

        Returns:
            TopologicalMetrics: beta_0 (componentes), beta_1 (ciclos).
        """
        if graph.number_of_nodes() == 0:
            return TopologicalMetrics(0, 0, 0, 1.0)

        # Usar MultiGraph para preservar aristas y calcular beta_1 correctamente
        undirected = nx.MultiGraph()
        undirected.add_nodes_from(graph.nodes(data=True))
        undirected.add_edges_from(graph.edges(data=True))

        beta_0 = nx.number_connected_components(undirected)
        n_edges = undirected.number_of_edges()
        n_nodes = undirected.number_of_nodes()

        # F√≥rmula de Euler-Poincar√©: œá = V - E = beta_0 - beta_1
        # beta_1 = E - V + beta_0
        beta_1 = max(0, n_edges - n_nodes + beta_0)
        euler_char = beta_0 - beta_1
        efficiency = self.calculate_euler_efficiency(graph)

        return TopologicalMetrics(
            beta_0=beta_0,
            beta_1=beta_1,
            euler_characteristic=euler_char,
            euler_efficiency=efficiency,
        )

    def calculate_pyramid_stability(self, graph: nx.DiGraph) -> float:
        """
        Calcula el √çndice de Estabilidad Piramidal (Œ®).

        Modelo F√≠sico:
        - Una pir√°mide estable tiene base ancha (insumos) y c√∫spide estrecha (APUs).
        - Œ® ‚àà [0, 1], donde 1 = m√°xima estabilidad.

        Args:
            graph: Grafo a analizar.

        Returns:
            float: √çndice de estabilidad normalizado.
        """
        if graph.number_of_nodes() == 0:
            return 0.0

        type_counts = {"APU": 0, "INSUMO": 0, "CAPITULO": 0}
        for _, data in graph.nodes(data=True):
            node_type = data.get("type", "")
            if node_type in type_counts:
                type_counts[node_type] += 1

        num_apus = type_counts["APU"]
        num_insumos = type_counts["INSUMO"]

        # Casos degenerados
        if num_apus == 0:
            return 0.0 if num_insumos == 0 else 0.5
        if num_insumos == 0:
            return 0.3

        # 1. Ratio de Base (Escalamiento logar√≠tmico)
        base_ratio = num_insumos / num_apus
        ratio_score = min(1.0, np.log10(1 + base_ratio) / np.log10(11))

        # 2. Factor de Densidad
        density = nx.density(graph)
        if density < 0.001:
            density_score = 0.5  # Muy disperso
        elif density > 0.5:
            density_score = 0.3  # Muy denso
        else:
            density_score = 1.0 - min(0.7, density)

        # 3. Factor de Aciclicidad
        is_dag = nx.is_directed_acyclic_graph(graph)
        if is_dag:
            acyclic_score = 1.0
        else:
            # Penalizaci√≥n por componentes fuertemente conexos
            sccs = [c for c in nx.strongly_connected_components(graph) if len(c) > 1]
            cycle_penalty = min(0.5, len(sccs) * 0.1)
            acyclic_score = 0.5 - cycle_penalty

        # 4. Factor de Conectividad
        if nx.is_weakly_connected(graph):
            connectivity_score = 1.0
        else:
            num_components = nx.number_weakly_connected_components(graph)
            connectivity_score = 1.0 / num_components

        # Ponderaci√≥n
        weights = {
            "ratio": 0.35,
            "density": 0.20,
            "acyclic": 0.30,
            "connectivity": 0.15,
        }

        stability = (
            weights["ratio"] * ratio_score
            + weights["density"] * density_score
            + weights["acyclic"] * acyclic_score
            + weights["connectivity"] * connectivity_score
        )

        return round(max(0.0, min(1.0, stability)), 4)

    def audit_integration_homology(
        self, graph_a: nx.DiGraph, graph_b: nx.DiGraph
    ) -> Dict[str, Any]:
        """
        Ejecuta el Test de Mayer-Vietoris para auditor√≠a de fusi√≥n.

        Eval√∫a si la uni√≥n de dos estructuras (A ‚à™ B) introduce complejidad
        c√≠clica no presente en los componentes originales (A, B).

        Args:
            graph_a: Primer grafo (ej. Presupuesto).
            graph_b: Segundo grafo (ej. Detalle APUs).

        Returns:
            Dict: Diagn√≥stico de la fusi√≥n.
        """
        metrics_a = self.calculate_betti_numbers(graph_a)
        metrics_b = self.calculate_betti_numbers(graph_b)

        graph_union = nx.compose(graph_a, graph_b)
        metrics_union = self.calculate_betti_numbers(graph_union)

        nodes_a = set(graph_a.nodes())
        nodes_b = set(graph_b.nodes())
        common_nodes = nodes_a & nodes_b

        # Grafo Intersecci√≥n (A ‚à© B)
        graph_intersection = nx.DiGraph()
        if common_nodes:
            graph_intersection.add_nodes_from((n, graph_a.nodes[n]) for n in common_nodes)
            for u, v in graph_a.edges():
                if u in common_nodes and v in common_nodes and graph_b.has_edge(u, v):
                    graph_intersection.add_edge(u, v)

        metrics_intersection = self.calculate_betti_numbers(graph_intersection)

        # C√°lculo de Mayer-Vietoris para delta_beta_1
        delta_connectivity = (
            metrics_intersection.beta_0
            - metrics_a.beta_0
            - metrics_b.beta_0
            + metrics_union.beta_0
        )

        beta1_theoretical = (
            metrics_a.beta_1
            + metrics_b.beta_1
            - metrics_intersection.beta_1
            + max(0, delta_connectivity)
        )

        emergent_observed = metrics_union.beta_1 - (metrics_a.beta_1 + metrics_b.beta_1)
        discrepancy = abs(metrics_union.beta_1 - beta1_theoretical)

        if discrepancy <= 1:
            if emergent_observed > 0:
                verdict = "INTEGRATION_CONFLICT"
                severity = "warning"
            elif emergent_observed < 0:
                verdict = "TOPOLOGY_SIMPLIFIED"
                severity = "info"
            else:
                verdict = "CLEAN_MERGE"
                severity = "success"
        else:
            verdict = "INCONSISTENT_TOPOLOGY"
            severity = "error"

        # Identificar nodos frontera
        boundary_nodes = []
        for node in common_nodes:
            neighbors_a = set(graph_a.successors(node)) | set(graph_a.predecessors(node))
            neighbors_b = set(graph_b.successors(node)) | set(graph_b.predecessors(node))
            if (neighbors_a - nodes_b) | (neighbors_b - nodes_a):
                boundary_nodes.append(node)

        narrative = self._generate_mayer_vietoris_narrative(
            emergent_observed, discrepancy, len(boundary_nodes)
        )

        return {
            "status": verdict,
            "severity": severity,
            "delta_beta_1": emergent_observed,
            "beta_1_observed": metrics_union.beta_1,
            "beta_1_theoretical": beta1_theoretical,
            "discrepancy": discrepancy,
            "boundary_nodes": boundary_nodes,
            "details": {
                "beta_1_A": metrics_a.beta_1,
                "beta_1_B": metrics_b.beta_1,
                "beta_1_intersection": metrics_intersection.beta_1,
                "beta_1_union": metrics_union.beta_1,
                "common_nodes_count": len(common_nodes),
            },
            "narrative": narrative,
        }

    def _generate_mayer_vietoris_narrative(
        self, observed: int, discrepancy: float, boundary_count: int
    ) -> str:
        """Genera la narrativa del an√°lisis Mayer-Vietoris."""
        parts = []

        if discrepancy > 2:
            parts.append(
                f"‚ö†Ô∏è ANOMAL√çA TOPOL√ìGICA: Discrepancia significativa (Œî={discrepancy:.1f}). "
                f"Revisar coherencia en {boundary_count} nodos de frontera."
            )
        elif discrepancy > 1:
            parts.append(f"‚ö†Ô∏è Discrepancia menor (Œî={discrepancy:.1f}). Posible redundancia.")

        if observed > 0:
            parts.append(
                f"üö® CONFLICTO DE INTEGRACI√ìN: {observed} nuevo(s) ciclo(s) emergentes."
            )
        elif observed < 0:
            parts.append(
                f"‚úÖ SIMPLIFICACI√ìN TOPOL√ìGICA: Eliminaci√≥n de {abs(observed)} ciclo(s)."
            )
        elif discrepancy <= 1:
            parts.append("‚úÖ FUSI√ìN LIMPIA: Integraci√≥n topol√≥gicamente neutral.")

        return " ".join(parts) if parts else "An√°lisis completado."

    def _get_raw_cycles(self, graph: nx.DiGraph) -> Tuple[List[List[str]], bool]:
        """Obtiene los ciclos crudos del grafo."""
        cycles = []
        truncated = False
        try:
            cycle_generator = nx.simple_cycles(graph)
            max_cycle_length = 10
            for count, cycle in enumerate(cycle_generator):
                if count >= self.max_cycles:
                    truncated = True
                    self.logger.warning(f"Truncado de ciclos en {self.max_cycles}")
                    break
                if len(cycle) <= max_cycle_length:
                    cycles.append(cycle)
        except Exception as e:
            self.logger.error(f"Error en detecci√≥n de ciclos: {e}")

        cycles.sort(key=len)
        return cycles, truncated

    def detect_risk_synergy(
        self, graph: nx.DiGraph, raw_cycles: Optional[List[List[str]]] = None, weighted: bool = False
    ) -> Dict[str, Any]:
        """
        Detecta Sinergia de Riesgo (Efecto Domin√≥).

        Identifica si m√∫ltiples ciclos comparten nodos cr√≠ticos (puentes),
        lo que amplifica el riesgo de fallos en cascada.

        Args:
            graph: Grafo a analizar.
            raw_cycles: Ciclos pre-calculados (opcional).
            weighted: Si es True, considera el peso de los nodos puente.

        Returns:
            Dict: An√°lisis de sinergia y nodos puente.
        """
        if raw_cycles is None:
            raw_cycles, _ = self._get_raw_cycles(graph)

        default_result = {
            "synergy_detected": False,
            "synergy_score": 0.0,
            "risk_level": "NINGUNO",
            "bridge_nodes": [],
            "intersecting_cycles_count": 0,
            "synergy_strength": 0.0, # Added for tests
        }

        if len(raw_cycles) < 2:
            return default_result

        # Centralidad de intermediaci√≥n (Approx para grafos grandes)
        try:
            k = min(100, graph.number_of_nodes())
            betweenness = nx.betweenness_centrality(graph, normalized=True, k=k)
        except Exception:
            betweenness = {n: 0.0 for n in graph.nodes()}

        # Umbral adaptativo
        vals = list(betweenness.values())
        threshold = np.percentile(vals, 75) if len(vals) >= 4 else np.mean(vals)
        critical_nodes = {n for n, c in betweenness.items() if c >= threshold and c > 0}

        synergy_pairs = []
        bridge_occ = {}

        cycle_sets = [set(c) for c in raw_cycles]
        n_cycles = len(cycle_sets)

        for i in range(n_cycles):
            for j in range(i + 1, n_cycles):
                inter = cycle_sets[i] & cycle_sets[j]
                # Se requiere compartir al menos 2 nodos (una arista) para que haya sinergia real
                # o si weighted=True, permitimos nodos cr√≠ticos como puentes
                if len(inter) >= 2 or (weighted and (inter & critical_nodes)):
                    synergy_pairs.append((i, j))
                    for node in inter:
                        bridge_occ.setdefault(node, []).append((i, j))

        if not synergy_pairs:
            return default_result

        # Ranking de puentes
        bridge_nodes = sorted(
            [
                {
                    "id": n,
                    "cycles_connected": len(lst),
                    "betweenness": round(betweenness.get(n, 0), 4),
                }
                for n, lst in bridge_occ.items()
            ],
            key=lambda x: (x["cycles_connected"], x["betweenness"]),
            reverse=True,
        )

        # C√°lculo de Score
        total_pairs = n_cycles * (n_cycles - 1) / 2
        pair_ratio = len(synergy_pairs) / total_pairs if total_pairs > 0 else 0
        synergy_score = round(min(1.0, pair_ratio * 2.0), 4)  # Heur√≠stica simple

        if weighted:
            synergy_score *= 1.1 # Modifier for weighted calculation

        risk_level = "BAJO"
        if synergy_score > 0.6:
            risk_level = "CR√çTICO"
        elif synergy_score > 0.3:
            risk_level = "ALTO"
        elif synergy_score > 0.1:
            risk_level = "MEDIO"

        return {
            "synergy_detected": True,
            "synergy_score": synergy_score,
            "risk_level": risk_level,
            "bridge_nodes": bridge_nodes[:10],
            "intersecting_cycles_count": len(synergy_pairs),
            "synergy_strength": synergy_score, # Alias for tests
        }

    def analyze_thermal_flow(self, graph: nx.DiGraph) -> Dict[str, Any]:
        """
        Calcula el Flujo T√©rmico Estructural (Modelo de Difusi√≥n).

        Simula c√≥mo la volatilidad de precios (temperatura) se propaga
        desde los insumos (hojas) hacia el proyecto total (ra√≠z).

        Args:
            graph: Grafo del presupuesto.

        Returns:
            Dict: Mapa t√©rmico del proyecto.
        """
        # Temperaturas base seg√∫n tipo de insumo (Volatilidad estimada)
        BASE_TEMPERATURES = {
            "COMBUSTIBLE": 95.0,
            "ACERO": 85.0,
            "CEMENTO": 60.0,
            "TRANSPORTE": 75.0,
            "MANO DE OBRA": 25.0,
        }
        DEFAULT_TEMP = 30.0

        def get_temp(desc: str, tipo: str) -> float:
            txt = f"{desc} {tipo}".upper()
            t = DEFAULT_TEMP
            for k, v in BASE_TEMPERATURES.items():
                if k in txt:
                    t = max(t, v)
            return t

        node_temps = {}
        node_costs = {}

        # 1. Asignar temperatura a hojas (Insumos)
        for node, data in graph.nodes(data=True):
            if data.get("type") == "INSUMO":
                desc = str(data.get("description", ""))
                tipo = str(data.get("tipo_insumo", ""))
                node_temps[node] = get_temp(desc, tipo)
                # Costo acumulado en aristas entrantes
                total = sum(
                    graph[pred][node].get("total_cost", 0.0)
                    for pred in graph.predecessors(node)
                )
                node_costs[node] = max(total, data.get("unit_cost", 0.0))
            else:
                node_temps[node] = 0.0
                node_costs[node] = 0.0

        # 2. Propagaci√≥n Bottom-Up (Hojas -> Ra√≠z)
        try:
            topo_order = list(reversed(list(nx.topological_sort(graph))))
        except nx.NetworkXUnfeasible:
            # Fallback para ciclos: ordenar por nivel inverso
            topo_order = sorted(
                graph.nodes(), key=lambda n: graph.nodes[n].get("level", 0), reverse=True
            )

        for node in topo_order:
            if graph.nodes[node].get("type") == "INSUMO":
                continue

            children = list(graph.successors(node))
            if not children:
                node_temps[node] = DEFAULT_TEMP * 0.5
                continue

            weighted_sum = 0.0
            cost_sum = 0.0

            for child in children:
                edge = graph[node][child]
                w = edge.get("total_cost", edge.get("weight", 0.0))
                # Fallback si peso es 0
                if w == 0.0:
                    w = node_costs.get(child, 1.0)

                t = node_temps.get(child, DEFAULT_TEMP)
                weighted_sum += t * w
                cost_sum += w

            if cost_sum > 0:
                node_temps[node] = weighted_sum / cost_sum
                node_costs[node] = cost_sum
            else:
                node_temps[node] = DEFAULT_TEMP * 0.5

        # 3. Temperatura del Sistema Ponderada (T_sys)
        # Esto es vital: No es un promedio simple. Los nodos con mayor costo (peso)
        # aportan m√°s temperatura al sistema.

        total_project_cost = 0.0
        weighted_temp_sum = 0.0

        for node in graph.nodes():
            if graph.nodes[node].get("type") == "APU":
                # Ensure we have the cost from node_costs or data
                cost = node_costs.get(node, 0.0)
                temp = node_temps.get(node, 0.0)

                weighted_temp_sum += cost * temp
                total_project_cost += cost

        if total_project_cost > 0:
            system_temp = weighted_temp_sum / total_project_cost
        else:
            system_temp = 25.0 # Temperatura ambiente base

        # Clasificaci√≥n
        risk = "BAJO"
        if system_temp > 70:
            risk = "CR√çTICO"
        elif system_temp > 50:
            risk = "ALTO"
        elif system_temp > 35:
            risk = "MEDIO"

        # Identificar hotspots reales (> 50 grados)
        hotspots = [
            {"id": n, "temp": round(t, 2)} for n, t in node_temps.items() if t > 50.0
        ]
        hotspots.sort(key=lambda x: x["temp"], reverse=True)

        return {
            "system_temperature": round(system_temp, 2),
            "thermal_risk_level": risk,
            "hotspots": hotspots,
            "node_temperatures": node_temps, # Exponer mapa completo para visualizaci√≥n detallada
        }

    def analyze_inflationary_convection(
        self, graph: nx.DiGraph, fluid_nodes: List[str]
    ) -> Dict[str, Any]:
        """
        Analiza el contagio inflacionario por convecci√≥n.

        Args:
            graph: Grafo del presupuesto.
            fluid_nodes: Nodos que act√∫an como fluido (ej. Transporte).

        Returns:
            Dict: Impacto convectivo.
        """
        convection_impact = {}
        affected_nodes = []

        # Calculate impact per node
        # For simplicity in this V2 implementation, we check the ratio of fluid cost / total cost
        # for each affected node.

        for node in graph.nodes():
            if graph.nodes[node].get("type") != "APU":
                continue

            total_cost = 0.0
            fluid_cost = 0.0

            for succ in graph.successors(node):
                edge = graph[node][succ]
                cost = edge.get("total_cost", edge.get("weight", 0.0))
                total_cost += cost

                if succ in fluid_nodes:
                    fluid_cost += cost

            if total_cost > 0:
                impact = fluid_cost / total_cost
                if impact > 0:
                    convection_impact[node] = impact
                    if impact > 0.2:  # Threshold for high risk
                        affected_nodes.append(node)

        return {
            "affected_nodes_count": len(affected_nodes),
            "high_risk_nodes": affected_nodes,
            "convection_impact": convection_impact,
        }

    def _classify_anomalous_nodes(self, graph: nx.DiGraph) -> Dict[str, List[Dict]]:
        """Clasifica nodos an√≥malos (aislados, hu√©rfanos)."""
        res = {"isolated_nodes": [], "orphan_insumos": [], "empty_apus": []}
        for n, d in graph.nodes(data=True):
            if d.get("type") == "ROOT":
                continue
            ind = graph.in_degree(n)
            outd = graph.out_degree(n)
            info = d.copy()
            info.update({"id": n, "in_degree": ind, "out_degree": outd})

            if ind == 0 and outd == 0:
                res["isolated_nodes"].append(info)
                if d.get("type") == "INSUMO":
                    res["orphan_insumos"].append(info)
            elif d.get("type") == "INSUMO" and ind == 0:
                res["orphan_insumos"].append(info)
            elif d.get("type") == "APU" and outd == 0:
                res["empty_apus"].append(info)
        return res

    def generate_executive_report(
        self, graph: nx.DiGraph, financial_metrics: Optional[Dict[str, Any]] = None
    ) -> ConstructionRiskReport:
        """
        Genera el Reporte Ejecutivo con Scoring Bayesiano.

        Integra m√©tricas topol√≥gicas, espectrales, t√©rmicas y financieras
        en un veredicto unificado.

        Args:
            graph: Grafo del proyecto.
            financial_metrics: M√©tricas financieras externas.

        Returns:
            ConstructionRiskReport: Informe consolidado.
        """
        metrics = self.calculate_betti_numbers(graph)
        raw_cycles, truncated = self._get_raw_cycles(graph)
        synergy = self.detect_risk_synergy(graph, raw_cycles)
        pyramid_stability = self.calculate_pyramid_stability(graph)
        spectral = self.analyze_spectral_stability(graph)
        thermal = self.analyze_thermal_flow(graph)
        anomalies = self._classify_anomalous_nodes(graph)

        # --- Scoring ---
        euler_factor = metrics.euler_efficiency
        stability_factor = pyramid_stability
        density = nx.density(graph) if graph.number_of_nodes() > 0 else 0
        density_factor = 1.0 - min(0.5, density)

        base_score = 100.0 * (
            0.30 * euler_factor + 0.25 * stability_factor + 0.20 * density_factor + 0.25
        )

        # Penalizaciones
        penalty = 0.0
        if metrics.beta_1 > 0:
            penalty += min(25.0, metrics.beta_1 * 5.0)
        if synergy["synergy_detected"]:
            penalty += min(20.0, synergy["synergy_score"] * 30.0)
        if spectral.get("resonance_risk"):
            penalty += 10.0

        integrity_score = max(0.0, min(100.0, base_score - penalty))

        # Niveles
        complexity_level = "Baja"
        if integrity_score < 40:
            complexity_level = "CR√çTICA"
        elif integrity_score < 70:
            complexity_level = "Alta"
        elif integrity_score < 85:
            complexity_level = "Media"

        # Riesgos y Alertas
        waste_alerts = []
        if anomalies["isolated_nodes"]:
            waste_alerts.append(f"üö® {len(anomalies['isolated_nodes'])} nodo(s) aislado(s).")

        circular_risks = []
        if metrics.beta_1 > 0:
            circular_risks.append(f"üö® {metrics.beta_1} ciclo(s) detectado(s).")
        if synergy["synergy_detected"]:
            circular_risks.append(f"‚ò£Ô∏è Riesgo Sist√©mico: {synergy['risk_level']}")

        # Narrativa
        narrative = self._generate_strategic_narrative(
            metrics, synergy, pyramid_stability, None, thermal, spectral
        )

        return ConstructionRiskReport(
            integrity_score=round(integrity_score, 1),
            waste_alerts=waste_alerts,
            circular_risks=circular_risks,
            complexity_level=complexity_level,
            financial_risk_level="Pendiente",
            strategic_narrative=narrative,
            details={
                "metrics": asdict(metrics),
                "pyramid_stability": pyramid_stability,
                "synergy_risk": synergy,
                "anomalies": anomalies,
                "thermal": thermal,
                "spectral": spectral,
                "spectral_analysis": spectral,  # Alias for V3.0 tests
                "density": density,
                "connectivity": {"is_dag": nx.is_directed_acyclic_graph(graph)},
                "cycles": [" -> ".join(c) for c in raw_cycles[:5]],
                "convection_risk": {"high_risk_nodes": []},  # Placeholder
            },
        )

    def _generate_strategic_narrative(
        self,
        metrics: TopologicalMetrics,
        synergy: Dict,
        stability: float,
        financial_risk: Any,
        thermal: Dict,
        spectral: Dict,
    ) -> str:
        """Genera la narrativa estrat√©gica ("La Voz del Consejo")."""
        sections = []

        # 1. Diagn√≥stico Estructural
        if stability > 0.7:
            sections.append("üèóÔ∏è **ESTRUCTURA S√ìLIDA**: Base robusta.")
        elif stability > 0.4:
            sections.append("‚ö†Ô∏è **ESTRUCTURA MODERADA**: Oportunidades de mejora.")
        else:
            sections.append("üö® **PIR√ÅMIDE INVERTIDA**: Cimentaci√≥n insuficiente.")

        # 2. Integridad L√≥gica
        if metrics.beta_1 > 0:
            sections.append(f"üîÑ **COMPLEJIDAD C√çCLICA**: {metrics.beta_1} ciclos.")
        else:
            sections.append("‚úÖ **TRAZABILIDAD LIMPIA**: Flujo ac√≠clico.")

        # 3. Riesgo T√©rmico
        if thermal.get("thermal_risk_level") in ["ALTO", "CR√çTICO"]:
            t = thermal.get("system_temperature", 0)
            sections.append(f"üå°Ô∏è **FIEBRE INFLACIONARIA**: Temp. sist√©mica {t}¬∞.")

        return " ".join(sections)

    def analyze_structural_integrity(self, graph: nx.DiGraph) -> Dict[str, Any]:
        """
        Analiza la integridad estructural del grafo (Compatibilidad V2).

        M√©todo wrapper para mantener compatibilidad con tests antiguos que
        esperan una estructura plana de m√©tricas.

        Args:
            graph: Grafo a analizar.

        Returns:
            Dict: M√©tricas estructurales.
        """
        report = self.generate_executive_report(graph)
        metrics = report.details["metrics"]

        # Estructura plana legacy
        result = {
            "business.betti_b0": metrics["beta_0"],
            "business.betti_b1": metrics["beta_1"],
            "business.euler_characteristic": metrics["euler_characteristic"],
            "business.cycles_count": metrics["beta_1"],  # Aprox
            "business.is_dag": 1 if report.details["connectivity"]["is_dag"] else 0,
            "business.isolated_count": len(report.details["anomalies"]["isolated_nodes"]),
            "business.empty_apus_count": len(report.details["anomalies"]["empty_apus"]),
            "details": report.details,
        }

        # A√±adir secciones extra que esperan los tests
        result["details"]["critical_resources"] = self._identify_critical_resources(graph)
        result["details"]["graph_summary"] = {
            "node_count": graph.number_of_nodes(),
            "edge_count": graph.number_of_edges(),
        }

        # Mapear topology en details
        result["details"]["topology"] = {"betti_numbers": metrics}

        if self.telemetry:
            self.telemetry.record_metric("business_topology.analysis_complete", 1)

        return result

    def get_audit_report(self, analysis_result: Dict[str, Any]) -> List[str]:
        """
        Genera un reporte de auditor√≠a en formato lista de strings (Legacy).

        Args:
            analysis_result: Resultado de analyze_structural_integrity.

        Returns:
            List[str]: L√≠neas del reporte.
        """
        lines = [
            "=== AUDITORIA ESTRUCTURAL ===",
            f"Integridad: {analysis_result.get('integrity_score', 'N/A')}",
            f"Ciclos de Costo: {analysis_result.get('business.betti_b1', 0)}",
            f"Eficiencia de Euler: {analysis_result.get('details', {}).get('metrics', {}).get('euler_efficiency', 0.0)}",
        ]

        if analysis_result.get("business.betti_b1", 0) > 0:
            lines.append("ALERTA: Estructura Circular Detectada (CRITICAS)")

        if analysis_result.get("business.isolated_count", 0) > 0:
            lines.append("ADVERTENCIA: Nodos aislados detectados (desperdicio potential)")

        lines.append("Puntuacion de Integridad: Calculada")

        return lines

    def _identify_critical_resources(self, graph: nx.DiGraph, top_n: int = 5) -> List[Dict]:
        """Identifica recursos cr√≠ticos (Legacy Helper)."""
        resources = []
        for n, d in graph.nodes(data=True):
            if d.get("type") == "INSUMO":
                in_degree = graph.in_degree(n)
                if in_degree > 0:
                    resources.append({"id": n, "in_degree": in_degree})

        resources.sort(key=lambda x: x["in_degree"], reverse=True)
        return resources[:top_n]

    def _compute_connectivity_analysis(self, graph: nx.DiGraph) -> Dict[str, Any]:
        """An√°lisis de conectividad (Legacy Helper)."""
        return {
            "is_dag": nx.is_directed_acyclic_graph(graph),
            "num_wcc": nx.number_weakly_connected_components(graph),
            "is_weakly_connected": nx.is_weakly_connected(graph),
            "num_non_trivial_scc": len(
                [c for c in nx.strongly_connected_components(graph) if len(c) > 1]
            ),
            "non_trivial_scc": [
                list(c) for c in nx.strongly_connected_components(graph) if len(c) > 1
            ],
        }

    def _interpret_topology(self, metrics: TopologicalMetrics) -> Dict[str, str]:
        """Interpreta m√©tricas (Legacy Helper)."""
        return {
            "beta_0": f"{metrics.beta_0} componente(s) conexo(s)"
            if metrics.beta_0 == 1
            else f"{metrics.beta_0} componentes desconexas",
            "beta_1": f"{metrics.beta_1} ciclo(s)"
            if metrics.beta_1 > 0
            else "Ac√≠clico (Sin ciclos)",
        }

    def _detect_cycles(self, graph: nx.DiGraph) -> Tuple[List[str], bool]:
        """Legacy wrapper for cycle detection."""
        raw, truncated = self._get_raw_cycles(graph)
        # Convert list of nodes to string representation "A -> B -> A"
        formatted = [" -> ".join(map(str, c + [c[0]])) for c in raw]
        return formatted, truncated
