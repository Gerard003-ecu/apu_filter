# APU Filter: Plataforma de Inteligencia de Costos para ConstrucciÃ³n

## Resumen Ejecutivo

APU Filter es una plataforma de inteligencia de negocio diseÃ±ada para el sector de la construcciÃ³n. Transforma la compleja tarea de analizar los costos de un proyecto, convirtiendo los tediosos reportes de AnÃ¡lisis de Precios Unitarios (APU) en una fuente de datos interactiva y estratÃ©gica. No es solo un lector de archivos, es una herramienta para tomar decisiones rÃ¡pidas y precisas que impactan directamente en la rentabilidad y competitividad de su empresa.

## Â¿Por quÃ© APU Filter?

### De Horas a Segundos: Ahorro de Tiempo y ReducciÃ³n de Errores
- **AutomatizaciÃ³n Inteligente:** APU Filter automatiza el procesamiento de los complejos reportes de costos, una tarea que tradicionalmente consume horas de trabajo de ingenierÃ­a.
- **Fiabilidad Garantizada:** Minimiza los errores humanos de transcripciÃ³n y cÃ¡lculo que se producen en flujos de trabajo manuales basados en Excel, garantizando cifras fiables para sus anÃ¡lisis.

### Una Herramienta de DecisiÃ³n EstratÃ©gica
- **Simulador de Costos (AIU):** No se limite a ver los costos; proyÃ©ctelos. El simulador permite analizar en tiempo real el impacto de los costos indirectos (AdministraciÃ³n, Imprevistos, Utilidad) en la rentabilidad final del proyecto.
- **Estimador SemÃ¡ntico:** Genere cotizaciones precisas para nuevos proyectos en segundos. Utilizando embeddings de texto y bÃºsqueda vectorial (FAISS), el estimador encuentra los APUs mÃ¡s *conceptualmente similares*, no solo los que coinciden por palabras clave.

### CentralizaciÃ³n y Consistencia
- **LÃ³gica de Negocio Unificada:** Centralice las reglas de cÃ¡lculo y anÃ¡lisis, que a menudo estÃ¡n dispersas en frÃ¡giles macros de Excel, difÃ­ciles de mantener y escalar.
- **Fuente Ãšnica de Verdad:** Asegure que todo el equipo de costos y presupuestos trabaje con las mismas reglas y los datos mÃ¡s actualizados, eliminando inconsistencias.

## Arquitectura del Proyecto

APU Filter estÃ¡ construido sobre una arquitectura modular que separa claramente las responsabilidades, garantizando robustez y escalabilidad. Sus tres pilares fundamentales son:

### 1. Condensador de Flujo de Datos (Data Flux Condenser)
- **Componente Clave:** `app/flux_condenser.py`
- **FunciÃ³n:** ActÃºa como un estabilizador dinÃ¡mico de seÃ±al a la entrada del sistema. Implementa una arquitectura de **"Caja de Cristal"**, transformando la ingesta de datos en un proceso observable, medible y auto-regulado mediante principios de fÃ­sica y teorÃ­a de control.

#### âš™ï¸ Nivel 1: Motor de FÃ­sica RLC (El Sensor)
El sistema no ve "archivos"; ve "corrientes de energÃ­a". Modela la ingesta usando un circuito elÃ©ctrico de segundo orden para generar telemetrÃ­a en tiempo real:

1.  **Capacitancia ($C$) - Capacidad de Carga:**
    *   Representa la capacidad del sistema para absorber registros.
    *   Calcula el **Nivel de SaturaciÃ³n** del sistema. Un nivel bajo indica "Flujo Laminar" (estable); un nivel alto indica "Flujo Turbulento" (riesgo de desbordamiento).
2.  **Resistencia ($R$) - FricciÃ³n de Procesamiento:**
    *   Mide la complejidad o "suciedad" de los datos. Se calcula dinÃ¡micamente basÃ¡ndose en la tasa de fallos del parser y la necesidad de re-procesamiento.
3.  **Inductancia ($L$) - Inercia de Calidad:**
    *   **InnovaciÃ³n Clave:** Mide la resistencia del flujo a cambiar su estado.
    *   Calcula la **TensiÃ³n de Flyback** (Fuerza Contra-Electromotriz) generada por cambios violentos en la calidad de los datos (ej. un archivo limpio que se corrompe sÃºbitamente).
    *   **Mecanismo de ProtecciÃ³n (Diodo Flyback):** Si se detecta un pico inductivo peligroso, el sistema activa un "Diodo LÃ³gico" para disipar la energÃ­a del error sin colapsar el proceso.

#### ðŸ§  Nivel 2: Controlador PI Discreto (El Cerebro)
Sobre la capa fÃ­sica, opera un **Lazo de Control Cerrado (Feedback Loop)** que ajusta el comportamiento del sistema en tiempo real:

*   **Algoritmo:** ImplementaciÃ³n de un controlador **Proporcional-Integral (PI)** discreto.
*   **Setpoint:** El sistema busca mantener una saturaciÃ³n estable del 30% (Flujo Laminar ideal).
*   **Variable de Control:** El tamaÃ±o del lote de procesamiento (*Batch Size*).
*   **Comportamiento Adaptativo:**
    *   Si los datos son limpios (baja resistencia), el controlador **acelera**, aumentando el tamaÃ±o del lote para maximizar el rendimiento.
    *   Si detecta datos complejos o inestables (alta saturaciÃ³n/resistencia), el controlador **frena** suavemente, reduciendo el flujo para garantizar la precisiÃ³n y prevenir errores de memoria.

**Resultado:** Un sistema bi-mimÃ©tico que "siente" la data y adapta su velocidad de ingestiÃ³n para garantizar una estabilidad del 100% bajo cualquier condiciÃ³n.

### 2. Pipeline de Procesamiento de Datos
- **Componente Clave:** `app/procesador_csv.py`
- **FunciÃ³n:** Es el orquestador central que toma los datos crudos del parser y los transforma en un modelo de costos consolidado.
- **Mecanismo:** Utiliza un patrÃ³n `Pipeline` con pasos secuenciales y bien definidos:
    1.  **Carga de Datos:** Ingiere los tres archivos principales (Presupuesto, APUs, Insumos).
    2.  **FusiÃ³n de Datos:** Enriquece los insumos de los APUs con los precios del catÃ¡logo maestro de insumos.
    3.  **CÃ¡lculo de Costos:** Agrega los costos de los insumos para calcular el valor total de cada APU, desglosado por categorÃ­a (Materiales, Mano de Obra, Equipo).
    4.  **Merge Final:** Une los costos calculados de los APUs con las cantidades del archivo de presupuesto para generar el informe final.

### 3. Estimador Inteligente
- **Componente Clave:** `app/estimator.py`
- **FunciÃ³n:** Proporciona una capacidad de bÃºsqueda avanzada para generar cotizaciones rÃ¡pidas para nuevos proyectos, basÃ¡ndose en el conocimiento extraÃ­do de APUs histÃ³ricos.
- **Mecanismo Dual:**
    - **BÃºsqueda por Palabras Clave:** Un mÃ©todo tradicional y rÃ¡pido que busca coincidencias directas de texto.
    - **BÃºsqueda SemÃ¡ntica (Vectorial):** Su capacidad mÃ¡s potente. Utiliza modelos de `sentence-transformers` para convertir las descripciones de los APUs en vectores numÃ©ricos (embeddings). Luego, usa **FAISS** para encontrar los APUs mÃ¡s *conceptualmente similares* a una nueva descripciÃ³n, incluso si no comparten las mismas palabras.

## TecnologÃ­as Utilizadas

La plataforma estÃ¡ construida sobre una pila de tecnologÃ­as modernas de alto rendimiento:

- **Backend:** **Flask** para la API web.
- **AnÃ¡lisis de Datos y ML:**
    - **Pandas:** Utilizado como la base para la manipulaciÃ³n de datos.
    - **Sentence-Transformers:** Para la generaciÃ³n de embeddings de texto que potencian la bÃºsqueda semÃ¡ntica.
    - **FAISS (Facebook AI Similarity Search):** Para la bÃºsqueda vectorial de alta velocidad de los APUs mÃ¡s similares.
- **Parsing y Estructura de Datos:**
    - **Lark:** Para el parsing robusto de la gramÃ¡tica de los insumos en los archivos de APU.
    - **Dataclasses:** Para la creaciÃ³n de esquemas de datos (`schemas.py`) que garantizan la consistencia y validaciÃ³n.
- **Entorno y Dependencias:**
    - **Conda:** Para gestionar el entorno y las dependencias complejas con componentes binarios (ej. `faiss-cpu`).
- **Redis:** Para la gestiÃ³n de sesiones de usuario, garantizando la persistencia de datos entre solicitudes.
    - **uv & pip:** Para la gestiÃ³n rÃ¡pida y eficiente del resto de las dependencias de Python.
- **Calidad de CÃ³digo y Pruebas:**
    - **Pytest:** Para una suite de pruebas exhaustiva que cubre desde unidades hasta la integraciÃ³n completa.
    - **Ruff:** Para el formateo y linting del cÃ³digo, asegurando un estilo consistente y de alta calidad.

## InstalaciÃ³n y Uso

Esta secciÃ³n describe cÃ³mo configurar un entorno de desarrollo robusto utilizando un enfoque hÃ­brido que combina **Conda**, **pip** y **uv**. Este mÃ©todo es esencial para garantizar una instalaciÃ³n estable y reproducible.

### La Arquitectura de la InstalaciÃ³n: Una AnalogÃ­a de Engranajes

Para entender por quÃ© seguimos un orden de instalaciÃ³n especÃ­fico, podemos visualizar nuestro entorno como una caja de cambios de precisiÃ³n compuesta por tres engranajes diferentes, cada uno con una funciÃ³n especializada.

1.  **Conda: El Engranaje Principal y de Potencia (El Engranaje Grande)**
    *   **Rol:** Mueve las piezas mÃ¡s pesadas y complejas que no son de Python puro y dependen del sistema operativo (ej. librerÃ­as C++).
    *   **CaracterÃ­stica:** Es potente y fiable, diseÃ±ado para buscar e instalar paquetes pre-compilados que encajan perfectamente con la arquitectura de la mÃ¡quina.
    *   **En APU Filter:** Su Ãºnica tarea es instalar `faiss-cpu`, una librerÃ­a con dependencias complejas a nivel de sistema.

2.  **Pip (con `--index-url`): La Herramienta Especializada**
    *   **Rol:** Se utiliza para una pieza crÃ­tica que necesita una instalaciÃ³n muy especÃ­fica desde un repositorio exclusivo.
    *   **CaracterÃ­stica:** Comunica una intenciÃ³n precisa: "Ve Ãºnicamente a este almacÃ©n especÃ­fico (el de PyTorch para CPU) y trae la pieza exacta que encuentres allÃ­".
    *   **En APU Filter:** Su Ãºnica tarea es instalar la versiÃ³n `torch` optimizada exclusivamente para CPU, evitando la descarga de las pesadas librerÃ­as de CUDA.

3.  **uv/pip: El Engranaje de Alta Velocidad y PrecisiÃ³n (El Engranaje PequeÃ±o)**
    *   **Rol:** Ensambla todos los componentes de la aplicaciÃ³n que son de Python puro, comunicÃ¡ndose directamente con el ecosistema de Python (PyPI).
    *   **CaracterÃ­stica:** Es ultrarrÃ¡pido y Ã¡gil, ideal para manejar dependencias estÃ¡ndar de Python, pero no tiene la fuerza para gestionar las piezas pesadas que maneja Conda.
    *   **En APU Filter:** Su tarea es instalar todo lo demÃ¡s desde `requirements.txt` de forma eficiente.

Este enfoque de "engranajes" asegura que cada componente se instale con la herramienta adecuada, en el orden correcto, garantizando la estabilidad y el rendimiento del sistema.

### Diagrama del Flujo de InstalaciÃ³n

```mermaid
graph TD
    A[Inicio: Entorno Limpio] --> B{Paso 1: Crear Entorno Conda con Python 3.10};
    B --> C[Paso 2: Activar Entorno];
    C --> D{Paso 3: Instalar Paquetes Especiales};
    D -- "conda install -c pytorch" --> E[faiss-cpu];
    D -- "pip install --index-url ..." --> F[torch (cpu)];
    E --> H;
    F --> H;
    G[requirements.txt (sin faiss/torch)] --> H{Paso 4: Instalar Dependencias de la AplicaciÃ³n};
    H -- "uv pip install -r" --> I[LibrerÃ­as Restantes];
    I --> J{Paso 5: Instalar Servidor de Sesiones};
    J -- "conda install -c conda-forge" --> K[Redis];
    K --> L[Fin: Entorno Listo âœ…];
```

### Pasos Detallados de InstalaciÃ³n

**Requisito Previo:** AsegÃºrese de tener instalado Miniconda o Anaconda. Puede descargarlo desde [aquÃ­](https://www.anaconda.com/products/distribution).

**Paso 1: Crear el Entorno Base (Conda)**
Cree un nuevo entorno Conda llamado `apu_filter_env` con Python 3.10, la versiÃ³n sobre la cual se construirÃ¡n los demÃ¡s componentes.
```bash
conda create --name apu_filter_env python=3.10
```

**Paso 2: Activar el Entorno**
Active el entorno reciÃ©n creado. **Debe hacer esto cada vez que trabaje en el proyecto.**
```bash
conda activate apu_filter_env
```

**Paso 3: Instalar Componentes Pesados (Conda y Pip Especializado)**
Instale los "engranajes" principales que requieren compilaciones y dependencias complejas.

*   **Instalar `faiss-cpu` (El Engranaje de Potencia):**
    ```bash
    conda install -c pytorch faiss-cpu
    ```

*   **Instalar `torch` (La Herramienta Especializada):**
    ```bash
    pip install torch --index-url https://download.pytorch.org/whl/cpu
    ```

**Paso 4: Instalar Dependencias de la AplicaciÃ³n (uv)**
Instale todas las demÃ¡s dependencias de Python puro con el "engranaje de alta velocidad".
```bash
uv pip install -r requirements.txt
uv pip install -r requirements-dev.txt
```

**Paso 5: Instalar y Configurar el Servidor de Sesiones (Redis)**
Para garantizar la persistencia de los datos del usuario entre solicitudes, la aplicaciÃ³n utiliza Redis.

*   **Instalar `redis` (El Engranaje de Estabilidad):**
    Es crucial instalar Redis a travÃ©s del canal `conda-forge` para asegurar la compatibilidad entre diferentes sistemas operativos, incluyendo macOS y Linux.
    ```bash
    conda install -c conda-forge redis
    ```ejecutar redis-server

**Nota Importante:** El archivo `requirements.txt` no debe contener `faiss-cpu` ni `torch`. Si alguna vez necesita regenerar este archivo (ej. usando `uv pip compile requirements.in`), asegÃºrese de excluir estas dos librerÃ­as para evitar conflictos de instalaciÃ³n.

Flujo de Trabajo del Proyecto
El ciclo de vida del desarrollo y uso de la aplicaciÃ³n sigue estos pasos:
ConfiguraciÃ³n: La lÃ³gica de negocio (mapeo de columnas, umbrales, reglas del estimador) se gestiona en app/config.json.
Pre-procesamiento (si los datos cambian): La bÃºsqueda semÃ¡ntica requiere un Ã­ndice. Si los datos de los APUs cambian, debe regenerar los embeddings ejecutando:
python scripts/generate_embeddings.py --input path/to/processed_apus.json

EjecuciÃ³n de la AplicaciÃ³n: Con el entorno activado, inicie el servidor Flask:
python -m flask run --port=5002

InteracciÃ³n con la API:
Un usuario sube los archivos (presupuesto, apus, insumos) al endpoint /upload.
La aplicaciÃ³n procesa los datos y los almacena en una sesiÃ³n.
El usuario interactÃºa con los endpoints /api/estimate y /api/apu/{code} para realizar anÃ¡lisis.
ValidaciÃ³n y Pruebas: Para verificar la integridad del cÃ³digo, ejecute la suite de pruebas completa:

pytest -vv

Estructura del Directorio
El proyecto estÃ¡ organizado con una clara separaciÃ³n de responsabilidades para facilitar la mantenibilidad y la escalabilidad.

```
apu_filter/
â”‚
â”œâ”€â”€ app/                        # LÃ³gica principal de la aplicaciÃ³n Flask
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ app.py                  # Factory de la app, endpoints API y carga de modelos
â”‚   â”œâ”€â”€ procesador_csv.py       # Orquestador del pipeline de procesamiento de datos
â”‚   â”œâ”€â”€ report_parser_crudo.py  # Parser especializado para archivos de APU semi-estructurados
â”‚   â”œâ”€â”€ apu_processor.py        # Motor de transformaciÃ³n que aplica lÃ³gica de negocio a los datos parseados
â”‚   â”œâ”€â”€ estimator.py            # LÃ³gica de estimaciÃ³n con bÃºsqueda semÃ¡ntica y por keywords
â”‚   â”œâ”€â”€ flux_condenser.py       # LÃ³gica del condensador de flujos de datos
â”‚   â”œâ”€â”€ data_loader.py          # Capa de abstracciÃ³n para leer datos (.csv, .xlsx, .pdf)
â”‚   â”œâ”€â”€ schemas.py              # DefiniciÃ³n de los esquemas de datos (dataclasses)
â”‚   â”œâ”€â”€ utils.py                # Funciones de utilidad generales (normalizaciÃ³n, parsing, etc.)
â”‚   â”œâ”€â”€ config.json             # Archivo de configuraciÃ³n de la lÃ³gica de negocio
â”‚   â””â”€â”€ embeddings/             # Directorio para los artefactos de ML (Ã­ndice FAISS, mapeo)
â”‚
â”œâ”€â”€ data/                       # Datos de entrada y resultados intermedmedios
â”‚   â”œâ”€â”€ presupuesto_clean.csv   # VersiÃ³n sanitizada del presupuesto, lista para el pipeline
â”‚   â”œâ”€â”€ insumos_clean.csv       # VersiÃ³n sanitizada de insumos, lista para el pipeline
â”‚   â””â”€â”€ apus_clean.csv          # VersiÃ³n sanitizada de apus, lista para el pipeline  
â”‚
â”œâ”€â”€ data_dirty/                 # Datos crudos y sin procesar
â”‚   â”œâ”€â”€ presupuesto.csv         # Archivo de presupuesto original con posibles errores
â”‚   â”œâ”€â”€ insumos.csv             # Archivo de insumos original con posibles errores
â”‚   â””â”€â”€ apus.csv                # Archivo de apus original con posibles errores  
â”‚
â”œâ”€â”€ models/                     # MÃ³dulos de lÃ³gica de negocio y anÃ¡lisis avanzado
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ probability_models.py   # Motor de simulaciÃ³n Monte Carlo para anÃ¡lisis de riesgos
â”‚
â”œâ”€â”€ scripts/                    # Herramientas de lÃ­nea de comandos para desarrolladores
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ generate_embeddings.py       # Script para generar el Ã­ndice de bÃºsqueda semÃ¡ntica
â”‚   â”œâ”€â”€ diagnose_apus_file.py        # Herramienta para analizar formatos de archivo de APU
â”‚   â”œâ”€â”€ diagnose_insumos_file.py     # Herramienta para analizar formatos de archivo de insumos
â”‚   â”œâ”€â”€ diagnose_presupuesto_file.py # Herramienta para analizar formatos de archivo de presupuesto
â”‚   â””â”€â”€ clean_csv.py                 # Herramienta para limpiar caracteres sucios y crear un archivo csv limpio 
â”‚
â”œâ”€â”€ tests/                      # Suite de pruebas completa del proyecto
â”‚   â”œâ”€â”€ test_app.py             # Pruebas de integraciÃ³n para los endpoints de la API
â”‚   â”œâ”€â”€ test_procesador_csv.py  # Pruebas para el orquestador del pipeline
â”‚   â”œâ”€â”€ test_apu_processor.py   # Pruebas para el motor de transformaciÃ³n
â”‚   â”œâ”€â”€ test_estimator.py       # Pruebas para la lÃ³gica de estimaciÃ³n
â”‚   â”œâ”€â”€ test_data_loader.py     # Pruebas para la capa de carga de datos
â”‚   â””â”€â”€ test_data.py            # Datos de prueba centralizados
â”‚
â”œâ”€â”€ templates/                  # Plantillas HTML para la interfaz (si aplica)
â”œâ”€â”€ uploads/                    # Directorio temporal para archivos subidos
â”‚
â”œâ”€â”€ requirements.in             # Archivo fuente para definir dependencias
â”œâ”€â”€ requirements.txt            # Archivo de dependencias "congelado" generado por uv
â””â”€â”€ pyproject.toml              # Archivo de configuraciÃ³n del proyecto Python
```